Sun Dec 22 16:13:41 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Quadro RTX 8000                On  | 00000000:3B:00.0 Off |                    0 |
| 33%   27C    P8              14W / 260W |      1MiB / 46080MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  Quadro RTX 8000                On  | 00000000:5E:00.0 Off |                    0 |
| 33%   27C    P8              17W / 260W |      1MiB / 46080MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   2  Quadro RTX 8000                On  | 00000000:AF:00.0 Off |                    0 |
| 33%   27C    P8               9W / 260W |      1MiB / 46080MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   3  Quadro RTX 8000                On  | 00000000:D8:00.0 Off |                    0 |
| 33%   28C    P8              16W / 260W |      1MiB / 46080MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
attention mode is flash
attention mode is flash
attention mode is flash
attention mode is flash
[12/22 16:13:59 TiTok]: Saving config to titok_b64_stage1_run1/config.yaml
[12/22 16:13:59 TiTok]: Config:
experiment:
  project: titok_b64_stage1
  name: titok_b64_stage1_run1
  output_dir: titok_b64_stage1_run1
  max_train_examples: 1281167
  save_every: 50000
  eval_every: 50000
  generate_every: 5000
  log_every: 50
  log_grad_norm_every: 1000
  resume: true
  init_weight: ''
  logging_dir: titok_b64_stage1_run1/logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: base
    vit_dec_model_size: base
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 64
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
  reconstruction_regularization:
    name: matryoshka
    mask_ratio_method: hierarchical
losses:
  quantizer_weight: 1.0
dataset:
  params:
    train_shards_path_or_url: /mnt/rdata8/imagenet_wds/imagenet-train-{000000..000252}.tar
    eval_shards_path_or_url: /mnt/rdata8/imagenet_wds/imagenet-val-{000000..000009}.tar
    num_workers_per_gpu: 12
  preprocessing:
    resize_shorter_edge: 256
    crop_size: 256
    random_crop: true
    random_flip: true
optimizer:
  name: adamw
  params:
    learning_rate: 0.0001
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 32
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: true
  use_ema: true
  seed: 42
  max_train_steps: 1000000
  num_generated_images: 2
  max_grad_norm: 1.0
config: configs/training/stage1/titok_b64_matryoshka.yaml

[12/22 16:14:03 TiTok]: Creating model and loss module.
self.train_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    RandomCrop(size=(256, 256), padding=None)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.eval_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(256, 256))
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.train_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    RandomCrop(size=(256, 256), padding=None)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.eval_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(256, 256))
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.train_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    RandomCrop(size=(256, 256), padding=None)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.eval_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(256, 256))
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
===================================================================================================================================================================================================
Layer (type:depth-idx)                        Input Shape               Output Shape              Param #                   Param %                   Kernel Shape              Mult-Adds
===================================================================================================================================================================================================
TiTok                                         [1, 3, 256, 256]          [1, 1024, 16, 16]         49,152                      0.03%                   --                        --
├─TiTokEncoder: 1-1                           --                        [1, 12, 1, 64]            247,296                     0.14%                   --                        --
│    └─Conv2d: 2-1                            [1, 3, 256, 256]          [1, 768, 16, 16]          590,592                     0.34%                   [16, 16]                  151,191,552
│    └─LayerNorm: 2-2                         [1, 321, 768]             [1, 321, 768]             1,536                       0.00%                   --                        1,536
│    └─ModuleList: 2-3                        --                        --                        --                             --                   --                        --
│    │    └─ResidualAttentionBlock: 3-1       [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-1               [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-2      [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-3               [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-4              [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-1             [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-2               [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-3             [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-2       [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-5               [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-6      [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-7               [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-8              [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-4             [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-5               [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-6             [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-3       [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-9               [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-10     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-11              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-12             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-7             [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-8               [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-9             [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-4       [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-13              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-14     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-15              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-16             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-10            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-11              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-12            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-5       [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-17              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-18     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-19              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-20             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-13            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-14              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-15            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-6       [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-21              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-22     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-23              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-24             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-16            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-17              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-18            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-7       [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-25              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-26     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-27              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-28             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-19            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-20              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-21            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-8       [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-29              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-30     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-31              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-32             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-22            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-23              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-24            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-9       [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-33              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-34     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-35              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-36             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-25            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-26              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-27            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-10      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-37              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-38     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-39              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-40             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-28            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-29              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-30            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-11      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-41              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-42     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-43              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-44             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-31            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-32              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-33            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-12      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-45              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-46     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-47              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-48             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-34            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-35              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-36            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    └─LayerNorm: 2-4                         [1, 64, 768]              [1, 64, 768]              1,536                       0.00%                   --                        1,536
│    └─Conv2d: 2-5                            [1, 768, 64, 1]           [1, 12, 64, 1]            9,228                       0.01%                   [1, 1]                    590,592
├─VectorQuantizer: 1-2                        [1, 12, 1, 64]            [1, 12, 1, 64]            --                             --                   --                        --
│    └─Embedding: 2-6                         [64]                      [64, 12]                  49,152                      0.03%                   --                        3,145,728
├─TiTokDecoder: 1-3                           [1, 12, 1, 64]            [1, 1024, 16, 16]         248,064                     0.14%                   --                        --
│    └─Linear: 2-7                            [1, 64, 12]               [1, 64, 768]              9,984                       0.01%                   --                        9,984
│    └─LayerNorm: 2-8                         [1, 321, 768]             [1, 321, 768]             1,536                       0.00%                   --                        1,536
│    └─ModuleList: 2-9                        --                        --                        --                             --                   --                        --
│    │    └─ResidualAttentionBlock: 3-13      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-49              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-50     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-51              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-52             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-37            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-38              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-39            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-14      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-53              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-54     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-55              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-56             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-40            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-41              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-42            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-15      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-57              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-58     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-59              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-60             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-43            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-44              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-45            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-16      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-61              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-62     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-63              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-64             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-46            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-47              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-48            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-17      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-65              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-66     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-67              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-68             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-49            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-50              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-51            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-18      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-69              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-70     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-71              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-72             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-52            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-53              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-54            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-19      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-73              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-74     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-75              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-76             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-55            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-56              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-57            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-20      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-77              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-78     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-79              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-80             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-58            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-59              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-60            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-21      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-81              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-82     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-83              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-84             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-61            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-62              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-63            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-22      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-85              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-86     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-87              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-88             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-64            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-65              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-66            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-23      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-89              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-90     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-91              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-92             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-67            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-68              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-69            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-24      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-93              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-94     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-95              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-96             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-70            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-71              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-72            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    └─LayerNorm: 2-10                        [1, 256, 768]             [1, 256, 768]             1,536                       0.00%                   --                        1,536
│    └─Sequential: 2-11                       [1, 768, 16, 16]          [1, 1024, 16, 16]         --                             --                   --                        --
│    │    └─Conv2d: 3-25                      [1, 768, 16, 16]          [1, 1536, 16, 16]         1,181,184                   0.68%                   [1, 1]                    302,383,104
│    │    └─Tanh: 3-26                        [1, 1536, 16, 16]         [1, 1536, 16, 16]         --                             --                   --                        --
│    │    └─Conv2d: 3-27                      [1, 1536, 16, 16]         [1, 1024, 16, 16]         1,573,888                   0.90%                   [1, 1]                    402,915,328
│    └─Identity: 2-12                         [1, 1024, 16, 16]         [1, 1024, 16, 16]         --                             --                   --                        --
===================================================================================================================================================================================================
Total params: 174,073,612
Trainable params: 174,073,612
Non-trainable params: 0
Total mult-adds (G): 37.27
===================================================================================================================================================================================================
Input size (MB): 0.79
Forward/backward pass size (MB): 344.47
Params size (MB): 467.33
Estimated Total Size (MB): 812.58
===================================================================================================================================================================================================
[12/22 16:14:20 TiTok]: ===================================================================================================================================================================================================
Layer (type:depth-idx)                        Input Shape               Output Shape              Param #                   Param %                   Kernel Shape              Mult-Adds
===================================================================================================================================================================================================
TiTok                                         [1, 3, 256, 256]          [1, 1024, 16, 16]         49,152                      0.03%                   --                        --
├─TiTokEncoder: 1-1                           --                        [1, 12, 1, 64]            247,296                     0.14%                   --                        --
│    └─Conv2d: 2-1                            [1, 3, 256, 256]          [1, 768, 16, 16]          590,592                     0.34%                   [16, 16]                  151,191,552
│    └─LayerNorm: 2-2                         [1, 321, 768]             [1, 321, 768]             1,536                       0.00%                   --                        1,536
│    └─ModuleList: 2-3                        --                        --                        --                             --                   --                        --
│    │    └─ResidualAttentionBlock: 3-1       [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-1               [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-2      [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-3               [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-4              [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-1             [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-2               [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-3             [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-2       [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-5               [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-6      [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-7               [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-8              [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-4             [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-5               [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-6             [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-3       [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-9               [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-10     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-11              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-12             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-7             [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-8               [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-9             [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-4       [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-13              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-14     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-15              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-16             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-10            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-11              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-12            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-5       [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-17              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-18     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-19              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-20             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-13            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-14              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-15            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-6       [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-21              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-22     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-23              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-24             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-16            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-17              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-18            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-7       [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-25              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-26     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-27              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-28             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-19            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-20              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-21            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-8       [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-29              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-30     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-31              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-32             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-22            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-23              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-24            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-9       [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-33              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-34     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-35              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-36             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-25            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-26              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-27            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-10      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-37              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-38     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-39              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-40             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-28            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-29              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-30            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-11      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-41              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-42     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-43              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-44             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-31            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-32              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-33            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-12      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-45              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-46     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-47              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-48             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-34            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-35              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-36            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    └─LayerNorm: 2-4                         [1, 64, 768]              [1, 64, 768]              1,536                       0.00%                   --                        1,536
│    └─Conv2d: 2-5                            [1, 768, 64, 1]           [1, 12, 64, 1]            9,228                       0.01%                   [1, 1]                    590,592
├─VectorQuantizer: 1-2                        [1, 12, 1, 64]            [1, 12, 1, 64]            --                             --                   --                        --
│    └─Embedding: 2-6                         [64]                      [64, 12]                  49,152                      0.03%                   --                        3,145,728
├─TiTokDecoder: 1-3                           [1, 12, 1, 64]            [1, 1024, 16, 16]         248,064                     0.14%                   --                        --
│    └─Linear: 2-7                            [1, 64, 12]               [1, 64, 768]              9,984                       0.01%                   --                        9,984
│    └─LayerNorm: 2-8                         [1, 321, 768]             [1, 321, 768]             1,536                       0.00%                   --                        1,536
│    └─ModuleList: 2-9                        --                        --                        --                             --                   --                        --
│    │    └─ResidualAttentionBlock: 3-13      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-49              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-50     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-51              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-52             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-37            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-38              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-39            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-14      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-53              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-54     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-55              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-56             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-40            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-41              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-42            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-15      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-57              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-58     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-59              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-60             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-43            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-44              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-45            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-16      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-61              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-62     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-63              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-64             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-46            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-47              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-48            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-17      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-65              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-66     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-67              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-68             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-49            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-50              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-51            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-18      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-69              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-70     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-71              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-72             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-52            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-53              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-54            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-19      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-73              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-74     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-75              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-76             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-55            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-56              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-57            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-20      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-77              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-78     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-79              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-80             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-58            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-59              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-60            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-21      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-81              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-82     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-83              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-84             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-61            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-62              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-63            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-22      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-85              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-86     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-87              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-88             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-64            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-65              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-66            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-23      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-89              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-90     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-91              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-92             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-67            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-68              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-69            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    │    └─ResidualAttentionBlock: 3-24      [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-93              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─MultiheadAttention: 4-94     [321, 1, 768]             [321, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-95              [321, 1, 768]             [321, 1, 768]             1,536                       0.00%                   --                        493,056
│    │    │    └─Sequential: 4-96             [321, 1, 768]             [321, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-70            [321, 1, 768]             [321, 1, 3072]            2,362,368                   1.36%                   --                        758,320,128
│    │    │    │    └─GELU: 5-71              [321, 1, 3072]            [321, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-72            [321, 1, 3072]            [321, 1, 768]             2,360,064                   1.36%                   --                        757,580,544
│    └─LayerNorm: 2-10                        [1, 256, 768]             [1, 256, 768]             1,536                       0.00%                   --                        1,536
│    └─Sequential: 2-11                       [1, 768, 16, 16]          [1, 1024, 16, 16]         --                             --                   --                        --
│    │    └─Conv2d: 3-25                      [1, 768, 16, 16]          [1, 1536, 16, 16]         1,181,184                   0.68%                   [1, 1]                    302,383,104
│    │    └─Tanh: 3-26                        [1, 1536, 16, 16]         [1, 1536, 16, 16]         --                             --                   --                        --
│    │    └─Conv2d: 3-27                      [1, 1536, 16, 16]         [1, 1024, 16, 16]         1,573,888                   0.90%                   [1, 1]                    402,915,328
│    └─Identity: 2-12                         [1, 1024, 16, 16]         [1, 1024, 16, 16]         --                             --                   --                        --
===================================================================================================================================================================================================
Total params: 174,073,612
Trainable params: 174,073,612
Non-trainable params: 0
Total mult-adds (G): 37.27
===================================================================================================================================================================================================
Input size (MB): 0.79
Forward/backward pass size (MB): 344.47
Params size (MB): 467.33
Estimated Total Size (MB): 812.58
===================================================================================================================================================================================================
[12/22 16:14:20 TiTok]: Creating optimizers.
[12/22 16:14:20 TiTok]: Creating lr_schedulers.
[12/22 16:14:20 TiTok]: Creating dataloaders.
self.train_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    RandomCrop(size=(256, 256), padding=None)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.eval_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(256, 256))
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
[12/22 16:14:20 TiTok]: Creating evaluator.
[12/22 16:14:25 TiTok]: Preparing model, optimizer and dataloaders
[12/22 16:14:26 TiTok]: ***** Running training *****
[12/22 16:14:26 TiTok]:   Num training steps = 1000000
[12/22 16:14:26 TiTok]:   Gradient Accumulation steps = 1
[12/22 16:14:26 TiTok]:   Instantaneous batch size per gpu = 32
[12/22 16:14:26 TiTok]:   Total train batch size (w. parallel, distributed & accumulation) = 128
[12/22 16:14:26 TiTok]: All globbed checkpoints are: []
[12/22 16:14:26 TiTok]: Training from scratch.
Epoch 0/99 started.
[12/22 16:15:34 TiTok]: Data (t): 0.0010, 39.56/s/gpu Batch (t): 0.8089 LR: 0.000001 Step: 50 Total Loss: 6.9911 Recon Loss: 6.9800 
[12/22 16:16:15 TiTok]: Data (t): 0.0010, 39.52/s/gpu Batch (t): 0.8097 LR: 0.000001 Step: 100 Total Loss: 6.9322 Recon Loss: 6.9315 
[12/22 16:16:55 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000002 Step: 150 Total Loss: 6.9109 Recon Loss: 6.9106 
[12/22 16:17:36 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000002 Step: 200 Total Loss: 6.8992 Recon Loss: 6.8991 
[12/22 16:18:17 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8160 LR: 0.000003 Step: 250 Total Loss: 6.8912 Recon Loss: 6.8911 
[12/22 16:18:57 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000003 Step: 300 Total Loss: 6.8751 Recon Loss: 6.8750 
[12/22 16:19:38 TiTok]: Data (t): 0.0011, 39.65/s/gpu Batch (t): 0.8071 LR: 0.000004 Step: 350 Total Loss: 6.8756 Recon Loss: 6.8755 
[12/22 16:20:19 TiTok]: Data (t): 0.0010, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000004 Step: 400 Total Loss: 6.8600 Recon Loss: 6.8599 
[12/22 16:20:59 TiTok]: Data (t): 0.0010, 39.43/s/gpu Batch (t): 0.8115 LR: 0.000005 Step: 450 Total Loss: 6.8917 Recon Loss: 6.8917 
[12/22 16:21:40 TiTok]: Data (t): 0.0011, 39.46/s/gpu Batch (t): 0.8110 LR: 0.000005 Step: 500 Total Loss: 6.8838 Recon Loss: 6.8838 
[12/22 16:22:20 TiTok]: Data (t): 0.0010, 39.43/s/gpu Batch (t): 0.8115 LR: 0.000006 Step: 550 Total Loss: 6.8715 Recon Loss: 6.8715 
[12/22 16:23:01 TiTok]: Data (t): 0.0011, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000006 Step: 600 Total Loss: 6.8769 Recon Loss: 6.8769 
[12/22 16:23:41 TiTok]: Data (t): 0.0010, 39.56/s/gpu Batch (t): 0.8090 LR: 0.000007 Step: 650 Total Loss: 6.8836 Recon Loss: 6.8836 
[12/22 16:24:22 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8150 LR: 0.000007 Step: 700 Total Loss: 6.8791 Recon Loss: 6.8791 
[12/22 16:25:03 TiTok]: Data (t): 0.0010, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000008 Step: 750 Total Loss: 6.8610 Recon Loss: 6.8609 
[12/22 16:25:43 TiTok]: Data (t): 0.0010, 39.64/s/gpu Batch (t): 0.8072 LR: 0.000008 Step: 800 Total Loss: 6.8518 Recon Loss: 6.8515 
[12/22 16:26:24 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000009 Step: 850 Total Loss: 6.8387 Recon Loss: 6.8385 
[12/22 16:27:04 TiTok]: Data (t): 0.0010, 39.40/s/gpu Batch (t): 0.8122 LR: 0.000009 Step: 900 Total Loss: 6.8606 Recon Loss: 6.8602 
[12/22 16:27:45 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000010 Step: 950 Total Loss: 6.8261 Recon Loss: 6.8256 
[12/22 16:28:26 TiTok]: Data (t): 0.0011, 35.50/s/gpu Batch (t): 0.9014 LR: 0.000010 Step: 1000 Total Loss: 6.8338 Recon Loss: 6.8320 
[12/22 16:29:06 TiTok]: Data (t): 0.0010, 39.66/s/gpu Batch (t): 0.8068 LR: 0.000010 Step: 1050 Total Loss: 6.8363 Recon Loss: 6.8336 
[12/22 16:29:47 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000011 Step: 1100 Total Loss: 6.8054 Recon Loss: 6.8029 
[12/22 16:30:28 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8131 LR: 0.000012 Step: 1150 Total Loss: 6.8155 Recon Loss: 6.8137 
[12/22 16:31:09 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000012 Step: 1200 Total Loss: 6.8229 Recon Loss: 6.8214 
[12/22 16:31:50 TiTok]: Data (t): 0.0011, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000013 Step: 1250 Total Loss: 6.8030 Recon Loss: 6.8018 
[12/22 16:32:30 TiTok]: Data (t): 0.0011, 39.46/s/gpu Batch (t): 0.8109 LR: 0.000013 Step: 1300 Total Loss: 6.7803 Recon Loss: 6.7792 
[12/22 16:33:11 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000014 Step: 1350 Total Loss: 6.7906 Recon Loss: 6.7891 
[12/22 16:33:52 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000014 Step: 1400 Total Loss: 6.7909 Recon Loss: 6.7889 
[12/22 16:34:33 TiTok]: Data (t): 0.0010, 39.50/s/gpu Batch (t): 0.8100 LR: 0.000015 Step: 1450 Total Loss: 6.8382 Recon Loss: 6.8356 
[12/22 16:35:14 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000015 Step: 1500 Total Loss: 6.7586 Recon Loss: 6.7561 
[12/22 16:35:54 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8204 LR: 0.000016 Step: 1550 Total Loss: 6.7926 Recon Loss: 6.7897 
[12/22 16:36:35 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000016 Step: 1600 Total Loss: 6.7405 Recon Loss: 6.7380 
[12/22 16:37:16 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000017 Step: 1650 Total Loss: 6.7269 Recon Loss: 6.7239 
[12/22 16:37:57 TiTok]: Data (t): 0.0012, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000017 Step: 1700 Total Loss: 6.8066 Recon Loss: 6.8034 
[12/22 16:38:38 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8235 LR: 0.000017 Step: 1750 Total Loss: 6.7375 Recon Loss: 6.7344 
[12/22 16:39:19 TiTok]: Data (t): 0.0010, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000018 Step: 1800 Total Loss: 6.7260 Recon Loss: 6.7234 
[12/22 16:39:59 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000018 Step: 1850 Total Loss: 6.6927 Recon Loss: 6.6896 
[12/22 16:40:40 TiTok]: Data (t): 0.0010, 39.48/s/gpu Batch (t): 0.8106 LR: 0.000019 Step: 1900 Total Loss: 6.7627 Recon Loss: 6.7596 
[12/22 16:41:21 TiTok]: Data (t): 0.0012, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000020 Step: 1950 Total Loss: 6.6876 Recon Loss: 6.6846 
[12/22 16:42:02 TiTok]: Data (t): 0.0011, 35.05/s/gpu Batch (t): 0.9129 LR: 0.000020 Step: 2000 Total Loss: 6.6623 Recon Loss: 6.6592 
[12/22 16:42:43 TiTok]: Data (t): 0.0011, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000021 Step: 2050 Total Loss: 6.7310 Recon Loss: 6.7281 
[12/22 16:43:24 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000021 Step: 2100 Total Loss: 6.6673 Recon Loss: 6.6644 
[12/22 16:44:04 TiTok]: Data (t): 0.0021, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000022 Step: 2150 Total Loss: 6.6892 Recon Loss: 6.6865 
[12/22 16:44:45 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000022 Step: 2200 Total Loss: 6.7173 Recon Loss: 6.7146 
[12/22 16:45:26 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000023 Step: 2250 Total Loss: 6.7270 Recon Loss: 6.7240 
[12/22 16:46:07 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000023 Step: 2300 Total Loss: 6.7525 Recon Loss: 6.7494 
[12/22 16:46:48 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8127 LR: 0.000023 Step: 2350 Total Loss: 6.7675 Recon Loss: 6.7645 
[12/22 16:47:29 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000024 Step: 2400 Total Loss: 6.6583 Recon Loss: 6.6555 
[12/22 16:48:09 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000024 Step: 2450 Total Loss: 6.7706 Recon Loss: 6.7681 
[12/22 16:48:50 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000025 Step: 2500 Total Loss: 6.7809 Recon Loss: 6.7782 
[12/22 16:49:31 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000026 Step: 2550 Total Loss: 6.6466 Recon Loss: 6.6438 
[12/22 16:50:12 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000026 Step: 2600 Total Loss: 6.6785 Recon Loss: 6.6757 
[12/22 16:50:53 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000027 Step: 2650 Total Loss: 6.6902 Recon Loss: 6.6874 
[12/22 16:51:34 TiTok]: Data (t): 0.0012, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000027 Step: 2700 Total Loss: 6.6787 Recon Loss: 6.6758 
[12/22 16:52:15 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8200 LR: 0.000028 Step: 2750 Total Loss: 6.7441 Recon Loss: 6.7412 
[12/22 16:52:55 TiTok]: Data (t): 0.0010, 38.64/s/gpu Batch (t): 0.8281 LR: 0.000028 Step: 2800 Total Loss: 6.6581 Recon Loss: 6.6546 
[12/22 16:53:36 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8202 LR: 0.000028 Step: 2850 Total Loss: 6.7310 Recon Loss: 6.7281 
[12/22 16:54:17 TiTok]: Data (t): 0.0011, 39.49/s/gpu Batch (t): 0.8104 LR: 0.000029 Step: 2900 Total Loss: 6.7476 Recon Loss: 6.7447 
[12/22 16:54:58 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000029 Step: 2950 Total Loss: 6.6468 Recon Loss: 6.6439 
[12/22 16:55:39 TiTok]: Data (t): 0.0011, 35.40/s/gpu Batch (t): 0.9038 LR: 0.000030 Step: 3000 Total Loss: 6.6014 Recon Loss: 6.5987 
[12/22 16:56:20 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000030 Step: 3050 Total Loss: 6.7056 Recon Loss: 6.7027 
[12/22 16:57:01 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000031 Step: 3100 Total Loss: 6.6402 Recon Loss: 6.6374 
[12/22 16:57:41 TiTok]: Data (t): 0.0014, 39.47/s/gpu Batch (t): 0.8108 LR: 0.000031 Step: 3150 Total Loss: 6.6294 Recon Loss: 6.6264 
[12/22 16:58:22 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000032 Step: 3200 Total Loss: 6.5748 Recon Loss: 6.5718 
[12/22 16:59:03 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000033 Step: 3250 Total Loss: 6.7018 Recon Loss: 6.6988 
[12/22 16:59:44 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000033 Step: 3300 Total Loss: 6.7271 Recon Loss: 6.7241 
[12/22 17:00:25 TiTok]: Data (t): 0.0010, 39.40/s/gpu Batch (t): 0.8122 LR: 0.000034 Step: 3350 Total Loss: 6.6234 Recon Loss: 6.6205 
[12/22 17:01:06 TiTok]: Data (t): 0.0011, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000034 Step: 3400 Total Loss: 6.7151 Recon Loss: 6.7121 
[12/22 17:01:46 TiTok]: Data (t): 0.0011, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000034 Step: 3450 Total Loss: 6.7073 Recon Loss: 6.7044 
[12/22 17:02:27 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000035 Step: 3500 Total Loss: 6.5701 Recon Loss: 6.5670 
[12/22 17:03:08 TiTok]: Data (t): 0.0010, 39.45/s/gpu Batch (t): 0.8111 LR: 0.000036 Step: 3550 Total Loss: 6.5555 Recon Loss: 6.5527 
[12/22 17:03:49 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000036 Step: 3600 Total Loss: 6.5616 Recon Loss: 6.5587 
[12/22 17:04:30 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8150 LR: 0.000036 Step: 3650 Total Loss: 6.5654 Recon Loss: 6.5623 
[12/22 17:05:11 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000037 Step: 3700 Total Loss: 6.6730 Recon Loss: 6.6700 
[12/22 17:05:52 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000038 Step: 3750 Total Loss: 6.6197 Recon Loss: 6.6166 
[12/22 17:06:33 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000038 Step: 3800 Total Loss: 6.6000 Recon Loss: 6.5969 
[12/22 17:07:13 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000039 Step: 3850 Total Loss: 6.6968 Recon Loss: 6.6938 
[12/22 17:07:54 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000039 Step: 3900 Total Loss: 6.5268 Recon Loss: 6.5235 
[12/22 17:08:35 TiTok]: Data (t): 0.0010, 39.49/s/gpu Batch (t): 0.8104 LR: 0.000040 Step: 3950 Total Loss: 6.6925 Recon Loss: 6.6894 
[12/22 17:09:16 TiTok]: Data (t): 0.0010, 35.18/s/gpu Batch (t): 0.9095 LR: 0.000040 Step: 4000 Total Loss: 6.6546 Recon Loss: 6.6514 
[12/22 17:09:57 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000041 Step: 4050 Total Loss: 6.5508 Recon Loss: 6.5475 
[12/22 17:10:38 TiTok]: Data (t): 0.0011, 39.52/s/gpu Batch (t): 0.8098 LR: 0.000041 Step: 4100 Total Loss: 6.7206 Recon Loss: 6.7172 
[12/22 17:11:19 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000041 Step: 4150 Total Loss: 6.4927 Recon Loss: 6.4894 
[12/22 17:11:59 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8131 LR: 0.000042 Step: 4200 Total Loss: 6.6917 Recon Loss: 6.6884 
[12/22 17:12:40 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000043 Step: 4250 Total Loss: 6.5982 Recon Loss: 6.5949 
[12/22 17:13:21 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000043 Step: 4300 Total Loss: 6.5372 Recon Loss: 6.5337 
[12/22 17:14:02 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000044 Step: 4350 Total Loss: 6.6723 Recon Loss: 6.6688 
[12/22 17:14:43 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000044 Step: 4400 Total Loss: 6.5544 Recon Loss: 6.5508 
[12/22 17:15:24 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000045 Step: 4450 Total Loss: 6.5563 Recon Loss: 6.5528 
[12/22 17:16:05 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8171 LR: 0.000045 Step: 4500 Total Loss: 6.5815 Recon Loss: 6.5781 
[12/22 17:16:45 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000046 Step: 4550 Total Loss: 6.6610 Recon Loss: 6.6576 
[12/22 17:17:26 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000046 Step: 4600 Total Loss: 6.4496 Recon Loss: 6.4462 
[12/22 17:18:07 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000047 Step: 4650 Total Loss: 6.5811 Recon Loss: 6.5775 
[12/22 17:18:48 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000047 Step: 4700 Total Loss: 6.5500 Recon Loss: 6.5463 
[12/22 17:19:29 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000048 Step: 4750 Total Loss: 6.6243 Recon Loss: 6.6204 
[12/22 17:20:10 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8150 LR: 0.000048 Step: 4800 Total Loss: 6.6403 Recon Loss: 6.6367 
[12/22 17:20:51 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000048 Step: 4850 Total Loss: 6.5468 Recon Loss: 6.5432 
[12/22 17:21:31 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000049 Step: 4900 Total Loss: 6.5447 Recon Loss: 6.5412 
[12/22 17:22:12 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000050 Step: 4950 Total Loss: 6.5152 Recon Loss: 6.5116 
[12/22 17:22:53 TiTok]: Data (t): 0.0010, 35.60/s/gpu Batch (t): 0.8989 LR: 0.000050 Step: 5000 Total Loss: 6.5323 Recon Loss: 6.5286 
[12/22 17:22:54 TiTok]: Reconstructing images...
[12/22 17:23:39 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000051 Step: 5050 Total Loss: 6.5223 Recon Loss: 6.5187 
[12/22 17:24:20 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000051 Step: 5100 Total Loss: 6.5278 Recon Loss: 6.5243 
[12/22 17:25:01 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000052 Step: 5150 Total Loss: 6.3981 Recon Loss: 6.3945 
[12/22 17:25:42 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8221 LR: 0.000052 Step: 5200 Total Loss: 6.5231 Recon Loss: 6.5195 
[12/22 17:26:22 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000053 Step: 5250 Total Loss: 6.3965 Recon Loss: 6.3929 
[12/22 17:27:03 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000053 Step: 5300 Total Loss: 6.4209 Recon Loss: 6.4172 
[12/22 17:27:44 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000054 Step: 5350 Total Loss: 6.4850 Recon Loss: 6.4814 
[12/22 17:28:25 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000054 Step: 5400 Total Loss: 6.4916 Recon Loss: 6.4879 
[12/22 17:29:06 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000055 Step: 5450 Total Loss: 6.6248 Recon Loss: 6.6211 
[12/22 17:29:47 TiTok]: Data (t): 0.0012, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000055 Step: 5500 Total Loss: 6.3419 Recon Loss: 6.3380 
[12/22 17:30:28 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000056 Step: 5550 Total Loss: 6.5892 Recon Loss: 6.5854 
[12/22 17:31:08 TiTok]: Data (t): 0.0011, 38.95/s/gpu Batch (t): 0.8217 LR: 0.000056 Step: 5600 Total Loss: 6.3317 Recon Loss: 6.3279 
[12/22 17:31:49 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000056 Step: 5650 Total Loss: 6.4872 Recon Loss: 6.4833 
[12/22 17:32:30 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000057 Step: 5700 Total Loss: 6.5882 Recon Loss: 6.5843 
[12/22 17:33:11 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000057 Step: 5750 Total Loss: 6.3475 Recon Loss: 6.3439 
[12/22 17:33:52 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000058 Step: 5800 Total Loss: 6.3119 Recon Loss: 6.3081 
[12/22 17:34:33 TiTok]: Data (t): 0.0010, 38.73/s/gpu Batch (t): 0.8262 LR: 0.000058 Step: 5850 Total Loss: 6.3278 Recon Loss: 6.3241 
[12/22 17:35:14 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000059 Step: 5900 Total Loss: 6.4709 Recon Loss: 6.4672 
[12/22 17:35:55 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000060 Step: 5950 Total Loss: 6.4885 Recon Loss: 6.4849 
[12/22 17:36:35 TiTok]: Data (t): 0.0010, 33.83/s/gpu Batch (t): 0.9459 LR: 0.000060 Step: 6000 Total Loss: 6.4860 Recon Loss: 6.4824 
[12/22 17:37:16 TiTok]: Data (t): 0.0011, 39.47/s/gpu Batch (t): 0.8107 LR: 0.000061 Step: 6050 Total Loss: 6.5634 Recon Loss: 6.5598 
[12/22 17:37:57 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000061 Step: 6100 Total Loss: 6.5731 Recon Loss: 6.5694 
[12/22 17:38:38 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000062 Step: 6150 Total Loss: 6.3280 Recon Loss: 6.3243 
[12/22 17:39:19 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000062 Step: 6200 Total Loss: 6.2743 Recon Loss: 6.2705 
[12/22 17:40:00 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000063 Step: 6250 Total Loss: 6.5586 Recon Loss: 6.5548 
[12/22 17:40:41 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000063 Step: 6300 Total Loss: 6.3999 Recon Loss: 6.3961 
[12/22 17:41:21 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000063 Step: 6350 Total Loss: 6.3932 Recon Loss: 6.3893 
[12/22 17:42:02 TiTok]: Data (t): 0.0011, 38.76/s/gpu Batch (t): 0.8257 LR: 0.000064 Step: 6400 Total Loss: 6.7119 Recon Loss: 6.7081 
[12/22 17:42:43 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000065 Step: 6450 Total Loss: 6.5849 Recon Loss: 6.5811 
[12/22 17:43:24 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000065 Step: 6500 Total Loss: 6.2663 Recon Loss: 6.2623 
[12/22 17:44:05 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000066 Step: 6550 Total Loss: 6.1873 Recon Loss: 6.1836 
[12/22 17:44:46 TiTok]: Data (t): 0.0010, 39.45/s/gpu Batch (t): 0.8111 LR: 0.000066 Step: 6600 Total Loss: 6.3635 Recon Loss: 6.3596 
[12/22 17:45:27 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000067 Step: 6650 Total Loss: 6.3477 Recon Loss: 6.3438 
[12/22 17:46:07 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000067 Step: 6700 Total Loss: 6.5218 Recon Loss: 6.5179 
[12/22 17:46:48 TiTok]: Data (t): 0.0011, 39.44/s/gpu Batch (t): 0.8114 LR: 0.000068 Step: 6750 Total Loss: 6.3204 Recon Loss: 6.3168 
[12/22 17:47:29 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8158 LR: 0.000068 Step: 6800 Total Loss: 6.5394 Recon Loss: 6.5357 
[12/22 17:48:10 TiTok]: Data (t): 0.0019, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000069 Step: 6850 Total Loss: 6.3377 Recon Loss: 6.3339 
[12/22 17:48:51 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8164 LR: 0.000069 Step: 6900 Total Loss: 6.3936 Recon Loss: 6.3900 
[12/22 17:49:32 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000069 Step: 6950 Total Loss: 6.2199 Recon Loss: 6.2162 
[12/22 17:50:13 TiTok]: Data (t): 0.0010, 35.53/s/gpu Batch (t): 0.9007 LR: 0.000070 Step: 7000 Total Loss: 6.3619 Recon Loss: 6.3582 
[12/22 17:50:53 TiTok]: Data (t): 0.0012, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000071 Step: 7050 Total Loss: 6.1618 Recon Loss: 6.1581 
[12/22 17:51:34 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000071 Step: 7100 Total Loss: 6.2916 Recon Loss: 6.2878 
[12/22 17:52:15 TiTok]: Data (t): 0.0011, 39.38/s/gpu Batch (t): 0.8125 LR: 0.000072 Step: 7150 Total Loss: 6.4433 Recon Loss: 6.4395 
[12/22 17:52:56 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000072 Step: 7200 Total Loss: 6.4973 Recon Loss: 6.4935 
[12/22 17:53:37 TiTok]: Data (t): 0.0011, 39.44/s/gpu Batch (t): 0.8115 LR: 0.000073 Step: 7250 Total Loss: 6.6849 Recon Loss: 6.6811 
[12/22 17:54:18 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000073 Step: 7300 Total Loss: 6.6659 Recon Loss: 6.6620 
[12/22 17:54:59 TiTok]: Data (t): 0.0010, 39.37/s/gpu Batch (t): 0.8129 LR: 0.000073 Step: 7350 Total Loss: 6.4689 Recon Loss: 6.4650 
[12/22 17:55:39 TiTok]: Data (t): 0.0011, 38.48/s/gpu Batch (t): 0.8315 LR: 0.000074 Step: 7400 Total Loss: 6.1071 Recon Loss: 6.1031 
[12/22 17:56:20 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000075 Step: 7450 Total Loss: 6.3185 Recon Loss: 6.3146 
[12/22 17:57:01 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000075 Step: 7500 Total Loss: 6.2403 Recon Loss: 6.2363 
[12/22 17:57:42 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000076 Step: 7550 Total Loss: 6.1292 Recon Loss: 6.1252 
[12/22 17:58:23 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8160 LR: 0.000076 Step: 7600 Total Loss: 6.4279 Recon Loss: 6.4240 
[12/22 17:59:04 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000077 Step: 7650 Total Loss: 6.2795 Recon Loss: 6.2756 
[12/22 17:59:45 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000077 Step: 7700 Total Loss: 6.0716 Recon Loss: 6.0676 
[12/22 18:00:26 TiTok]: Data (t): 0.0011, 36.36/s/gpu Batch (t): 0.8800 LR: 0.000077 Step: 7750 Total Loss: 6.2410 Recon Loss: 6.2369 
[12/22 18:01:06 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000078 Step: 7800 Total Loss: 6.2828 Recon Loss: 6.2786 
[12/22 18:01:47 TiTok]: Data (t): 0.0010, 39.39/s/gpu Batch (t): 0.8123 LR: 0.000079 Step: 7850 Total Loss: 6.1093 Recon Loss: 6.1051 
[12/22 18:02:28 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000079 Step: 7900 Total Loss: 6.0731 Recon Loss: 6.0689 
[12/22 18:03:09 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000080 Step: 7950 Total Loss: 6.0574 Recon Loss: 6.0531 
[12/22 18:03:50 TiTok]: Data (t): 0.0010, 35.02/s/gpu Batch (t): 0.9138 LR: 0.000080 Step: 8000 Total Loss: 6.2651 Recon Loss: 6.2609 
[12/22 18:04:31 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000080 Step: 8050 Total Loss: 6.0747 Recon Loss: 6.0706 
[12/22 18:05:12 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8166 LR: 0.000081 Step: 8100 Total Loss: 6.6336 Recon Loss: 6.6293 
[12/22 18:05:52 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000081 Step: 8150 Total Loss: 6.2890 Recon Loss: 6.2849 
[12/22 18:06:33 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8227 LR: 0.000082 Step: 8200 Total Loss: 6.2150 Recon Loss: 6.2106 
[12/22 18:07:14 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000082 Step: 8250 Total Loss: 6.0601 Recon Loss: 6.0558 
[12/22 18:07:55 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8183 LR: 0.000083 Step: 8300 Total Loss: 6.0507 Recon Loss: 6.0463 
[12/22 18:08:36 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000083 Step: 8350 Total Loss: 5.9967 Recon Loss: 5.9924 
[12/22 18:09:17 TiTok]: Data (t): 0.0010, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000084 Step: 8400 Total Loss: 6.0081 Recon Loss: 6.0040 
[12/22 18:09:58 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000084 Step: 8450 Total Loss: 6.0458 Recon Loss: 6.0416 
[12/22 18:10:38 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8231 LR: 0.000085 Step: 8500 Total Loss: 6.1781 Recon Loss: 6.1739 
[12/22 18:11:19 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8234 LR: 0.000085 Step: 8550 Total Loss: 6.0229 Recon Loss: 6.0188 
[12/22 18:12:00 TiTok]: Data (t): 0.0010, 39.46/s/gpu Batch (t): 0.8109 LR: 0.000086 Step: 8600 Total Loss: 6.1876 Recon Loss: 6.1832 
[12/22 18:12:41 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000086 Step: 8650 Total Loss: 6.4019 Recon Loss: 6.3977 
[12/22 18:13:22 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000087 Step: 8700 Total Loss: 5.9994 Recon Loss: 5.9950 
[12/22 18:14:03 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000087 Step: 8750 Total Loss: 6.1364 Recon Loss: 6.1319 
[12/22 18:14:44 TiTok]: Data (t): 0.0010, 39.51/s/gpu Batch (t): 0.8099 LR: 0.000088 Step: 8800 Total Loss: 6.4113 Recon Loss: 6.4067 
[12/22 18:15:24 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000088 Step: 8850 Total Loss: 6.3736 Recon Loss: 6.3690 
[12/22 18:16:05 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8160 LR: 0.000089 Step: 8900 Total Loss: 6.1289 Recon Loss: 6.1243 
[12/22 18:16:46 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000089 Step: 8950 Total Loss: 6.4339 Recon Loss: 6.4292 
[12/22 18:17:27 TiTok]: Data (t): 0.0010, 35.34/s/gpu Batch (t): 0.9056 LR: 0.000090 Step: 9000 Total Loss: 5.9337 Recon Loss: 5.9290 
[12/22 18:18:08 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000090 Step: 9050 Total Loss: 6.1650 Recon Loss: 6.1605 
[12/22 18:18:49 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8150 LR: 0.000091 Step: 9100 Total Loss: 5.9403 Recon Loss: 5.9358 
[12/22 18:19:30 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8189 LR: 0.000091 Step: 9150 Total Loss: 5.9213 Recon Loss: 5.9170 
[12/22 18:20:11 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000092 Step: 9200 Total Loss: 5.9604 Recon Loss: 5.9560 
[12/22 18:20:52 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000092 Step: 9250 Total Loss: 6.1426 Recon Loss: 6.1382 
[12/22 18:21:32 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000093 Step: 9300 Total Loss: 6.3339 Recon Loss: 6.3295 
[12/22 18:22:13 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8154 LR: 0.000093 Step: 9350 Total Loss: 6.1585 Recon Loss: 6.1542 
[12/22 18:22:54 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000094 Step: 9400 Total Loss: 6.1715 Recon Loss: 6.1671 
[12/22 18:23:35 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000094 Step: 9450 Total Loss: 6.1186 Recon Loss: 6.1144 
[12/22 18:24:16 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8204 LR: 0.000095 Step: 9500 Total Loss: 6.1140 Recon Loss: 6.1096 
[12/22 18:24:57 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8166 LR: 0.000095 Step: 9550 Total Loss: 5.8854 Recon Loss: 5.8810 
[12/22 18:25:38 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8200 LR: 0.000096 Step: 9600 Total Loss: 6.0956 Recon Loss: 6.0912 
[12/22 18:26:19 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000096 Step: 9650 Total Loss: 6.1463 Recon Loss: 6.1419 
[12/22 18:26:59 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000097 Step: 9700 Total Loss: 6.1671 Recon Loss: 6.1626 
[12/22 18:27:40 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000097 Step: 9750 Total Loss: 5.8356 Recon Loss: 5.8312 
[12/22 18:28:21 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8133 LR: 0.000098 Step: 9800 Total Loss: 6.3534 Recon Loss: 6.3489 
[12/22 18:29:02 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000098 Step: 9850 Total Loss: 6.0970 Recon Loss: 6.0926 
[12/22 18:29:43 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000099 Step: 9900 Total Loss: 6.3283 Recon Loss: 6.3238 
[12/22 18:30:24 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000099 Step: 9950 Total Loss: 6.5851 Recon Loss: 6.5808 
[12/22 18:31:05 TiTok]: Data (t): 0.0010, 35.26/s/gpu Batch (t): 0.9075 LR: 0.000100 Step: 10000 Total Loss: 6.3272 Recon Loss: 6.3229 
[12/22 18:31:06 TiTok]: Reconstructing images...
Epoch 1/99 started.
[12/22 18:31:56 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000100 Step: 10050 Total Loss: 6.3230 Recon Loss: 6.3188 
[12/22 18:32:37 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000100 Step: 10100 Total Loss: 6.1250 Recon Loss: 6.1209 
[12/22 18:33:18 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000100 Step: 10150 Total Loss: 5.7908 Recon Loss: 5.7867 
[12/22 18:33:59 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000100 Step: 10200 Total Loss: 6.0680 Recon Loss: 6.0638 
[12/22 18:34:40 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000100 Step: 10250 Total Loss: 6.0993 Recon Loss: 6.0952 
[12/22 18:35:20 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000100 Step: 10300 Total Loss: 6.0373 Recon Loss: 6.0332 
[12/22 18:36:01 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000100 Step: 10350 Total Loss: 6.0335 Recon Loss: 6.0294 
[12/22 18:36:42 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000100 Step: 10400 Total Loss: 5.7872 Recon Loss: 5.7830 
[12/22 18:37:23 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000100 Step: 10450 Total Loss: 6.0152 Recon Loss: 6.0110 
[12/22 18:38:04 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8221 LR: 0.000100 Step: 10500 Total Loss: 6.0251 Recon Loss: 6.0209 
[12/22 18:38:45 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000100 Step: 10550 Total Loss: 6.2862 Recon Loss: 6.2821 
[12/22 18:39:26 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000100 Step: 10600 Total Loss: 6.1255 Recon Loss: 6.1214 
[12/22 18:40:07 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000100 Step: 10650 Total Loss: 6.5966 Recon Loss: 6.5924 
[12/22 18:40:48 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000100 Step: 10700 Total Loss: 6.0052 Recon Loss: 6.0010 
[12/22 18:41:28 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8206 LR: 0.000100 Step: 10750 Total Loss: 6.0537 Recon Loss: 6.0495 
[12/22 18:42:09 TiTok]: Data (t): 0.0013, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000100 Step: 10800 Total Loss: 5.7031 Recon Loss: 5.6989 
[12/22 18:42:50 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000100 Step: 10850 Total Loss: 6.3092 Recon Loss: 6.3049 
[12/22 18:43:31 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000100 Step: 10900 Total Loss: 5.7479 Recon Loss: 5.7436 
[12/22 18:44:12 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000100 Step: 10950 Total Loss: 6.0013 Recon Loss: 5.9971 
[12/22 18:44:53 TiTok]: Data (t): 0.0010, 35.51/s/gpu Batch (t): 0.9012 LR: 0.000100 Step: 11000 Total Loss: 5.9949 Recon Loss: 5.9906 
[12/22 18:45:34 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000100 Step: 11050 Total Loss: 5.7694 Recon Loss: 5.7652 
[12/22 18:46:14 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8228 LR: 0.000100 Step: 11100 Total Loss: 5.9620 Recon Loss: 5.9578 
[12/22 18:46:55 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8125 LR: 0.000100 Step: 11150 Total Loss: 5.9685 Recon Loss: 5.9641 
[12/22 18:47:36 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000100 Step: 11200 Total Loss: 6.3033 Recon Loss: 6.2989 
[12/22 18:48:17 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000100 Step: 11250 Total Loss: 6.0029 Recon Loss: 5.9986 
[12/22 18:48:58 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000100 Step: 11300 Total Loss: 5.9551 Recon Loss: 5.9508 
[12/22 18:49:39 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8183 LR: 0.000100 Step: 11350 Total Loss: 5.6222 Recon Loss: 5.6177 
[12/22 18:50:20 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000100 Step: 11400 Total Loss: 6.2180 Recon Loss: 6.2135 
[12/22 18:51:01 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000100 Step: 11450 Total Loss: 5.6494 Recon Loss: 5.6450 
[12/22 18:51:41 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000100 Step: 11500 Total Loss: 5.9467 Recon Loss: 5.9424 
[12/22 18:52:22 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000100 Step: 11550 Total Loss: 5.9644 Recon Loss: 5.9599 
[12/22 18:53:03 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000100 Step: 11600 Total Loss: 5.6616 Recon Loss: 5.6572 
[12/22 18:53:44 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000100 Step: 11650 Total Loss: 5.9162 Recon Loss: 5.9119 
[12/22 18:54:25 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8137 LR: 0.000100 Step: 11700 Total Loss: 5.7137 Recon Loss: 5.7094 
[12/22 18:55:06 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000100 Step: 11750 Total Loss: 5.6158 Recon Loss: 5.6114 
[12/22 18:55:47 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000100 Step: 11800 Total Loss: 5.9584 Recon Loss: 5.9539 
[12/22 18:56:27 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000100 Step: 11850 Total Loss: 6.2586 Recon Loss: 6.2541 
[12/22 18:57:08 TiTok]: Data (t): 0.0011, 39.64/s/gpu Batch (t): 0.8073 LR: 0.000100 Step: 11900 Total Loss: 6.5549 Recon Loss: 6.5504 
[12/22 18:57:49 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8204 LR: 0.000100 Step: 11950 Total Loss: 5.9857 Recon Loss: 5.9810 
[12/22 18:58:30 TiTok]: Data (t): 0.0011, 34.89/s/gpu Batch (t): 0.9172 LR: 0.000100 Step: 12000 Total Loss: 5.9576 Recon Loss: 5.9529 
[12/22 18:59:11 TiTok]: Data (t): 0.0011, 39.40/s/gpu Batch (t): 0.8121 LR: 0.000100 Step: 12050 Total Loss: 5.9641 Recon Loss: 5.9593 
[12/22 18:59:52 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000100 Step: 12100 Total Loss: 5.6679 Recon Loss: 5.6629 
[12/22 19:00:33 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8181 LR: 0.000100 Step: 12150 Total Loss: 6.2327 Recon Loss: 6.2275 
[12/22 19:01:14 TiTok]: Data (t): 0.0011, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000100 Step: 12200 Total Loss: 5.6345 Recon Loss: 5.6292 
[12/22 19:01:54 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000100 Step: 12250 Total Loss: 5.8734 Recon Loss: 5.8679 
[12/22 19:02:35 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000100 Step: 12300 Total Loss: 5.9293 Recon Loss: 5.9237 
[12/22 19:03:16 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000100 Step: 12350 Total Loss: 5.9312 Recon Loss: 5.9256 
[12/22 19:03:57 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8177 LR: 0.000100 Step: 12400 Total Loss: 5.9214 Recon Loss: 5.9156 
[12/22 19:04:38 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000100 Step: 12450 Total Loss: 6.1790 Recon Loss: 6.1731 
[12/22 19:05:19 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000100 Step: 12500 Total Loss: 6.2051 Recon Loss: 6.1991 
[12/22 19:06:00 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000100 Step: 12550 Total Loss: 6.2047 Recon Loss: 6.1985 
[12/22 19:06:40 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000100 Step: 12600 Total Loss: 6.2163 Recon Loss: 6.2102 
[12/22 19:07:21 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000100 Step: 12650 Total Loss: 5.5811 Recon Loss: 5.5749 
[12/22 19:08:02 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000100 Step: 12700 Total Loss: 5.9220 Recon Loss: 5.9157 
[12/22 19:08:43 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000100 Step: 12750 Total Loss: 5.6127 Recon Loss: 5.6064 
[12/22 19:09:24 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000100 Step: 12800 Total Loss: 5.8838 Recon Loss: 5.8773 
[12/22 19:10:05 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000100 Step: 12850 Total Loss: 6.2203 Recon Loss: 6.2139 
[12/22 19:10:46 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000100 Step: 12900 Total Loss: 5.9035 Recon Loss: 5.8971 
[12/22 19:11:27 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000100 Step: 12950 Total Loss: 6.2371 Recon Loss: 6.2307 
[12/22 19:12:08 TiTok]: Data (t): 0.0010, 34.94/s/gpu Batch (t): 0.9158 LR: 0.000100 Step: 13000 Total Loss: 5.5455 Recon Loss: 5.5390 
[12/22 19:12:48 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000100 Step: 13050 Total Loss: 6.1913 Recon Loss: 6.1846 
[12/22 19:13:29 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8198 LR: 0.000100 Step: 13100 Total Loss: 5.9281 Recon Loss: 5.9216 
[12/22 19:14:10 TiTok]: Data (t): 0.0010, 39.45/s/gpu Batch (t): 0.8112 LR: 0.000100 Step: 13150 Total Loss: 6.1995 Recon Loss: 6.1929 
[12/22 19:14:51 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000100 Step: 13200 Total Loss: 5.8196 Recon Loss: 5.8131 
[12/22 19:15:32 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000100 Step: 13250 Total Loss: 5.5385 Recon Loss: 5.5319 
[12/22 19:16:13 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000100 Step: 13300 Total Loss: 5.8587 Recon Loss: 5.8520 
[12/22 19:16:54 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8198 LR: 0.000100 Step: 13350 Total Loss: 5.5166 Recon Loss: 5.5099 
[12/22 19:17:35 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000100 Step: 13400 Total Loss: 5.8461 Recon Loss: 5.8395 
[12/22 19:18:16 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000100 Step: 13450 Total Loss: 5.5737 Recon Loss: 5.5672 
[12/22 19:18:56 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000100 Step: 13500 Total Loss: 5.8001 Recon Loss: 5.7935 
[12/22 19:19:37 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000100 Step: 13550 Total Loss: 5.5298 Recon Loss: 5.5230 
[12/22 19:20:18 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000100 Step: 13600 Total Loss: 5.8042 Recon Loss: 5.7975 
[12/22 19:20:59 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000100 Step: 13650 Total Loss: 6.1626 Recon Loss: 6.1558 
[12/22 19:21:40 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000100 Step: 13700 Total Loss: 5.8255 Recon Loss: 5.8187 
[12/22 19:22:21 TiTok]: Data (t): 0.0013, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000100 Step: 13750 Total Loss: 5.5159 Recon Loss: 5.5091 
[12/22 19:23:02 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000100 Step: 13800 Total Loss: 5.4835 Recon Loss: 5.4766 
[12/22 19:23:43 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8236 LR: 0.000100 Step: 13850 Total Loss: 5.8656 Recon Loss: 5.8588 
[12/22 19:24:23 TiTok]: Data (t): 0.0010, 38.77/s/gpu Batch (t): 0.8253 LR: 0.000100 Step: 13900 Total Loss: 5.7906 Recon Loss: 5.7837 
[12/22 19:25:04 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000100 Step: 13950 Total Loss: 5.8203 Recon Loss: 5.8134 
[12/22 19:25:45 TiTok]: Data (t): 0.0011, 34.64/s/gpu Batch (t): 0.9238 LR: 0.000100 Step: 14000 Total Loss: 5.5228 Recon Loss: 5.5160 
[12/22 19:26:26 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000100 Step: 14050 Total Loss: 5.5116 Recon Loss: 5.5045 
[12/22 19:27:07 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8183 LR: 0.000100 Step: 14100 Total Loss: 5.7842 Recon Loss: 5.7772 
[12/22 19:27:48 TiTok]: Data (t): 0.0011, 39.46/s/gpu Batch (t): 0.8110 LR: 0.000100 Step: 14150 Total Loss: 6.1382 Recon Loss: 6.1313 
[12/22 19:28:29 TiTok]: Data (t): 0.0010, 38.87/s/gpu Batch (t): 0.8233 LR: 0.000100 Step: 14200 Total Loss: 5.8285 Recon Loss: 5.8214 
[12/22 19:29:10 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8131 LR: 0.000100 Step: 14250 Total Loss: 6.1579 Recon Loss: 6.1508 
[12/22 19:29:51 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8137 LR: 0.000100 Step: 14300 Total Loss: 5.4222 Recon Loss: 5.4151 
[12/22 19:30:31 TiTok]: Data (t): 0.0010, 39.47/s/gpu Batch (t): 0.8108 LR: 0.000100 Step: 14350 Total Loss: 6.1227 Recon Loss: 6.1156 
[12/22 19:31:12 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8148 LR: 0.000100 Step: 14400 Total Loss: 6.1066 Recon Loss: 6.0995 
[12/22 19:31:53 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000100 Step: 14450 Total Loss: 5.4544 Recon Loss: 5.4473 
[12/22 19:32:34 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000100 Step: 14500 Total Loss: 5.8414 Recon Loss: 5.8343 
[12/22 19:33:15 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000100 Step: 14550 Total Loss: 5.7659 Recon Loss: 5.7588 
[12/22 19:33:56 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000100 Step: 14600 Total Loss: 5.7846 Recon Loss: 5.7775 
[12/22 19:34:37 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8202 LR: 0.000100 Step: 14650 Total Loss: 5.7802 Recon Loss: 5.7732 
[12/22 19:35:17 TiTok]: Data (t): 0.0010, 39.50/s/gpu Batch (t): 0.8101 LR: 0.000100 Step: 14700 Total Loss: 5.8080 Recon Loss: 5.8009 
[12/22 19:35:58 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8228 LR: 0.000100 Step: 14750 Total Loss: 5.3955 Recon Loss: 5.3884 
[12/22 19:36:39 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000100 Step: 14800 Total Loss: 5.7585 Recon Loss: 5.7515 
[12/22 19:37:20 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000100 Step: 14850 Total Loss: 5.7777 Recon Loss: 5.7706 
[12/22 19:38:01 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000100 Step: 14900 Total Loss: 5.7826 Recon Loss: 5.7755 
[12/22 19:38:42 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000100 Step: 14950 Total Loss: 5.4178 Recon Loss: 5.4107 
[12/22 19:39:23 TiTok]: Data (t): 0.0010, 35.21/s/gpu Batch (t): 0.9087 LR: 0.000100 Step: 15000 Total Loss: 6.1150 Recon Loss: 6.1079 
[12/22 19:39:24 TiTok]: Reconstructing images...
[12/22 19:40:05 TiTok]: Data (t): 0.0010, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000100 Step: 15050 Total Loss: 5.4508 Recon Loss: 5.4437 
[12/22 19:40:46 TiTok]: Data (t): 0.0012, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000100 Step: 15100 Total Loss: 6.1637 Recon Loss: 6.1566 
[12/22 19:41:27 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000100 Step: 15150 Total Loss: 5.7320 Recon Loss: 5.7248 
[12/22 19:42:08 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000100 Step: 15200 Total Loss: 6.4515 Recon Loss: 6.4443 
[12/22 19:42:49 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8217 LR: 0.000100 Step: 15250 Total Loss: 5.7381 Recon Loss: 5.7310 
[12/22 19:43:30 TiTok]: Data (t): 0.0012, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000100 Step: 15300 Total Loss: 6.0833 Recon Loss: 6.0762 
[12/22 19:44:11 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000100 Step: 15350 Total Loss: 5.7574 Recon Loss: 5.7502 
[12/22 19:44:52 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000100 Step: 15400 Total Loss: 5.7550 Recon Loss: 5.7478 
[12/22 19:45:33 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000100 Step: 15450 Total Loss: 5.4117 Recon Loss: 5.4045 
[12/22 19:46:13 TiTok]: Data (t): 0.0011, 38.46/s/gpu Batch (t): 0.8319 LR: 0.000100 Step: 15500 Total Loss: 6.1002 Recon Loss: 6.0931 
[12/22 19:46:54 TiTok]: Data (t): 0.0009, 39.50/s/gpu Batch (t): 0.8101 LR: 0.000100 Step: 15550 Total Loss: 6.4399 Recon Loss: 6.4327 
[12/22 19:47:35 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000100 Step: 15600 Total Loss: 5.7537 Recon Loss: 5.7465 
[12/22 19:48:16 TiTok]: Data (t): 0.0012, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000100 Step: 15650 Total Loss: 5.7587 Recon Loss: 5.7514 
[12/22 19:48:57 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000100 Step: 15700 Total Loss: 6.0509 Recon Loss: 6.0437 
[12/22 19:49:38 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000100 Step: 15750 Total Loss: 5.7250 Recon Loss: 5.7177 
[12/22 19:50:19 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000100 Step: 15800 Total Loss: 6.1448 Recon Loss: 6.1375 
[12/22 19:51:00 TiTok]: Data (t): 0.0011, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000100 Step: 15850 Total Loss: 5.3676 Recon Loss: 5.3604 
[12/22 19:51:41 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000100 Step: 15900 Total Loss: 5.3874 Recon Loss: 5.3802 
[12/22 19:52:22 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000100 Step: 15950 Total Loss: 5.3783 Recon Loss: 5.3712 
[12/22 19:53:03 TiTok]: Data (t): 0.0011, 34.86/s/gpu Batch (t): 0.9180 LR: 0.000100 Step: 16000 Total Loss: 6.1117 Recon Loss: 6.1045 
[12/22 19:53:43 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000100 Step: 16050 Total Loss: 6.4546 Recon Loss: 6.4475 
[12/22 19:54:24 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8164 LR: 0.000100 Step: 16100 Total Loss: 5.3767 Recon Loss: 5.3695 
[12/22 19:55:05 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000100 Step: 16150 Total Loss: 5.7071 Recon Loss: 5.7001 
[12/22 19:55:46 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000100 Step: 16200 Total Loss: 5.6868 Recon Loss: 5.6797 
[12/22 19:56:27 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000100 Step: 16250 Total Loss: 5.7076 Recon Loss: 5.7006 
[12/22 19:57:08 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000100 Step: 16300 Total Loss: 5.6646 Recon Loss: 5.6577 
[12/22 19:57:49 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8198 LR: 0.000100 Step: 16350 Total Loss: 5.7148 Recon Loss: 5.7078 
[12/22 19:58:30 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000100 Step: 16400 Total Loss: 5.6793 Recon Loss: 5.6723 
[12/22 19:59:11 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000100 Step: 16450 Total Loss: 5.3269 Recon Loss: 5.3199 
[12/22 19:59:51 TiTok]: Data (t): 0.0012, 39.36/s/gpu Batch (t): 0.8131 LR: 0.000100 Step: 16500 Total Loss: 5.6708 Recon Loss: 5.6639 
[12/22 20:00:32 TiTok]: Data (t): 0.0011, 38.87/s/gpu Batch (t): 0.8233 LR: 0.000100 Step: 16550 Total Loss: 5.6789 Recon Loss: 5.6720 
[12/22 20:01:13 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000100 Step: 16600 Total Loss: 5.3295 Recon Loss: 5.3225 
[12/22 20:01:54 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000100 Step: 16650 Total Loss: 5.6809 Recon Loss: 5.6739 
[12/22 20:02:35 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000100 Step: 16700 Total Loss: 6.0640 Recon Loss: 6.0571 
[12/22 20:03:16 TiTok]: Data (t): 0.0010, 39.41/s/gpu Batch (t): 0.8119 LR: 0.000100 Step: 16750 Total Loss: 5.7051 Recon Loss: 5.6982 
[12/22 20:03:57 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000100 Step: 16800 Total Loss: 6.0641 Recon Loss: 6.0571 
[12/22 20:04:38 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8179 LR: 0.000100 Step: 16850 Total Loss: 5.6874 Recon Loss: 5.6805 
[12/22 20:05:18 TiTok]: Data (t): 0.0010, 39.41/s/gpu Batch (t): 0.8120 LR: 0.000100 Step: 16900 Total Loss: 6.0371 Recon Loss: 6.0301 
[12/22 20:05:59 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000100 Step: 16950 Total Loss: 5.2899 Recon Loss: 5.2830 
[12/22 20:06:40 TiTok]: Data (t): 0.0011, 35.58/s/gpu Batch (t): 0.8995 LR: 0.000100 Step: 17000 Total Loss: 5.6560 Recon Loss: 5.6490 
[12/22 20:07:21 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000100 Step: 17050 Total Loss: 5.3444 Recon Loss: 5.3375 
[12/22 20:08:02 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000100 Step: 17100 Total Loss: 5.2994 Recon Loss: 5.2925 
[12/22 20:08:43 TiTok]: Data (t): 0.0011, 38.89/s/gpu Batch (t): 0.8229 LR: 0.000100 Step: 17150 Total Loss: 5.2113 Recon Loss: 5.2043 
[12/22 20:09:24 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000100 Step: 17200 Total Loss: 5.6696 Recon Loss: 5.6627 
[12/22 20:10:05 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8131 LR: 0.000100 Step: 17250 Total Loss: 5.6231 Recon Loss: 5.6162 
[12/22 20:10:46 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000100 Step: 17300 Total Loss: 6.0161 Recon Loss: 6.0091 
[12/22 20:11:27 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000100 Step: 17350 Total Loss: 6.0183 Recon Loss: 6.0115 
[12/22 20:12:08 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8158 LR: 0.000100 Step: 17400 Total Loss: 5.2386 Recon Loss: 5.2317 
[12/22 20:12:48 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000100 Step: 17450 Total Loss: 5.2802 Recon Loss: 5.2732 
[12/22 20:13:29 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000100 Step: 17500 Total Loss: 5.3468 Recon Loss: 5.3400 
[12/22 20:14:10 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000100 Step: 17550 Total Loss: 5.6106 Recon Loss: 5.6036 
[12/22 20:14:51 TiTok]: Data (t): 0.0011, 39.42/s/gpu Batch (t): 0.8117 LR: 0.000100 Step: 17600 Total Loss: 6.0603 Recon Loss: 6.0535 
[12/22 20:15:32 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000100 Step: 17650 Total Loss: 5.2577 Recon Loss: 5.2508 
[12/22 20:16:13 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000100 Step: 17700 Total Loss: 5.6546 Recon Loss: 5.6478 
[12/22 20:16:54 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000100 Step: 17750 Total Loss: 5.2050 Recon Loss: 5.1981 
[12/22 20:17:35 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000100 Step: 17800 Total Loss: 5.1624 Recon Loss: 5.1555 
[12/22 20:18:16 TiTok]: Data (t): 0.0009, 38.98/s/gpu Batch (t): 0.8208 LR: 0.000100 Step: 17850 Total Loss: 5.6240 Recon Loss: 5.6171 
[12/22 20:18:56 TiTok]: Data (t): 0.0011, 38.86/s/gpu Batch (t): 0.8235 LR: 0.000100 Step: 17900 Total Loss: 5.2229 Recon Loss: 5.2161 
[12/22 20:19:37 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000100 Step: 17950 Total Loss: 5.6448 Recon Loss: 5.6379 
[12/22 20:20:18 TiTok]: Data (t): 0.0011, 35.37/s/gpu Batch (t): 0.9048 LR: 0.000100 Step: 18000 Total Loss: 5.6524 Recon Loss: 5.6455 
[12/22 20:20:59 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000100 Step: 18050 Total Loss: 5.6655 Recon Loss: 5.6586 
[12/22 20:21:40 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000100 Step: 18100 Total Loss: 5.2551 Recon Loss: 5.2482 
[12/22 20:22:21 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000100 Step: 18150 Total Loss: 5.6789 Recon Loss: 5.6720 
[12/22 20:23:02 TiTok]: Data (t): 0.0011, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000100 Step: 18200 Total Loss: 6.0168 Recon Loss: 6.0099 
[12/22 20:23:43 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8219 LR: 0.000100 Step: 18250 Total Loss: 5.2093 Recon Loss: 5.2024 
[12/22 20:24:23 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000100 Step: 18300 Total Loss: 5.6149 Recon Loss: 5.6080 
[12/22 20:25:04 TiTok]: Data (t): 0.0010, 38.83/s/gpu Batch (t): 0.8242 LR: 0.000100 Step: 18350 Total Loss: 5.6599 Recon Loss: 5.6529 
[12/22 20:25:45 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000100 Step: 18400 Total Loss: 5.6433 Recon Loss: 5.6364 
[12/22 20:26:26 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000100 Step: 18450 Total Loss: 6.0122 Recon Loss: 6.0053 
[12/22 20:27:07 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8137 LR: 0.000100 Step: 18500 Total Loss: 6.0500 Recon Loss: 6.0431 
[12/22 20:27:48 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8187 LR: 0.000100 Step: 18550 Total Loss: 5.6808 Recon Loss: 5.6740 
[12/22 20:28:29 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000100 Step: 18600 Total Loss: 5.5852 Recon Loss: 5.5784 
[12/22 20:29:10 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000100 Step: 18650 Total Loss: 5.2877 Recon Loss: 5.2809 
[12/22 20:29:50 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000100 Step: 18700 Total Loss: 5.5889 Recon Loss: 5.5819 
[12/22 20:30:31 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000100 Step: 18750 Total Loss: 5.1594 Recon Loss: 5.1525 
[12/22 20:31:12 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000100 Step: 18800 Total Loss: 5.6238 Recon Loss: 5.6169 
[12/22 20:31:53 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000100 Step: 18850 Total Loss: 5.5725 Recon Loss: 5.5656 
[12/22 20:32:34 TiTok]: Data (t): 0.0010, 39.47/s/gpu Batch (t): 0.8108 LR: 0.000100 Step: 18900 Total Loss: 6.4146 Recon Loss: 6.4077 
[12/22 20:33:15 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000100 Step: 18950 Total Loss: 5.6329 Recon Loss: 5.6259 
[12/22 20:33:56 TiTok]: Data (t): 0.0010, 35.13/s/gpu Batch (t): 0.9109 LR: 0.000100 Step: 19000 Total Loss: 5.6016 Recon Loss: 5.5947 
[12/22 20:34:37 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000100 Step: 19050 Total Loss: 6.4470 Recon Loss: 6.4402 
[12/22 20:35:18 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000100 Step: 19100 Total Loss: 5.1456 Recon Loss: 5.1387 
[12/22 20:35:59 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000100 Step: 19150 Total Loss: 5.5775 Recon Loss: 5.5706 
[12/22 20:36:39 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000100 Step: 19200 Total Loss: 5.9951 Recon Loss: 5.9882 
[12/22 20:37:20 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000100 Step: 19250 Total Loss: 5.1607 Recon Loss: 5.1538 
[12/22 20:38:01 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000100 Step: 19300 Total Loss: 6.4491 Recon Loss: 6.4422 
[12/22 20:38:42 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000100 Step: 19350 Total Loss: 5.2222 Recon Loss: 5.2153 
[12/22 20:39:23 TiTok]: Data (t): 0.0010, 39.53/s/gpu Batch (t): 0.8096 LR: 0.000100 Step: 19400 Total Loss: 5.5475 Recon Loss: 5.5405 
[12/22 20:40:04 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000100 Step: 19450 Total Loss: 5.1540 Recon Loss: 5.1472 
[12/22 20:40:45 TiTok]: Data (t): 0.0010, 39.40/s/gpu Batch (t): 0.8122 LR: 0.000100 Step: 19500 Total Loss: 5.5730 Recon Loss: 5.5660 
[12/22 20:41:26 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000100 Step: 19550 Total Loss: 6.4369 Recon Loss: 6.4300 
[12/22 20:42:07 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000100 Step: 19600 Total Loss: 6.0282 Recon Loss: 6.0213 
[12/22 20:42:47 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000100 Step: 19650 Total Loss: 6.4500 Recon Loss: 6.4431 
[12/22 20:43:28 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000100 Step: 19700 Total Loss: 5.1490 Recon Loss: 5.1421 
[12/22 20:44:09 TiTok]: Data (t): 0.0010, 38.85/s/gpu Batch (t): 0.8236 LR: 0.000100 Step: 19750 Total Loss: 5.9912 Recon Loss: 5.9843 
[12/22 20:44:50 TiTok]: Data (t): 0.0011, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000100 Step: 19800 Total Loss: 5.1449 Recon Loss: 5.1380 
[12/22 20:45:31 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000100 Step: 19850 Total Loss: 5.1666 Recon Loss: 5.1596 
[12/22 20:46:12 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000100 Step: 19900 Total Loss: 5.1342 Recon Loss: 5.1273 
[12/22 20:46:53 TiTok]: Data (t): 0.0010, 39.45/s/gpu Batch (t): 0.8111 LR: 0.000100 Step: 19950 Total Loss: 5.9409 Recon Loss: 5.9339 
[12/22 20:47:34 TiTok]: Data (t): 0.0011, 35.09/s/gpu Batch (t): 0.9120 LR: 0.000100 Step: 20000 Total Loss: 5.9974 Recon Loss: 5.9904 
[12/22 20:47:35 TiTok]: Reconstructing images...
Epoch 2/99 started.
[12/22 20:48:20 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000100 Step: 20050 Total Loss: 5.6350 Recon Loss: 5.6281 
[12/22 20:49:01 TiTok]: Data (t): 0.0011, 39.45/s/gpu Batch (t): 0.8112 LR: 0.000100 Step: 20100 Total Loss: 5.2239 Recon Loss: 5.2170 
[12/22 20:49:42 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000100 Step: 20150 Total Loss: 5.5728 Recon Loss: 5.5659 
[12/22 20:50:23 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000100 Step: 20200 Total Loss: 5.5784 Recon Loss: 5.5715 
[12/22 20:51:04 TiTok]: Data (t): 0.0011, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000100 Step: 20250 Total Loss: 5.1709 Recon Loss: 5.1639 
[12/22 20:51:45 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000100 Step: 20300 Total Loss: 5.5537 Recon Loss: 5.5467 
[12/22 20:52:26 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000100 Step: 20350 Total Loss: 5.5562 Recon Loss: 5.5493 
[12/22 20:53:06 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000100 Step: 20400 Total Loss: 6.0212 Recon Loss: 6.0143 
[12/22 20:53:47 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8221 LR: 0.000100 Step: 20450 Total Loss: 5.5296 Recon Loss: 5.5227 
[12/22 20:54:28 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000100 Step: 20500 Total Loss: 6.4483 Recon Loss: 6.4413 
[12/22 20:55:09 TiTok]: Data (t): 0.0010, 38.85/s/gpu Batch (t): 0.8236 LR: 0.000100 Step: 20550 Total Loss: 5.5669 Recon Loss: 5.5600 
[12/22 20:55:50 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8235 LR: 0.000100 Step: 20600 Total Loss: 5.5388 Recon Loss: 5.5318 
[12/22 20:56:31 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000100 Step: 20650 Total Loss: 5.5622 Recon Loss: 5.5553 
[12/22 20:57:12 TiTok]: Data (t): 0.0010, 39.39/s/gpu Batch (t): 0.8123 LR: 0.000100 Step: 20700 Total Loss: 5.5457 Recon Loss: 5.5389 
[12/22 20:57:53 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000100 Step: 20750 Total Loss: 5.1674 Recon Loss: 5.1606 
[12/22 20:58:34 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8137 LR: 0.000100 Step: 20800 Total Loss: 5.5342 Recon Loss: 5.5273 
[12/22 20:59:14 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000100 Step: 20850 Total Loss: 5.5307 Recon Loss: 5.5238 
[12/22 20:59:55 TiTok]: Data (t): 0.0011, 38.81/s/gpu Batch (t): 0.8246 LR: 0.000100 Step: 20900 Total Loss: 5.6289 Recon Loss: 5.6220 
[12/22 21:00:36 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000100 Step: 20950 Total Loss: 5.4972 Recon Loss: 5.4903 
[12/22 21:01:17 TiTok]: Data (t): 0.0010, 34.98/s/gpu Batch (t): 0.9149 LR: 0.000100 Step: 21000 Total Loss: 5.5805 Recon Loss: 5.5736 
[12/22 21:01:58 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000100 Step: 21050 Total Loss: 5.1139 Recon Loss: 5.1069 
[12/22 21:02:39 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000100 Step: 21100 Total Loss: 5.9813 Recon Loss: 5.9744 
[12/22 21:03:20 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000100 Step: 21150 Total Loss: 5.1435 Recon Loss: 5.1365 
[12/22 21:04:01 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000100 Step: 21200 Total Loss: 5.0568 Recon Loss: 5.0498 
[12/22 21:04:42 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000100 Step: 21250 Total Loss: 5.0878 Recon Loss: 5.0808 
[12/22 21:05:23 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000100 Step: 21300 Total Loss: 5.5359 Recon Loss: 5.5289 
[12/22 21:06:03 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8162 LR: 0.000100 Step: 21350 Total Loss: 5.1428 Recon Loss: 5.1358 
[12/22 21:06:44 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000100 Step: 21400 Total Loss: 5.1153 Recon Loss: 5.1083 
[12/22 21:07:25 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000100 Step: 21450 Total Loss: 5.5222 Recon Loss: 5.5153 
[12/22 21:08:06 TiTok]: Data (t): 0.0011, 38.73/s/gpu Batch (t): 0.8262 LR: 0.000100 Step: 21500 Total Loss: 5.5965 Recon Loss: 5.5896 
[12/22 21:08:47 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000100 Step: 21550 Total Loss: 5.4900 Recon Loss: 5.4831 
[12/22 21:09:28 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8150 LR: 0.000100 Step: 21600 Total Loss: 5.5358 Recon Loss: 5.5289 
[12/22 21:10:09 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000100 Step: 21650 Total Loss: 5.9737 Recon Loss: 5.9667 
[12/22 21:10:50 TiTok]: Data (t): 0.0011, 38.86/s/gpu Batch (t): 0.8235 LR: 0.000100 Step: 21700 Total Loss: 5.9420 Recon Loss: 5.9350 
[12/22 21:11:31 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000100 Step: 21750 Total Loss: 5.9657 Recon Loss: 5.9588 
[12/22 21:12:11 TiTok]: Data (t): 0.0012, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000100 Step: 21800 Total Loss: 5.4713 Recon Loss: 5.4643 
[12/22 21:12:52 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000100 Step: 21850 Total Loss: 5.5476 Recon Loss: 5.5407 
[12/22 21:13:33 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000100 Step: 21900 Total Loss: 5.0713 Recon Loss: 5.0644 
[12/22 21:14:14 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000100 Step: 21950 Total Loss: 5.1222 Recon Loss: 5.1153 
[12/22 21:14:55 TiTok]: Data (t): 0.0011, 35.28/s/gpu Batch (t): 0.9071 LR: 0.000100 Step: 22000 Total Loss: 5.9901 Recon Loss: 5.9832 
[12/22 21:15:36 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000100 Step: 22050 Total Loss: 5.9582 Recon Loss: 5.9513 
[12/22 21:16:17 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000100 Step: 22100 Total Loss: 5.0903 Recon Loss: 5.0833 
[12/22 21:16:58 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000100 Step: 22150 Total Loss: 5.9393 Recon Loss: 5.9324 
[12/22 21:17:39 TiTok]: Data (t): 0.0010, 38.83/s/gpu Batch (t): 0.8241 LR: 0.000100 Step: 22200 Total Loss: 5.0417 Recon Loss: 5.0347 
[12/22 21:18:19 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000100 Step: 22250 Total Loss: 5.0300 Recon Loss: 5.0231 
[12/22 21:19:00 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8183 LR: 0.000100 Step: 22300 Total Loss: 5.5713 Recon Loss: 5.5644 
[12/22 21:19:41 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000100 Step: 22350 Total Loss: 5.5086 Recon Loss: 5.5017 
[12/22 21:20:22 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000100 Step: 22400 Total Loss: 5.1105 Recon Loss: 5.1036 
[12/22 21:21:03 TiTok]: Data (t): 0.0012, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000100 Step: 22450 Total Loss: 5.9484 Recon Loss: 5.9414 
[12/22 21:21:44 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8164 LR: 0.000100 Step: 22500 Total Loss: 5.0698 Recon Loss: 5.0630 
[12/22 21:22:25 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000100 Step: 22550 Total Loss: 5.9191 Recon Loss: 5.9122 
[12/22 21:23:06 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000100 Step: 22600 Total Loss: 5.9492 Recon Loss: 5.9422 
[12/22 21:23:47 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8137 LR: 0.000100 Step: 22650 Total Loss: 5.4438 Recon Loss: 5.4369 
[12/22 21:24:27 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000100 Step: 22700 Total Loss: 5.4351 Recon Loss: 5.4282 
[12/22 21:25:08 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000100 Step: 22750 Total Loss: 5.0780 Recon Loss: 5.0711 
[12/22 21:25:49 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000100 Step: 22800 Total Loss: 5.0097 Recon Loss: 5.0028 
[12/22 21:26:30 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000100 Step: 22850 Total Loss: 5.0578 Recon Loss: 5.0508 
[12/22 21:27:11 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8189 LR: 0.000100 Step: 22900 Total Loss: 5.0849 Recon Loss: 5.0780 
[12/22 21:27:52 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000100 Step: 22950 Total Loss: 5.0868 Recon Loss: 5.0798 
[12/22 21:28:33 TiTok]: Data (t): 0.0011, 34.49/s/gpu Batch (t): 0.9278 LR: 0.000100 Step: 23000 Total Loss: 6.3577 Recon Loss: 6.3507 
[12/22 21:29:14 TiTok]: Data (t): 0.0010, 38.85/s/gpu Batch (t): 0.8237 LR: 0.000100 Step: 23050 Total Loss: 5.1049 Recon Loss: 5.0980 
[12/22 21:29:54 TiTok]: Data (t): 0.0011, 38.87/s/gpu Batch (t): 0.8232 LR: 0.000100 Step: 23100 Total Loss: 4.9592 Recon Loss: 4.9523 
[12/22 21:30:35 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000100 Step: 23150 Total Loss: 5.9692 Recon Loss: 5.9623 
[12/22 21:31:16 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000100 Step: 23200 Total Loss: 5.4766 Recon Loss: 5.4697 
[12/22 21:31:57 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000100 Step: 23250 Total Loss: 5.9322 Recon Loss: 5.9252 
[12/22 21:32:38 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8162 LR: 0.000100 Step: 23300 Total Loss: 5.4711 Recon Loss: 5.4642 
[12/22 21:33:19 TiTok]: Data (t): 0.0010, 38.82/s/gpu Batch (t): 0.8243 LR: 0.000100 Step: 23350 Total Loss: 5.1280 Recon Loss: 5.1210 
[12/22 21:34:00 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000100 Step: 23400 Total Loss: 5.9489 Recon Loss: 5.9420 
[12/22 21:34:41 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000100 Step: 23450 Total Loss: 5.1040 Recon Loss: 5.0971 
[12/22 21:35:22 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000100 Step: 23500 Total Loss: 5.8926 Recon Loss: 5.8857 
[12/22 21:36:03 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000100 Step: 23550 Total Loss: 5.1116 Recon Loss: 5.1047 
[12/22 21:36:43 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000100 Step: 23600 Total Loss: 5.4728 Recon Loss: 5.4658 
[12/22 21:37:24 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000100 Step: 23650 Total Loss: 5.5105 Recon Loss: 5.5036 
[12/22 21:38:05 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8131 LR: 0.000100 Step: 23700 Total Loss: 5.5139 Recon Loss: 5.5070 
[12/22 21:38:46 TiTok]: Data (t): 0.0012, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000100 Step: 23750 Total Loss: 5.4789 Recon Loss: 5.4719 
[12/22 21:39:27 TiTok]: Data (t): 0.0011, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000100 Step: 23800 Total Loss: 5.0029 Recon Loss: 4.9960 
[12/22 21:40:08 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000100 Step: 23850 Total Loss: 5.9050 Recon Loss: 5.8980 
[12/22 21:40:49 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000100 Step: 23900 Total Loss: 4.9907 Recon Loss: 4.9838 
[12/22 21:41:30 TiTok]: Data (t): 0.0010, 38.78/s/gpu Batch (t): 0.8253 LR: 0.000100 Step: 23950 Total Loss: 5.4932 Recon Loss: 5.4864 
[12/22 21:42:11 TiTok]: Data (t): 0.0011, 35.16/s/gpu Batch (t): 0.9101 LR: 0.000100 Step: 24000 Total Loss: 5.4486 Recon Loss: 5.4417 
[12/22 21:42:51 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000100 Step: 24050 Total Loss: 5.4792 Recon Loss: 5.4722 
[12/22 21:43:32 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000100 Step: 24100 Total Loss: 5.0555 Recon Loss: 5.0486 
[12/22 21:44:13 TiTok]: Data (t): 0.0010, 39.37/s/gpu Batch (t): 0.8127 LR: 0.000100 Step: 24150 Total Loss: 5.4783 Recon Loss: 5.4714 
[12/22 21:44:54 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000100 Step: 24200 Total Loss: 5.8721 Recon Loss: 5.8652 
[12/22 21:45:35 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8154 LR: 0.000100 Step: 24250 Total Loss: 5.9429 Recon Loss: 5.9361 
[12/22 21:46:16 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8200 LR: 0.000100 Step: 24300 Total Loss: 4.9891 Recon Loss: 4.9822 
[12/22 21:46:57 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000100 Step: 24350 Total Loss: 5.5013 Recon Loss: 5.4944 
[12/22 21:47:38 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000100 Step: 24400 Total Loss: 5.4734 Recon Loss: 5.4665 
[12/22 21:48:19 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000100 Step: 24450 Total Loss: 5.4899 Recon Loss: 5.4829 
[12/22 21:48:59 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000100 Step: 24500 Total Loss: 5.0026 Recon Loss: 4.9957 
[12/22 21:49:40 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8150 LR: 0.000100 Step: 24550 Total Loss: 5.5019 Recon Loss: 5.4950 
[12/22 21:50:21 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000100 Step: 24600 Total Loss: 5.5008 Recon Loss: 5.4939 
[12/22 21:51:02 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000100 Step: 24650 Total Loss: 6.4131 Recon Loss: 6.4062 
[12/22 21:51:43 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000100 Step: 24700 Total Loss: 5.8801 Recon Loss: 5.8732 
[12/22 21:52:24 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000100 Step: 24750 Total Loss: 5.9081 Recon Loss: 5.9012 
[12/22 21:53:05 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000100 Step: 24800 Total Loss: 5.4302 Recon Loss: 5.4233 
[12/22 21:53:46 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000100 Step: 24850 Total Loss: 4.9923 Recon Loss: 4.9854 
[12/22 21:54:26 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000100 Step: 24900 Total Loss: 5.8989 Recon Loss: 5.8920 
[12/22 21:55:07 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000100 Step: 24950 Total Loss: 5.0587 Recon Loss: 5.0518 
[12/22 21:55:48 TiTok]: Data (t): 0.0010, 35.54/s/gpu Batch (t): 0.9004 LR: 0.000100 Step: 25000 Total Loss: 5.9020 Recon Loss: 5.8951 
[12/22 21:55:49 TiTok]: Reconstructing images...
[12/22 21:56:31 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8200 LR: 0.000100 Step: 25050 Total Loss: 6.3703 Recon Loss: 6.3634 
[12/22 21:57:11 TiTok]: Data (t): 0.0012, 39.39/s/gpu Batch (t): 0.8124 LR: 0.000100 Step: 25100 Total Loss: 4.9900 Recon Loss: 4.9831 
[12/22 21:57:52 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000100 Step: 25150 Total Loss: 5.9042 Recon Loss: 5.8973 
[12/22 21:58:33 TiTok]: Data (t): 0.0010, 39.43/s/gpu Batch (t): 0.8115 LR: 0.000100 Step: 25200 Total Loss: 5.4168 Recon Loss: 5.4098 
[12/22 21:59:14 TiTok]: Data (t): 0.0011, 38.87/s/gpu Batch (t): 0.8234 LR: 0.000100 Step: 25250 Total Loss: 5.9121 Recon Loss: 5.9052 
[12/22 21:59:55 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000100 Step: 25300 Total Loss: 5.8884 Recon Loss: 5.8815 
[12/22 22:00:36 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000100 Step: 25350 Total Loss: 5.4767 Recon Loss: 5.4698 
[12/22 22:01:17 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000100 Step: 25400 Total Loss: 5.4655 Recon Loss: 5.4585 
[12/22 22:01:58 TiTok]: Data (t): 0.0010, 39.49/s/gpu Batch (t): 0.8103 LR: 0.000100 Step: 25450 Total Loss: 5.8963 Recon Loss: 5.8894 
[12/22 22:02:39 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8162 LR: 0.000100 Step: 25500 Total Loss: 5.0038 Recon Loss: 4.9969 
[12/22 22:03:19 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000100 Step: 25550 Total Loss: 5.4583 Recon Loss: 5.4513 
[12/22 22:04:00 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000100 Step: 25600 Total Loss: 5.9015 Recon Loss: 5.8946 
[12/22 22:04:41 TiTok]: Data (t): 0.0010, 38.71/s/gpu Batch (t): 0.8267 LR: 0.000100 Step: 25650 Total Loss: 5.0378 Recon Loss: 5.0310 
[12/22 22:05:22 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000100 Step: 25700 Total Loss: 5.0118 Recon Loss: 5.0049 
[12/22 22:06:03 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8223 LR: 0.000100 Step: 25750 Total Loss: 5.9233 Recon Loss: 5.9164 
[12/22 22:06:44 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000100 Step: 25800 Total Loss: 5.4958 Recon Loss: 5.4889 
[12/22 22:07:25 TiTok]: Data (t): 0.0011, 39.40/s/gpu Batch (t): 0.8121 LR: 0.000100 Step: 25850 Total Loss: 5.9051 Recon Loss: 5.8982 
[12/22 22:08:06 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8166 LR: 0.000100 Step: 25900 Total Loss: 5.9115 Recon Loss: 5.9046 
[12/22 22:08:46 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8150 LR: 0.000100 Step: 25950 Total Loss: 5.4329 Recon Loss: 5.4259 
[12/22 22:09:27 TiTok]: Data (t): 0.0010, 34.37/s/gpu Batch (t): 0.9309 LR: 0.000100 Step: 26000 Total Loss: 5.5134 Recon Loss: 5.5065 
[12/22 22:10:08 TiTok]: Data (t): 0.0011, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000100 Step: 26050 Total Loss: 5.4099 Recon Loss: 5.4030 
[12/22 22:10:49 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000100 Step: 26100 Total Loss: 5.4523 Recon Loss: 5.4454 
[12/22 22:11:30 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000100 Step: 26150 Total Loss: 4.9637 Recon Loss: 4.9568 
[12/22 22:12:11 TiTok]: Data (t): 0.0011, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000100 Step: 26200 Total Loss: 5.8895 Recon Loss: 5.8826 
[12/22 22:12:52 TiTok]: Data (t): 0.0010, 39.40/s/gpu Batch (t): 0.8122 LR: 0.000100 Step: 26250 Total Loss: 5.9388 Recon Loss: 5.9319 
[12/22 22:13:33 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000100 Step: 26300 Total Loss: 5.4834 Recon Loss: 5.4765 
[12/22 22:14:13 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8154 LR: 0.000100 Step: 26350 Total Loss: 5.4591 Recon Loss: 5.4522 
[12/22 22:14:54 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000100 Step: 26400 Total Loss: 5.4122 Recon Loss: 5.4053 
[12/22 22:15:35 TiTok]: Data (t): 0.0011, 38.87/s/gpu Batch (t): 0.8232 LR: 0.000100 Step: 26450 Total Loss: 5.9295 Recon Loss: 5.9226 
[12/22 22:16:16 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8221 LR: 0.000100 Step: 26500 Total Loss: 4.9181 Recon Loss: 4.9111 
[12/22 22:16:57 TiTok]: Data (t): 0.0011, 39.46/s/gpu Batch (t): 0.8110 LR: 0.000100 Step: 26550 Total Loss: 5.5055 Recon Loss: 5.4986 
[12/22 22:17:38 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000100 Step: 26600 Total Loss: 5.9265 Recon Loss: 5.9196 
[12/22 22:18:19 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000100 Step: 26650 Total Loss: 5.4622 Recon Loss: 5.4552 
[12/22 22:19:00 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000100 Step: 26700 Total Loss: 5.4639 Recon Loss: 5.4571 
[12/22 22:19:41 TiTok]: Data (t): 0.0013, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000100 Step: 26750 Total Loss: 5.4077 Recon Loss: 5.4008 
[12/22 22:20:21 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000100 Step: 26800 Total Loss: 5.8662 Recon Loss: 5.8593 
[12/22 22:21:02 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000100 Step: 26850 Total Loss: 5.9478 Recon Loss: 5.9409 
[12/22 22:21:43 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000100 Step: 26900 Total Loss: 5.4456 Recon Loss: 5.4387 
[12/22 22:22:24 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000100 Step: 26950 Total Loss: 4.9753 Recon Loss: 4.9684 
[12/22 22:23:05 TiTok]: Data (t): 0.0011, 34.90/s/gpu Batch (t): 0.9170 LR: 0.000100 Step: 27000 Total Loss: 5.9140 Recon Loss: 5.9070 
[12/22 22:23:46 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000100 Step: 27050 Total Loss: 5.9542 Recon Loss: 5.9472 
[12/22 22:24:27 TiTok]: Data (t): 0.0011, 39.44/s/gpu Batch (t): 0.8114 LR: 0.000100 Step: 27100 Total Loss: 5.3755 Recon Loss: 5.3686 
[12/22 22:25:08 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8202 LR: 0.000100 Step: 27150 Total Loss: 5.4598 Recon Loss: 5.4528 
[12/22 22:25:48 TiTok]: Data (t): 0.0012, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000100 Step: 27200 Total Loss: 5.8903 Recon Loss: 5.8834 
[12/22 22:26:29 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000100 Step: 27250 Total Loss: 4.9748 Recon Loss: 4.9678 
[12/22 22:27:10 TiTok]: Data (t): 0.0015, 40.11/s/gpu Batch (t): 0.7978 LR: 0.000100 Step: 27300 Total Loss: 4.9578 Recon Loss: 4.9508 
[12/22 22:27:51 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000100 Step: 27350 Total Loss: 4.9478 Recon Loss: 4.9408 
[12/22 22:28:32 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000100 Step: 27400 Total Loss: 5.8861 Recon Loss: 5.8792 
[12/22 22:29:13 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000100 Step: 27450 Total Loss: 5.3182 Recon Loss: 5.3112 
[12/22 22:29:54 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000100 Step: 27500 Total Loss: 5.4150 Recon Loss: 5.4080 
[12/22 22:30:35 TiTok]: Data (t): 0.0010, 39.40/s/gpu Batch (t): 0.8121 LR: 0.000100 Step: 27550 Total Loss: 4.9793 Recon Loss: 4.9723 
[12/22 22:31:16 TiTok]: Data (t): 0.0011, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000100 Step: 27600 Total Loss: 5.3709 Recon Loss: 5.3640 
[12/22 22:31:56 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000100 Step: 27650 Total Loss: 5.0274 Recon Loss: 5.0204 
[12/22 22:32:37 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8150 LR: 0.000100 Step: 27700 Total Loss: 5.4146 Recon Loss: 5.4077 
[12/22 22:33:18 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8152 LR: 0.000100 Step: 27750 Total Loss: 4.9981 Recon Loss: 4.9911 
[12/22 22:33:59 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000100 Step: 27800 Total Loss: 5.4282 Recon Loss: 5.4211 
[12/22 22:34:40 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000100 Step: 27850 Total Loss: 5.4204 Recon Loss: 5.4133 
[12/22 22:35:21 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000100 Step: 27900 Total Loss: 5.4172 Recon Loss: 5.4102 
[12/22 22:36:02 TiTok]: Data (t): 0.0010, 39.47/s/gpu Batch (t): 0.8108 LR: 0.000100 Step: 27950 Total Loss: 5.4105 Recon Loss: 5.4034 
[12/22 22:36:43 TiTok]: Data (t): 0.0011, 35.55/s/gpu Batch (t): 0.9003 LR: 0.000100 Step: 28000 Total Loss: 5.4043 Recon Loss: 5.3973 
[12/22 22:37:24 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8221 LR: 0.000100 Step: 28050 Total Loss: 5.4495 Recon Loss: 5.4425 
[12/22 22:38:04 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000100 Step: 28100 Total Loss: 4.9254 Recon Loss: 4.9184 
[12/22 22:38:45 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000100 Step: 28150 Total Loss: 5.8927 Recon Loss: 5.8856 
[12/22 22:39:26 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000100 Step: 28200 Total Loss: 5.3936 Recon Loss: 5.3866 
[12/22 22:40:07 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000100 Step: 28250 Total Loss: 4.9081 Recon Loss: 4.9011 
[12/22 22:40:48 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000100 Step: 28300 Total Loss: 5.4549 Recon Loss: 5.4478 
[12/22 22:41:29 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000100 Step: 28350 Total Loss: 5.8483 Recon Loss: 5.8412 
[12/22 22:42:10 TiTok]: Data (t): 0.0010, 38.87/s/gpu Batch (t): 0.8232 LR: 0.000100 Step: 28400 Total Loss: 5.9019 Recon Loss: 5.8949 
[12/22 22:42:51 TiTok]: Data (t): 0.0010, 39.51/s/gpu Batch (t): 0.8099 LR: 0.000100 Step: 28450 Total Loss: 5.4035 Recon Loss: 5.3965 
[12/22 22:43:31 TiTok]: Data (t): 0.0011, 38.38/s/gpu Batch (t): 0.8337 LR: 0.000100 Step: 28500 Total Loss: 5.3556 Recon Loss: 5.3485 
[12/22 22:44:12 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000100 Step: 28550 Total Loss: 5.3754 Recon Loss: 5.3684 
[12/22 22:44:53 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000100 Step: 28600 Total Loss: 5.3906 Recon Loss: 5.3834 
[12/22 22:45:34 TiTok]: Data (t): 0.0010, 39.39/s/gpu Batch (t): 0.8123 LR: 0.000100 Step: 28650 Total Loss: 5.8475 Recon Loss: 5.8404 
[12/22 22:46:15 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8198 LR: 0.000100 Step: 28700 Total Loss: 5.3469 Recon Loss: 5.3399 
[12/22 22:46:56 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000100 Step: 28750 Total Loss: 5.4969 Recon Loss: 5.4898 
[12/22 22:47:37 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000100 Step: 28800 Total Loss: 4.9015 Recon Loss: 4.8943 
[12/22 22:48:17 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000100 Step: 28850 Total Loss: 5.8875 Recon Loss: 5.8804 
[12/22 22:48:58 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000100 Step: 28900 Total Loss: 5.4040 Recon Loss: 5.3968 
[12/22 22:49:39 TiTok]: Data (t): 0.0012, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000100 Step: 28950 Total Loss: 4.8992 Recon Loss: 4.8920 
[12/22 22:50:20 TiTok]: Data (t): 0.0011, 35.60/s/gpu Batch (t): 0.8989 LR: 0.000100 Step: 29000 Total Loss: 4.8945 Recon Loss: 4.8874 
[12/22 22:51:01 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8150 LR: 0.000100 Step: 29050 Total Loss: 5.8604 Recon Loss: 5.8533 
[12/22 22:51:42 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000100 Step: 29100 Total Loss: 5.3357 Recon Loss: 5.3285 
[12/22 22:52:23 TiTok]: Data (t): 0.0011, 39.37/s/gpu Batch (t): 0.8129 LR: 0.000100 Step: 29150 Total Loss: 5.4339 Recon Loss: 5.4267 
[12/22 22:53:04 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000100 Step: 29200 Total Loss: 5.8792 Recon Loss: 5.8719 
[12/22 22:53:45 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8135 LR: 0.000100 Step: 29250 Total Loss: 5.9000 Recon Loss: 5.8927 
[12/22 22:54:25 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000100 Step: 29300 Total Loss: 5.8153 Recon Loss: 5.8079 
[12/22 22:55:06 TiTok]: Data (t): 0.0011, 38.98/s/gpu Batch (t): 0.8209 LR: 0.000100 Step: 29350 Total Loss: 5.4008 Recon Loss: 5.3935 
[12/22 22:55:47 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000100 Step: 29400 Total Loss: 5.3476 Recon Loss: 5.3403 
[12/22 22:56:28 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8162 LR: 0.000100 Step: 29450 Total Loss: 5.4030 Recon Loss: 5.3955 
[12/22 22:57:09 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8154 LR: 0.000100 Step: 29500 Total Loss: 6.3385 Recon Loss: 6.3309 
[12/22 22:57:50 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000100 Step: 29550 Total Loss: 6.8210 Recon Loss: 6.8132 
[12/22 22:58:31 TiTok]: Data (t): 0.0011, 38.88/s/gpu Batch (t): 0.8231 LR: 0.000100 Step: 29600 Total Loss: 4.8561 Recon Loss: 4.8482 
[12/22 22:59:11 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000100 Step: 29650 Total Loss: 5.3496 Recon Loss: 5.3414 
[12/22 22:59:52 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000100 Step: 29700 Total Loss: 5.8374 Recon Loss: 5.8289 
[12/22 23:00:33 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000100 Step: 29750 Total Loss: 5.4291 Recon Loss: 5.4201 
[12/22 23:01:14 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000100 Step: 29800 Total Loss: 6.3201 Recon Loss: 6.3106 
[12/22 23:01:55 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000100 Step: 29850 Total Loss: 5.4468 Recon Loss: 5.4372 
[12/22 23:02:36 TiTok]: Data (t): 0.0012, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000100 Step: 29900 Total Loss: 6.3288 Recon Loss: 6.3191 
[12/22 23:03:17 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8235 LR: 0.000100 Step: 29950 Total Loss: 4.8917 Recon Loss: 4.8821 
[12/22 23:03:58 TiTok]: Data (t): 0.0010, 35.14/s/gpu Batch (t): 0.9106 LR: 0.000100 Step: 30000 Total Loss: 5.3789 Recon Loss: 5.3692 
[12/22 23:03:59 TiTok]: Reconstructing images...
[12/22 23:04:40 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000100 Step: 30050 Total Loss: 5.3895 Recon Loss: 5.3797 
Epoch 3/99 started.
[12/22 23:05:22 TiTok]: Data (t): 0.0011, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000100 Step: 30100 Total Loss: 4.8951 Recon Loss: 4.8851 
[12/22 23:06:03 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8137 LR: 0.000100 Step: 30150 Total Loss: 5.4069 Recon Loss: 5.3970 
[12/22 23:06:44 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000100 Step: 30200 Total Loss: 5.4191 Recon Loss: 5.4092 
[12/22 23:07:25 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000100 Step: 30250 Total Loss: 5.3582 Recon Loss: 5.3482 
[12/22 23:08:06 TiTok]: Data (t): 0.0011, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000100 Step: 30300 Total Loss: 5.9013 Recon Loss: 5.8913 
[12/22 23:08:46 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000100 Step: 30350 Total Loss: 6.3378 Recon Loss: 6.3278 
[12/22 23:09:27 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000100 Step: 30400 Total Loss: 5.3793 Recon Loss: 5.3692 
[12/22 23:10:08 TiTok]: Data (t): 0.0011, 39.40/s/gpu Batch (t): 0.8122 LR: 0.000100 Step: 30450 Total Loss: 5.8251 Recon Loss: 5.8150 
[12/22 23:10:49 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8219 LR: 0.000100 Step: 30500 Total Loss: 5.8414 Recon Loss: 5.8313 
[12/22 23:11:30 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8202 LR: 0.000100 Step: 30550 Total Loss: 5.3692 Recon Loss: 5.3590 
[12/22 23:12:11 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000100 Step: 30600 Total Loss: 5.3216 Recon Loss: 5.3115 
[12/22 23:12:52 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000100 Step: 30650 Total Loss: 5.3670 Recon Loss: 5.3567 
[12/22 23:13:33 TiTok]: Data (t): 0.0011, 39.40/s/gpu Batch (t): 0.8122 LR: 0.000100 Step: 30700 Total Loss: 5.3828 Recon Loss: 5.3727 
[12/22 23:14:14 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000100 Step: 30750 Total Loss: 5.3498 Recon Loss: 5.3395 
[12/22 23:14:54 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8228 LR: 0.000100 Step: 30800 Total Loss: 5.8625 Recon Loss: 5.8523 
[12/22 23:15:35 TiTok]: Data (t): 0.0011, 39.48/s/gpu Batch (t): 0.8106 LR: 0.000100 Step: 30850 Total Loss: 5.8360 Recon Loss: 5.8257 
[12/22 23:16:16 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000100 Step: 30900 Total Loss: 5.8472 Recon Loss: 5.8369 
[12/22 23:16:57 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8208 LR: 0.000100 Step: 30950 Total Loss: 4.8716 Recon Loss: 4.8613 
[12/22 23:17:38 TiTok]: Data (t): 0.0010, 35.33/s/gpu Batch (t): 0.9058 LR: 0.000100 Step: 31000 Total Loss: 6.3534 Recon Loss: 6.3432 
[12/22 23:18:19 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000100 Step: 31050 Total Loss: 4.8873 Recon Loss: 4.8771 
[12/22 23:19:00 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8137 LR: 0.000100 Step: 31100 Total Loss: 6.3128 Recon Loss: 6.3025 
[12/22 23:19:41 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000100 Step: 31150 Total Loss: 5.3351 Recon Loss: 5.3248 
[12/22 23:20:22 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000100 Step: 31200 Total Loss: 5.3395 Recon Loss: 5.3292 
[12/22 23:21:02 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000100 Step: 31250 Total Loss: 5.3360 Recon Loss: 5.3256 
[12/22 23:21:43 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000100 Step: 31300 Total Loss: 5.3924 Recon Loss: 5.3821 
[12/22 23:22:24 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8202 LR: 0.000100 Step: 31350 Total Loss: 5.3378 Recon Loss: 5.3275 
[12/22 23:23:05 TiTok]: Data (t): 0.0011, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000100 Step: 31400 Total Loss: 5.3687 Recon Loss: 5.3585 
[12/22 23:23:46 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000100 Step: 31450 Total Loss: 6.3367 Recon Loss: 6.3264 
[12/22 23:24:27 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000100 Step: 31500 Total Loss: 4.8178 Recon Loss: 4.8074 
[12/22 23:25:08 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000100 Step: 31550 Total Loss: 5.4134 Recon Loss: 5.4030 
[12/22 23:25:49 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000100 Step: 31600 Total Loss: 5.8260 Recon Loss: 5.8157 
[12/22 23:26:30 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000100 Step: 31650 Total Loss: 4.8304 Recon Loss: 4.8201 
[12/22 23:27:11 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000100 Step: 31700 Total Loss: 5.3685 Recon Loss: 5.3580 
[12/22 23:27:51 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000100 Step: 31750 Total Loss: 5.3428 Recon Loss: 5.3324 
[12/22 23:28:32 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000100 Step: 31800 Total Loss: 5.3491 Recon Loss: 5.3388 
[12/22 23:29:13 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000100 Step: 31850 Total Loss: 5.3763 Recon Loss: 5.3660 
[12/22 23:29:54 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8179 LR: 0.000100 Step: 31900 Total Loss: 5.3415 Recon Loss: 5.3311 
[12/22 23:30:35 TiTok]: Data (t): 0.0011, 39.51/s/gpu Batch (t): 0.8099 LR: 0.000100 Step: 31950 Total Loss: 5.3883 Recon Loss: 5.3779 
[12/22 23:31:16 TiTok]: Data (t): 0.0011, 35.21/s/gpu Batch (t): 0.9088 LR: 0.000100 Step: 32000 Total Loss: 5.2721 Recon Loss: 5.2617 
[12/22 23:31:57 TiTok]: Data (t): 0.0010, 39.61/s/gpu Batch (t): 0.8079 LR: 0.000100 Step: 32050 Total Loss: 5.7936 Recon Loss: 5.7831 
[12/22 23:32:38 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000100 Step: 32100 Total Loss: 4.8827 Recon Loss: 4.8722 
[12/22 23:33:19 TiTok]: Data (t): 0.0011, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000100 Step: 32150 Total Loss: 5.3507 Recon Loss: 5.3403 
[12/22 23:33:59 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8200 LR: 0.000100 Step: 32200 Total Loss: 5.3626 Recon Loss: 5.3522 
[12/22 23:34:40 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000100 Step: 32250 Total Loss: 5.8232 Recon Loss: 5.8127 
[12/22 23:35:21 TiTok]: Data (t): 0.0011, 38.81/s/gpu Batch (t): 0.8246 LR: 0.000100 Step: 32300 Total Loss: 4.8140 Recon Loss: 4.8035 
[12/22 23:36:02 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000100 Step: 32350 Total Loss: 4.9018 Recon Loss: 4.8914 
[12/22 23:36:43 TiTok]: Data (t): 0.0012, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000100 Step: 32400 Total Loss: 5.8262 Recon Loss: 5.8158 
[12/22 23:37:24 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000100 Step: 32450 Total Loss: 4.8806 Recon Loss: 4.8702 
[12/22 23:38:05 TiTok]: Data (t): 0.0013, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000100 Step: 32500 Total Loss: 5.3341 Recon Loss: 5.3235 
[12/22 23:38:45 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000100 Step: 32550 Total Loss: 5.3872 Recon Loss: 5.3768 
[12/22 23:39:26 TiTok]: Data (t): 0.0013, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000100 Step: 32600 Total Loss: 5.3431 Recon Loss: 5.3327 
[12/22 23:40:07 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000100 Step: 32650 Total Loss: 5.8435 Recon Loss: 5.8329 
[12/22 23:40:48 TiTok]: Data (t): 0.0010, 39.46/s/gpu Batch (t): 0.8108 LR: 0.000100 Step: 32700 Total Loss: 5.3259 Recon Loss: 5.3154 
[12/22 23:41:29 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000100 Step: 32750 Total Loss: 5.8250 Recon Loss: 5.8146 
[12/22 23:42:10 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8235 LR: 0.000100 Step: 32800 Total Loss: 4.7804 Recon Loss: 4.7699 
[12/22 23:42:51 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000100 Step: 32850 Total Loss: 5.8570 Recon Loss: 5.8466 
[12/22 23:43:32 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000100 Step: 32900 Total Loss: 5.3262 Recon Loss: 5.3158 
[12/22 23:44:12 TiTok]: Data (t): 0.0010, 39.52/s/gpu Batch (t): 0.8096 LR: 0.000100 Step: 32950 Total Loss: 5.2703 Recon Loss: 5.2598 
[12/22 23:44:53 TiTok]: Data (t): 0.0011, 35.35/s/gpu Batch (t): 0.9052 LR: 0.000100 Step: 33000 Total Loss: 4.8186 Recon Loss: 4.8082 
[12/22 23:45:34 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000100 Step: 33050 Total Loss: 5.8090 Recon Loss: 5.7984 
[12/22 23:46:15 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8209 LR: 0.000100 Step: 33100 Total Loss: 5.7774 Recon Loss: 5.7669 
[12/22 23:46:56 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000100 Step: 33150 Total Loss: 5.8171 Recon Loss: 5.8066 
[12/22 23:47:37 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000100 Step: 33200 Total Loss: 5.3440 Recon Loss: 5.3335 
[12/22 23:48:18 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000100 Step: 33250 Total Loss: 5.2195 Recon Loss: 5.2090 
[12/22 23:48:59 TiTok]: Data (t): 0.0010, 39.49/s/gpu Batch (t): 0.8104 LR: 0.000100 Step: 33300 Total Loss: 6.3403 Recon Loss: 6.3298 
[12/22 23:49:40 TiTok]: Data (t): 0.0010, 39.46/s/gpu Batch (t): 0.8110 LR: 0.000100 Step: 33350 Total Loss: 5.8228 Recon Loss: 5.8123 
[12/22 23:50:20 TiTok]: Data (t): 0.0012, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000100 Step: 33400 Total Loss: 5.7956 Recon Loss: 5.7851 
[12/22 23:51:01 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000100 Step: 33450 Total Loss: 4.7927 Recon Loss: 4.7821 
[12/22 23:51:42 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000100 Step: 33500 Total Loss: 5.8001 Recon Loss: 5.7898 
[12/22 23:52:23 TiTok]: Data (t): 0.0013, 38.94/s/gpu Batch (t): 0.8217 LR: 0.000100 Step: 33550 Total Loss: 4.8229 Recon Loss: 4.8123 
[12/22 23:53:04 TiTok]: Data (t): 0.0011, 38.87/s/gpu Batch (t): 0.8232 LR: 0.000100 Step: 33600 Total Loss: 4.7566 Recon Loss: 4.7461 
[12/22 23:53:45 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000100 Step: 33650 Total Loss: 4.8309 Recon Loss: 4.8203 
[12/22 23:54:26 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000100 Step: 33700 Total Loss: 4.7410 Recon Loss: 4.7305 
[12/22 23:55:07 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000100 Step: 33750 Total Loss: 5.2924 Recon Loss: 5.2819 
[12/22 23:55:48 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000100 Step: 33800 Total Loss: 5.2888 Recon Loss: 5.2783 
[12/22 23:56:28 TiTok]: Data (t): 0.0011, 39.37/s/gpu Batch (t): 0.8129 LR: 0.000100 Step: 33850 Total Loss: 4.7384 Recon Loss: 4.7279 
[12/22 23:57:09 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8204 LR: 0.000100 Step: 33900 Total Loss: 5.2505 Recon Loss: 5.2400 
[12/22 23:57:50 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000100 Step: 33950 Total Loss: 4.7458 Recon Loss: 4.7353 
[12/22 23:58:31 TiTok]: Data (t): 0.0010, 35.58/s/gpu Batch (t): 0.8994 LR: 0.000100 Step: 34000 Total Loss: 5.2315 Recon Loss: 5.2210 
[12/22 23:59:12 TiTok]: Data (t): 0.0011, 39.39/s/gpu Batch (t): 0.8123 LR: 0.000100 Step: 34050 Total Loss: 5.2219 Recon Loss: 5.2114 
[12/22 23:59:53 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000100 Step: 34100 Total Loss: 5.7894 Recon Loss: 5.7789 
[12/23 00:00:34 TiTok]: Data (t): 0.0013, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000100 Step: 34150 Total Loss: 5.3191 Recon Loss: 5.3084 
[12/23 00:01:15 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8204 LR: 0.000100 Step: 34200 Total Loss: 5.3250 Recon Loss: 5.3146 
[12/23 00:01:56 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000100 Step: 34250 Total Loss: 5.2718 Recon Loss: 5.2612 
[12/23 00:02:37 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000100 Step: 34300 Total Loss: 5.7911 Recon Loss: 5.7807 
[12/23 00:03:18 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000100 Step: 34350 Total Loss: 5.8190 Recon Loss: 5.8084 
[12/23 00:03:58 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000100 Step: 34400 Total Loss: 4.7942 Recon Loss: 4.7836 
[12/23 00:04:39 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000100 Step: 34450 Total Loss: 4.7904 Recon Loss: 4.7799 
[12/23 00:05:20 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000100 Step: 34500 Total Loss: 4.7378 Recon Loss: 4.7273 
[12/23 00:06:01 TiTok]: Data (t): 0.0011, 38.83/s/gpu Batch (t): 0.8240 LR: 0.000100 Step: 34550 Total Loss: 5.3170 Recon Loss: 5.3064 
[12/23 00:06:42 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000100 Step: 34600 Total Loss: 4.7691 Recon Loss: 4.7586 
[12/23 00:07:23 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000100 Step: 34650 Total Loss: 5.2720 Recon Loss: 5.2615 
[12/23 00:08:04 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000100 Step: 34700 Total Loss: 5.8408 Recon Loss: 5.8303 
[12/23 00:08:45 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000100 Step: 34750 Total Loss: 4.8088 Recon Loss: 4.7982 
[12/23 00:09:25 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8129 LR: 0.000100 Step: 34800 Total Loss: 5.2766 Recon Loss: 5.2661 
[12/23 00:10:06 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000100 Step: 34850 Total Loss: 4.7887 Recon Loss: 4.7782 
[12/23 00:10:47 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000100 Step: 34900 Total Loss: 5.7859 Recon Loss: 5.7754 
[12/23 00:11:28 TiTok]: Data (t): 0.0010, 38.79/s/gpu Batch (t): 0.8249 LR: 0.000100 Step: 34950 Total Loss: 5.3297 Recon Loss: 5.3191 
[12/23 00:12:09 TiTok]: Data (t): 0.0016, 35.96/s/gpu Batch (t): 0.8899 LR: 0.000100 Step: 35000 Total Loss: 5.3154 Recon Loss: 5.3048 
[12/23 00:12:10 TiTok]: Reconstructing images...
[12/23 00:12:51 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8154 LR: 0.000100 Step: 35050 Total Loss: 5.2426 Recon Loss: 5.2319 
[12/23 00:13:32 TiTok]: Data (t): 0.0010, 38.85/s/gpu Batch (t): 0.8237 LR: 0.000100 Step: 35100 Total Loss: 4.7877 Recon Loss: 4.7772 
[12/23 00:14:13 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000100 Step: 35150 Total Loss: 4.7886 Recon Loss: 4.7780 
[12/23 00:14:54 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8173 LR: 0.000100 Step: 35200 Total Loss: 4.7708 Recon Loss: 4.7602 
[12/23 00:15:35 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8137 LR: 0.000100 Step: 35250 Total Loss: 5.3082 Recon Loss: 5.2977 
[12/23 00:16:16 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000100 Step: 35300 Total Loss: 4.7671 Recon Loss: 4.7565 
[12/23 00:16:57 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8204 LR: 0.000100 Step: 35350 Total Loss: 5.3039 Recon Loss: 5.2934 
[12/23 00:17:38 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000100 Step: 35400 Total Loss: 5.2028 Recon Loss: 5.1922 
[12/23 00:18:19 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000100 Step: 35450 Total Loss: 5.8164 Recon Loss: 5.8058 
[12/23 00:19:00 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000100 Step: 35500 Total Loss: 5.2711 Recon Loss: 5.2606 
[12/23 00:19:40 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8208 LR: 0.000100 Step: 35550 Total Loss: 5.2193 Recon Loss: 5.2087 
[12/23 00:20:21 TiTok]: Data (t): 0.0012, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000100 Step: 35600 Total Loss: 4.7420 Recon Loss: 4.7314 
[12/23 00:21:02 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000100 Step: 35650 Total Loss: 4.7372 Recon Loss: 4.7266 
[12/23 00:21:43 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000100 Step: 35700 Total Loss: 5.2704 Recon Loss: 5.2599 
[12/23 00:22:24 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8209 LR: 0.000100 Step: 35750 Total Loss: 4.7739 Recon Loss: 4.7634 
[12/23 00:23:05 TiTok]: Data (t): 0.0011, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000100 Step: 35800 Total Loss: 5.7721 Recon Loss: 5.7616 
[12/23 00:23:46 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8133 LR: 0.000100 Step: 35850 Total Loss: 5.2283 Recon Loss: 5.2178 
[12/23 00:24:27 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000100 Step: 35900 Total Loss: 4.7792 Recon Loss: 4.7686 
[12/23 00:25:08 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000100 Step: 35950 Total Loss: 5.3131 Recon Loss: 5.3025 
[12/23 00:25:49 TiTok]: Data (t): 0.0010, 34.73/s/gpu Batch (t): 0.9213 LR: 0.000100 Step: 36000 Total Loss: 5.3023 Recon Loss: 5.2917 
[12/23 00:26:30 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000100 Step: 36050 Total Loss: 4.8056 Recon Loss: 4.7951 
[12/23 00:27:10 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000100 Step: 36100 Total Loss: 5.1881 Recon Loss: 5.1775 
[12/23 00:27:51 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000100 Step: 36150 Total Loss: 5.7450 Recon Loss: 5.7345 
[12/23 00:28:32 TiTok]: Data (t): 0.0010, 39.39/s/gpu Batch (t): 0.8124 LR: 0.000100 Step: 36200 Total Loss: 5.7677 Recon Loss: 5.7571 
[12/23 00:29:13 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8183 LR: 0.000100 Step: 36250 Total Loss: 4.7795 Recon Loss: 4.7689 
[12/23 00:29:54 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000100 Step: 36300 Total Loss: 5.3058 Recon Loss: 5.2952 
[12/23 00:30:35 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000100 Step: 36350 Total Loss: 4.7467 Recon Loss: 4.7362 
[12/23 00:31:16 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8234 LR: 0.000100 Step: 36400 Total Loss: 4.7975 Recon Loss: 4.7869 
[12/23 00:31:57 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000100 Step: 36450 Total Loss: 4.7554 Recon Loss: 4.7447 
[12/23 00:32:38 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000100 Step: 36500 Total Loss: 5.2815 Recon Loss: 5.2709 
[12/23 00:33:18 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000100 Step: 36550 Total Loss: 5.2782 Recon Loss: 5.2677 
[12/23 00:33:59 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000100 Step: 36600 Total Loss: 4.7295 Recon Loss: 4.7189 
[12/23 00:34:40 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000100 Step: 36650 Total Loss: 4.7885 Recon Loss: 4.7779 
[12/23 00:35:21 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000100 Step: 36700 Total Loss: 5.8059 Recon Loss: 5.7954 
[12/23 00:36:02 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000100 Step: 36750 Total Loss: 5.2655 Recon Loss: 5.2549 
[12/23 00:36:43 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000100 Step: 36800 Total Loss: 4.6732 Recon Loss: 4.6626 
[12/23 00:37:24 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8137 LR: 0.000100 Step: 36850 Total Loss: 5.2683 Recon Loss: 5.2578 
[12/23 00:38:05 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000100 Step: 36900 Total Loss: 5.7676 Recon Loss: 5.7571 
[12/23 00:38:46 TiTok]: Data (t): 0.0011, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000100 Step: 36950 Total Loss: 5.2335 Recon Loss: 5.2228 
[12/23 00:39:27 TiTok]: Data (t): 0.0010, 35.14/s/gpu Batch (t): 0.9107 LR: 0.000100 Step: 37000 Total Loss: 4.7795 Recon Loss: 4.7689 
[12/23 00:40:07 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8150 LR: 0.000100 Step: 37050 Total Loss: 4.7078 Recon Loss: 4.6972 
[12/23 00:40:48 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000100 Step: 37100 Total Loss: 5.7456 Recon Loss: 5.7350 
[12/23 00:41:29 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000100 Step: 37150 Total Loss: 5.2508 Recon Loss: 5.2402 
[12/23 00:42:10 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000100 Step: 37200 Total Loss: 5.2704 Recon Loss: 5.2598 
[12/23 00:42:51 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000100 Step: 37250 Total Loss: 5.2883 Recon Loss: 5.2778 
[12/23 00:43:32 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000100 Step: 37300 Total Loss: 5.2862 Recon Loss: 5.2755 
[12/23 00:44:13 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000100 Step: 37350 Total Loss: 5.7684 Recon Loss: 5.7578 
[12/23 00:44:54 TiTok]: Data (t): 0.0010, 39.44/s/gpu Batch (t): 0.8114 LR: 0.000100 Step: 37400 Total Loss: 5.7357 Recon Loss: 5.7250 
[12/23 00:45:34 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8154 LR: 0.000100 Step: 37450 Total Loss: 5.2991 Recon Loss: 5.2886 
[12/23 00:46:15 TiTok]: Data (t): 0.0012, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000100 Step: 37500 Total Loss: 5.2498 Recon Loss: 5.2392 
[12/23 00:46:56 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000100 Step: 37550 Total Loss: 5.2306 Recon Loss: 5.2200 
[12/23 00:47:37 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000100 Step: 37600 Total Loss: 5.2689 Recon Loss: 5.2583 
[12/23 00:48:18 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000100 Step: 37650 Total Loss: 6.2806 Recon Loss: 6.2700 
[12/23 00:48:59 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000100 Step: 37700 Total Loss: 5.2859 Recon Loss: 5.2753 
[12/23 00:49:40 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000100 Step: 37750 Total Loss: 4.6810 Recon Loss: 4.6706 
[12/23 00:50:21 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000100 Step: 37800 Total Loss: 5.3238 Recon Loss: 5.3131 
[12/23 00:51:02 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000100 Step: 37850 Total Loss: 5.2936 Recon Loss: 5.2830 
[12/23 00:51:43 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000100 Step: 37900 Total Loss: 5.7871 Recon Loss: 5.7765 
[12/23 00:52:24 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000100 Step: 37950 Total Loss: 5.2346 Recon Loss: 5.2241 
[12/23 00:53:05 TiTok]: Data (t): 0.0011, 35.15/s/gpu Batch (t): 0.9104 LR: 0.000100 Step: 38000 Total Loss: 4.6605 Recon Loss: 4.6500 
[12/23 00:53:45 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8150 LR: 0.000100 Step: 38050 Total Loss: 5.2874 Recon Loss: 5.2769 
[12/23 00:54:26 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000100 Step: 38100 Total Loss: 5.8129 Recon Loss: 5.8023 
[12/23 00:55:07 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8200 LR: 0.000100 Step: 38150 Total Loss: 4.6689 Recon Loss: 4.6584 
[12/23 00:55:48 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8189 LR: 0.000100 Step: 38200 Total Loss: 5.1936 Recon Loss: 5.1831 
[12/23 00:56:29 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000100 Step: 38250 Total Loss: 4.7113 Recon Loss: 4.7008 
[12/23 00:57:10 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8229 LR: 0.000100 Step: 38300 Total Loss: 5.7906 Recon Loss: 5.7801 
[12/23 00:57:51 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8135 LR: 0.000100 Step: 38350 Total Loss: 5.2572 Recon Loss: 5.2466 
[12/23 00:58:32 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000100 Step: 38400 Total Loss: 5.2312 Recon Loss: 5.2206 
[12/23 00:59:13 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000100 Step: 38450 Total Loss: 5.2537 Recon Loss: 5.2431 
[12/23 00:59:54 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000100 Step: 38500 Total Loss: 5.2150 Recon Loss: 5.2044 
[12/23 01:00:34 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000100 Step: 38550 Total Loss: 5.2704 Recon Loss: 5.2599 
[12/23 01:01:15 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000100 Step: 38600 Total Loss: 4.6903 Recon Loss: 4.6797 
[12/23 01:01:56 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8204 LR: 0.000100 Step: 38650 Total Loss: 5.2462 Recon Loss: 5.2357 
[12/23 01:02:37 TiTok]: Data (t): 0.0010, 39.41/s/gpu Batch (t): 0.8120 LR: 0.000100 Step: 38700 Total Loss: 4.6459 Recon Loss: 4.6353 
[12/23 01:03:18 TiTok]: Data (t): 0.0011, 38.89/s/gpu Batch (t): 0.8229 LR: 0.000100 Step: 38750 Total Loss: 4.7436 Recon Loss: 4.7330 
[12/23 01:03:59 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8179 LR: 0.000100 Step: 38800 Total Loss: 4.7078 Recon Loss: 4.6973 
[12/23 01:04:40 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000100 Step: 38850 Total Loss: 5.2696 Recon Loss: 5.2590 
[12/23 01:05:20 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8234 LR: 0.000100 Step: 38900 Total Loss: 4.7440 Recon Loss: 4.7334 
[12/23 01:06:01 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000100 Step: 38950 Total Loss: 5.2485 Recon Loss: 5.2379 
[12/23 01:06:42 TiTok]: Data (t): 0.0011, 35.57/s/gpu Batch (t): 0.8997 LR: 0.000100 Step: 39000 Total Loss: 5.2340 Recon Loss: 5.2233 
[12/23 01:07:23 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000100 Step: 39050 Total Loss: 4.7954 Recon Loss: 4.7847 
[12/23 01:08:04 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8181 LR: 0.000100 Step: 39100 Total Loss: 6.2747 Recon Loss: 6.2642 
[12/23 01:08:45 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8231 LR: 0.000100 Step: 39150 Total Loss: 4.7412 Recon Loss: 4.7307 
[12/23 01:09:26 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000100 Step: 39200 Total Loss: 4.6503 Recon Loss: 4.6397 
[12/23 01:10:07 TiTok]: Data (t): 0.0011, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000100 Step: 39250 Total Loss: 5.2388 Recon Loss: 5.2282 
[12/23 01:10:48 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000100 Step: 39300 Total Loss: 5.7416 Recon Loss: 5.7309 
[12/23 01:11:29 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8162 LR: 0.000100 Step: 39350 Total Loss: 5.2477 Recon Loss: 5.2371 
[12/23 01:12:09 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000100 Step: 39400 Total Loss: 5.2565 Recon Loss: 5.2459 
[12/23 01:12:50 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000100 Step: 39450 Total Loss: 5.2033 Recon Loss: 5.1927 
[12/23 01:13:31 TiTok]: Data (t): 0.0010, 39.44/s/gpu Batch (t): 0.8114 LR: 0.000100 Step: 39500 Total Loss: 6.8145 Recon Loss: 6.8039 
[12/23 01:14:12 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000100 Step: 39550 Total Loss: 5.6963 Recon Loss: 5.6857 
[12/23 01:14:53 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000100 Step: 39600 Total Loss: 5.1505 Recon Loss: 5.1399 
[12/23 01:15:34 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8146 LR: 0.000100 Step: 39650 Total Loss: 5.2804 Recon Loss: 5.2698 
[12/23 01:16:15 TiTok]: Data (t): 0.0011, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000100 Step: 39700 Total Loss: 5.7615 Recon Loss: 5.7509 
[12/23 01:16:56 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000100 Step: 39750 Total Loss: 6.2860 Recon Loss: 6.2754 
[12/23 01:17:37 TiTok]: Data (t): 0.0012, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000100 Step: 39800 Total Loss: 5.7407 Recon Loss: 5.7301 
[12/23 01:18:18 TiTok]: Data (t): 0.0011, 38.89/s/gpu Batch (t): 0.8228 LR: 0.000100 Step: 39850 Total Loss: 4.6463 Recon Loss: 4.6357 
[12/23 01:18:58 TiTok]: Data (t): 0.0011, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000100 Step: 39900 Total Loss: 5.2100 Recon Loss: 5.1993 
[12/23 01:19:39 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000100 Step: 39950 Total Loss: 5.2278 Recon Loss: 5.2172 
[12/23 01:20:20 TiTok]: Data (t): 0.0010, 35.09/s/gpu Batch (t): 0.9120 LR: 0.000100 Step: 40000 Total Loss: 5.1994 Recon Loss: 5.1888 
[12/23 01:20:21 TiTok]: Reconstructing images...
[12/23 01:21:03 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000100 Step: 40050 Total Loss: 5.2128 Recon Loss: 5.2021 
Epoch 4/99 started.
[12/23 01:21:45 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000100 Step: 40100 Total Loss: 5.2117 Recon Loss: 5.2011 
[12/23 01:22:26 TiTok]: Data (t): 0.0010, 38.38/s/gpu Batch (t): 0.8339 LR: 0.000100 Step: 40150 Total Loss: 5.1683 Recon Loss: 5.1578 
[12/23 01:23:06 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000100 Step: 40200 Total Loss: 5.2409 Recon Loss: 5.2303 
[12/23 01:23:47 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8160 LR: 0.000100 Step: 40250 Total Loss: 4.6962 Recon Loss: 4.6856 
[12/23 01:24:28 TiTok]: Data (t): 0.0011, 39.59/s/gpu Batch (t): 0.8083 LR: 0.000100 Step: 40300 Total Loss: 5.2539 Recon Loss: 5.2433 
[12/23 01:25:09 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000100 Step: 40350 Total Loss: 4.6992 Recon Loss: 4.6885 
[12/23 01:25:50 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000100 Step: 40400 Total Loss: 5.7522 Recon Loss: 5.7417 
[12/23 01:26:31 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000100 Step: 40450 Total Loss: 5.6762 Recon Loss: 5.6656 
[12/23 01:27:12 TiTok]: Data (t): 0.0011, 39.38/s/gpu Batch (t): 0.8125 LR: 0.000100 Step: 40500 Total Loss: 5.2180 Recon Loss: 5.2073 
[12/23 01:27:53 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000100 Step: 40550 Total Loss: 5.2228 Recon Loss: 5.2122 
[12/23 01:28:34 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000100 Step: 40600 Total Loss: 5.2307 Recon Loss: 5.2201 
[12/23 01:29:15 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000100 Step: 40650 Total Loss: 4.7394 Recon Loss: 4.7288 
[12/23 01:29:56 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000100 Step: 40700 Total Loss: 5.2138 Recon Loss: 5.2032 
[12/23 01:30:36 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000100 Step: 40750 Total Loss: 5.1465 Recon Loss: 5.1359 
[12/23 01:31:17 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000100 Step: 40800 Total Loss: 5.7690 Recon Loss: 5.7584 
[12/23 01:31:58 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000100 Step: 40850 Total Loss: 4.6329 Recon Loss: 4.6222 
[12/23 01:32:39 TiTok]: Data (t): 0.0011, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000100 Step: 40900 Total Loss: 4.6071 Recon Loss: 4.5965 
[12/23 01:33:20 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000100 Step: 40950 Total Loss: 4.7214 Recon Loss: 4.7108 
[12/23 01:34:01 TiTok]: Data (t): 0.0011, 35.31/s/gpu Batch (t): 0.9062 LR: 0.000100 Step: 41000 Total Loss: 5.1560 Recon Loss: 5.1453 
[12/23 01:34:42 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000100 Step: 41050 Total Loss: 5.1689 Recon Loss: 5.1582 
[12/23 01:35:23 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8154 LR: 0.000100 Step: 41100 Total Loss: 5.1720 Recon Loss: 5.1615 
[12/23 01:36:04 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000100 Step: 41150 Total Loss: 5.2055 Recon Loss: 5.1948 
[12/23 01:36:44 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000100 Step: 41200 Total Loss: 4.6599 Recon Loss: 4.6492 
[12/23 01:37:25 TiTok]: Data (t): 0.0013, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000100 Step: 41250 Total Loss: 5.1969 Recon Loss: 5.1863 
[12/23 01:38:06 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000100 Step: 41300 Total Loss: 5.7958 Recon Loss: 5.7852 
[12/23 01:38:47 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000100 Step: 41350 Total Loss: 5.1796 Recon Loss: 5.1689 
[12/23 01:39:28 TiTok]: Data (t): 0.0010, 39.48/s/gpu Batch (t): 0.8104 LR: 0.000100 Step: 41400 Total Loss: 4.6671 Recon Loss: 4.6565 
[12/23 01:40:09 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000100 Step: 41450 Total Loss: 5.1473 Recon Loss: 5.1366 
[12/23 01:40:50 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000100 Step: 41500 Total Loss: 5.8347 Recon Loss: 5.8241 
[12/23 01:41:31 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000100 Step: 41550 Total Loss: 4.6692 Recon Loss: 4.6586 
[12/23 01:42:12 TiTok]: Data (t): 0.0011, 39.40/s/gpu Batch (t): 0.8123 LR: 0.000100 Step: 41600 Total Loss: 4.6488 Recon Loss: 4.6382 
[12/23 01:42:52 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000100 Step: 41650 Total Loss: 5.2281 Recon Loss: 5.2174 
[12/23 01:43:33 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000100 Step: 41700 Total Loss: 5.7267 Recon Loss: 5.7161 
[12/23 01:44:14 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000100 Step: 41750 Total Loss: 4.6400 Recon Loss: 4.6293 
[12/23 01:44:55 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000100 Step: 41800 Total Loss: 5.7305 Recon Loss: 5.7199 
[12/23 01:45:36 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000100 Step: 41850 Total Loss: 4.6408 Recon Loss: 4.6301 
[12/23 01:46:17 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000100 Step: 41900 Total Loss: 4.7141 Recon Loss: 4.7034 
[12/23 01:46:58 TiTok]: Data (t): 0.0012, 38.67/s/gpu Batch (t): 0.8276 LR: 0.000100 Step: 41950 Total Loss: 5.2789 Recon Loss: 5.2682 
[12/23 01:47:39 TiTok]: Data (t): 0.0010, 34.69/s/gpu Batch (t): 0.9223 LR: 0.000100 Step: 42000 Total Loss: 5.2133 Recon Loss: 5.2027 
[12/23 01:48:20 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000100 Step: 42050 Total Loss: 4.6908 Recon Loss: 4.6801 
[12/23 01:49:00 TiTok]: Data (t): 0.0011, 39.48/s/gpu Batch (t): 0.8106 LR: 0.000100 Step: 42100 Total Loss: 5.7278 Recon Loss: 5.7172 
[12/23 01:49:41 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000100 Step: 42150 Total Loss: 5.2392 Recon Loss: 5.2286 
[12/23 01:50:22 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000100 Step: 42200 Total Loss: 5.2367 Recon Loss: 5.2261 
[12/23 01:51:03 TiTok]: Data (t): 0.0010, 38.80/s/gpu Batch (t): 0.8247 LR: 0.000100 Step: 42250 Total Loss: 4.5941 Recon Loss: 4.5834 
[12/23 01:51:44 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000100 Step: 42300 Total Loss: 5.2345 Recon Loss: 5.2239 
[12/23 01:52:25 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000100 Step: 42350 Total Loss: 5.7723 Recon Loss: 5.7617 
[12/23 01:53:06 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000100 Step: 42400 Total Loss: 6.2928 Recon Loss: 6.2822 
[12/23 01:53:47 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000100 Step: 42450 Total Loss: 6.8344 Recon Loss: 6.8238 
[12/23 01:54:28 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000100 Step: 42500 Total Loss: 5.2332 Recon Loss: 5.2226 
[12/23 01:55:08 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000100 Step: 42550 Total Loss: 5.7505 Recon Loss: 5.7399 
[12/23 01:55:49 TiTok]: Data (t): 0.0011, 39.36/s/gpu Batch (t): 0.8131 LR: 0.000100 Step: 42600 Total Loss: 5.7267 Recon Loss: 5.7161 
[12/23 01:56:30 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000100 Step: 42650 Total Loss: 5.1465 Recon Loss: 5.1358 
[12/23 01:57:11 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8158 LR: 0.000100 Step: 42700 Total Loss: 4.6639 Recon Loss: 4.6533 
[12/23 01:57:52 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000100 Step: 42750 Total Loss: 4.6641 Recon Loss: 4.6534 
[12/23 01:58:33 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000100 Step: 42800 Total Loss: 5.1476 Recon Loss: 5.1369 
[12/23 01:59:14 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000100 Step: 42850 Total Loss: 5.7016 Recon Loss: 5.6910 
[12/23 01:59:55 TiTok]: Data (t): 0.0011, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000100 Step: 42900 Total Loss: 4.6897 Recon Loss: 4.6791 
[12/23 02:00:35 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000100 Step: 42950 Total Loss: 5.7439 Recon Loss: 5.7332 
[12/23 02:01:16 TiTok]: Data (t): 0.0011, 35.22/s/gpu Batch (t): 0.9084 LR: 0.000100 Step: 43000 Total Loss: 6.2653 Recon Loss: 6.2548 
[12/23 02:01:57 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000100 Step: 43050 Total Loss: 4.6762 Recon Loss: 4.6655 
[12/23 02:02:38 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000100 Step: 43100 Total Loss: 4.6927 Recon Loss: 4.6821 
[12/23 02:03:19 TiTok]: Data (t): 0.0010, 39.43/s/gpu Batch (t): 0.8115 LR: 0.000100 Step: 43150 Total Loss: 5.7328 Recon Loss: 5.7221 
[12/23 02:04:00 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000100 Step: 43200 Total Loss: 5.2061 Recon Loss: 5.1955 
[12/23 02:04:41 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000100 Step: 43250 Total Loss: 5.2137 Recon Loss: 5.2031 
[12/23 02:05:22 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000100 Step: 43300 Total Loss: 4.6489 Recon Loss: 4.6383 
[12/23 02:06:03 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000100 Step: 43350 Total Loss: 5.7122 Recon Loss: 5.7017 
[12/23 02:06:43 TiTok]: Data (t): 0.0011, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000100 Step: 43400 Total Loss: 5.1689 Recon Loss: 5.1582 
[12/23 02:07:24 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000100 Step: 43450 Total Loss: 4.6252 Recon Loss: 4.6146 
[12/23 02:08:05 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8204 LR: 0.000100 Step: 43500 Total Loss: 6.3208 Recon Loss: 6.3102 
[12/23 02:08:46 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000100 Step: 43550 Total Loss: 5.1880 Recon Loss: 5.1773 
[12/23 02:09:27 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8202 LR: 0.000100 Step: 43600 Total Loss: 5.1934 Recon Loss: 5.1827 
[12/23 02:10:08 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000100 Step: 43650 Total Loss: 5.1718 Recon Loss: 5.1612 
[12/23 02:10:49 TiTok]: Data (t): 0.0011, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000100 Step: 43700 Total Loss: 5.1056 Recon Loss: 5.0949 
[12/23 02:11:30 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000100 Step: 43750 Total Loss: 4.6699 Recon Loss: 4.6592 
[12/23 02:12:10 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8185 LR: 0.000100 Step: 43800 Total Loss: 5.7012 Recon Loss: 5.6906 
[12/23 02:12:51 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000100 Step: 43850 Total Loss: 5.1557 Recon Loss: 5.1452 
[12/23 02:13:32 TiTok]: Data (t): 0.0011, 38.87/s/gpu Batch (t): 0.8233 LR: 0.000100 Step: 43900 Total Loss: 5.1806 Recon Loss: 5.1699 
[12/23 02:14:13 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000100 Step: 43950 Total Loss: 5.7286 Recon Loss: 5.7179 
[12/23 02:14:54 TiTok]: Data (t): 0.0010, 35.40/s/gpu Batch (t): 0.9039 LR: 0.000100 Step: 44000 Total Loss: 4.6695 Recon Loss: 4.6589 
[12/23 02:15:35 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000100 Step: 44050 Total Loss: 4.6121 Recon Loss: 4.6015 
[12/23 02:16:16 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000100 Step: 44100 Total Loss: 5.2012 Recon Loss: 5.1906 
[12/23 02:16:57 TiTok]: Data (t): 0.0010, 39.46/s/gpu Batch (t): 0.8110 LR: 0.000100 Step: 44150 Total Loss: 6.2522 Recon Loss: 6.2416 
[12/23 02:17:38 TiTok]: Data (t): 0.0011, 38.84/s/gpu Batch (t): 0.8239 LR: 0.000100 Step: 44200 Total Loss: 5.7262 Recon Loss: 5.7156 
[12/23 02:18:19 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000100 Step: 44250 Total Loss: 5.2258 Recon Loss: 5.2151 
[12/23 02:18:59 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000100 Step: 44300 Total Loss: 5.7189 Recon Loss: 5.7083 
[12/23 02:19:40 TiTok]: Data (t): 0.0010, 39.64/s/gpu Batch (t): 0.8074 LR: 0.000100 Step: 44350 Total Loss: 5.7539 Recon Loss: 5.7432 
[12/23 02:20:21 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000100 Step: 44400 Total Loss: 5.1890 Recon Loss: 5.1783 
[12/23 02:21:02 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8210 LR: 0.000100 Step: 44450 Total Loss: 4.5481 Recon Loss: 4.5375 
[12/23 02:21:43 TiTok]: Data (t): 0.0010, 39.42/s/gpu Batch (t): 0.8118 LR: 0.000100 Step: 44500 Total Loss: 5.2190 Recon Loss: 5.2083 
[12/23 02:22:24 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000100 Step: 44550 Total Loss: 4.6426 Recon Loss: 4.6319 
[12/23 02:23:05 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000100 Step: 44600 Total Loss: 5.7347 Recon Loss: 5.7240 
[12/23 02:23:46 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000100 Step: 44650 Total Loss: 5.1953 Recon Loss: 5.1846 
[12/23 02:24:26 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000100 Step: 44700 Total Loss: 5.1592 Recon Loss: 5.1485 
[12/23 02:25:07 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8129 LR: 0.000100 Step: 44750 Total Loss: 5.7234 Recon Loss: 5.7127 
[12/23 02:25:48 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000100 Step: 44800 Total Loss: 4.6231 Recon Loss: 4.6124 
[12/23 02:26:29 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8156 LR: 0.000100 Step: 44850 Total Loss: 4.6749 Recon Loss: 4.6643 
[12/23 02:27:10 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000100 Step: 44900 Total Loss: 4.6031 Recon Loss: 4.5925 
[12/23 02:27:51 TiTok]: Data (t): 0.0010, 39.46/s/gpu Batch (t): 0.8110 LR: 0.000100 Step: 44950 Total Loss: 5.1709 Recon Loss: 5.1601 
[12/23 02:28:32 TiTok]: Data (t): 0.0011, 34.99/s/gpu Batch (t): 0.9144 LR: 0.000100 Step: 45000 Total Loss: 5.7678 Recon Loss: 5.7571 
[12/23 02:28:33 TiTok]: Reconstructing images...
[12/23 02:29:14 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000100 Step: 45050 Total Loss: 5.2427 Recon Loss: 5.2322 
[12/23 02:29:55 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000100 Step: 45100 Total Loss: 5.1483 Recon Loss: 5.1377 
[12/23 02:30:36 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8221 LR: 0.000100 Step: 45150 Total Loss: 5.1715 Recon Loss: 5.1609 
[12/23 02:31:17 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000100 Step: 45200 Total Loss: 6.2404 Recon Loss: 6.2298 
[12/23 02:31:58 TiTok]: Data (t): 0.0012, 39.50/s/gpu Batch (t): 0.8101 LR: 0.000100 Step: 45250 Total Loss: 5.2197 Recon Loss: 5.2090 
[12/23 02:32:39 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000100 Step: 45300 Total Loss: 4.6021 Recon Loss: 4.5914 
[12/23 02:33:20 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000100 Step: 45350 Total Loss: 4.5739 Recon Loss: 4.5632 
[12/23 02:34:01 TiTok]: Data (t): 0.0010, 39.45/s/gpu Batch (t): 0.8112 LR: 0.000100 Step: 45400 Total Loss: 4.5384 Recon Loss: 4.5278 
[12/23 02:34:41 TiTok]: Data (t): 0.0010, 39.64/s/gpu Batch (t): 0.8073 LR: 0.000100 Step: 45450 Total Loss: 5.7225 Recon Loss: 5.7119 
[12/23 02:35:22 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000100 Step: 45500 Total Loss: 5.1566 Recon Loss: 5.1460 
[12/23 02:36:03 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000100 Step: 45550 Total Loss: 5.1005 Recon Loss: 5.0899 
[12/23 02:36:44 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000100 Step: 45600 Total Loss: 5.1125 Recon Loss: 5.1018 
[12/23 02:37:25 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000100 Step: 45650 Total Loss: 5.1342 Recon Loss: 5.1236 
[12/23 02:38:06 TiTok]: Data (t): 0.0011, 39.50/s/gpu Batch (t): 0.8102 LR: 0.000100 Step: 45700 Total Loss: 5.6994 Recon Loss: 5.6887 
[12/23 02:38:47 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000100 Step: 45750 Total Loss: 5.7293 Recon Loss: 5.7187 
[12/23 02:39:28 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000100 Step: 45800 Total Loss: 6.2892 Recon Loss: 6.2785 
[12/23 02:40:08 TiTok]: Data (t): 0.0012, 39.11/s/gpu Batch (t): 0.8181 LR: 0.000100 Step: 45850 Total Loss: 4.5771 Recon Loss: 4.5665 
[12/23 02:40:49 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000100 Step: 45900 Total Loss: 4.5968 Recon Loss: 4.5860 
[12/23 02:41:30 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8202 LR: 0.000100 Step: 45950 Total Loss: 5.7243 Recon Loss: 5.7136 
[12/23 02:42:11 TiTok]: Data (t): 0.0011, 34.71/s/gpu Batch (t): 0.9218 LR: 0.000100 Step: 46000 Total Loss: 4.5779 Recon Loss: 4.5672 
[12/23 02:42:52 TiTok]: Data (t): 0.0011, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000100 Step: 46050 Total Loss: 5.6649 Recon Loss: 5.6543 
[12/23 02:43:33 TiTok]: Data (t): 0.0012, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000100 Step: 46100 Total Loss: 4.5435 Recon Loss: 4.5329 
[12/23 02:44:14 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000100 Step: 46150 Total Loss: 5.1583 Recon Loss: 5.1477 
[12/23 02:44:54 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000100 Step: 46200 Total Loss: 5.1229 Recon Loss: 5.1123 
[12/23 02:45:35 TiTok]: Data (t): 0.0011, 39.41/s/gpu Batch (t): 0.8120 LR: 0.000100 Step: 46250 Total Loss: 5.1365 Recon Loss: 5.1258 
[12/23 02:46:16 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8200 LR: 0.000100 Step: 46300 Total Loss: 5.1496 Recon Loss: 5.1389 
[12/23 02:46:57 TiTok]: Data (t): 0.0011, 39.45/s/gpu Batch (t): 0.8112 LR: 0.000100 Step: 46350 Total Loss: 5.1644 Recon Loss: 5.1538 
[12/23 02:47:38 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000100 Step: 46400 Total Loss: 4.6622 Recon Loss: 4.6515 
[12/23 02:48:19 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000100 Step: 46450 Total Loss: 5.7236 Recon Loss: 5.7129 
[12/23 02:49:00 TiTok]: Data (t): 0.0010, 38.78/s/gpu Batch (t): 0.8252 LR: 0.000100 Step: 46500 Total Loss: 4.5664 Recon Loss: 4.5557 
[12/23 02:49:41 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000100 Step: 46550 Total Loss: 4.6911 Recon Loss: 4.6805 
[12/23 02:50:22 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000100 Step: 46600 Total Loss: 5.1614 Recon Loss: 5.1508 
[12/23 02:51:02 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000100 Step: 46650 Total Loss: 5.1609 Recon Loss: 5.1502 
[12/23 02:51:43 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000100 Step: 46700 Total Loss: 4.6387 Recon Loss: 4.6280 
[12/23 02:52:24 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8217 LR: 0.000100 Step: 46750 Total Loss: 5.6994 Recon Loss: 5.6887 
[12/23 02:53:05 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000100 Step: 46800 Total Loss: 5.0796 Recon Loss: 5.0690 
[12/23 02:53:46 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000100 Step: 46850 Total Loss: 5.0957 Recon Loss: 5.0851 
[12/23 02:54:27 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000100 Step: 46900 Total Loss: 5.1994 Recon Loss: 5.1888 
[12/23 02:55:08 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000100 Step: 46950 Total Loss: 5.1684 Recon Loss: 5.1578 
[12/23 02:55:49 TiTok]: Data (t): 0.0010, 34.92/s/gpu Batch (t): 0.9163 LR: 0.000100 Step: 47000 Total Loss: 5.1552 Recon Loss: 5.1446 
[12/23 02:56:30 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000100 Step: 47050 Total Loss: 5.2055 Recon Loss: 5.1947 
[12/23 02:57:10 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000100 Step: 47100 Total Loss: 4.5666 Recon Loss: 4.5560 
[12/23 02:57:51 TiTok]: Data (t): 0.0010, 39.42/s/gpu Batch (t): 0.8118 LR: 0.000100 Step: 47150 Total Loss: 4.5919 Recon Loss: 4.5813 
[12/23 02:58:32 TiTok]: Data (t): 0.0011, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000100 Step: 47200 Total Loss: 5.7189 Recon Loss: 5.7082 
[12/23 02:59:13 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000100 Step: 47250 Total Loss: 5.7988 Recon Loss: 5.7882 
[12/23 02:59:54 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000100 Step: 47300 Total Loss: 4.5493 Recon Loss: 4.5387 
[12/23 03:00:35 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8202 LR: 0.000100 Step: 47350 Total Loss: 5.2066 Recon Loss: 5.1959 
[12/23 03:01:16 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000100 Step: 47400 Total Loss: 5.7155 Recon Loss: 5.7048 
[12/23 03:01:56 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000100 Step: 47450 Total Loss: 4.6305 Recon Loss: 4.6198 
[12/23 03:02:37 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000100 Step: 47500 Total Loss: 5.2108 Recon Loss: 5.2001 
[12/23 03:03:18 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000100 Step: 47550 Total Loss: 5.6980 Recon Loss: 5.6875 
[12/23 03:03:59 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000100 Step: 47600 Total Loss: 5.1080 Recon Loss: 5.0974 
[12/23 03:04:40 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000100 Step: 47650 Total Loss: 4.5586 Recon Loss: 4.5481 
[12/23 03:05:21 TiTok]: Data (t): 0.0011, 39.39/s/gpu Batch (t): 0.8124 LR: 0.000100 Step: 47700 Total Loss: 4.5424 Recon Loss: 4.5319 
[12/23 03:06:02 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000100 Step: 47750 Total Loss: 5.7038 Recon Loss: 5.6932 
[12/23 03:06:43 TiTok]: Data (t): 0.0011, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000100 Step: 47800 Total Loss: 4.5726 Recon Loss: 4.5620 
[12/23 03:07:23 TiTok]: Data (t): 0.0010, 39.41/s/gpu Batch (t): 0.8119 LR: 0.000100 Step: 47850 Total Loss: 4.5570 Recon Loss: 4.5464 
[12/23 03:08:04 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000100 Step: 47900 Total Loss: 5.1031 Recon Loss: 5.0924 
[12/23 03:08:45 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000100 Step: 47950 Total Loss: 5.1956 Recon Loss: 5.1850 
[12/23 03:09:26 TiTok]: Data (t): 0.0011, 35.34/s/gpu Batch (t): 0.9055 LR: 0.000100 Step: 48000 Total Loss: 5.6796 Recon Loss: 5.6689 
[12/23 03:10:07 TiTok]: Data (t): 0.0011, 38.80/s/gpu Batch (t): 0.8247 LR: 0.000100 Step: 48050 Total Loss: 5.1591 Recon Loss: 5.1484 
[12/23 03:10:48 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8223 LR: 0.000100 Step: 48100 Total Loss: 5.7473 Recon Loss: 5.7366 
[12/23 03:11:29 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8179 LR: 0.000100 Step: 48150 Total Loss: 4.6300 Recon Loss: 4.6193 
[12/23 03:12:10 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000100 Step: 48200 Total Loss: 5.6985 Recon Loss: 5.6879 
[12/23 03:12:51 TiTok]: Data (t): 0.0012, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000100 Step: 48250 Total Loss: 5.1619 Recon Loss: 5.1512 
[12/23 03:13:31 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8221 LR: 0.000100 Step: 48300 Total Loss: 5.1468 Recon Loss: 5.1362 
[12/23 03:14:12 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000100 Step: 48350 Total Loss: 5.1584 Recon Loss: 5.1479 
[12/23 03:14:53 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000100 Step: 48400 Total Loss: 5.1571 Recon Loss: 5.1465 
[12/23 03:15:34 TiTok]: Data (t): 0.0010, 39.37/s/gpu Batch (t): 0.8127 LR: 0.000100 Step: 48450 Total Loss: 5.1126 Recon Loss: 5.1020 
[12/23 03:16:15 TiTok]: Data (t): 0.0012, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000100 Step: 48500 Total Loss: 4.6127 Recon Loss: 4.6020 
[12/23 03:16:56 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000100 Step: 48550 Total Loss: 6.2952 Recon Loss: 6.2845 
[12/23 03:17:37 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000100 Step: 48600 Total Loss: 5.6596 Recon Loss: 5.6491 
[12/23 03:18:18 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000100 Step: 48650 Total Loss: 4.5472 Recon Loss: 4.5366 
[12/23 03:18:59 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000100 Step: 48700 Total Loss: 5.1482 Recon Loss: 5.1376 
[12/23 03:19:40 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000100 Step: 48750 Total Loss: 4.5626 Recon Loss: 4.5520 
[12/23 03:20:20 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000100 Step: 48800 Total Loss: 5.1531 Recon Loss: 5.1424 
[12/23 03:21:01 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000100 Step: 48850 Total Loss: 4.6009 Recon Loss: 4.5902 
[12/23 03:21:42 TiTok]: Data (t): 0.0011, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000100 Step: 48900 Total Loss: 4.4812 Recon Loss: 4.4705 
[12/23 03:22:23 TiTok]: Data (t): 0.0010, 39.46/s/gpu Batch (t): 0.8110 LR: 0.000100 Step: 48950 Total Loss: 5.7263 Recon Loss: 5.7156 
[12/23 03:23:04 TiTok]: Data (t): 0.0010, 35.49/s/gpu Batch (t): 0.9017 LR: 0.000100 Step: 49000 Total Loss: 5.2034 Recon Loss: 5.1928 
[12/23 03:23:45 TiTok]: Data (t): 0.0010, 38.83/s/gpu Batch (t): 0.8240 LR: 0.000100 Step: 49050 Total Loss: 4.5459 Recon Loss: 4.5353 
[12/23 03:24:26 TiTok]: Data (t): 0.0011, 38.86/s/gpu Batch (t): 0.8234 LR: 0.000100 Step: 49100 Total Loss: 4.5800 Recon Loss: 4.5693 
[12/23 03:25:07 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8150 LR: 0.000100 Step: 49150 Total Loss: 5.7205 Recon Loss: 5.7098 
[12/23 03:25:48 TiTok]: Data (t): 0.0011, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000100 Step: 49200 Total Loss: 4.6100 Recon Loss: 4.5993 
[12/23 03:26:28 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8198 LR: 0.000100 Step: 49250 Total Loss: 5.1233 Recon Loss: 5.1127 
[12/23 03:27:09 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000100 Step: 49300 Total Loss: 4.5636 Recon Loss: 4.5529 
[12/23 03:27:50 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8227 LR: 0.000100 Step: 49350 Total Loss: 5.2135 Recon Loss: 5.2029 
[12/23 03:28:31 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000100 Step: 49400 Total Loss: 4.5572 Recon Loss: 4.5466 
[12/23 03:29:12 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8202 LR: 0.000100 Step: 49450 Total Loss: 5.1196 Recon Loss: 5.1090 
[12/23 03:29:53 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000100 Step: 49500 Total Loss: 4.5066 Recon Loss: 4.4960 
[12/23 03:30:34 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000100 Step: 49550 Total Loss: 5.6789 Recon Loss: 5.6682 
[12/23 03:31:15 TiTok]: Data (t): 0.0010, 39.41/s/gpu Batch (t): 0.8120 LR: 0.000100 Step: 49600 Total Loss: 5.1704 Recon Loss: 5.1598 
[12/23 03:31:55 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8131 LR: 0.000100 Step: 49650 Total Loss: 5.6292 Recon Loss: 5.6186 
[12/23 03:32:36 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000100 Step: 49700 Total Loss: 5.6631 Recon Loss: 5.6526 
[12/23 03:33:17 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000100 Step: 49750 Total Loss: 4.6056 Recon Loss: 4.5950 
[12/23 03:33:58 TiTok]: Data (t): 0.0010, 39.39/s/gpu Batch (t): 0.8124 LR: 0.000100 Step: 49800 Total Loss: 5.1735 Recon Loss: 5.1629 
[12/23 03:34:39 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8200 LR: 0.000100 Step: 49850 Total Loss: 5.6595 Recon Loss: 5.6488 
[12/23 03:35:20 TiTok]: Data (t): 0.0011, 39.46/s/gpu Batch (t): 0.8110 LR: 0.000100 Step: 49900 Total Loss: 5.1492 Recon Loss: 5.1386 
[12/23 03:36:01 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8200 LR: 0.000100 Step: 49950 Total Loss: 6.2705 Recon Loss: 6.2599 
[12/23 03:36:42 TiTok]: Data (t): 0.0010, 35.79/s/gpu Batch (t): 0.8941 LR: 0.000100 Step: 50000 Total Loss: 4.5246 Recon Loss: 4.5139 
Model weights saved in titok_b64_stage1_run1/checkpoint-50000/unwrapped_model/pytorch_model.bin
[12/23 03:36:44 TiTok]: Saved state to titok_b64_stage1_run1/checkpoint-50000
Model weights saved in titok_b64_stage1_run1/checkpoint-50000/ema_model/pytorch_model.bin
[12/23 03:37:00 TiTok]: Reconstructing images...
[12/23 03:37:01 TiTok]: Computing metrics on the validation set.
[12/23 03:52:08 TiTok]: EMA EVALUATION Step: 50000 
[12/23 03:52:08 TiTok]: {'CodebookEntropy': tensor(11.5026, device='cuda:0', dtype=torch.float64),
 'CodebookUsage': 0.712890625,
 'InceptionScore': 38.47759172809835,
 'rFID': 72.06927838416675}
[12/23 03:53:24 TiTok]: Data (t): 0.0010, 39.47/s/gpu Batch (t): 0.8107 LR: 0.000100 Step: 50050 Total Loss: 5.1329 Recon Loss: 5.1222 
[12/23 03:54:04 TiTok]: Data (t): 0.0010, 39.41/s/gpu Batch (t): 0.8120 LR: 0.000100 Step: 50100 Total Loss: 5.0739 Recon Loss: 5.0632 
Epoch 5/99 started.
[12/23 03:54:46 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000100 Step: 50150 Total Loss: 5.0794 Recon Loss: 5.0688 
[12/23 03:55:27 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000100 Step: 50200 Total Loss: 5.1296 Recon Loss: 5.1190 
[12/23 03:56:08 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000100 Step: 50250 Total Loss: 5.1241 Recon Loss: 5.1135 
[12/23 03:56:49 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000100 Step: 50300 Total Loss: 5.6247 Recon Loss: 5.6140 
[12/23 03:57:29 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000100 Step: 50350 Total Loss: 4.6358 Recon Loss: 4.6252 
[12/23 03:58:10 TiTok]: Data (t): 0.0010, 38.85/s/gpu Batch (t): 0.8237 LR: 0.000100 Step: 50400 Total Loss: 4.5221 Recon Loss: 4.5114 
[12/23 03:58:51 TiTok]: Data (t): 0.0012, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000100 Step: 50450 Total Loss: 5.1265 Recon Loss: 5.1158 
[12/23 03:59:32 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000100 Step: 50500 Total Loss: 5.1202 Recon Loss: 5.1095 
[12/23 04:00:13 TiTok]: Data (t): 0.0011, 38.68/s/gpu Batch (t): 0.8273 LR: 0.000100 Step: 50550 Total Loss: 5.0444 Recon Loss: 5.0337 
[12/23 04:00:54 TiTok]: Data (t): 0.0010, 38.56/s/gpu Batch (t): 0.8298 LR: 0.000100 Step: 50600 Total Loss: 4.5660 Recon Loss: 4.5553 
[12/23 04:01:35 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000100 Step: 50650 Total Loss: 4.4388 Recon Loss: 4.4281 
[12/23 04:02:16 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8131 LR: 0.000100 Step: 50700 Total Loss: 5.6956 Recon Loss: 5.6849 
[12/23 04:02:57 TiTok]: Data (t): 0.0010, 39.44/s/gpu Batch (t): 0.8113 LR: 0.000100 Step: 50750 Total Loss: 5.6991 Recon Loss: 5.6884 
[12/23 04:03:38 TiTok]: Data (t): 0.0020, 38.82/s/gpu Batch (t): 0.8243 LR: 0.000100 Step: 50800 Total Loss: 4.5997 Recon Loss: 4.5890 
[12/23 04:04:19 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000100 Step: 50850 Total Loss: 5.6865 Recon Loss: 5.6759 
[12/23 04:05:00 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000100 Step: 50900 Total Loss: 5.0914 Recon Loss: 5.0807 
[12/23 04:05:40 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000100 Step: 50950 Total Loss: 4.5073 Recon Loss: 4.4966 
[12/23 04:06:22 TiTok]: Data (t): 0.0010, 34.99/s/gpu Batch (t): 0.9145 LR: 0.000100 Step: 51000 Total Loss: 5.1738 Recon Loss: 5.1631 
[12/23 04:07:02 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000100 Step: 51050 Total Loss: 4.5681 Recon Loss: 4.5575 
[12/23 04:07:43 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000100 Step: 51100 Total Loss: 5.0791 Recon Loss: 5.0684 
[12/23 04:08:24 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000100 Step: 51150 Total Loss: 5.0713 Recon Loss: 5.0606 
[12/23 04:09:05 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000100 Step: 51200 Total Loss: 4.5961 Recon Loss: 4.5855 
[12/23 04:09:46 TiTok]: Data (t): 0.0010, 38.84/s/gpu Batch (t): 0.8238 LR: 0.000100 Step: 51250 Total Loss: 4.6056 Recon Loss: 4.5949 
[12/23 04:10:27 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000100 Step: 51300 Total Loss: 5.1380 Recon Loss: 5.1274 
[12/23 04:11:08 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000100 Step: 51350 Total Loss: 5.1665 Recon Loss: 5.1559 
[12/23 04:11:49 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000100 Step: 51400 Total Loss: 5.1140 Recon Loss: 5.1034 
[12/23 04:12:30 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000100 Step: 51450 Total Loss: 4.4749 Recon Loss: 4.4641 
[12/23 04:13:11 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8183 LR: 0.000100 Step: 51500 Total Loss: 4.5464 Recon Loss: 4.5358 
[12/23 04:13:52 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000100 Step: 51550 Total Loss: 6.8352 Recon Loss: 6.8246 
[12/23 04:14:32 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000100 Step: 51600 Total Loss: 4.5816 Recon Loss: 4.5709 
[12/23 04:15:13 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000100 Step: 51650 Total Loss: 5.0933 Recon Loss: 5.0826 
[12/23 04:15:54 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8158 LR: 0.000100 Step: 51700 Total Loss: 4.5048 Recon Loss: 4.4941 
[12/23 04:16:35 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000100 Step: 51750 Total Loss: 4.5592 Recon Loss: 4.5485 
[12/23 04:17:16 TiTok]: Data (t): 0.0011, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000100 Step: 51800 Total Loss: 5.7010 Recon Loss: 5.6903 
[12/23 04:17:57 TiTok]: Data (t): 0.0011, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000100 Step: 51850 Total Loss: 5.6815 Recon Loss: 5.6709 
[12/23 04:18:38 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000100 Step: 51900 Total Loss: 4.4897 Recon Loss: 4.4791 
[12/23 04:19:19 TiTok]: Data (t): 0.0010, 38.85/s/gpu Batch (t): 0.8237 LR: 0.000100 Step: 51950 Total Loss: 4.5647 Recon Loss: 4.5541 
[12/23 04:20:00 TiTok]: Data (t): 0.0010, 35.68/s/gpu Batch (t): 0.8970 LR: 0.000100 Step: 52000 Total Loss: 4.5187 Recon Loss: 4.5080 
[12/23 04:20:41 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000100 Step: 52050 Total Loss: 5.6162 Recon Loss: 5.6056 
[12/23 04:21:22 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8162 LR: 0.000100 Step: 52100 Total Loss: 5.1375 Recon Loss: 5.1267 
[12/23 04:22:02 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000100 Step: 52150 Total Loss: 6.2566 Recon Loss: 6.2460 
[12/23 04:22:43 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8154 LR: 0.000100 Step: 52200 Total Loss: 5.6947 Recon Loss: 5.6839 
[12/23 04:23:24 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000100 Step: 52250 Total Loss: 5.6667 Recon Loss: 5.6561 
[12/23 04:24:05 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000100 Step: 52300 Total Loss: 4.5803 Recon Loss: 4.5697 
[12/23 04:24:46 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000100 Step: 52350 Total Loss: 5.7315 Recon Loss: 5.7208 
[12/23 04:25:27 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8154 LR: 0.000100 Step: 52400 Total Loss: 5.6787 Recon Loss: 5.6680 
[12/23 04:26:08 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000100 Step: 52450 Total Loss: 4.4485 Recon Loss: 4.4378 
[12/23 04:26:49 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000100 Step: 52500 Total Loss: 4.4707 Recon Loss: 4.4600 
[12/23 04:27:30 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8152 LR: 0.000100 Step: 52550 Total Loss: 5.7028 Recon Loss: 5.6921 
[12/23 04:28:11 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000100 Step: 52600 Total Loss: 5.0888 Recon Loss: 5.0782 
[12/23 04:28:52 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8152 LR: 0.000100 Step: 52650 Total Loss: 4.5511 Recon Loss: 4.5404 
[12/23 04:29:33 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000100 Step: 52700 Total Loss: 5.0723 Recon Loss: 5.0617 
[12/23 04:30:14 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000100 Step: 52750 Total Loss: 5.1455 Recon Loss: 5.1348 
[12/23 04:30:54 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000100 Step: 52800 Total Loss: 6.2141 Recon Loss: 6.2035 
[12/23 04:31:35 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000100 Step: 52850 Total Loss: 5.1359 Recon Loss: 5.1253 
[12/23 04:32:16 TiTok]: Data (t): 0.0010, 39.44/s/gpu Batch (t): 0.8114 LR: 0.000100 Step: 52900 Total Loss: 5.0440 Recon Loss: 5.0333 
[12/23 04:32:57 TiTok]: Data (t): 0.0010, 38.73/s/gpu Batch (t): 0.8263 LR: 0.000100 Step: 52950 Total Loss: 5.6809 Recon Loss: 5.6702 
[12/23 04:33:38 TiTok]: Data (t): 0.0011, 34.73/s/gpu Batch (t): 0.9215 LR: 0.000100 Step: 53000 Total Loss: 5.1223 Recon Loss: 5.1116 
[12/23 04:34:19 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000100 Step: 53050 Total Loss: 5.6537 Recon Loss: 5.6430 
[12/23 04:35:00 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000100 Step: 53100 Total Loss: 5.7022 Recon Loss: 5.6917 
[12/23 04:35:41 TiTok]: Data (t): 0.0011, 38.70/s/gpu Batch (t): 0.8269 LR: 0.000100 Step: 53150 Total Loss: 5.1410 Recon Loss: 5.1304 
[12/23 04:36:22 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000100 Step: 53200 Total Loss: 5.1217 Recon Loss: 5.1111 
[12/23 04:37:03 TiTok]: Data (t): 0.0010, 38.68/s/gpu Batch (t): 0.8274 LR: 0.000100 Step: 53250 Total Loss: 5.1534 Recon Loss: 5.1427 
[12/23 04:37:44 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000100 Step: 53300 Total Loss: 5.0761 Recon Loss: 5.0654 
[12/23 04:38:24 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000100 Step: 53350 Total Loss: 5.7046 Recon Loss: 5.6939 
[12/23 04:39:05 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8181 LR: 0.000100 Step: 53400 Total Loss: 4.5342 Recon Loss: 4.5236 
[12/23 04:39:46 TiTok]: Data (t): 0.0011, 38.86/s/gpu Batch (t): 0.8234 LR: 0.000100 Step: 53450 Total Loss: 4.5499 Recon Loss: 4.5393 
[12/23 04:40:27 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000100 Step: 53500 Total Loss: 5.1186 Recon Loss: 5.1079 
[12/23 04:41:08 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000100 Step: 53550 Total Loss: 5.1159 Recon Loss: 5.1052 
[12/23 04:41:49 TiTok]: Data (t): 0.0017, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000100 Step: 53600 Total Loss: 5.6292 Recon Loss: 5.6185 
[12/23 04:42:30 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000100 Step: 53650 Total Loss: 4.4683 Recon Loss: 4.4577 
[12/23 04:43:11 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000100 Step: 53700 Total Loss: 4.6090 Recon Loss: 4.5984 
[12/23 04:43:52 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8229 LR: 0.000100 Step: 53750 Total Loss: 4.4828 Recon Loss: 4.4721 
[12/23 04:44:32 TiTok]: Data (t): 0.0010, 39.48/s/gpu Batch (t): 0.8105 LR: 0.000100 Step: 53800 Total Loss: 5.6334 Recon Loss: 5.6227 
[12/23 04:45:13 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000100 Step: 53850 Total Loss: 5.0572 Recon Loss: 5.0465 
[12/23 04:45:54 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000100 Step: 53900 Total Loss: 4.4603 Recon Loss: 4.4496 
[12/23 04:46:35 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000100 Step: 53950 Total Loss: 5.1037 Recon Loss: 5.0930 
[12/23 04:47:16 TiTok]: Data (t): 0.0010, 35.20/s/gpu Batch (t): 0.9092 LR: 0.000100 Step: 54000 Total Loss: 5.6702 Recon Loss: 5.6596 
[12/23 04:47:57 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000100 Step: 54050 Total Loss: 5.0691 Recon Loss: 5.0585 
[12/23 04:48:38 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8166 LR: 0.000100 Step: 54100 Total Loss: 5.0445 Recon Loss: 5.0339 
[12/23 04:49:19 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000100 Step: 54150 Total Loss: 5.0425 Recon Loss: 5.0320 
[12/23 04:50:00 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000100 Step: 54200 Total Loss: 4.5797 Recon Loss: 4.5690 
[12/23 04:50:41 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000100 Step: 54250 Total Loss: 5.0641 Recon Loss: 5.0535 
[12/23 04:51:22 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000100 Step: 54300 Total Loss: 5.6575 Recon Loss: 5.6469 
[12/23 04:52:02 TiTok]: Data (t): 0.0011, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000100 Step: 54350 Total Loss: 5.0774 Recon Loss: 5.0668 
[12/23 04:52:43 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8227 LR: 0.000100 Step: 54400 Total Loss: 5.6569 Recon Loss: 5.6462 
[12/23 04:53:24 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8229 LR: 0.000100 Step: 54450 Total Loss: 4.5125 Recon Loss: 4.5019 
[12/23 04:54:05 TiTok]: Data (t): 0.0011, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000100 Step: 54500 Total Loss: 4.4229 Recon Loss: 4.4123 
[12/23 04:54:46 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000100 Step: 54550 Total Loss: 5.6670 Recon Loss: 5.6564 
[12/23 04:55:27 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000100 Step: 54600 Total Loss: 4.9984 Recon Loss: 4.9878 
[12/23 04:56:08 TiTok]: Data (t): 0.0010, 39.40/s/gpu Batch (t): 0.8121 LR: 0.000100 Step: 54650 Total Loss: 5.6864 Recon Loss: 5.6757 
[12/23 04:56:49 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000100 Step: 54700 Total Loss: 4.5481 Recon Loss: 4.5374 
[12/23 04:57:30 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000100 Step: 54750 Total Loss: 5.7007 Recon Loss: 5.6901 
[12/23 04:58:11 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8162 LR: 0.000100 Step: 54800 Total Loss: 5.0833 Recon Loss: 5.0727 
[12/23 04:58:52 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000100 Step: 54850 Total Loss: 5.1005 Recon Loss: 5.0898 
[12/23 04:59:32 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000100 Step: 54900 Total Loss: 4.5518 Recon Loss: 4.5411 
[12/23 05:00:13 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000100 Step: 54950 Total Loss: 4.5570 Recon Loss: 4.5463 
[12/23 05:00:54 TiTok]: Data (t): 0.0011, 35.03/s/gpu Batch (t): 0.9135 LR: 0.000100 Step: 55000 Total Loss: 5.0982 Recon Loss: 5.0875 
[12/23 05:00:56 TiTok]: Reconstructing images...
[12/23 05:01:37 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8162 LR: 0.000100 Step: 55050 Total Loss: 5.6329 Recon Loss: 5.6223 
[12/23 05:02:18 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000100 Step: 55100 Total Loss: 5.6821 Recon Loss: 5.6714 
[12/23 05:02:59 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000100 Step: 55150 Total Loss: 4.5065 Recon Loss: 4.4958 
[12/23 05:03:40 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000100 Step: 55200 Total Loss: 5.0787 Recon Loss: 5.0680 
[12/23 05:04:21 TiTok]: Data (t): 0.0011, 38.91/s/gpu Batch (t): 0.8225 LR: 0.000100 Step: 55250 Total Loss: 4.4910 Recon Loss: 4.4803 
[12/23 05:05:02 TiTok]: Data (t): 0.0012, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000100 Step: 55300 Total Loss: 4.4878 Recon Loss: 4.4771 
[12/23 05:05:42 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000100 Step: 55350 Total Loss: 5.0858 Recon Loss: 5.0752 
[12/23 05:06:23 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000100 Step: 55400 Total Loss: 5.6617 Recon Loss: 5.6510 
[12/23 05:07:04 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000100 Step: 55450 Total Loss: 5.6605 Recon Loss: 5.6498 
[12/23 05:07:45 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000100 Step: 55500 Total Loss: 4.4647 Recon Loss: 4.4541 
[12/23 05:08:26 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000100 Step: 55550 Total Loss: 6.2372 Recon Loss: 6.2265 
[12/23 05:09:07 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000100 Step: 55600 Total Loss: 5.0849 Recon Loss: 5.0742 
[12/23 05:09:48 TiTok]: Data (t): 0.0010, 39.44/s/gpu Batch (t): 0.8113 LR: 0.000100 Step: 55650 Total Loss: 5.6520 Recon Loss: 5.6413 
[12/23 05:10:29 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000100 Step: 55700 Total Loss: 5.6819 Recon Loss: 5.6712 
[12/23 05:11:10 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000100 Step: 55750 Total Loss: 4.5126 Recon Loss: 4.5019 
[12/23 05:11:51 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000100 Step: 55800 Total Loss: 4.4992 Recon Loss: 4.4885 
[12/23 05:12:32 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000100 Step: 55850 Total Loss: 4.4925 Recon Loss: 4.4818 
[12/23 05:13:13 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000100 Step: 55900 Total Loss: 5.0864 Recon Loss: 5.0757 
[12/23 05:13:53 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000100 Step: 55950 Total Loss: 4.4513 Recon Loss: 4.4407 
[12/23 05:14:35 TiTok]: Data (t): 0.0010, 34.22/s/gpu Batch (t): 0.9351 LR: 0.000100 Step: 56000 Total Loss: 5.1354 Recon Loss: 5.1248 
[12/23 05:15:15 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000100 Step: 56050 Total Loss: 5.0646 Recon Loss: 5.0540 
[12/23 05:15:56 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8189 LR: 0.000100 Step: 56100 Total Loss: 5.6810 Recon Loss: 5.6703 
[12/23 05:16:37 TiTok]: Data (t): 0.0010, 39.57/s/gpu Batch (t): 0.8086 LR: 0.000100 Step: 56150 Total Loss: 6.2445 Recon Loss: 6.2338 
[12/23 05:17:18 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000100 Step: 56200 Total Loss: 5.0773 Recon Loss: 5.0667 
[12/23 05:17:59 TiTok]: Data (t): 0.0010, 39.51/s/gpu Batch (t): 0.8100 LR: 0.000100 Step: 56250 Total Loss: 5.7323 Recon Loss: 5.7216 
[12/23 05:18:40 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000100 Step: 56300 Total Loss: 5.0942 Recon Loss: 5.0835 
[12/23 05:19:21 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000100 Step: 56350 Total Loss: 5.1384 Recon Loss: 5.1278 
[12/23 05:20:02 TiTok]: Data (t): 0.0017, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000100 Step: 56400 Total Loss: 5.0873 Recon Loss: 5.0767 
[12/23 05:20:43 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8200 LR: 0.000100 Step: 56450 Total Loss: 4.4507 Recon Loss: 4.4400 
[12/23 05:21:24 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000100 Step: 56500 Total Loss: 4.4916 Recon Loss: 4.4809 
[12/23 05:22:05 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000100 Step: 56550 Total Loss: 6.2680 Recon Loss: 6.2573 
[12/23 05:22:45 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000100 Step: 56600 Total Loss: 4.4328 Recon Loss: 4.4222 
[12/23 05:23:26 TiTok]: Data (t): 0.0011, 39.45/s/gpu Batch (t): 0.8112 LR: 0.000100 Step: 56650 Total Loss: 5.0885 Recon Loss: 5.0778 
[12/23 05:24:07 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000100 Step: 56700 Total Loss: 5.0185 Recon Loss: 5.0078 
[12/23 05:24:48 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8208 LR: 0.000100 Step: 56750 Total Loss: 4.5397 Recon Loss: 4.5291 
[12/23 05:25:29 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000100 Step: 56800 Total Loss: 4.5420 Recon Loss: 4.5313 
[12/23 05:26:10 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000100 Step: 56850 Total Loss: 5.0025 Recon Loss: 4.9919 
[12/23 05:26:51 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000100 Step: 56900 Total Loss: 4.5128 Recon Loss: 4.5021 
[12/23 05:27:32 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8181 LR: 0.000100 Step: 56950 Total Loss: 5.6941 Recon Loss: 5.6834 
[12/23 05:28:13 TiTok]: Data (t): 0.0011, 34.78/s/gpu Batch (t): 0.9200 LR: 0.000100 Step: 57000 Total Loss: 5.6460 Recon Loss: 5.6353 
[12/23 05:28:54 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8154 LR: 0.000099 Step: 57050 Total Loss: 5.0642 Recon Loss: 5.0535 
[12/23 05:29:35 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000099 Step: 57100 Total Loss: 4.9994 Recon Loss: 4.9887 
[12/23 05:30:15 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8154 LR: 0.000099 Step: 57150 Total Loss: 5.6772 Recon Loss: 5.6665 
[12/23 05:30:56 TiTok]: Data (t): 0.0011, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000099 Step: 57200 Total Loss: 5.6474 Recon Loss: 5.6368 
[12/23 05:31:37 TiTok]: Data (t): 0.0011, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000099 Step: 57250 Total Loss: 4.5437 Recon Loss: 4.5330 
[12/23 05:32:18 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000099 Step: 57300 Total Loss: 4.4704 Recon Loss: 4.4597 
[12/23 05:32:59 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000099 Step: 57350 Total Loss: 4.4356 Recon Loss: 4.4249 
[12/23 05:33:40 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8125 LR: 0.000099 Step: 57400 Total Loss: 5.6786 Recon Loss: 5.6679 
[12/23 05:34:21 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8127 LR: 0.000099 Step: 57450 Total Loss: 6.2247 Recon Loss: 6.2141 
[12/23 05:35:02 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000099 Step: 57500 Total Loss: 5.6965 Recon Loss: 5.6858 
[12/23 05:35:43 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000099 Step: 57550 Total Loss: 5.5996 Recon Loss: 5.5889 
[12/23 05:36:24 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000099 Step: 57600 Total Loss: 5.0398 Recon Loss: 5.0291 
[12/23 05:37:04 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000099 Step: 57650 Total Loss: 4.4757 Recon Loss: 4.4650 
[12/23 05:37:45 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000099 Step: 57700 Total Loss: 5.1470 Recon Loss: 5.1364 
[12/23 05:38:26 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000099 Step: 57750 Total Loss: 5.0628 Recon Loss: 5.0522 
[12/23 05:39:07 TiTok]: Data (t): 0.0011, 39.42/s/gpu Batch (t): 0.8118 LR: 0.000099 Step: 57800 Total Loss: 5.6940 Recon Loss: 5.6832 
[12/23 05:39:48 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000099 Step: 57850 Total Loss: 5.0546 Recon Loss: 5.0439 
[12/23 05:40:29 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8225 LR: 0.000099 Step: 57900 Total Loss: 5.0593 Recon Loss: 5.0487 
[12/23 05:41:10 TiTok]: Data (t): 0.0019, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000099 Step: 57950 Total Loss: 5.6440 Recon Loss: 5.6334 
[12/23 05:41:51 TiTok]: Data (t): 0.0010, 35.09/s/gpu Batch (t): 0.9120 LR: 0.000099 Step: 58000 Total Loss: 5.0976 Recon Loss: 5.0868 
[12/23 05:42:32 TiTok]: Data (t): 0.0010, 38.79/s/gpu Batch (t): 0.8249 LR: 0.000099 Step: 58050 Total Loss: 4.4098 Recon Loss: 4.3992 
[12/23 05:43:13 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8162 LR: 0.000099 Step: 58100 Total Loss: 5.6761 Recon Loss: 5.6655 
[12/23 05:43:54 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000099 Step: 58150 Total Loss: 4.4247 Recon Loss: 4.4141 
[12/23 05:44:34 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000099 Step: 58200 Total Loss: 5.7068 Recon Loss: 5.6960 
[12/23 05:45:15 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000099 Step: 58250 Total Loss: 4.5204 Recon Loss: 4.5098 
[12/23 05:45:56 TiTok]: Data (t): 0.0011, 38.95/s/gpu Batch (t): 0.8217 LR: 0.000099 Step: 58300 Total Loss: 4.4459 Recon Loss: 4.4352 
[12/23 05:46:37 TiTok]: Data (t): 0.0011, 38.78/s/gpu Batch (t): 0.8251 LR: 0.000099 Step: 58350 Total Loss: 5.0179 Recon Loss: 5.0072 
[12/23 05:47:18 TiTok]: Data (t): 0.0010, 39.41/s/gpu Batch (t): 0.8119 LR: 0.000099 Step: 58400 Total Loss: 5.0090 Recon Loss: 4.9983 
[12/23 05:47:59 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8160 LR: 0.000099 Step: 58450 Total Loss: 4.4482 Recon Loss: 4.4375 
[12/23 05:48:40 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000099 Step: 58500 Total Loss: 6.2196 Recon Loss: 6.2089 
[12/23 05:49:21 TiTok]: Data (t): 0.0011, 38.98/s/gpu Batch (t): 0.8208 LR: 0.000099 Step: 58550 Total Loss: 5.0292 Recon Loss: 5.0185 
[12/23 05:50:02 TiTok]: Data (t): 0.0010, 38.74/s/gpu Batch (t): 0.8260 LR: 0.000099 Step: 58600 Total Loss: 4.4411 Recon Loss: 4.4304 
[12/23 05:50:43 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8221 LR: 0.000099 Step: 58650 Total Loss: 4.4617 Recon Loss: 4.4510 
[12/23 05:51:24 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000099 Step: 58700 Total Loss: 4.4677 Recon Loss: 4.4570 
[12/23 05:52:04 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000099 Step: 58750 Total Loss: 5.6358 Recon Loss: 5.6251 
[12/23 05:52:45 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8150 LR: 0.000099 Step: 58800 Total Loss: 4.5344 Recon Loss: 4.5237 
[12/23 05:53:26 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000099 Step: 58850 Total Loss: 4.4882 Recon Loss: 4.4775 
[12/23 05:54:07 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000099 Step: 58900 Total Loss: 6.2309 Recon Loss: 6.2201 
[12/23 05:54:48 TiTok]: Data (t): 0.0010, 39.42/s/gpu Batch (t): 0.8118 LR: 0.000099 Step: 58950 Total Loss: 5.6334 Recon Loss: 5.6227 
[12/23 05:55:29 TiTok]: Data (t): 0.0011, 33.95/s/gpu Batch (t): 0.9425 LR: 0.000099 Step: 59000 Total Loss: 5.0718 Recon Loss: 5.0611 
[12/23 05:56:10 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000099 Step: 59050 Total Loss: 4.4922 Recon Loss: 4.4814 
[12/23 05:56:51 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000099 Step: 59100 Total Loss: 5.0994 Recon Loss: 5.0886 
[12/23 05:57:32 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8209 LR: 0.000099 Step: 59150 Total Loss: 5.0025 Recon Loss: 4.9919 
[12/23 05:58:13 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000099 Step: 59200 Total Loss: 5.0359 Recon Loss: 5.0251 
[12/23 05:58:54 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000099 Step: 59250 Total Loss: 5.6479 Recon Loss: 5.6371 
[12/23 05:59:34 TiTok]: Data (t): 0.0010, 38.76/s/gpu Batch (t): 0.8256 LR: 0.000099 Step: 59300 Total Loss: 6.2091 Recon Loss: 6.1984 
[12/23 06:00:15 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000099 Step: 59350 Total Loss: 5.0440 Recon Loss: 5.0333 
[12/23 06:00:56 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000099 Step: 59400 Total Loss: 4.5170 Recon Loss: 4.5064 
[12/23 06:01:37 TiTok]: Data (t): 0.0010, 38.83/s/gpu Batch (t): 0.8240 LR: 0.000099 Step: 59450 Total Loss: 5.0904 Recon Loss: 5.0798 
[12/23 06:02:18 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000099 Step: 59500 Total Loss: 6.2376 Recon Loss: 6.2270 
[12/23 06:02:59 TiTok]: Data (t): 0.0011, 39.43/s/gpu Batch (t): 0.8115 LR: 0.000099 Step: 59550 Total Loss: 6.2634 Recon Loss: 6.2527 
[12/23 06:03:40 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000099 Step: 59600 Total Loss: 5.0506 Recon Loss: 5.0399 
[12/23 06:04:21 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000099 Step: 59650 Total Loss: 4.4636 Recon Loss: 4.4529 
[12/23 06:05:02 TiTok]: Data (t): 0.0011, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000099 Step: 59700 Total Loss: 4.4194 Recon Loss: 4.4088 
[12/23 06:05:43 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000099 Step: 59750 Total Loss: 5.0614 Recon Loss: 5.0507 
[12/23 06:06:23 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000099 Step: 59800 Total Loss: 5.6418 Recon Loss: 5.6311 
[12/23 06:07:04 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000099 Step: 59850 Total Loss: 5.1552 Recon Loss: 5.1445 
[12/23 06:07:45 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000099 Step: 59900 Total Loss: 4.4622 Recon Loss: 4.4515 
[12/23 06:08:26 TiTok]: Data (t): 0.0011, 38.81/s/gpu Batch (t): 0.8245 LR: 0.000099 Step: 59950 Total Loss: 4.4525 Recon Loss: 4.4417 
[12/23 06:09:07 TiTok]: Data (t): 0.0010, 35.22/s/gpu Batch (t): 0.9084 LR: 0.000099 Step: 60000 Total Loss: 6.2636 Recon Loss: 6.2529 
[12/23 06:09:08 TiTok]: Reconstructing images...
[12/23 06:09:50 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000099 Step: 60050 Total Loss: 5.0637 Recon Loss: 5.0530 
[12/23 06:10:30 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000099 Step: 60100 Total Loss: 5.0264 Recon Loss: 5.0157 
Epoch 6/99 started.
[12/23 06:11:12 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8162 LR: 0.000099 Step: 60150 Total Loss: 5.0680 Recon Loss: 5.0574 
[12/23 06:11:53 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000099 Step: 60200 Total Loss: 5.6099 Recon Loss: 5.5992 
[12/23 06:12:34 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000099 Step: 60250 Total Loss: 5.6073 Recon Loss: 5.5966 
[12/23 06:13:15 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000099 Step: 60300 Total Loss: 5.0782 Recon Loss: 5.0676 
[12/23 06:13:56 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000099 Step: 60350 Total Loss: 4.5018 Recon Loss: 4.4911 
[12/23 06:14:37 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8162 LR: 0.000099 Step: 60400 Total Loss: 5.0971 Recon Loss: 5.0865 
[12/23 06:15:18 TiTok]: Data (t): 0.0011, 39.44/s/gpu Batch (t): 0.8114 LR: 0.000099 Step: 60450 Total Loss: 5.6296 Recon Loss: 5.6190 
[12/23 06:15:58 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000099 Step: 60500 Total Loss: 6.2175 Recon Loss: 6.2068 
[12/23 06:16:39 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000099 Step: 60550 Total Loss: 4.9904 Recon Loss: 4.9797 
[12/23 06:17:20 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8228 LR: 0.000099 Step: 60600 Total Loss: 5.6608 Recon Loss: 5.6501 
[12/23 06:18:01 TiTok]: Data (t): 0.0010, 38.49/s/gpu Batch (t): 0.8313 LR: 0.000099 Step: 60650 Total Loss: 6.2124 Recon Loss: 6.2017 
[12/23 06:18:42 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000099 Step: 60700 Total Loss: 5.0318 Recon Loss: 5.0212 
[12/23 06:19:23 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8225 LR: 0.000099 Step: 60750 Total Loss: 5.0585 Recon Loss: 5.0477 
[12/23 06:20:04 TiTok]: Data (t): 0.0011, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000099 Step: 60800 Total Loss: 4.4301 Recon Loss: 4.4195 
[12/23 06:20:45 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8162 LR: 0.000099 Step: 60850 Total Loss: 4.3959 Recon Loss: 4.3853 
[12/23 06:21:26 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000099 Step: 60900 Total Loss: 4.5263 Recon Loss: 4.5157 
[12/23 06:22:07 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000099 Step: 60950 Total Loss: 5.0550 Recon Loss: 5.0444 
[12/23 06:22:48 TiTok]: Data (t): 0.0010, 34.59/s/gpu Batch (t): 0.9251 LR: 0.000099 Step: 61000 Total Loss: 4.4616 Recon Loss: 4.4509 
[12/23 06:23:28 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8229 LR: 0.000099 Step: 61050 Total Loss: 5.6230 Recon Loss: 5.6123 
[12/23 06:24:09 TiTok]: Data (t): 0.0011, 38.67/s/gpu Batch (t): 0.8275 LR: 0.000099 Step: 61100 Total Loss: 4.4599 Recon Loss: 4.4492 
[12/23 06:24:50 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000099 Step: 61150 Total Loss: 4.4327 Recon Loss: 4.4220 
[12/23 06:25:31 TiTok]: Data (t): 0.0010, 39.39/s/gpu Batch (t): 0.8123 LR: 0.000099 Step: 61200 Total Loss: 5.0246 Recon Loss: 5.0140 
[12/23 06:26:12 TiTok]: Data (t): 0.0010, 38.62/s/gpu Batch (t): 0.8286 LR: 0.000099 Step: 61250 Total Loss: 5.6369 Recon Loss: 5.6263 
[12/23 06:26:53 TiTok]: Data (t): 0.0010, 39.59/s/gpu Batch (t): 0.8084 LR: 0.000099 Step: 61300 Total Loss: 5.6319 Recon Loss: 5.6212 
[12/23 06:27:34 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000099 Step: 61350 Total Loss: 6.2030 Recon Loss: 6.1922 
[12/23 06:28:15 TiTok]: Data (t): 0.0011, 38.80/s/gpu Batch (t): 0.8248 LR: 0.000099 Step: 61400 Total Loss: 5.0305 Recon Loss: 5.0199 
[12/23 06:28:56 TiTok]: Data (t): 0.0010, 39.46/s/gpu Batch (t): 0.8110 LR: 0.000099 Step: 61450 Total Loss: 5.0659 Recon Loss: 5.0552 
[12/23 06:29:37 TiTok]: Data (t): 0.0009, 39.10/s/gpu Batch (t): 0.8183 LR: 0.000099 Step: 61500 Total Loss: 5.0617 Recon Loss: 5.0511 
[12/23 06:30:17 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000099 Step: 61550 Total Loss: 4.4074 Recon Loss: 4.3967 
[12/23 06:30:58 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000099 Step: 61600 Total Loss: 5.0436 Recon Loss: 5.0330 
[12/23 06:31:39 TiTok]: Data (t): 0.0010, 39.48/s/gpu Batch (t): 0.8106 LR: 0.000099 Step: 61650 Total Loss: 5.0008 Recon Loss: 4.9901 
[12/23 06:32:20 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000099 Step: 61700 Total Loss: 5.0718 Recon Loss: 5.0611 
[12/23 06:33:01 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000099 Step: 61750 Total Loss: 5.6531 Recon Loss: 5.6424 
[12/23 06:33:42 TiTok]: Data (t): 0.0017, 39.67/s/gpu Batch (t): 0.8067 LR: 0.000099 Step: 61800 Total Loss: 5.6039 Recon Loss: 5.5932 
[12/23 06:34:23 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000099 Step: 61850 Total Loss: 5.0668 Recon Loss: 5.0562 
[12/23 06:35:04 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000099 Step: 61900 Total Loss: 5.0493 Recon Loss: 5.0386 
[12/23 06:35:45 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000099 Step: 61950 Total Loss: 5.6270 Recon Loss: 5.6162 
[12/23 06:36:26 TiTok]: Data (t): 0.0011, 35.06/s/gpu Batch (t): 0.9126 LR: 0.000099 Step: 62000 Total Loss: 5.0402 Recon Loss: 5.0296 
[12/23 06:37:07 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8217 LR: 0.000099 Step: 62050 Total Loss: 5.6590 Recon Loss: 5.6483 
[12/23 06:37:48 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000099 Step: 62100 Total Loss: 5.6884 Recon Loss: 5.6777 
[12/23 06:38:28 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000099 Step: 62150 Total Loss: 5.1055 Recon Loss: 5.0948 
[12/23 06:39:09 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000099 Step: 62200 Total Loss: 5.0274 Recon Loss: 5.0167 
[12/23 06:39:50 TiTok]: Data (t): 0.0010, 38.84/s/gpu Batch (t): 0.8240 LR: 0.000099 Step: 62250 Total Loss: 4.4660 Recon Loss: 4.4553 
[12/23 06:40:31 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000099 Step: 62300 Total Loss: 4.4108 Recon Loss: 4.4001 
[12/23 06:41:12 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8154 LR: 0.000099 Step: 62350 Total Loss: 4.9559 Recon Loss: 4.9453 
[12/23 06:41:53 TiTok]: Data (t): 0.0011, 38.88/s/gpu Batch (t): 0.8231 LR: 0.000099 Step: 62400 Total Loss: 5.6011 Recon Loss: 5.5904 
[12/23 06:42:34 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000099 Step: 62450 Total Loss: 5.0999 Recon Loss: 5.0892 
[12/23 06:43:15 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000099 Step: 62500 Total Loss: 4.4762 Recon Loss: 4.4655 
[12/23 06:43:56 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8217 LR: 0.000099 Step: 62550 Total Loss: 4.3877 Recon Loss: 4.3770 
[12/23 06:44:37 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000099 Step: 62600 Total Loss: 4.4347 Recon Loss: 4.4241 
[12/23 06:45:18 TiTok]: Data (t): 0.0010, 37.95/s/gpu Batch (t): 0.8432 LR: 0.000099 Step: 62650 Total Loss: 5.6010 Recon Loss: 5.5902 
[12/23 06:45:59 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000099 Step: 62700 Total Loss: 5.0087 Recon Loss: 4.9980 
[12/23 06:46:40 TiTok]: Data (t): 0.0010, 39.43/s/gpu Batch (t): 0.8116 LR: 0.000099 Step: 62750 Total Loss: 5.6109 Recon Loss: 5.6003 
[12/23 06:47:20 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8154 LR: 0.000099 Step: 62800 Total Loss: 4.4660 Recon Loss: 4.4554 
[12/23 06:48:01 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000099 Step: 62850 Total Loss: 5.0477 Recon Loss: 5.0370 
[12/23 06:48:42 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000099 Step: 62900 Total Loss: 4.4365 Recon Loss: 4.4259 
[12/23 06:49:23 TiTok]: Data (t): 0.0009, 39.00/s/gpu Batch (t): 0.8204 LR: 0.000099 Step: 62950 Total Loss: 4.4993 Recon Loss: 4.4886 
[12/23 06:50:04 TiTok]: Data (t): 0.0010, 34.54/s/gpu Batch (t): 0.9264 LR: 0.000099 Step: 63000 Total Loss: 5.0350 Recon Loss: 5.0244 
[12/23 06:50:45 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8202 LR: 0.000099 Step: 63050 Total Loss: 5.6176 Recon Loss: 5.6069 
[12/23 06:51:26 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000099 Step: 63100 Total Loss: 5.0083 Recon Loss: 4.9976 
[12/23 06:52:07 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000099 Step: 63150 Total Loss: 5.0939 Recon Loss: 5.0832 
[12/23 06:52:48 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8231 LR: 0.000099 Step: 63200 Total Loss: 5.0072 Recon Loss: 4.9965 
[12/23 06:53:29 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000099 Step: 63250 Total Loss: 5.6512 Recon Loss: 5.6406 
[12/23 06:54:10 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8202 LR: 0.000099 Step: 63300 Total Loss: 5.0434 Recon Loss: 5.0327 
[12/23 06:54:51 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8200 LR: 0.000099 Step: 63350 Total Loss: 4.4171 Recon Loss: 4.4065 
[12/23 06:55:31 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8158 LR: 0.000099 Step: 63400 Total Loss: 4.4370 Recon Loss: 4.4263 
[12/23 06:56:12 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8223 LR: 0.000099 Step: 63450 Total Loss: 5.0637 Recon Loss: 5.0530 
[12/23 06:56:53 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000099 Step: 63500 Total Loss: 5.6311 Recon Loss: 5.6205 
[12/23 06:57:34 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8217 LR: 0.000099 Step: 63550 Total Loss: 5.0044 Recon Loss: 4.9938 
[12/23 06:58:15 TiTok]: Data (t): 0.0010, 38.82/s/gpu Batch (t): 0.8242 LR: 0.000099 Step: 63600 Total Loss: 5.0792 Recon Loss: 5.0686 
[12/23 06:58:56 TiTok]: Data (t): 0.0011, 36.82/s/gpu Batch (t): 0.8691 LR: 0.000099 Step: 63650 Total Loss: 4.4545 Recon Loss: 4.4438 
[12/23 06:59:37 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000099 Step: 63700 Total Loss: 5.0928 Recon Loss: 5.0821 
[12/23 07:00:18 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000099 Step: 63750 Total Loss: 5.6257 Recon Loss: 5.6151 
[12/23 07:00:59 TiTok]: Data (t): 0.0011, 39.41/s/gpu Batch (t): 0.8121 LR: 0.000099 Step: 63800 Total Loss: 5.5752 Recon Loss: 5.5645 
[12/23 07:01:39 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000099 Step: 63850 Total Loss: 5.6170 Recon Loss: 5.6064 
[12/23 07:02:20 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8160 LR: 0.000099 Step: 63900 Total Loss: 4.4325 Recon Loss: 4.4218 
[12/23 07:03:01 TiTok]: Data (t): 0.0010, 38.85/s/gpu Batch (t): 0.8236 LR: 0.000099 Step: 63950 Total Loss: 4.4228 Recon Loss: 4.4122 
[12/23 07:03:42 TiTok]: Data (t): 0.0010, 35.17/s/gpu Batch (t): 0.9100 LR: 0.000099 Step: 64000 Total Loss: 4.4466 Recon Loss: 4.4359 
[12/23 07:04:23 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000099 Step: 64050 Total Loss: 5.5913 Recon Loss: 5.5806 
[12/23 07:05:04 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000099 Step: 64100 Total Loss: 5.0347 Recon Loss: 5.0240 
[12/23 07:05:45 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8133 LR: 0.000099 Step: 64150 Total Loss: 5.6584 Recon Loss: 5.6477 
[12/23 07:06:26 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000099 Step: 64200 Total Loss: 5.0484 Recon Loss: 5.0378 
[12/23 07:07:07 TiTok]: Data (t): 0.0011, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000099 Step: 64250 Total Loss: 5.0368 Recon Loss: 5.0262 
[12/23 07:07:47 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000099 Step: 64300 Total Loss: 5.0127 Recon Loss: 5.0020 
[12/23 07:08:28 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000099 Step: 64350 Total Loss: 6.2566 Recon Loss: 6.2459 
[12/23 07:09:09 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000099 Step: 64400 Total Loss: 5.6275 Recon Loss: 5.6169 
[12/23 07:09:50 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8133 LR: 0.000099 Step: 64450 Total Loss: 4.9788 Recon Loss: 4.9682 
[12/23 07:10:31 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8183 LR: 0.000099 Step: 64500 Total Loss: 5.0242 Recon Loss: 5.0136 
[12/23 07:11:12 TiTok]: Data (t): 0.0011, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000099 Step: 64550 Total Loss: 5.0602 Recon Loss: 5.0496 
[12/23 07:11:53 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000099 Step: 64600 Total Loss: 5.0486 Recon Loss: 5.0378 
[12/23 07:12:34 TiTok]: Data (t): 0.0011, 38.53/s/gpu Batch (t): 0.8305 LR: 0.000099 Step: 64650 Total Loss: 4.4189 Recon Loss: 4.4083 
[12/23 07:13:15 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8135 LR: 0.000099 Step: 64700 Total Loss: 5.0001 Recon Loss: 4.9894 
[12/23 07:13:55 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000099 Step: 64750 Total Loss: 4.9858 Recon Loss: 4.9751 
[12/23 07:14:36 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000099 Step: 64800 Total Loss: 4.3633 Recon Loss: 4.3526 
[12/23 07:15:17 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000099 Step: 64850 Total Loss: 5.6222 Recon Loss: 5.6115 
[12/23 07:15:58 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000099 Step: 64900 Total Loss: 5.0457 Recon Loss: 5.0350 
[12/23 07:16:39 TiTok]: Data (t): 0.0011, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000099 Step: 64950 Total Loss: 5.0450 Recon Loss: 5.0344 
[12/23 07:17:20 TiTok]: Data (t): 0.0011, 35.44/s/gpu Batch (t): 0.9029 LR: 0.000099 Step: 65000 Total Loss: 5.0584 Recon Loss: 5.0477 
[12/23 07:17:21 TiTok]: Reconstructing images...
[12/23 07:18:02 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000099 Step: 65050 Total Loss: 4.9642 Recon Loss: 4.9535 
[12/23 07:18:43 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8204 LR: 0.000099 Step: 65100 Total Loss: 4.4402 Recon Loss: 4.4295 
[12/23 07:19:24 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000099 Step: 65150 Total Loss: 4.2959 Recon Loss: 4.2853 
[12/23 07:20:05 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000099 Step: 65200 Total Loss: 4.4478 Recon Loss: 4.4371 
[12/23 07:20:46 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000099 Step: 65250 Total Loss: 4.9225 Recon Loss: 4.9118 
[12/23 07:21:27 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000099 Step: 65300 Total Loss: 5.6854 Recon Loss: 5.6747 
[12/23 07:22:08 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8183 LR: 0.000099 Step: 65350 Total Loss: 4.4675 Recon Loss: 4.4568 
[12/23 07:22:49 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000099 Step: 65400 Total Loss: 4.4190 Recon Loss: 4.4083 
[12/23 07:23:30 TiTok]: Data (t): 0.0033, 39.44/s/gpu Batch (t): 0.8114 LR: 0.000099 Step: 65450 Total Loss: 5.0352 Recon Loss: 5.0245 
[12/23 07:24:10 TiTok]: Data (t): 0.0021, 39.65/s/gpu Batch (t): 0.8070 LR: 0.000099 Step: 65500 Total Loss: 5.0028 Recon Loss: 4.9922 
[12/23 07:24:51 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000099 Step: 65550 Total Loss: 4.9958 Recon Loss: 4.9852 
[12/23 07:25:32 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000099 Step: 65600 Total Loss: 5.0620 Recon Loss: 5.0513 
[12/23 07:26:13 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000099 Step: 65650 Total Loss: 4.4166 Recon Loss: 4.4059 
[12/23 07:26:54 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000099 Step: 65700 Total Loss: 4.4014 Recon Loss: 4.3907 
[12/23 07:27:35 TiTok]: Data (t): 0.0009, 38.77/s/gpu Batch (t): 0.8254 LR: 0.000099 Step: 65750 Total Loss: 4.3550 Recon Loss: 4.3443 
[12/23 07:28:16 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8228 LR: 0.000099 Step: 65800 Total Loss: 4.9875 Recon Loss: 4.9768 
[12/23 07:28:57 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8175 LR: 0.000099 Step: 65850 Total Loss: 5.0319 Recon Loss: 5.0212 
[12/23 07:29:38 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000099 Step: 65900 Total Loss: 4.3926 Recon Loss: 4.3820 
[12/23 07:30:19 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000099 Step: 65950 Total Loss: 4.9925 Recon Loss: 4.9818 
[12/23 07:31:00 TiTok]: Data (t): 0.0010, 33.57/s/gpu Batch (t): 0.9533 LR: 0.000099 Step: 66000 Total Loss: 5.0489 Recon Loss: 5.0382 
[12/23 07:31:40 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000099 Step: 66050 Total Loss: 4.3504 Recon Loss: 4.3398 
[12/23 07:32:21 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000099 Step: 66100 Total Loss: 5.0320 Recon Loss: 5.0212 
[12/23 07:33:02 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000099 Step: 66150 Total Loss: 5.0686 Recon Loss: 5.0580 
[12/23 07:33:43 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000099 Step: 66200 Total Loss: 4.4042 Recon Loss: 4.3935 
[12/23 07:34:24 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000099 Step: 66250 Total Loss: 5.0000 Recon Loss: 4.9893 
[12/23 07:35:05 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000099 Step: 66300 Total Loss: 5.6422 Recon Loss: 5.6315 
[12/23 07:35:46 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000099 Step: 66350 Total Loss: 4.3802 Recon Loss: 4.3695 
[12/23 07:36:27 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000099 Step: 66400 Total Loss: 5.0914 Recon Loss: 5.0807 
[12/23 07:37:07 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000099 Step: 66450 Total Loss: 5.0154 Recon Loss: 5.0047 
[12/23 07:37:48 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000099 Step: 66500 Total Loss: 4.3541 Recon Loss: 4.3434 
[12/23 07:38:29 TiTok]: Data (t): 0.0010, 39.40/s/gpu Batch (t): 0.8123 LR: 0.000099 Step: 66550 Total Loss: 4.3478 Recon Loss: 4.3371 
[12/23 07:39:10 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000099 Step: 66600 Total Loss: 4.4157 Recon Loss: 4.4051 
[12/23 07:39:51 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000099 Step: 66650 Total Loss: 4.9816 Recon Loss: 4.9709 
[12/23 07:40:32 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000099 Step: 66700 Total Loss: 5.6243 Recon Loss: 5.6136 
[12/23 07:41:13 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000099 Step: 66750 Total Loss: 4.4801 Recon Loss: 4.4694 
[12/23 07:41:54 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000099 Step: 66800 Total Loss: 5.0684 Recon Loss: 5.0577 
[12/23 07:42:35 TiTok]: Data (t): 0.0010, 38.62/s/gpu Batch (t): 0.8286 LR: 0.000099 Step: 66850 Total Loss: 4.4269 Recon Loss: 4.4161 
[12/23 07:43:15 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000099 Step: 66900 Total Loss: 4.4409 Recon Loss: 4.4302 
[12/23 07:43:56 TiTok]: Data (t): 0.0017, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000099 Step: 66950 Total Loss: 5.6057 Recon Loss: 5.5951 
[12/23 07:44:37 TiTok]: Data (t): 0.0010, 35.22/s/gpu Batch (t): 0.9085 LR: 0.000099 Step: 67000 Total Loss: 5.0221 Recon Loss: 5.0113 
[12/23 07:45:18 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8217 LR: 0.000099 Step: 67050 Total Loss: 5.0663 Recon Loss: 5.0556 
[12/23 07:45:59 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000099 Step: 67100 Total Loss: 4.9943 Recon Loss: 4.9835 
[12/23 07:46:40 TiTok]: Data (t): 0.0013, 39.56/s/gpu Batch (t): 0.8089 LR: 0.000099 Step: 67150 Total Loss: 5.5374 Recon Loss: 5.5268 
[12/23 07:47:21 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000099 Step: 67200 Total Loss: 5.0479 Recon Loss: 5.0373 
[12/23 07:48:02 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000099 Step: 67250 Total Loss: 4.4172 Recon Loss: 4.4065 
[12/23 07:48:43 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000099 Step: 67300 Total Loss: 5.0452 Recon Loss: 5.0345 
[12/23 07:49:24 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000099 Step: 67350 Total Loss: 4.4305 Recon Loss: 4.4197 
[12/23 07:50:04 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000099 Step: 67400 Total Loss: 5.6239 Recon Loss: 5.6132 
[12/23 07:50:45 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000099 Step: 67450 Total Loss: 4.4324 Recon Loss: 4.4217 
[12/23 07:51:26 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8154 LR: 0.000099 Step: 67500 Total Loss: 5.0852 Recon Loss: 5.0745 
[12/23 07:52:07 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000099 Step: 67550 Total Loss: 5.0319 Recon Loss: 5.0211 
[12/23 07:52:48 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000099 Step: 67600 Total Loss: 6.1903 Recon Loss: 6.1795 
[12/23 07:53:29 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8152 LR: 0.000099 Step: 67650 Total Loss: 5.0524 Recon Loss: 5.0418 
[12/23 07:54:10 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000099 Step: 67700 Total Loss: 5.6452 Recon Loss: 5.6346 
[12/23 07:54:51 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8129 LR: 0.000099 Step: 67750 Total Loss: 5.6401 Recon Loss: 5.6294 
[12/23 07:55:32 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000099 Step: 67800 Total Loss: 4.9917 Recon Loss: 4.9810 
[12/23 07:56:12 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000099 Step: 67850 Total Loss: 5.0055 Recon Loss: 4.9948 
[12/23 07:56:53 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000099 Step: 67900 Total Loss: 5.6096 Recon Loss: 5.5989 
[12/23 07:57:34 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000099 Step: 67950 Total Loss: 4.2913 Recon Loss: 4.2806 
[12/23 07:58:15 TiTok]: Data (t): 0.0010, 35.21/s/gpu Batch (t): 0.9089 LR: 0.000099 Step: 68000 Total Loss: 5.0499 Recon Loss: 5.0392 
[12/23 07:58:56 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000099 Step: 68050 Total Loss: 4.4276 Recon Loss: 4.4169 
[12/23 07:59:37 TiTok]: Data (t): 0.0018, 39.89/s/gpu Batch (t): 0.8022 LR: 0.000099 Step: 68100 Total Loss: 5.5764 Recon Loss: 5.5657 
[12/23 08:00:18 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000099 Step: 68150 Total Loss: 4.3923 Recon Loss: 4.3815 
[12/23 08:00:59 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8158 LR: 0.000099 Step: 68200 Total Loss: 5.5992 Recon Loss: 5.5885 
[12/23 08:01:39 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000099 Step: 68250 Total Loss: 5.0166 Recon Loss: 5.0059 
[12/23 08:02:20 TiTok]: Data (t): 0.0010, 38.85/s/gpu Batch (t): 0.8237 LR: 0.000099 Step: 68300 Total Loss: 5.0458 Recon Loss: 5.0352 
[12/23 08:03:01 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8209 LR: 0.000099 Step: 68350 Total Loss: 5.6277 Recon Loss: 5.6170 
[12/23 08:03:42 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000099 Step: 68400 Total Loss: 5.6101 Recon Loss: 5.5994 
[12/23 08:04:23 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000099 Step: 68450 Total Loss: 5.0253 Recon Loss: 5.0146 
[12/23 08:05:04 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000099 Step: 68500 Total Loss: 4.3708 Recon Loss: 4.3601 
[12/23 08:05:45 TiTok]: Data (t): 0.0011, 38.82/s/gpu Batch (t): 0.8242 LR: 0.000099 Step: 68550 Total Loss: 5.5838 Recon Loss: 5.5731 
[12/23 08:06:26 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8154 LR: 0.000099 Step: 68600 Total Loss: 4.3418 Recon Loss: 4.3311 
[12/23 08:07:06 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000099 Step: 68650 Total Loss: 5.5823 Recon Loss: 5.5716 
[12/23 08:07:47 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000099 Step: 68700 Total Loss: 5.6424 Recon Loss: 5.6317 
[12/23 08:08:28 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000099 Step: 68750 Total Loss: 4.3594 Recon Loss: 4.3487 
[12/23 08:09:09 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000099 Step: 68800 Total Loss: 5.6425 Recon Loss: 5.6318 
[12/23 08:09:50 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000099 Step: 68850 Total Loss: 5.0429 Recon Loss: 5.0322 
[12/23 08:10:31 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000099 Step: 68900 Total Loss: 4.9619 Recon Loss: 4.9512 
[12/23 08:11:12 TiTok]: Data (t): 0.0010, 38.84/s/gpu Batch (t): 0.8239 LR: 0.000099 Step: 68950 Total Loss: 5.0191 Recon Loss: 5.0085 
[12/23 08:11:53 TiTok]: Data (t): 0.0010, 34.86/s/gpu Batch (t): 0.9180 LR: 0.000099 Step: 69000 Total Loss: 4.9918 Recon Loss: 4.9810 
[12/23 08:12:34 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000099 Step: 69050 Total Loss: 4.3053 Recon Loss: 4.2946 
[12/23 08:13:15 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000099 Step: 69100 Total Loss: 5.6226 Recon Loss: 5.6119 
[12/23 08:13:55 TiTok]: Data (t): 0.0011, 38.89/s/gpu Batch (t): 0.8228 LR: 0.000099 Step: 69150 Total Loss: 4.4074 Recon Loss: 4.3967 
[12/23 08:14:36 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000099 Step: 69200 Total Loss: 4.9925 Recon Loss: 4.9818 
[12/23 08:15:17 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000099 Step: 69250 Total Loss: 5.0383 Recon Loss: 5.0276 
[12/23 08:15:58 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8235 LR: 0.000099 Step: 69300 Total Loss: 4.9913 Recon Loss: 4.9805 
[12/23 08:16:39 TiTok]: Data (t): 0.0010, 38.83/s/gpu Batch (t): 0.8241 LR: 0.000099 Step: 69350 Total Loss: 4.9909 Recon Loss: 4.9803 
[12/23 08:17:20 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000099 Step: 69400 Total Loss: 5.0040 Recon Loss: 4.9933 
[12/23 08:18:01 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8154 LR: 0.000099 Step: 69450 Total Loss: 6.1810 Recon Loss: 6.1703 
[12/23 08:18:42 TiTok]: Data (t): 0.0011, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000099 Step: 69500 Total Loss: 5.0555 Recon Loss: 5.0448 
[12/23 08:19:23 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000099 Step: 69550 Total Loss: 4.9732 Recon Loss: 4.9626 
[12/23 08:20:04 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000099 Step: 69600 Total Loss: 5.6286 Recon Loss: 5.6180 
[12/23 08:20:45 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000099 Step: 69650 Total Loss: 5.6074 Recon Loss: 5.5968 
[12/23 08:21:25 TiTok]: Data (t): 0.0010, 38.87/s/gpu Batch (t): 0.8232 LR: 0.000099 Step: 69700 Total Loss: 5.6010 Recon Loss: 5.5903 
[12/23 08:22:06 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000099 Step: 69750 Total Loss: 5.6000 Recon Loss: 5.5894 
[12/23 08:22:47 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000099 Step: 69800 Total Loss: 4.3813 Recon Loss: 4.3706 
[12/23 08:23:28 TiTok]: Data (t): 0.0011, 39.48/s/gpu Batch (t): 0.8105 LR: 0.000099 Step: 69850 Total Loss: 5.0156 Recon Loss: 5.0049 
[12/23 08:24:09 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8158 LR: 0.000099 Step: 69900 Total Loss: 4.3723 Recon Loss: 4.3617 
[12/23 08:24:50 TiTok]: Data (t): 0.0011, 39.47/s/gpu Batch (t): 0.8107 LR: 0.000099 Step: 69950 Total Loss: 5.5707 Recon Loss: 5.5599 
[12/23 08:25:31 TiTok]: Data (t): 0.0010, 35.49/s/gpu Batch (t): 0.9017 LR: 0.000099 Step: 70000 Total Loss: 5.0053 Recon Loss: 4.9946 
[12/23 08:25:32 TiTok]: Reconstructing images...
[12/23 08:26:13 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000099 Step: 70050 Total Loss: 4.9632 Recon Loss: 4.9525 
[12/23 08:26:54 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8162 LR: 0.000099 Step: 70100 Total Loss: 4.9738 Recon Loss: 4.9630 
Epoch 7/99 started.
[12/23 08:27:36 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8177 LR: 0.000099 Step: 70150 Total Loss: 4.9961 Recon Loss: 4.9855 
[12/23 08:28:17 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8158 LR: 0.000099 Step: 70200 Total Loss: 5.0345 Recon Loss: 5.0238 
[12/23 08:28:58 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000099 Step: 70250 Total Loss: 4.3563 Recon Loss: 4.3457 
[12/23 08:29:38 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000099 Step: 70300 Total Loss: 4.3710 Recon Loss: 4.3603 
[12/23 08:30:19 TiTok]: Data (t): 0.0011, 39.36/s/gpu Batch (t): 0.8131 LR: 0.000099 Step: 70350 Total Loss: 5.6200 Recon Loss: 5.6093 
[12/23 08:31:00 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8219 LR: 0.000099 Step: 70400 Total Loss: 5.6180 Recon Loss: 5.6073 
[12/23 08:31:41 TiTok]: Data (t): 0.0011, 38.82/s/gpu Batch (t): 0.8242 LR: 0.000099 Step: 70450 Total Loss: 5.6238 Recon Loss: 5.6131 
[12/23 08:32:22 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000099 Step: 70500 Total Loss: 5.0220 Recon Loss: 5.0113 
[12/23 08:33:03 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000099 Step: 70550 Total Loss: 5.6578 Recon Loss: 5.6471 
[12/23 08:33:44 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000099 Step: 70600 Total Loss: 4.9998 Recon Loss: 4.9891 
[12/23 08:34:25 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000099 Step: 70650 Total Loss: 4.4238 Recon Loss: 4.4131 
[12/23 08:35:06 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8217 LR: 0.000099 Step: 70700 Total Loss: 5.0270 Recon Loss: 5.0163 
[12/23 08:35:47 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000099 Step: 70750 Total Loss: 5.0049 Recon Loss: 4.9942 
[12/23 08:36:27 TiTok]: Data (t): 0.0010, 38.80/s/gpu Batch (t): 0.8247 LR: 0.000099 Step: 70800 Total Loss: 4.4373 Recon Loss: 4.4266 
[12/23 08:37:08 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000099 Step: 70850 Total Loss: 4.3573 Recon Loss: 4.3465 
[12/23 08:37:49 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000099 Step: 70900 Total Loss: 5.0082 Recon Loss: 4.9975 
[12/23 08:38:30 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000099 Step: 70950 Total Loss: 5.0259 Recon Loss: 5.0152 
[12/23 08:39:11 TiTok]: Data (t): 0.0011, 34.51/s/gpu Batch (t): 0.9272 LR: 0.000099 Step: 71000 Total Loss: 5.5943 Recon Loss: 5.5836 
[12/23 08:39:52 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000099 Step: 71050 Total Loss: 5.0240 Recon Loss: 5.0132 
[12/23 08:40:33 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000099 Step: 71100 Total Loss: 5.6094 Recon Loss: 5.5986 
[12/23 08:41:14 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000099 Step: 71150 Total Loss: 5.0286 Recon Loss: 5.0179 
[12/23 08:41:55 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000099 Step: 71200 Total Loss: 4.4770 Recon Loss: 4.4663 
[12/23 08:42:36 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000099 Step: 71250 Total Loss: 5.0388 Recon Loss: 5.0282 
[12/23 08:43:17 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000099 Step: 71300 Total Loss: 4.3966 Recon Loss: 4.3860 
[12/23 08:43:57 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8236 LR: 0.000099 Step: 71350 Total Loss: 4.3640 Recon Loss: 4.3533 
[12/23 08:44:38 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8227 LR: 0.000099 Step: 71400 Total Loss: 4.9715 Recon Loss: 4.9608 
[12/23 08:45:19 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000099 Step: 71450 Total Loss: 4.3757 Recon Loss: 4.3650 
[12/23 08:46:00 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000099 Step: 71500 Total Loss: 5.0002 Recon Loss: 4.9894 
[12/23 08:46:41 TiTok]: Data (t): 0.0010, 39.41/s/gpu Batch (t): 0.8120 LR: 0.000099 Step: 71550 Total Loss: 5.0288 Recon Loss: 5.0181 
[12/23 08:47:22 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8160 LR: 0.000099 Step: 71600 Total Loss: 5.5263 Recon Loss: 5.5157 
[12/23 08:48:03 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000099 Step: 71650 Total Loss: 5.0115 Recon Loss: 5.0008 
[12/23 08:48:44 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000099 Step: 71700 Total Loss: 5.0101 Recon Loss: 4.9995 
[12/23 08:49:25 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000099 Step: 71750 Total Loss: 5.0323 Recon Loss: 5.0217 
[12/23 08:50:06 TiTok]: Data (t): 0.0016, 39.70/s/gpu Batch (t): 0.8061 LR: 0.000099 Step: 71800 Total Loss: 5.5858 Recon Loss: 5.5752 
[12/23 08:50:46 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000099 Step: 71850 Total Loss: 4.3950 Recon Loss: 4.3844 
[12/23 08:51:27 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000099 Step: 71900 Total Loss: 4.9246 Recon Loss: 4.9139 
[12/23 08:52:08 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000099 Step: 71950 Total Loss: 5.6191 Recon Loss: 5.6083 
[12/23 08:52:49 TiTok]: Data (t): 0.0011, 35.45/s/gpu Batch (t): 0.9026 LR: 0.000099 Step: 72000 Total Loss: 5.0044 Recon Loss: 4.9937 
[12/23 08:53:30 TiTok]: Data (t): 0.0022, 40.07/s/gpu Batch (t): 0.7986 LR: 0.000099 Step: 72050 Total Loss: 5.0077 Recon Loss: 4.9970 
[12/23 08:54:11 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8137 LR: 0.000099 Step: 72100 Total Loss: 4.9345 Recon Loss: 4.9238 
[12/23 08:54:52 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000099 Step: 72150 Total Loss: 4.3370 Recon Loss: 4.3264 
[12/23 08:55:33 TiTok]: Data (t): 0.0010, 39.46/s/gpu Batch (t): 0.8109 LR: 0.000099 Step: 72200 Total Loss: 4.9702 Recon Loss: 4.9595 
[12/23 08:56:14 TiTok]: Data (t): 0.0010, 38.85/s/gpu Batch (t): 0.8237 LR: 0.000099 Step: 72250 Total Loss: 4.9331 Recon Loss: 4.9224 
[12/23 08:56:55 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000099 Step: 72300 Total Loss: 5.0699 Recon Loss: 5.0592 
[12/23 08:57:35 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8200 LR: 0.000099 Step: 72350 Total Loss: 4.9897 Recon Loss: 4.9789 
[12/23 08:58:16 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8206 LR: 0.000099 Step: 72400 Total Loss: 4.4329 Recon Loss: 4.4222 
[12/23 08:58:57 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000099 Step: 72450 Total Loss: 4.9901 Recon Loss: 4.9794 
[12/23 08:59:38 TiTok]: Data (t): 0.0011, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000099 Step: 72500 Total Loss: 4.3757 Recon Loss: 4.3649 
[12/23 09:00:19 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000099 Step: 72550 Total Loss: 5.5818 Recon Loss: 5.5711 
[12/23 09:01:00 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000099 Step: 72600 Total Loss: 4.3805 Recon Loss: 4.3698 
[12/23 09:01:41 TiTok]: Data (t): 0.0010, 38.49/s/gpu Batch (t): 0.8314 LR: 0.000099 Step: 72650 Total Loss: 4.9972 Recon Loss: 4.9865 
[12/23 09:02:22 TiTok]: Data (t): 0.0010, 39.46/s/gpu Batch (t): 0.8110 LR: 0.000099 Step: 72700 Total Loss: 5.5616 Recon Loss: 5.5509 
[12/23 09:03:03 TiTok]: Data (t): 0.0010, 39.40/s/gpu Batch (t): 0.8121 LR: 0.000099 Step: 72750 Total Loss: 4.9856 Recon Loss: 4.9749 
[12/23 09:03:44 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000099 Step: 72800 Total Loss: 4.3686 Recon Loss: 4.3578 
[12/23 09:04:25 TiTok]: Data (t): 0.0009, 38.85/s/gpu Batch (t): 0.8237 LR: 0.000099 Step: 72850 Total Loss: 4.4578 Recon Loss: 4.4470 
[12/23 09:05:05 TiTok]: Data (t): 0.0011, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000099 Step: 72900 Total Loss: 4.9742 Recon Loss: 4.9635 
[12/23 09:05:46 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000099 Step: 72950 Total Loss: 4.9619 Recon Loss: 4.9512 
[12/23 09:06:27 TiTok]: Data (t): 0.0010, 35.16/s/gpu Batch (t): 0.9101 LR: 0.000099 Step: 73000 Total Loss: 4.9750 Recon Loss: 4.9643 
[12/23 09:07:08 TiTok]: Data (t): 0.0011, 38.85/s/gpu Batch (t): 0.8236 LR: 0.000099 Step: 73050 Total Loss: 4.3438 Recon Loss: 4.3330 
[12/23 09:07:49 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000099 Step: 73100 Total Loss: 4.4259 Recon Loss: 4.4151 
[12/23 09:08:30 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000099 Step: 73150 Total Loss: 4.9764 Recon Loss: 4.9657 
[12/23 09:09:11 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000099 Step: 73200 Total Loss: 5.6207 Recon Loss: 5.6099 
[12/23 09:09:52 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000099 Step: 73250 Total Loss: 6.2097 Recon Loss: 6.1989 
[12/23 09:10:33 TiTok]: Data (t): 0.0011, 39.35/s/gpu Batch (t): 0.8133 LR: 0.000099 Step: 73300 Total Loss: 5.0298 Recon Loss: 5.0191 
[12/23 09:11:13 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8235 LR: 0.000099 Step: 73350 Total Loss: 4.3824 Recon Loss: 4.3717 
[12/23 09:11:54 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000099 Step: 73400 Total Loss: 4.9510 Recon Loss: 4.9402 
[12/23 09:12:35 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000099 Step: 73450 Total Loss: 4.9855 Recon Loss: 4.9747 
[12/23 09:13:16 TiTok]: Data (t): 0.0011, 38.65/s/gpu Batch (t): 0.8279 LR: 0.000099 Step: 73500 Total Loss: 4.3401 Recon Loss: 4.3294 
[12/23 09:13:57 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000099 Step: 73550 Total Loss: 4.3813 Recon Loss: 4.3707 
[12/23 09:14:38 TiTok]: Data (t): 0.0010, 39.40/s/gpu Batch (t): 0.8122 LR: 0.000099 Step: 73600 Total Loss: 4.9618 Recon Loss: 4.9510 
[12/23 09:15:19 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000099 Step: 73650 Total Loss: 5.0391 Recon Loss: 5.0283 
[12/23 09:16:00 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000099 Step: 73700 Total Loss: 4.9711 Recon Loss: 4.9604 
[12/23 09:16:41 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000099 Step: 73750 Total Loss: 4.9883 Recon Loss: 4.9776 
[12/23 09:17:22 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000099 Step: 73800 Total Loss: 6.2113 Recon Loss: 6.2006 
[12/23 09:18:02 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000099 Step: 73850 Total Loss: 4.3021 Recon Loss: 4.2915 
[12/23 09:18:43 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000099 Step: 73900 Total Loss: 4.3270 Recon Loss: 4.3163 
[12/23 09:19:24 TiTok]: Data (t): 0.0011, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000099 Step: 73950 Total Loss: 4.4000 Recon Loss: 4.3893 
[12/23 09:20:05 TiTok]: Data (t): 0.0010, 34.99/s/gpu Batch (t): 0.9144 LR: 0.000099 Step: 74000 Total Loss: 4.4299 Recon Loss: 4.4192 
[12/23 09:20:46 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000099 Step: 74050 Total Loss: 4.3307 Recon Loss: 4.3199 
[12/23 09:21:27 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000099 Step: 74100 Total Loss: 4.3804 Recon Loss: 4.3696 
[12/23 09:22:08 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000099 Step: 74150 Total Loss: 6.2308 Recon Loss: 6.2201 
[12/23 09:22:49 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000099 Step: 74200 Total Loss: 5.5260 Recon Loss: 5.5153 
[12/23 09:23:30 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000099 Step: 74250 Total Loss: 5.5810 Recon Loss: 5.5703 
[12/23 09:24:11 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000099 Step: 74300 Total Loss: 4.9237 Recon Loss: 4.9130 
[12/23 09:24:51 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000099 Step: 74350 Total Loss: 4.3408 Recon Loss: 4.3300 
[12/23 09:25:32 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000099 Step: 74400 Total Loss: 5.6455 Recon Loss: 5.6348 
[12/23 09:26:13 TiTok]: Data (t): 0.0011, 38.89/s/gpu Batch (t): 0.8228 LR: 0.000099 Step: 74450 Total Loss: 4.3343 Recon Loss: 4.3236 
[12/23 09:26:54 TiTok]: Data (t): 0.0012, 39.55/s/gpu Batch (t): 0.8091 LR: 0.000099 Step: 74500 Total Loss: 4.9868 Recon Loss: 4.9761 
[12/23 09:27:35 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000099 Step: 74550 Total Loss: 5.0349 Recon Loss: 5.0242 
[12/23 09:28:16 TiTok]: Data (t): 0.0011, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000099 Step: 74600 Total Loss: 5.0030 Recon Loss: 4.9923 
[12/23 09:28:57 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000099 Step: 74650 Total Loss: 5.0180 Recon Loss: 5.0073 
[12/23 09:29:38 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000099 Step: 74700 Total Loss: 4.3192 Recon Loss: 4.3085 
[12/23 09:30:19 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000099 Step: 74750 Total Loss: 4.2842 Recon Loss: 4.2735 
[12/23 09:31:00 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000099 Step: 74800 Total Loss: 5.0023 Recon Loss: 4.9916 
[12/23 09:31:40 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000099 Step: 74850 Total Loss: 4.3941 Recon Loss: 4.3834 
[12/23 09:32:21 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000099 Step: 74900 Total Loss: 5.0346 Recon Loss: 5.0239 
[12/23 09:33:02 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000099 Step: 74950 Total Loss: 4.9834 Recon Loss: 4.9727 
[12/23 09:33:43 TiTok]: Data (t): 0.0011, 35.27/s/gpu Batch (t): 0.9074 LR: 0.000099 Step: 75000 Total Loss: 6.2266 Recon Loss: 6.2158 
[12/23 09:33:44 TiTok]: Reconstructing images...
[12/23 09:34:26 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000099 Step: 75050 Total Loss: 4.3476 Recon Loss: 4.3368 
[12/23 09:35:06 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000099 Step: 75100 Total Loss: 4.9351 Recon Loss: 4.9244 
[12/23 09:35:47 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000099 Step: 75150 Total Loss: 4.3758 Recon Loss: 4.3651 
[12/23 09:36:28 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000099 Step: 75200 Total Loss: 4.3725 Recon Loss: 4.3618 
[12/23 09:37:09 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000099 Step: 75250 Total Loss: 5.5807 Recon Loss: 5.5699 
[12/23 09:37:50 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000099 Step: 75300 Total Loss: 4.3203 Recon Loss: 4.3096 
[12/23 09:38:31 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000099 Step: 75350 Total Loss: 4.9940 Recon Loss: 4.9832 
[12/23 09:39:12 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000099 Step: 75400 Total Loss: 5.0248 Recon Loss: 5.0140 
[12/23 09:39:53 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000099 Step: 75450 Total Loss: 5.0320 Recon Loss: 5.0212 
[12/23 09:40:33 TiTok]: Data (t): 0.0010, 38.79/s/gpu Batch (t): 0.8249 LR: 0.000099 Step: 75500 Total Loss: 4.3629 Recon Loss: 4.3521 
[12/23 09:41:14 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000099 Step: 75550 Total Loss: 4.3642 Recon Loss: 4.3535 
[12/23 09:41:55 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8204 LR: 0.000099 Step: 75600 Total Loss: 5.0050 Recon Loss: 4.9942 
[12/23 09:42:36 TiTok]: Data (t): 0.0010, 39.41/s/gpu Batch (t): 0.8120 LR: 0.000099 Step: 75650 Total Loss: 4.3968 Recon Loss: 4.3860 
[12/23 09:43:17 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000099 Step: 75700 Total Loss: 5.5863 Recon Loss: 5.5756 
[12/23 09:43:58 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000099 Step: 75750 Total Loss: 5.0089 Recon Loss: 4.9982 
[12/23 09:44:39 TiTok]: Data (t): 0.0010, 38.87/s/gpu Batch (t): 0.8232 LR: 0.000099 Step: 75800 Total Loss: 5.6388 Recon Loss: 5.6281 
[12/23 09:45:20 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000099 Step: 75850 Total Loss: 4.9327 Recon Loss: 4.9220 
[12/23 09:46:01 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000099 Step: 75900 Total Loss: 4.9716 Recon Loss: 4.9609 
[12/23 09:46:41 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000099 Step: 75950 Total Loss: 4.9836 Recon Loss: 4.9730 
[12/23 09:47:22 TiTok]: Data (t): 0.0010, 34.08/s/gpu Batch (t): 0.9389 LR: 0.000099 Step: 76000 Total Loss: 4.2957 Recon Loss: 4.2849 
[12/23 09:48:03 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000099 Step: 76050 Total Loss: 4.9772 Recon Loss: 4.9664 
[12/23 09:48:44 TiTok]: Data (t): 0.0010, 39.49/s/gpu Batch (t): 0.8103 LR: 0.000099 Step: 76100 Total Loss: 5.0106 Recon Loss: 4.9999 
[12/23 09:49:25 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000099 Step: 76150 Total Loss: 4.9629 Recon Loss: 4.9522 
[12/23 09:50:06 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000099 Step: 76200 Total Loss: 4.3448 Recon Loss: 4.3341 
[12/23 09:50:47 TiTok]: Data (t): 0.0011, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000099 Step: 76250 Total Loss: 4.9224 Recon Loss: 4.9117 
[12/23 09:51:28 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000099 Step: 76300 Total Loss: 5.5662 Recon Loss: 5.5554 
[12/23 09:52:09 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000099 Step: 76350 Total Loss: 4.9806 Recon Loss: 4.9699 
[12/23 09:52:49 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8152 LR: 0.000099 Step: 76400 Total Loss: 4.9888 Recon Loss: 4.9780 
[12/23 09:53:30 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000099 Step: 76450 Total Loss: 5.5906 Recon Loss: 5.5798 
[12/23 09:54:11 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000099 Step: 76500 Total Loss: 5.0066 Recon Loss: 4.9959 
[12/23 09:54:52 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8150 LR: 0.000099 Step: 76550 Total Loss: 4.9622 Recon Loss: 4.9515 
[12/23 09:55:33 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000099 Step: 76600 Total Loss: 4.4071 Recon Loss: 4.3964 
[12/23 09:56:14 TiTok]: Data (t): 0.0009, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000099 Step: 76650 Total Loss: 4.3667 Recon Loss: 4.3560 
[12/23 09:56:55 TiTok]: Data (t): 0.0011, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000099 Step: 76700 Total Loss: 4.3240 Recon Loss: 4.3133 
[12/23 09:57:36 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000099 Step: 76750 Total Loss: 4.3217 Recon Loss: 4.3109 
[12/23 09:58:17 TiTok]: Data (t): 0.0010, 39.37/s/gpu Batch (t): 0.8127 LR: 0.000099 Step: 76800 Total Loss: 5.0160 Recon Loss: 5.0052 
[12/23 09:58:57 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8217 LR: 0.000099 Step: 76850 Total Loss: 5.0232 Recon Loss: 5.0125 
[12/23 09:59:38 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000099 Step: 76900 Total Loss: 4.3821 Recon Loss: 4.3713 
[12/23 10:00:19 TiTok]: Data (t): 0.0010, 39.63/s/gpu Batch (t): 0.8074 LR: 0.000099 Step: 76950 Total Loss: 5.6141 Recon Loss: 5.6033 
[12/23 10:01:00 TiTok]: Data (t): 0.0010, 35.43/s/gpu Batch (t): 0.9033 LR: 0.000099 Step: 77000 Total Loss: 6.2035 Recon Loss: 6.1927 
[12/23 10:01:41 TiTok]: Data (t): 0.0011, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000099 Step: 77050 Total Loss: 4.9459 Recon Loss: 4.9352 
[12/23 10:02:22 TiTok]: Data (t): 0.0011, 39.39/s/gpu Batch (t): 0.8123 LR: 0.000099 Step: 77100 Total Loss: 5.0369 Recon Loss: 5.0262 
[12/23 10:03:03 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000099 Step: 77150 Total Loss: 4.4089 Recon Loss: 4.3981 
[12/23 10:03:44 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000099 Step: 77200 Total Loss: 4.9143 Recon Loss: 4.9036 
[12/23 10:04:25 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000099 Step: 77250 Total Loss: 4.4072 Recon Loss: 4.3964 
[12/23 10:05:05 TiTok]: Data (t): 0.0011, 38.17/s/gpu Batch (t): 0.8383 LR: 0.000099 Step: 77300 Total Loss: 4.3418 Recon Loss: 4.3311 
[12/23 10:05:46 TiTok]: Data (t): 0.0011, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000099 Step: 77350 Total Loss: 5.0225 Recon Loss: 5.0118 
[12/23 10:06:27 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000099 Step: 77400 Total Loss: 5.5839 Recon Loss: 5.5732 
[12/23 10:07:08 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8223 LR: 0.000099 Step: 77450 Total Loss: 4.3093 Recon Loss: 4.2985 
[12/23 10:07:49 TiTok]: Data (t): 0.0011, 39.45/s/gpu Batch (t): 0.8111 LR: 0.000099 Step: 77500 Total Loss: 5.5748 Recon Loss: 5.5640 
[12/23 10:08:30 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000099 Step: 77550 Total Loss: 4.8937 Recon Loss: 4.8830 
[12/23 10:09:11 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000099 Step: 77600 Total Loss: 4.9507 Recon Loss: 4.9399 
[12/23 10:09:52 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000099 Step: 77650 Total Loss: 4.3335 Recon Loss: 4.3228 
[12/23 10:10:33 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000099 Step: 77700 Total Loss: 5.6445 Recon Loss: 5.6338 
[12/23 10:11:14 TiTok]: Data (t): 0.0011, 39.45/s/gpu Batch (t): 0.8111 LR: 0.000099 Step: 77750 Total Loss: 5.6319 Recon Loss: 5.6211 
[12/23 10:11:54 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000099 Step: 77800 Total Loss: 4.9534 Recon Loss: 4.9426 
[12/23 10:12:35 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8177 LR: 0.000099 Step: 77850 Total Loss: 4.9705 Recon Loss: 4.9597 
[12/23 10:13:16 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000099 Step: 77900 Total Loss: 5.6074 Recon Loss: 5.5967 
[12/23 10:13:57 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8160 LR: 0.000099 Step: 77950 Total Loss: 6.2167 Recon Loss: 6.2060 
[12/23 10:14:38 TiTok]: Data (t): 0.0010, 35.61/s/gpu Batch (t): 0.8985 LR: 0.000099 Step: 78000 Total Loss: 4.9832 Recon Loss: 4.9723 
[12/23 10:15:19 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000099 Step: 78050 Total Loss: 4.3148 Recon Loss: 4.3040 
[12/23 10:16:00 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000099 Step: 78100 Total Loss: 4.9184 Recon Loss: 4.9076 
[12/23 10:16:41 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000099 Step: 78150 Total Loss: 4.9297 Recon Loss: 4.9190 
[12/23 10:17:22 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8154 LR: 0.000099 Step: 78200 Total Loss: 4.2893 Recon Loss: 4.2785 
[12/23 10:18:02 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000099 Step: 78250 Total Loss: 5.5904 Recon Loss: 5.5797 
[12/23 10:18:43 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000099 Step: 78300 Total Loss: 4.3640 Recon Loss: 4.3532 
[12/23 10:19:24 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000099 Step: 78350 Total Loss: 4.3231 Recon Loss: 4.3125 
[12/23 10:20:05 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000099 Step: 78400 Total Loss: 4.9038 Recon Loss: 4.8930 
[12/23 10:20:46 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000099 Step: 78450 Total Loss: 4.3203 Recon Loss: 4.3095 
[12/23 10:21:27 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000099 Step: 78500 Total Loss: 4.3238 Recon Loss: 4.3131 
[12/23 10:22:08 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000099 Step: 78550 Total Loss: 4.9307 Recon Loss: 4.9199 
[12/23 10:22:49 TiTok]: Data (t): 0.0011, 38.85/s/gpu Batch (t): 0.8236 LR: 0.000099 Step: 78600 Total Loss: 5.0287 Recon Loss: 5.0179 
[12/23 10:23:30 TiTok]: Data (t): 0.0011, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000099 Step: 78650 Total Loss: 4.9502 Recon Loss: 4.9394 
[12/23 10:24:10 TiTok]: Data (t): 0.0011, 38.83/s/gpu Batch (t): 0.8241 LR: 0.000099 Step: 78700 Total Loss: 4.3319 Recon Loss: 4.3212 
[12/23 10:24:51 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000099 Step: 78750 Total Loss: 4.9632 Recon Loss: 4.9524 
[12/23 10:25:32 TiTok]: Data (t): 0.0011, 39.47/s/gpu Batch (t): 0.8108 LR: 0.000099 Step: 78800 Total Loss: 5.5633 Recon Loss: 5.5526 
[12/23 10:26:13 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000099 Step: 78850 Total Loss: 4.3548 Recon Loss: 4.3441 
[12/23 10:26:54 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000099 Step: 78900 Total Loss: 4.9483 Recon Loss: 4.9375 
[12/23 10:27:35 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8231 LR: 0.000099 Step: 78950 Total Loss: 4.3097 Recon Loss: 4.2990 
[12/23 10:28:16 TiTok]: Data (t): 0.0011, 35.16/s/gpu Batch (t): 0.9102 LR: 0.000099 Step: 79000 Total Loss: 4.9386 Recon Loss: 4.9279 
[12/23 10:28:57 TiTok]: Data (t): 0.0014, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000099 Step: 79050 Total Loss: 5.6236 Recon Loss: 5.6129 
[12/23 10:29:38 TiTok]: Data (t): 0.0011, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000099 Step: 79100 Total Loss: 4.2959 Recon Loss: 4.2852 
[12/23 10:30:19 TiTok]: Data (t): 0.0011, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000099 Step: 79150 Total Loss: 4.9285 Recon Loss: 4.9177 
[12/23 10:30:59 TiTok]: Data (t): 0.0011, 38.72/s/gpu Batch (t): 0.8265 LR: 0.000099 Step: 79200 Total Loss: 4.9725 Recon Loss: 4.9618 
[12/23 10:31:40 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000099 Step: 79250 Total Loss: 4.3269 Recon Loss: 4.3162 
[12/23 10:32:21 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8131 LR: 0.000099 Step: 79300 Total Loss: 5.5987 Recon Loss: 5.5879 
[12/23 10:33:02 TiTok]: Data (t): 0.0011, 39.42/s/gpu Batch (t): 0.8119 LR: 0.000099 Step: 79350 Total Loss: 5.6344 Recon Loss: 5.6237 
[12/23 10:33:43 TiTok]: Data (t): 0.0011, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000099 Step: 79400 Total Loss: 4.3573 Recon Loss: 4.3465 
[12/23 10:34:24 TiTok]: Data (t): 0.0010, 38.87/s/gpu Batch (t): 0.8232 LR: 0.000099 Step: 79450 Total Loss: 5.6043 Recon Loss: 5.5936 
[12/23 10:35:05 TiTok]: Data (t): 0.0010, 39.49/s/gpu Batch (t): 0.8103 LR: 0.000099 Step: 79500 Total Loss: 4.9264 Recon Loss: 4.9157 
[12/23 10:35:46 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000099 Step: 79550 Total Loss: 4.9667 Recon Loss: 4.9560 
[12/23 10:36:27 TiTok]: Data (t): 0.0010, 39.51/s/gpu Batch (t): 0.8099 LR: 0.000099 Step: 79600 Total Loss: 5.5256 Recon Loss: 5.5148 
[12/23 10:37:07 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8234 LR: 0.000099 Step: 79650 Total Loss: 5.5574 Recon Loss: 5.5467 
[12/23 10:37:48 TiTok]: Data (t): 0.0010, 38.83/s/gpu Batch (t): 0.8242 LR: 0.000099 Step: 79700 Total Loss: 4.3091 Recon Loss: 4.2984 
[12/23 10:38:29 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000099 Step: 79750 Total Loss: 4.8697 Recon Loss: 4.8589 
[12/23 10:39:10 TiTok]: Data (t): 0.0010, 39.47/s/gpu Batch (t): 0.8108 LR: 0.000099 Step: 79800 Total Loss: 4.9489 Recon Loss: 4.9381 
[12/23 10:39:51 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000099 Step: 79850 Total Loss: 4.4170 Recon Loss: 4.4062 
[12/23 10:40:32 TiTok]: Data (t): 0.0012, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000099 Step: 79900 Total Loss: 5.6184 Recon Loss: 5.6077 
[12/23 10:41:13 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000099 Step: 79950 Total Loss: 4.9466 Recon Loss: 4.9358 
[12/23 10:41:54 TiTok]: Data (t): 0.0012, 35.03/s/gpu Batch (t): 0.9134 LR: 0.000099 Step: 80000 Total Loss: 4.9261 Recon Loss: 4.9153 
[12/23 10:41:55 TiTok]: Reconstructing images...
[12/23 10:42:36 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8202 LR: 0.000099 Step: 80050 Total Loss: 4.9232 Recon Loss: 4.9124 
[12/23 10:43:17 TiTok]: Data (t): 0.0010, 39.37/s/gpu Batch (t): 0.8127 LR: 0.000099 Step: 80100 Total Loss: 4.9462 Recon Loss: 4.9355 
[12/23 10:43:58 TiTok]: Data (t): 0.0012, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000099 Step: 80150 Total Loss: 5.5947 Recon Loss: 5.5840 
Epoch 8/99 started.
[12/23 10:44:39 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8219 LR: 0.000099 Step: 80200 Total Loss: 4.9317 Recon Loss: 4.9210 
[12/23 10:45:20 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8168 LR: 0.000099 Step: 80250 Total Loss: 4.9029 Recon Loss: 4.8921 
[12/23 10:46:01 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000099 Step: 80300 Total Loss: 4.3478 Recon Loss: 4.3371 
[12/23 10:46:42 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000099 Step: 80350 Total Loss: 6.2144 Recon Loss: 6.2036 
[12/23 10:47:23 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000099 Step: 80400 Total Loss: 4.9443 Recon Loss: 4.9336 
[12/23 10:48:04 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000099 Step: 80450 Total Loss: 5.5745 Recon Loss: 5.5638 
[12/23 10:48:45 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000099 Step: 80500 Total Loss: 4.9322 Recon Loss: 4.9215 
[12/23 10:49:26 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000099 Step: 80550 Total Loss: 4.9658 Recon Loss: 4.9551 
[12/23 10:50:07 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000099 Step: 80600 Total Loss: 4.3809 Recon Loss: 4.3702 
[12/23 10:50:48 TiTok]: Data (t): 0.0015, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000099 Step: 80650 Total Loss: 5.5487 Recon Loss: 5.5379 
[12/23 10:51:28 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000099 Step: 80700 Total Loss: 4.3739 Recon Loss: 4.3632 
[12/23 10:52:09 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000099 Step: 80750 Total Loss: 6.2055 Recon Loss: 6.1948 
[12/23 10:52:50 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000099 Step: 80800 Total Loss: 4.3219 Recon Loss: 4.3112 
[12/23 10:53:31 TiTok]: Data (t): 0.0011, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000099 Step: 80850 Total Loss: 4.3056 Recon Loss: 4.2948 
[12/23 10:54:12 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000099 Step: 80900 Total Loss: 4.9860 Recon Loss: 4.9752 
[12/23 10:54:53 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000099 Step: 80950 Total Loss: 4.3272 Recon Loss: 4.3164 
[12/23 10:55:34 TiTok]: Data (t): 0.0011, 33.81/s/gpu Batch (t): 0.9465 LR: 0.000099 Step: 81000 Total Loss: 5.5173 Recon Loss: 5.5066 
[12/23 10:56:15 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000099 Step: 81050 Total Loss: 4.9474 Recon Loss: 4.9366 
[12/23 10:56:56 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000099 Step: 81100 Total Loss: 4.9394 Recon Loss: 4.9286 
[12/23 10:57:36 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000099 Step: 81150 Total Loss: 4.3325 Recon Loss: 4.3217 
[12/23 10:58:17 TiTok]: Data (t): 0.0011, 38.88/s/gpu Batch (t): 0.8231 LR: 0.000099 Step: 81200 Total Loss: 4.3969 Recon Loss: 4.3862 
[12/23 10:58:58 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000099 Step: 81250 Total Loss: 4.9252 Recon Loss: 4.9145 
[12/23 10:59:39 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000099 Step: 81300 Total Loss: 4.9725 Recon Loss: 4.9617 
[12/23 11:00:20 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000099 Step: 81350 Total Loss: 5.5741 Recon Loss: 5.5633 
[12/23 11:01:01 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000099 Step: 81400 Total Loss: 5.5511 Recon Loss: 5.5404 
[12/23 11:01:42 TiTok]: Data (t): 0.0011, 38.91/s/gpu Batch (t): 0.8223 LR: 0.000099 Step: 81450 Total Loss: 4.9919 Recon Loss: 4.9811 
[12/23 11:02:23 TiTok]: Data (t): 0.0010, 38.62/s/gpu Batch (t): 0.8287 LR: 0.000099 Step: 81500 Total Loss: 5.5797 Recon Loss: 5.5689 
[12/23 11:03:04 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000099 Step: 81550 Total Loss: 4.9384 Recon Loss: 4.9276 
[12/23 11:03:44 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000099 Step: 81600 Total Loss: 4.9827 Recon Loss: 4.9719 
[12/23 11:04:25 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000099 Step: 81650 Total Loss: 5.5909 Recon Loss: 5.5801 
[12/23 11:05:06 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000099 Step: 81700 Total Loss: 4.2853 Recon Loss: 4.2745 
[12/23 11:05:47 TiTok]: Data (t): 0.0010, 38.81/s/gpu Batch (t): 0.8245 LR: 0.000099 Step: 81750 Total Loss: 4.9465 Recon Loss: 4.9358 
[12/23 11:06:28 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000099 Step: 81800 Total Loss: 4.3361 Recon Loss: 4.3253 
[12/23 11:07:09 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000099 Step: 81850 Total Loss: 4.9500 Recon Loss: 4.9393 
[12/23 11:07:50 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000099 Step: 81900 Total Loss: 4.9522 Recon Loss: 4.9414 
[12/23 11:08:31 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000099 Step: 81950 Total Loss: 4.8953 Recon Loss: 4.8845 
[12/23 11:09:12 TiTok]: Data (t): 0.0010, 35.59/s/gpu Batch (t): 0.8992 LR: 0.000099 Step: 82000 Total Loss: 5.5732 Recon Loss: 5.5625 
[12/23 11:09:52 TiTok]: Data (t): 0.0010, 39.52/s/gpu Batch (t): 0.8097 LR: 0.000099 Step: 82050 Total Loss: 4.9450 Recon Loss: 4.9342 
[12/23 11:10:33 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000099 Step: 82100 Total Loss: 4.2906 Recon Loss: 4.2798 
[12/23 11:11:14 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000099 Step: 82150 Total Loss: 6.2205 Recon Loss: 6.2097 
[12/23 11:11:55 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000099 Step: 82200 Total Loss: 5.0142 Recon Loss: 5.0035 
[12/23 11:12:36 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8146 LR: 0.000099 Step: 82250 Total Loss: 4.9773 Recon Loss: 4.9665 
[12/23 11:13:17 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8154 LR: 0.000099 Step: 82300 Total Loss: 5.5517 Recon Loss: 5.5409 
[12/23 11:13:58 TiTok]: Data (t): 0.0010, 38.85/s/gpu Batch (t): 0.8237 LR: 0.000099 Step: 82350 Total Loss: 4.2982 Recon Loss: 4.2873 
[12/23 11:14:39 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000099 Step: 82400 Total Loss: 4.9550 Recon Loss: 4.9442 
[12/23 11:15:20 TiTok]: Data (t): 0.0010, 39.42/s/gpu Batch (t): 0.8118 LR: 0.000099 Step: 82450 Total Loss: 6.2181 Recon Loss: 6.2073 
[12/23 11:16:00 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000099 Step: 82500 Total Loss: 4.2740 Recon Loss: 4.2633 
[12/23 11:16:41 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8223 LR: 0.000099 Step: 82550 Total Loss: 4.3695 Recon Loss: 4.3588 
[12/23 11:17:22 TiTok]: Data (t): 0.0011, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000099 Step: 82600 Total Loss: 4.3116 Recon Loss: 4.3009 
[12/23 11:18:03 TiTok]: Data (t): 0.0010, 38.70/s/gpu Batch (t): 0.8269 LR: 0.000099 Step: 82650 Total Loss: 5.5715 Recon Loss: 5.5608 
[12/23 11:18:44 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8154 LR: 0.000099 Step: 82700 Total Loss: 4.2683 Recon Loss: 4.2576 
[12/23 11:19:25 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000099 Step: 82750 Total Loss: 4.9128 Recon Loss: 4.9020 
[12/23 11:20:06 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000099 Step: 82800 Total Loss: 5.5852 Recon Loss: 5.5745 
[12/23 11:20:47 TiTok]: Data (t): 0.0010, 39.47/s/gpu Batch (t): 0.8107 LR: 0.000099 Step: 82850 Total Loss: 5.5722 Recon Loss: 5.5615 
[12/23 11:21:27 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000099 Step: 82900 Total Loss: 4.2715 Recon Loss: 4.2607 
[12/23 11:22:08 TiTok]: Data (t): 0.0011, 39.39/s/gpu Batch (t): 0.8125 LR: 0.000099 Step: 82950 Total Loss: 5.5763 Recon Loss: 5.5654 
[12/23 11:22:49 TiTok]: Data (t): 0.0011, 34.91/s/gpu Batch (t): 0.9167 LR: 0.000099 Step: 83000 Total Loss: 4.9485 Recon Loss: 4.9377 
[12/23 11:23:30 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000099 Step: 83050 Total Loss: 5.5674 Recon Loss: 5.5565 
[12/23 11:24:11 TiTok]: Data (t): 0.0010, 38.85/s/gpu Batch (t): 0.8237 LR: 0.000099 Step: 83100 Total Loss: 4.8989 Recon Loss: 4.8882 
[12/23 11:24:52 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000099 Step: 83150 Total Loss: 4.3607 Recon Loss: 4.3500 
[12/23 11:25:33 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000099 Step: 83200 Total Loss: 4.9458 Recon Loss: 4.9350 
[12/23 11:26:14 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000099 Step: 83250 Total Loss: 4.9535 Recon Loss: 4.9427 
[12/23 11:26:55 TiTok]: Data (t): 0.0010, 39.42/s/gpu Batch (t): 0.8118 LR: 0.000099 Step: 83300 Total Loss: 4.2867 Recon Loss: 4.2759 
[12/23 11:27:35 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000099 Step: 83350 Total Loss: 4.2920 Recon Loss: 4.2813 
[12/23 11:28:16 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000099 Step: 83400 Total Loss: 4.8992 Recon Loss: 4.8884 
[12/23 11:28:57 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8200 LR: 0.000099 Step: 83450 Total Loss: 6.2229 Recon Loss: 6.2121 
[12/23 11:29:38 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000099 Step: 83500 Total Loss: 4.9152 Recon Loss: 4.9045 
[12/23 11:30:19 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000099 Step: 83550 Total Loss: 4.9456 Recon Loss: 4.9349 
[12/23 11:31:00 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8164 LR: 0.000099 Step: 83600 Total Loss: 4.3336 Recon Loss: 4.3229 
[12/23 11:31:41 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000099 Step: 83650 Total Loss: 4.9970 Recon Loss: 4.9863 
[12/23 11:32:22 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000099 Step: 83700 Total Loss: 4.3466 Recon Loss: 4.3359 
[12/23 11:33:02 TiTok]: Data (t): 0.0010, 39.42/s/gpu Batch (t): 0.8117 LR: 0.000099 Step: 83750 Total Loss: 5.6113 Recon Loss: 5.6005 
[12/23 11:33:43 TiTok]: Data (t): 0.0019, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000099 Step: 83800 Total Loss: 4.9486 Recon Loss: 4.9379 
[12/23 11:34:24 TiTok]: Data (t): 0.0010, 38.81/s/gpu Batch (t): 0.8245 LR: 0.000099 Step: 83850 Total Loss: 4.3386 Recon Loss: 4.3278 
[12/23 11:35:05 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8179 LR: 0.000099 Step: 83900 Total Loss: 4.9810 Recon Loss: 4.9702 
[12/23 11:35:46 TiTok]: Data (t): 0.0011, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000099 Step: 83950 Total Loss: 4.9294 Recon Loss: 4.9186 
[12/23 11:36:27 TiTok]: Data (t): 0.0011, 35.00/s/gpu Batch (t): 0.9143 LR: 0.000099 Step: 84000 Total Loss: 5.5678 Recon Loss: 5.5570 
[12/23 11:37:08 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000099 Step: 84050 Total Loss: 4.2865 Recon Loss: 4.2758 
[12/23 11:37:49 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000099 Step: 84100 Total Loss: 5.5762 Recon Loss: 5.5655 
[12/23 11:38:29 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000099 Step: 84150 Total Loss: 5.0192 Recon Loss: 5.0084 
[12/23 11:39:10 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000099 Step: 84200 Total Loss: 4.3090 Recon Loss: 4.2982 
[12/23 11:39:51 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000099 Step: 84250 Total Loss: 4.9592 Recon Loss: 4.9485 
[12/23 11:40:32 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8217 LR: 0.000099 Step: 84300 Total Loss: 4.9291 Recon Loss: 4.9184 
[12/23 11:41:13 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000099 Step: 84350 Total Loss: 5.5911 Recon Loss: 5.5803 
[12/23 11:41:54 TiTok]: Data (t): 0.0011, 38.80/s/gpu Batch (t): 0.8247 LR: 0.000099 Step: 84400 Total Loss: 4.9720 Recon Loss: 4.9612 
[12/23 11:42:35 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000099 Step: 84450 Total Loss: 4.9457 Recon Loss: 4.9350 
[12/23 11:43:16 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000099 Step: 84500 Total Loss: 5.5583 Recon Loss: 5.5475 
[12/23 11:43:56 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000099 Step: 84550 Total Loss: 4.9604 Recon Loss: 4.9496 
[12/23 11:44:37 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000099 Step: 84600 Total Loss: 5.5953 Recon Loss: 5.5845 
[12/23 11:45:18 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000099 Step: 84650 Total Loss: 4.2998 Recon Loss: 4.2891 
[12/23 11:45:59 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000099 Step: 84700 Total Loss: 5.5718 Recon Loss: 5.5610 
[12/23 11:46:40 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000099 Step: 84750 Total Loss: 4.9006 Recon Loss: 4.8899 
[12/23 11:47:21 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000099 Step: 84800 Total Loss: 4.9478 Recon Loss: 4.9371 
[12/23 11:48:02 TiTok]: Data (t): 0.0012, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000099 Step: 84850 Total Loss: 4.9482 Recon Loss: 4.9375 
[12/23 11:48:43 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8231 LR: 0.000099 Step: 84900 Total Loss: 4.3056 Recon Loss: 4.2948 
[12/23 11:49:24 TiTok]: Data (t): 0.0010, 39.46/s/gpu Batch (t): 0.8109 LR: 0.000099 Step: 84950 Total Loss: 4.9419 Recon Loss: 4.9311 
[12/23 11:50:05 TiTok]: Data (t): 0.0010, 35.05/s/gpu Batch (t): 0.9129 LR: 0.000099 Step: 85000 Total Loss: 5.5662 Recon Loss: 5.5554 
[12/23 11:50:06 TiTok]: Reconstructing images...
[12/23 11:50:47 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000099 Step: 85050 Total Loss: 4.9893 Recon Loss: 4.9785 
[12/23 11:51:28 TiTok]: Data (t): 0.0010, 39.43/s/gpu Batch (t): 0.8115 LR: 0.000099 Step: 85100 Total Loss: 4.8827 Recon Loss: 4.8720 
[12/23 11:52:09 TiTok]: Data (t): 0.0013, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000099 Step: 85150 Total Loss: 4.9556 Recon Loss: 4.9448 
[12/23 11:52:50 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000099 Step: 85200 Total Loss: 4.9564 Recon Loss: 4.9457 
[12/23 11:53:31 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000099 Step: 85250 Total Loss: 4.2968 Recon Loss: 4.2860 
[12/23 11:54:11 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000099 Step: 85300 Total Loss: 5.5879 Recon Loss: 5.5771 
[12/23 11:54:52 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000099 Step: 85350 Total Loss: 4.9513 Recon Loss: 4.9406 
[12/23 11:55:33 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8181 LR: 0.000099 Step: 85400 Total Loss: 4.3501 Recon Loss: 4.3394 
[12/23 11:56:14 TiTok]: Data (t): 0.0010, 38.82/s/gpu Batch (t): 0.8242 LR: 0.000099 Step: 85450 Total Loss: 4.3195 Recon Loss: 4.3088 
[12/23 11:56:55 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000099 Step: 85500 Total Loss: 5.5328 Recon Loss: 5.5221 
[12/23 11:57:36 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000099 Step: 85550 Total Loss: 4.3564 Recon Loss: 4.3456 
[12/23 11:58:17 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000099 Step: 85600 Total Loss: 4.2855 Recon Loss: 4.2747 
[12/23 11:58:58 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000099 Step: 85650 Total Loss: 4.9012 Recon Loss: 4.8904 
[12/23 11:59:38 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8234 LR: 0.000099 Step: 85700 Total Loss: 4.3495 Recon Loss: 4.3387 
[12/23 12:00:19 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000099 Step: 85750 Total Loss: 4.8915 Recon Loss: 4.8808 
[12/23 12:01:00 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000099 Step: 85800 Total Loss: 4.9507 Recon Loss: 4.9399 
[12/23 12:01:41 TiTok]: Data (t): 0.0010, 39.44/s/gpu Batch (t): 0.8114 LR: 0.000099 Step: 85850 Total Loss: 4.2610 Recon Loss: 4.2502 
[12/23 12:02:22 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000099 Step: 85900 Total Loss: 4.9224 Recon Loss: 4.9116 
[12/23 12:03:03 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000099 Step: 85950 Total Loss: 4.3485 Recon Loss: 4.3378 
[12/23 12:03:44 TiTok]: Data (t): 0.0011, 35.07/s/gpu Batch (t): 0.9125 LR: 0.000099 Step: 86000 Total Loss: 5.5563 Recon Loss: 5.5456 
[12/23 12:04:25 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000099 Step: 86050 Total Loss: 6.1768 Recon Loss: 6.1659 
[12/23 12:05:06 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8228 LR: 0.000099 Step: 86100 Total Loss: 4.9866 Recon Loss: 4.9758 
[12/23 12:05:46 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000099 Step: 86150 Total Loss: 4.9413 Recon Loss: 4.9305 
[12/23 12:06:27 TiTok]: Data (t): 0.0011, 39.45/s/gpu Batch (t): 0.8112 LR: 0.000099 Step: 86200 Total Loss: 4.9551 Recon Loss: 4.9443 
[12/23 12:07:08 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000099 Step: 86250 Total Loss: 4.9019 Recon Loss: 4.8911 
[12/23 12:07:49 TiTok]: Data (t): 0.0011, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000099 Step: 86300 Total Loss: 4.3268 Recon Loss: 4.3161 
[12/23 12:08:30 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000099 Step: 86350 Total Loss: 4.9616 Recon Loss: 4.9508 
[12/23 12:09:11 TiTok]: Data (t): 0.0010, 38.75/s/gpu Batch (t): 0.8259 LR: 0.000099 Step: 86400 Total Loss: 4.9233 Recon Loss: 4.9126 
[12/23 12:09:52 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8131 LR: 0.000099 Step: 86450 Total Loss: 4.9351 Recon Loss: 4.9244 
[12/23 12:10:33 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000099 Step: 86500 Total Loss: 5.5760 Recon Loss: 5.5653 
[12/23 12:11:14 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000099 Step: 86550 Total Loss: 5.5467 Recon Loss: 5.5359 
[12/23 12:11:54 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000099 Step: 86600 Total Loss: 4.9274 Recon Loss: 4.9167 
[12/23 12:12:35 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000099 Step: 86650 Total Loss: 4.2741 Recon Loss: 4.2634 
[12/23 12:13:16 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8129 LR: 0.000099 Step: 86700 Total Loss: 5.5570 Recon Loss: 5.5463 
[12/23 12:13:57 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8154 LR: 0.000099 Step: 86750 Total Loss: 5.5571 Recon Loss: 5.5463 
[12/23 12:14:38 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000099 Step: 86800 Total Loss: 4.9137 Recon Loss: 4.9030 
[12/23 12:15:19 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000099 Step: 86850 Total Loss: 6.2207 Recon Loss: 6.2099 
[12/23 12:16:00 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000099 Step: 86900 Total Loss: 5.6129 Recon Loss: 5.6021 
[12/23 12:16:40 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000099 Step: 86950 Total Loss: 5.5678 Recon Loss: 5.5570 
[12/23 12:17:21 TiTok]: Data (t): 0.0011, 35.17/s/gpu Batch (t): 0.9098 LR: 0.000099 Step: 87000 Total Loss: 4.3136 Recon Loss: 4.3028 
[12/23 12:18:02 TiTok]: Data (t): 0.0010, 38.59/s/gpu Batch (t): 0.8293 LR: 0.000099 Step: 87050 Total Loss: 4.9139 Recon Loss: 4.9031 
[12/23 12:18:43 TiTok]: Data (t): 0.0010, 39.54/s/gpu Batch (t): 0.8092 LR: 0.000099 Step: 87100 Total Loss: 5.5677 Recon Loss: 5.5570 
[12/23 12:19:24 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000099 Step: 87150 Total Loss: 5.5917 Recon Loss: 5.5809 
[12/23 12:20:05 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000099 Step: 87200 Total Loss: 4.2577 Recon Loss: 4.2469 
[12/23 12:20:46 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000099 Step: 87250 Total Loss: 4.3264 Recon Loss: 4.3157 
[12/23 12:21:27 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8200 LR: 0.000099 Step: 87300 Total Loss: 4.9361 Recon Loss: 4.9254 
[12/23 12:22:07 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000099 Step: 87350 Total Loss: 4.2796 Recon Loss: 4.2688 
[12/23 12:22:48 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000099 Step: 87400 Total Loss: 6.1828 Recon Loss: 6.1721 
[12/23 12:23:29 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000099 Step: 87450 Total Loss: 4.2961 Recon Loss: 4.2853 
[12/23 12:24:10 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000099 Step: 87500 Total Loss: 6.1746 Recon Loss: 6.1639 
[12/23 12:24:51 TiTok]: Data (t): 0.0010, 39.42/s/gpu Batch (t): 0.8118 LR: 0.000099 Step: 87550 Total Loss: 5.5180 Recon Loss: 5.5072 
[12/23 12:25:32 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000099 Step: 87600 Total Loss: 4.2804 Recon Loss: 4.2696 
[12/23 12:26:13 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000099 Step: 87650 Total Loss: 4.8887 Recon Loss: 4.8780 
[12/23 12:26:54 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000099 Step: 87700 Total Loss: 5.0018 Recon Loss: 4.9910 
[12/23 12:27:34 TiTok]: Data (t): 0.0011, 39.33/s/gpu Batch (t): 0.8135 LR: 0.000099 Step: 87750 Total Loss: 4.9097 Recon Loss: 4.8990 
[12/23 12:28:15 TiTok]: Data (t): 0.0022, 40.27/s/gpu Batch (t): 0.7947 LR: 0.000099 Step: 87800 Total Loss: 5.5121 Recon Loss: 5.5014 
[12/23 12:28:56 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000099 Step: 87850 Total Loss: 4.9054 Recon Loss: 4.8946 
[12/23 12:29:37 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000099 Step: 87900 Total Loss: 4.9351 Recon Loss: 4.9243 
[12/23 12:30:18 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000099 Step: 87950 Total Loss: 5.5855 Recon Loss: 5.5747 
[12/23 12:30:59 TiTok]: Data (t): 0.0010, 35.06/s/gpu Batch (t): 0.9128 LR: 0.000099 Step: 88000 Total Loss: 6.1718 Recon Loss: 6.1610 
[12/23 12:31:40 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000099 Step: 88050 Total Loss: 4.2743 Recon Loss: 4.2636 
[12/23 12:32:21 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000099 Step: 88100 Total Loss: 4.9542 Recon Loss: 4.9435 
[12/23 12:33:01 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000099 Step: 88150 Total Loss: 4.9137 Recon Loss: 4.9030 
[12/23 12:33:42 TiTok]: Data (t): 0.0010, 38.80/s/gpu Batch (t): 0.8247 LR: 0.000099 Step: 88200 Total Loss: 4.9630 Recon Loss: 4.9522 
[12/23 12:34:23 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8229 LR: 0.000099 Step: 88250 Total Loss: 4.2697 Recon Loss: 4.2589 
[12/23 12:35:04 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8125 LR: 0.000099 Step: 88300 Total Loss: 6.1631 Recon Loss: 6.1524 
[12/23 12:35:45 TiTok]: Data (t): 0.0012, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000099 Step: 88350 Total Loss: 4.9217 Recon Loss: 4.9110 
[12/23 12:36:26 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000099 Step: 88400 Total Loss: 5.5407 Recon Loss: 5.5300 
[12/23 12:37:07 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000099 Step: 88450 Total Loss: 4.2786 Recon Loss: 4.2678 
[12/23 12:37:48 TiTok]: Data (t): 0.0010, 38.83/s/gpu Batch (t): 0.8241 LR: 0.000099 Step: 88500 Total Loss: 4.9088 Recon Loss: 4.8981 
[12/23 12:38:29 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000099 Step: 88550 Total Loss: 4.2524 Recon Loss: 4.2417 
[12/23 12:39:10 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000099 Step: 88600 Total Loss: 4.2850 Recon Loss: 4.2742 
[12/23 12:39:51 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000099 Step: 88650 Total Loss: 4.2689 Recon Loss: 4.2582 
[12/23 12:40:31 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000099 Step: 88700 Total Loss: 4.2635 Recon Loss: 4.2527 
[12/23 12:41:12 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8209 LR: 0.000099 Step: 88750 Total Loss: 5.6077 Recon Loss: 5.5969 
[12/23 12:41:53 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000099 Step: 88800 Total Loss: 5.5701 Recon Loss: 5.5594 
[12/23 12:42:34 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000099 Step: 88850 Total Loss: 4.9154 Recon Loss: 4.9046 
[12/23 12:43:15 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000099 Step: 88900 Total Loss: 4.2952 Recon Loss: 4.2844 
[12/23 12:43:56 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8183 LR: 0.000099 Step: 88950 Total Loss: 5.5967 Recon Loss: 5.5859 
[12/23 12:44:37 TiTok]: Data (t): 0.0011, 35.29/s/gpu Batch (t): 0.9068 LR: 0.000099 Step: 89000 Total Loss: 4.2119 Recon Loss: 4.2011 
[12/23 12:45:18 TiTok]: Data (t): 0.0011, 39.48/s/gpu Batch (t): 0.8106 LR: 0.000099 Step: 89050 Total Loss: 4.2982 Recon Loss: 4.2875 
[12/23 12:45:58 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000099 Step: 89100 Total Loss: 5.5220 Recon Loss: 5.5113 
[12/23 12:46:39 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000099 Step: 89150 Total Loss: 4.3457 Recon Loss: 4.3349 
[12/23 12:47:20 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000099 Step: 89200 Total Loss: 5.5045 Recon Loss: 5.4938 
[12/23 12:48:01 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000099 Step: 89250 Total Loss: 4.9267 Recon Loss: 4.9159 
[12/23 12:48:42 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000099 Step: 89300 Total Loss: 4.9376 Recon Loss: 4.9269 
[12/23 12:49:23 TiTok]: Data (t): 0.0011, 38.88/s/gpu Batch (t): 0.8231 LR: 0.000099 Step: 89350 Total Loss: 4.9477 Recon Loss: 4.9369 
[12/23 12:50:04 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000099 Step: 89400 Total Loss: 4.2531 Recon Loss: 4.2423 
[12/23 12:50:45 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000099 Step: 89450 Total Loss: 5.5364 Recon Loss: 5.5256 
[12/23 12:51:26 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8202 LR: 0.000099 Step: 89500 Total Loss: 5.5604 Recon Loss: 5.5497 
[12/23 12:52:06 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000099 Step: 89550 Total Loss: 4.9097 Recon Loss: 4.8989 
[12/23 12:52:47 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000099 Step: 89600 Total Loss: 5.5745 Recon Loss: 5.5638 
[12/23 12:53:28 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000099 Step: 89650 Total Loss: 5.5780 Recon Loss: 5.5672 
[12/23 12:54:09 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000099 Step: 89700 Total Loss: 4.9362 Recon Loss: 4.9254 
[12/23 12:54:50 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000099 Step: 89750 Total Loss: 4.3296 Recon Loss: 4.3188 
[12/23 12:55:31 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000099 Step: 89800 Total Loss: 4.9810 Recon Loss: 4.9702 
[12/23 12:56:12 TiTok]: Data (t): 0.0010, 38.84/s/gpu Batch (t): 0.8239 LR: 0.000099 Step: 89850 Total Loss: 4.8987 Recon Loss: 4.8880 
[12/23 12:56:53 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000099 Step: 89900 Total Loss: 6.1855 Recon Loss: 6.1747 
[12/23 12:57:34 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8223 LR: 0.000099 Step: 89950 Total Loss: 4.8979 Recon Loss: 4.8872 
[12/23 12:58:15 TiTok]: Data (t): 0.0011, 35.36/s/gpu Batch (t): 0.9050 LR: 0.000099 Step: 90000 Total Loss: 5.5371 Recon Loss: 5.5262 
[12/23 12:58:16 TiTok]: Reconstructing images...
[12/23 12:58:57 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000099 Step: 90050 Total Loss: 5.5582 Recon Loss: 5.5474 
[12/23 12:59:38 TiTok]: Data (t): 0.0011, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000099 Step: 90100 Total Loss: 4.9509 Recon Loss: 4.9402 
[12/23 13:00:19 TiTok]: Data (t): 0.0010, 38.84/s/gpu Batch (t): 0.8239 LR: 0.000099 Step: 90150 Total Loss: 4.8903 Recon Loss: 4.8796 
Epoch 9/99 started.
[12/23 13:01:00 TiTok]: Data (t): 0.0010, 38.82/s/gpu Batch (t): 0.8244 LR: 0.000099 Step: 90200 Total Loss: 4.9346 Recon Loss: 4.9239 
[12/23 13:01:41 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000099 Step: 90250 Total Loss: 4.9814 Recon Loss: 4.9707 
[12/23 13:02:22 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000099 Step: 90300 Total Loss: 6.1730 Recon Loss: 6.1622 
[12/23 13:03:03 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000099 Step: 90350 Total Loss: 4.2849 Recon Loss: 4.2742 
[12/23 13:03:44 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000099 Step: 90400 Total Loss: 4.9521 Recon Loss: 4.9412 
[12/23 13:04:25 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000099 Step: 90450 Total Loss: 4.9513 Recon Loss: 4.9405 
[12/23 13:05:05 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000099 Step: 90500 Total Loss: 4.8736 Recon Loss: 4.8628 
[12/23 13:05:46 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8221 LR: 0.000099 Step: 90550 Total Loss: 4.2356 Recon Loss: 4.2248 
[12/23 13:06:27 TiTok]: Data (t): 0.0012, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000099 Step: 90600 Total Loss: 4.8577 Recon Loss: 4.8469 
[12/23 13:07:08 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000099 Step: 90650 Total Loss: 4.3059 Recon Loss: 4.2951 
[12/23 13:07:49 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8200 LR: 0.000099 Step: 90700 Total Loss: 4.3631 Recon Loss: 4.3523 
[12/23 13:08:30 TiTok]: Data (t): 0.0011, 39.39/s/gpu Batch (t): 0.8123 LR: 0.000099 Step: 90750 Total Loss: 4.9570 Recon Loss: 4.9462 
[12/23 13:09:11 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000099 Step: 90800 Total Loss: 4.2056 Recon Loss: 4.1949 
[12/23 13:09:51 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000099 Step: 90850 Total Loss: 4.9286 Recon Loss: 4.9179 
[12/23 13:10:32 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000099 Step: 90900 Total Loss: 4.8795 Recon Loss: 4.8688 
[12/23 13:11:13 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000099 Step: 90950 Total Loss: 4.3013 Recon Loss: 4.2905 
[12/23 13:11:54 TiTok]: Data (t): 0.0010, 34.28/s/gpu Batch (t): 0.9335 LR: 0.000099 Step: 91000 Total Loss: 4.2818 Recon Loss: 4.2711 
[12/23 13:12:35 TiTok]: Data (t): 0.0011, 38.83/s/gpu Batch (t): 0.8241 LR: 0.000099 Step: 91050 Total Loss: 5.5834 Recon Loss: 5.5727 
[12/23 13:13:16 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000099 Step: 91100 Total Loss: 4.3111 Recon Loss: 4.3003 
[12/23 13:13:57 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000099 Step: 91150 Total Loss: 4.2583 Recon Loss: 4.2474 
[12/23 13:14:38 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000099 Step: 91200 Total Loss: 4.2220 Recon Loss: 4.2112 
[12/23 13:15:19 TiTok]: Data (t): 0.0010, 39.46/s/gpu Batch (t): 0.8110 LR: 0.000099 Step: 91250 Total Loss: 5.5605 Recon Loss: 5.5498 
[12/23 13:15:59 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000099 Step: 91300 Total Loss: 4.9907 Recon Loss: 4.9799 
[12/23 13:16:40 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000099 Step: 91350 Total Loss: 5.5791 Recon Loss: 5.5683 
[12/23 13:17:21 TiTok]: Data (t): 0.0012, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000099 Step: 91400 Total Loss: 4.8883 Recon Loss: 4.8776 
[12/23 13:18:02 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000099 Step: 91450 Total Loss: 4.8967 Recon Loss: 4.8860 
[12/23 13:18:43 TiTok]: Data (t): 0.0010, 39.43/s/gpu Batch (t): 0.8115 LR: 0.000099 Step: 91500 Total Loss: 5.5291 Recon Loss: 5.5183 
[12/23 13:19:24 TiTok]: Data (t): 0.0010, 38.79/s/gpu Batch (t): 0.8250 LR: 0.000099 Step: 91550 Total Loss: 4.3250 Recon Loss: 4.3143 
[12/23 13:20:05 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000099 Step: 91600 Total Loss: 5.5322 Recon Loss: 5.5214 
[12/23 13:20:46 TiTok]: Data (t): 0.0010, 38.81/s/gpu Batch (t): 0.8246 LR: 0.000098 Step: 91650 Total Loss: 5.5407 Recon Loss: 5.5299 
[12/23 13:21:26 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000098 Step: 91700 Total Loss: 4.9408 Recon Loss: 4.9299 
[12/23 13:22:07 TiTok]: Data (t): 0.0010, 39.43/s/gpu Batch (t): 0.8116 LR: 0.000098 Step: 91750 Total Loss: 5.6114 Recon Loss: 5.6005 
[12/23 13:22:48 TiTok]: Data (t): 0.0010, 39.49/s/gpu Batch (t): 0.8103 LR: 0.000098 Step: 91800 Total Loss: 4.8935 Recon Loss: 4.8827 
[12/23 13:23:29 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000098 Step: 91850 Total Loss: 4.9694 Recon Loss: 4.9587 
[12/23 13:24:10 TiTok]: Data (t): 0.0011, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000098 Step: 91900 Total Loss: 4.9237 Recon Loss: 4.9129 
[12/23 13:24:51 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000098 Step: 91950 Total Loss: 4.9402 Recon Loss: 4.9294 
[12/23 13:25:32 TiTok]: Data (t): 0.0011, 35.22/s/gpu Batch (t): 0.9086 LR: 0.000098 Step: 92000 Total Loss: 4.9191 Recon Loss: 4.9083 
[12/23 13:26:13 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000098 Step: 92050 Total Loss: 4.2589 Recon Loss: 4.2481 
[12/23 13:26:54 TiTok]: Data (t): 0.0011, 38.77/s/gpu Batch (t): 0.8255 LR: 0.000098 Step: 92100 Total Loss: 4.8980 Recon Loss: 4.8872 
[12/23 13:27:34 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000098 Step: 92150 Total Loss: 4.3077 Recon Loss: 4.2969 
[12/23 13:28:15 TiTok]: Data (t): 0.0010, 39.48/s/gpu Batch (t): 0.8104 LR: 0.000098 Step: 92200 Total Loss: 5.5282 Recon Loss: 5.5174 
[12/23 13:28:56 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000098 Step: 92250 Total Loss: 5.5372 Recon Loss: 5.5265 
[12/23 13:29:37 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000098 Step: 92300 Total Loss: 4.2869 Recon Loss: 4.2761 
[12/23 13:30:18 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8129 LR: 0.000098 Step: 92350 Total Loss: 4.9141 Recon Loss: 4.9033 
[12/23 13:30:59 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000098 Step: 92400 Total Loss: 4.8972 Recon Loss: 4.8865 
[12/23 13:31:40 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000098 Step: 92450 Total Loss: 4.8356 Recon Loss: 4.8247 
[12/23 13:32:21 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000098 Step: 92500 Total Loss: 4.2832 Recon Loss: 4.2724 
[12/23 13:33:01 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8162 LR: 0.000098 Step: 92550 Total Loss: 4.2481 Recon Loss: 4.2373 
[12/23 13:33:42 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000098 Step: 92600 Total Loss: 4.8683 Recon Loss: 4.8575 
[12/23 13:34:23 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000098 Step: 92650 Total Loss: 4.1995 Recon Loss: 4.1887 
[12/23 13:35:04 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000098 Step: 92700 Total Loss: 4.1989 Recon Loss: 4.1882 
[12/23 13:35:45 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000098 Step: 92750 Total Loss: 4.9650 Recon Loss: 4.9541 
[12/23 13:36:26 TiTok]: Data (t): 0.0009, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000098 Step: 92800 Total Loss: 5.5707 Recon Loss: 5.5599 
[12/23 13:37:07 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000098 Step: 92850 Total Loss: 5.5111 Recon Loss: 5.5003 
[12/23 13:37:48 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000098 Step: 92900 Total Loss: 5.5648 Recon Loss: 5.5540 
[12/23 13:38:28 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8200 LR: 0.000098 Step: 92950 Total Loss: 4.3340 Recon Loss: 4.3232 
[12/23 13:39:09 TiTok]: Data (t): 0.0010, 34.70/s/gpu Batch (t): 0.9222 LR: 0.000098 Step: 93000 Total Loss: 4.8184 Recon Loss: 4.8077 
[12/23 13:39:50 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000098 Step: 93050 Total Loss: 4.2582 Recon Loss: 4.2475 
[12/23 13:40:31 TiTok]: Data (t): 0.0011, 38.98/s/gpu Batch (t): 0.8209 LR: 0.000098 Step: 93100 Total Loss: 4.3372 Recon Loss: 4.3264 
[12/23 13:41:12 TiTok]: Data (t): 0.0011, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000098 Step: 93150 Total Loss: 4.2611 Recon Loss: 4.2504 
[12/23 13:41:53 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000098 Step: 93200 Total Loss: 4.2808 Recon Loss: 4.2701 
[12/23 13:42:34 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000098 Step: 93250 Total Loss: 4.8752 Recon Loss: 4.8643 
[12/23 13:43:15 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8234 LR: 0.000098 Step: 93300 Total Loss: 4.9056 Recon Loss: 4.8948 
[12/23 13:43:56 TiTok]: Data (t): 0.0011, 39.43/s/gpu Batch (t): 0.8116 LR: 0.000098 Step: 93350 Total Loss: 5.5786 Recon Loss: 5.5678 
[12/23 13:44:37 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000098 Step: 93400 Total Loss: 4.3246 Recon Loss: 4.3138 
[12/23 13:45:17 TiTok]: Data (t): 0.0011, 38.83/s/gpu Batch (t): 0.8240 LR: 0.000098 Step: 93450 Total Loss: 4.9243 Recon Loss: 4.9135 
[12/23 13:45:58 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8185 LR: 0.000098 Step: 93500 Total Loss: 4.8864 Recon Loss: 4.8756 
[12/23 13:46:39 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000098 Step: 93550 Total Loss: 4.2461 Recon Loss: 4.2354 
[12/23 13:47:20 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000098 Step: 93600 Total Loss: 6.1660 Recon Loss: 6.1553 
[12/23 13:48:01 TiTok]: Data (t): 0.0011, 39.33/s/gpu Batch (t): 0.8137 LR: 0.000098 Step: 93650 Total Loss: 4.3115 Recon Loss: 4.3008 
[12/23 13:48:42 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8150 LR: 0.000098 Step: 93700 Total Loss: 4.2859 Recon Loss: 4.2751 
[12/23 13:49:23 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000098 Step: 93750 Total Loss: 5.5954 Recon Loss: 5.5847 
[12/23 13:50:04 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8154 LR: 0.000098 Step: 93800 Total Loss: 4.8692 Recon Loss: 4.8584 
[12/23 13:50:45 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000098 Step: 93850 Total Loss: 4.2975 Recon Loss: 4.2867 
[12/23 13:51:25 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000098 Step: 93900 Total Loss: 4.9177 Recon Loss: 4.9070 
[12/23 13:52:06 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000098 Step: 93950 Total Loss: 4.9268 Recon Loss: 4.9159 
[12/23 13:52:47 TiTok]: Data (t): 0.0010, 35.14/s/gpu Batch (t): 0.9107 LR: 0.000098 Step: 94000 Total Loss: 4.9519 Recon Loss: 4.9411 
[12/23 13:53:28 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000098 Step: 94050 Total Loss: 4.9225 Recon Loss: 4.9118 
[12/23 13:54:09 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000098 Step: 94100 Total Loss: 5.5149 Recon Loss: 5.5041 
[12/23 13:54:50 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000098 Step: 94150 Total Loss: 5.5320 Recon Loss: 5.5212 
[12/23 13:55:31 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000098 Step: 94200 Total Loss: 4.2958 Recon Loss: 4.2851 
[12/23 13:56:12 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000098 Step: 94250 Total Loss: 4.2868 Recon Loss: 4.2759 
[12/23 13:56:53 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8217 LR: 0.000098 Step: 94300 Total Loss: 4.8534 Recon Loss: 4.8426 
[12/23 13:57:34 TiTok]: Data (t): 0.0011, 39.38/s/gpu Batch (t): 0.8125 LR: 0.000098 Step: 94350 Total Loss: 4.9101 Recon Loss: 4.8994 
[12/23 13:58:14 TiTok]: Data (t): 0.0011, 38.83/s/gpu Batch (t): 0.8240 LR: 0.000098 Step: 94400 Total Loss: 5.5460 Recon Loss: 5.5352 
[12/23 13:58:55 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000098 Step: 94450 Total Loss: 4.2360 Recon Loss: 4.2252 
[12/23 13:59:36 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000098 Step: 94500 Total Loss: 5.5521 Recon Loss: 5.5413 
[12/23 14:00:17 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000098 Step: 94550 Total Loss: 4.8992 Recon Loss: 4.8884 
[12/23 14:00:58 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000098 Step: 94600 Total Loss: 4.8954 Recon Loss: 4.8846 
[12/23 14:01:39 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000098 Step: 94650 Total Loss: 4.2787 Recon Loss: 4.2679 
[12/23 14:02:20 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000098 Step: 94700 Total Loss: 4.9003 Recon Loss: 4.8895 
[12/23 14:03:01 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000098 Step: 94750 Total Loss: 4.9331 Recon Loss: 4.9224 
[12/23 14:03:42 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8223 LR: 0.000098 Step: 94800 Total Loss: 5.6045 Recon Loss: 5.5938 
[12/23 14:04:22 TiTok]: Data (t): 0.0010, 38.79/s/gpu Batch (t): 0.8250 LR: 0.000098 Step: 94850 Total Loss: 4.8511 Recon Loss: 4.8403 
[12/23 14:05:03 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8235 LR: 0.000098 Step: 94900 Total Loss: 4.2781 Recon Loss: 4.2673 
[12/23 14:05:44 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8150 LR: 0.000098 Step: 94950 Total Loss: 5.5170 Recon Loss: 5.5062 
[12/23 14:06:25 TiTok]: Data (t): 0.0010, 34.94/s/gpu Batch (t): 0.9158 LR: 0.000098 Step: 95000 Total Loss: 4.3063 Recon Loss: 4.2955 
[12/23 14:06:26 TiTok]: Reconstructing images...
[12/23 14:07:08 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8152 LR: 0.000098 Step: 95050 Total Loss: 5.5645 Recon Loss: 5.5537 
[12/23 14:07:49 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000098 Step: 95100 Total Loss: 5.5372 Recon Loss: 5.5264 
[12/23 14:08:29 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000098 Step: 95150 Total Loss: 4.9825 Recon Loss: 4.9717 
[12/23 14:09:10 TiTok]: Data (t): 0.0011, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000098 Step: 95200 Total Loss: 4.8779 Recon Loss: 4.8671 
[12/23 14:09:51 TiTok]: Data (t): 0.0010, 38.79/s/gpu Batch (t): 0.8250 LR: 0.000098 Step: 95250 Total Loss: 4.2899 Recon Loss: 4.2791 
[12/23 14:10:32 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000098 Step: 95300 Total Loss: 4.9351 Recon Loss: 4.9243 
[12/23 14:11:13 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000098 Step: 95350 Total Loss: 4.9184 Recon Loss: 4.9077 
[12/23 14:11:54 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8158 LR: 0.000098 Step: 95400 Total Loss: 4.9120 Recon Loss: 4.9012 
[12/23 14:12:35 TiTok]: Data (t): 0.0013, 39.46/s/gpu Batch (t): 0.8110 LR: 0.000098 Step: 95450 Total Loss: 4.8782 Recon Loss: 4.8673 
[12/23 14:13:16 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8137 LR: 0.000098 Step: 95500 Total Loss: 4.9414 Recon Loss: 4.9307 
[12/23 14:13:57 TiTok]: Data (t): 0.0010, 38.82/s/gpu Batch (t): 0.8243 LR: 0.000098 Step: 95550 Total Loss: 4.8804 Recon Loss: 4.8697 
[12/23 14:14:37 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000098 Step: 95600 Total Loss: 4.3147 Recon Loss: 4.3038 
[12/23 14:15:18 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8177 LR: 0.000098 Step: 95650 Total Loss: 4.3289 Recon Loss: 4.3182 
[12/23 14:15:59 TiTok]: Data (t): 0.0011, 39.42/s/gpu Batch (t): 0.8118 LR: 0.000098 Step: 95700 Total Loss: 4.1977 Recon Loss: 4.1870 
[12/23 14:16:40 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8204 LR: 0.000098 Step: 95750 Total Loss: 4.2795 Recon Loss: 4.2688 
[12/23 14:17:21 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000098 Step: 95800 Total Loss: 5.5501 Recon Loss: 5.5392 
[12/23 14:18:02 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8150 LR: 0.000098 Step: 95850 Total Loss: 4.8646 Recon Loss: 4.8538 
[12/23 14:18:43 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000098 Step: 95900 Total Loss: 4.9108 Recon Loss: 4.8999 
[12/23 14:19:24 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000098 Step: 95950 Total Loss: 5.4572 Recon Loss: 5.4464 
[12/23 14:20:05 TiTok]: Data (t): 0.0010, 34.93/s/gpu Batch (t): 0.9160 LR: 0.000098 Step: 96000 Total Loss: 4.2751 Recon Loss: 4.2643 
[12/23 14:20:45 TiTok]: Data (t): 0.0010, 38.77/s/gpu Batch (t): 0.8254 LR: 0.000098 Step: 96050 Total Loss: 4.2640 Recon Loss: 4.2532 
[12/23 14:21:26 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000098 Step: 96100 Total Loss: 4.2479 Recon Loss: 4.2371 
[12/23 14:22:07 TiTok]: Data (t): 0.0010, 38.74/s/gpu Batch (t): 0.8259 LR: 0.000098 Step: 96150 Total Loss: 5.5603 Recon Loss: 5.5496 
[12/23 14:22:48 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8154 LR: 0.000098 Step: 96200 Total Loss: 5.5890 Recon Loss: 5.5782 
[12/23 14:23:29 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000098 Step: 96250 Total Loss: 5.5907 Recon Loss: 5.5798 
[12/23 14:24:10 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000098 Step: 96300 Total Loss: 4.9824 Recon Loss: 4.9717 
[12/23 14:24:51 TiTok]: Data (t): 0.0010, 39.45/s/gpu Batch (t): 0.8111 LR: 0.000098 Step: 96350 Total Loss: 5.5428 Recon Loss: 5.5321 
[12/23 14:25:32 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000098 Step: 96400 Total Loss: 4.2452 Recon Loss: 4.2344 
[12/23 14:26:13 TiTok]: Data (t): 0.0010, 38.85/s/gpu Batch (t): 0.8238 LR: 0.000098 Step: 96450 Total Loss: 4.9513 Recon Loss: 4.9405 
[12/23 14:26:54 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000098 Step: 96500 Total Loss: 4.2593 Recon Loss: 4.2485 
[12/23 14:27:34 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000098 Step: 96550 Total Loss: 4.2580 Recon Loss: 4.2472 
[12/23 14:28:15 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000098 Step: 96600 Total Loss: 5.5027 Recon Loss: 5.4919 
[12/23 14:28:56 TiTok]: Data (t): 0.0010, 38.73/s/gpu Batch (t): 0.8262 LR: 0.000098 Step: 96650 Total Loss: 4.8723 Recon Loss: 4.8616 
[12/23 14:29:37 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000098 Step: 96700 Total Loss: 4.2813 Recon Loss: 4.2705 
[12/23 14:30:18 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8206 LR: 0.000098 Step: 96750 Total Loss: 4.9045 Recon Loss: 4.8937 
[12/23 14:30:59 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8162 LR: 0.000098 Step: 96800 Total Loss: 5.5572 Recon Loss: 5.5464 
[12/23 14:31:40 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000098 Step: 96850 Total Loss: 4.2685 Recon Loss: 4.2577 
[12/23 14:32:21 TiTok]: Data (t): 0.0010, 39.40/s/gpu Batch (t): 0.8122 LR: 0.000098 Step: 96900 Total Loss: 6.1546 Recon Loss: 6.1438 
[12/23 14:33:02 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000098 Step: 96950 Total Loss: 6.1521 Recon Loss: 6.1414 
[12/23 14:33:43 TiTok]: Data (t): 0.0010, 35.59/s/gpu Batch (t): 0.8990 LR: 0.000098 Step: 97000 Total Loss: 6.2059 Recon Loss: 6.1951 
[12/23 14:34:23 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000098 Step: 97050 Total Loss: 4.8762 Recon Loss: 4.8654 
[12/23 14:35:04 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000098 Step: 97100 Total Loss: 6.1588 Recon Loss: 6.1481 
[12/23 14:35:45 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8183 LR: 0.000098 Step: 97150 Total Loss: 4.3050 Recon Loss: 4.2942 
[12/23 14:36:26 TiTok]: Data (t): 0.0010, 39.37/s/gpu Batch (t): 0.8129 LR: 0.000098 Step: 97200 Total Loss: 4.8522 Recon Loss: 4.8414 
[12/23 14:37:07 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000098 Step: 97250 Total Loss: 6.1740 Recon Loss: 6.1632 
[12/23 14:37:48 TiTok]: Data (t): 0.0009, 39.26/s/gpu Batch (t): 0.8150 LR: 0.000098 Step: 97300 Total Loss: 6.1950 Recon Loss: 6.1843 
[12/23 14:38:29 TiTok]: Data (t): 0.0010, 39.40/s/gpu Batch (t): 0.8121 LR: 0.000098 Step: 97350 Total Loss: 4.9209 Recon Loss: 4.9101 
[12/23 14:39:10 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000098 Step: 97400 Total Loss: 4.2731 Recon Loss: 4.2624 
[12/23 14:39:51 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000098 Step: 97450 Total Loss: 6.1836 Recon Loss: 6.1728 
[12/23 14:40:31 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000098 Step: 97500 Total Loss: 4.9153 Recon Loss: 4.9045 
[12/23 14:41:12 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000098 Step: 97550 Total Loss: 5.5955 Recon Loss: 5.5847 
[12/23 14:41:53 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000098 Step: 97600 Total Loss: 4.3380 Recon Loss: 4.3272 
[12/23 14:42:34 TiTok]: Data (t): 0.0011, 39.61/s/gpu Batch (t): 0.8079 LR: 0.000098 Step: 97650 Total Loss: 4.9143 Recon Loss: 4.9035 
[12/23 14:43:15 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000098 Step: 97700 Total Loss: 6.1507 Recon Loss: 6.1400 
[12/23 14:43:56 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000098 Step: 97750 Total Loss: 4.2286 Recon Loss: 4.2179 
[12/23 14:44:37 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000098 Step: 97800 Total Loss: 4.9150 Recon Loss: 4.9043 
[12/23 14:45:18 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000098 Step: 97850 Total Loss: 5.4946 Recon Loss: 5.4838 
[12/23 14:45:59 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8234 LR: 0.000098 Step: 97900 Total Loss: 4.9284 Recon Loss: 4.9176 
[12/23 14:46:39 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000098 Step: 97950 Total Loss: 4.2647 Recon Loss: 4.2539 
[12/23 14:47:20 TiTok]: Data (t): 0.0010, 35.16/s/gpu Batch (t): 0.9102 LR: 0.000098 Step: 98000 Total Loss: 4.2724 Recon Loss: 4.2615 
[12/23 14:48:01 TiTok]: Data (t): 0.0011, 39.41/s/gpu Batch (t): 0.8121 LR: 0.000098 Step: 98050 Total Loss: 4.9211 Recon Loss: 4.9103 
[12/23 14:48:42 TiTok]: Data (t): 0.0010, 38.84/s/gpu Batch (t): 0.8239 LR: 0.000098 Step: 98100 Total Loss: 4.9058 Recon Loss: 4.8951 
[12/23 14:49:23 TiTok]: Data (t): 0.0033, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000098 Step: 98150 Total Loss: 5.5139 Recon Loss: 5.5031 
[12/23 14:50:04 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8148 LR: 0.000098 Step: 98200 Total Loss: 4.9327 Recon Loss: 4.9219 
[12/23 14:50:45 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000098 Step: 98250 Total Loss: 4.9143 Recon Loss: 4.9035 
[12/23 14:51:26 TiTok]: Data (t): 0.0011, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000098 Step: 98300 Total Loss: 4.8906 Recon Loss: 4.8797 
[12/23 14:52:07 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000098 Step: 98350 Total Loss: 4.8757 Recon Loss: 4.8649 
[12/23 14:52:47 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8217 LR: 0.000098 Step: 98400 Total Loss: 4.3446 Recon Loss: 4.3338 
[12/23 14:53:28 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000098 Step: 98450 Total Loss: 4.2813 Recon Loss: 4.2705 
[12/23 14:54:09 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000098 Step: 98500 Total Loss: 4.2690 Recon Loss: 4.2582 
[12/23 14:54:50 TiTok]: Data (t): 0.0010, 38.82/s/gpu Batch (t): 0.8243 LR: 0.000098 Step: 98550 Total Loss: 4.2536 Recon Loss: 4.2429 
[12/23 14:55:31 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000098 Step: 98600 Total Loss: 4.2474 Recon Loss: 4.2365 
[12/23 14:56:12 TiTok]: Data (t): 0.0011, 38.87/s/gpu Batch (t): 0.8232 LR: 0.000098 Step: 98650 Total Loss: 4.9402 Recon Loss: 4.9294 
[12/23 14:56:53 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8185 LR: 0.000098 Step: 98700 Total Loss: 4.3472 Recon Loss: 4.3365 
[12/23 14:57:34 TiTok]: Data (t): 0.0011, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000098 Step: 98750 Total Loss: 6.1839 Recon Loss: 6.1731 
[12/23 14:58:15 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000098 Step: 98800 Total Loss: 4.8478 Recon Loss: 4.8370 
[12/23 14:58:55 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000098 Step: 98850 Total Loss: 4.8703 Recon Loss: 4.8595 
[12/23 14:59:36 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8152 LR: 0.000098 Step: 98900 Total Loss: 4.2481 Recon Loss: 4.2373 
[12/23 15:00:17 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000098 Step: 98950 Total Loss: 4.9419 Recon Loss: 4.9311 
[12/23 15:00:58 TiTok]: Data (t): 0.0010, 35.37/s/gpu Batch (t): 0.9048 LR: 0.000098 Step: 99000 Total Loss: 4.8884 Recon Loss: 4.8775 
[12/23 15:01:39 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8229 LR: 0.000098 Step: 99050 Total Loss: 5.5569 Recon Loss: 5.5461 
[12/23 15:02:20 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000098 Step: 99100 Total Loss: 5.5623 Recon Loss: 5.5515 
[12/23 15:03:01 TiTok]: Data (t): 0.0011, 39.43/s/gpu Batch (t): 0.8115 LR: 0.000098 Step: 99150 Total Loss: 5.5539 Recon Loss: 5.5432 
[12/23 15:03:42 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8183 LR: 0.000098 Step: 99200 Total Loss: 4.9122 Recon Loss: 4.9014 
[12/23 15:04:22 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000098 Step: 99250 Total Loss: 4.9123 Recon Loss: 4.9015 
[12/23 15:05:03 TiTok]: Data (t): 0.0013, 39.33/s/gpu Batch (t): 0.8135 LR: 0.000098 Step: 99300 Total Loss: 4.9105 Recon Loss: 4.8997 
[12/23 15:05:44 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000098 Step: 99350 Total Loss: 4.9125 Recon Loss: 4.9016 
[12/23 15:06:25 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000098 Step: 99400 Total Loss: 4.2691 Recon Loss: 4.2583 
[12/23 15:07:06 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8133 LR: 0.000098 Step: 99450 Total Loss: 6.2078 Recon Loss: 6.1969 
[12/23 15:07:47 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000098 Step: 99500 Total Loss: 4.2432 Recon Loss: 4.2324 
[12/23 15:08:28 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000098 Step: 99550 Total Loss: 5.5632 Recon Loss: 5.5525 
[12/23 15:09:09 TiTok]: Data (t): 0.0011, 39.42/s/gpu Batch (t): 0.8117 LR: 0.000098 Step: 99600 Total Loss: 4.9512 Recon Loss: 4.9404 
[12/23 15:09:49 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000098 Step: 99650 Total Loss: 5.5274 Recon Loss: 5.5165 
[12/23 15:10:30 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8152 LR: 0.000098 Step: 99700 Total Loss: 4.9101 Recon Loss: 4.8993 
[12/23 15:11:11 TiTok]: Data (t): 0.0011, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000098 Step: 99750 Total Loss: 4.9181 Recon Loss: 4.9074 
[12/23 15:11:52 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000098 Step: 99800 Total Loss: 4.8627 Recon Loss: 4.8519 
[12/23 15:12:33 TiTok]: Data (t): 0.0010, 38.80/s/gpu Batch (t): 0.8246 LR: 0.000098 Step: 99850 Total Loss: 4.2828 Recon Loss: 4.2720 
[12/23 15:13:14 TiTok]: Data (t): 0.0011, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000098 Step: 99900 Total Loss: 4.2882 Recon Loss: 4.2773 
[12/23 15:13:55 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000098 Step: 99950 Total Loss: 5.5412 Recon Loss: 5.5305 
[12/23 15:14:36 TiTok]: Data (t): 0.0010, 35.68/s/gpu Batch (t): 0.8968 LR: 0.000098 Step: 100000 Total Loss: 4.9377 Recon Loss: 4.9268 
Model weights saved in titok_b64_stage1_run1/checkpoint-100000/unwrapped_model/pytorch_model.bin
[12/23 15:14:37 TiTok]: Saved state to titok_b64_stage1_run1/checkpoint-100000
Model weights saved in titok_b64_stage1_run1/checkpoint-100000/ema_model/pytorch_model.bin
[12/23 15:14:55 TiTok]: Reconstructing images...
[12/23 15:14:55 TiTok]: Computing metrics on the validation set.
[12/23 15:29:35 TiTok]: EMA EVALUATION Step: 100000 
[12/23 15:29:35 TiTok]: {'CodebookEntropy': tensor(11.5693, device='cuda:0', dtype=torch.float64),
 'CodebookUsage': 0.746826171875,
 'InceptionScore': 36.83581000301445,
 'rFID': 59.04257459395183}
[12/23 15:30:48 TiTok]: Data (t): 0.0011, 39.81/s/gpu Batch (t): 0.8037 LR: 0.000098 Step: 100050 Total Loss: 4.2319 Recon Loss: 4.2211 
[12/23 15:31:29 TiTok]: Data (t): 0.0011, 39.78/s/gpu Batch (t): 0.8044 LR: 0.000098 Step: 100100 Total Loss: 4.9508 Recon Loss: 4.9401 
[12/23 15:32:10 TiTok]: Data (t): 0.0010, 38.82/s/gpu Batch (t): 0.8243 LR: 0.000098 Step: 100150 Total Loss: 4.2081 Recon Loss: 4.1973 
[12/23 15:32:50 TiTok]: Data (t): 0.0010, 39.42/s/gpu Batch (t): 0.8117 LR: 0.000098 Step: 100200 Total Loss: 4.8841 Recon Loss: 4.8733 
Epoch 10/99 started.
[12/23 15:33:32 TiTok]: Data (t): 0.0010, 38.67/s/gpu Batch (t): 0.8275 LR: 0.000098 Step: 100250 Total Loss: 4.8592 Recon Loss: 4.8484 
[12/23 15:34:13 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000098 Step: 100300 Total Loss: 5.4770 Recon Loss: 5.4662 
[12/23 15:34:54 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000098 Step: 100350 Total Loss: 4.9477 Recon Loss: 4.9369 
[12/23 15:35:35 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000098 Step: 100400 Total Loss: 4.2565 Recon Loss: 4.2457 
[12/23 15:36:16 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000098 Step: 100450 Total Loss: 6.1905 Recon Loss: 6.1797 
[12/23 15:36:57 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000098 Step: 100500 Total Loss: 4.9201 Recon Loss: 4.9094 
[12/23 15:37:37 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8229 LR: 0.000098 Step: 100550 Total Loss: 4.2459 Recon Loss: 4.2351 
[12/23 15:38:18 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000098 Step: 100600 Total Loss: 4.2551 Recon Loss: 4.2443 
[12/23 15:38:59 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000098 Step: 100650 Total Loss: 4.2791 Recon Loss: 4.2682 
[12/23 15:39:40 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000098 Step: 100700 Total Loss: 4.9041 Recon Loss: 4.8933 
[12/23 15:40:21 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000098 Step: 100750 Total Loss: 4.1749 Recon Loss: 4.1641 
[12/23 15:41:02 TiTok]: Data (t): 0.0010, 39.39/s/gpu Batch (t): 0.8124 LR: 0.000098 Step: 100800 Total Loss: 4.2098 Recon Loss: 4.1990 
[12/23 15:41:43 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000098 Step: 100850 Total Loss: 4.9306 Recon Loss: 4.9198 
[12/23 15:42:23 TiTok]: Data (t): 0.0011, 39.35/s/gpu Batch (t): 0.8133 LR: 0.000098 Step: 100900 Total Loss: 4.8873 Recon Loss: 4.8765 
[12/23 15:43:04 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000098 Step: 100950 Total Loss: 5.5131 Recon Loss: 5.5022 
[12/23 15:43:45 TiTok]: Data (t): 0.0010, 33.92/s/gpu Batch (t): 0.9433 LR: 0.000098 Step: 101000 Total Loss: 4.9117 Recon Loss: 4.9008 
[12/23 15:44:26 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000098 Step: 101050 Total Loss: 4.9217 Recon Loss: 4.9110 
[12/23 15:45:07 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8158 LR: 0.000098 Step: 101100 Total Loss: 4.3302 Recon Loss: 4.3194 
[12/23 15:45:48 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000098 Step: 101150 Total Loss: 4.2456 Recon Loss: 4.2348 
[12/23 15:46:29 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000098 Step: 101200 Total Loss: 4.2745 Recon Loss: 4.2636 
[12/23 15:47:09 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000098 Step: 101250 Total Loss: 4.3117 Recon Loss: 4.3008 
[12/23 15:47:50 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000098 Step: 101300 Total Loss: 4.8681 Recon Loss: 4.8572 
[12/23 15:48:31 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8156 LR: 0.000098 Step: 101350 Total Loss: 4.2078 Recon Loss: 4.1970 
[12/23 15:49:12 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000098 Step: 101400 Total Loss: 4.2251 Recon Loss: 4.2142 
[12/23 15:49:53 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000098 Step: 101450 Total Loss: 4.9461 Recon Loss: 4.9353 
[12/23 15:50:34 TiTok]: Data (t): 0.0010, 38.78/s/gpu Batch (t): 0.8251 LR: 0.000098 Step: 101500 Total Loss: 4.9066 Recon Loss: 4.8958 
[12/23 15:51:15 TiTok]: Data (t): 0.0011, 38.84/s/gpu Batch (t): 0.8238 LR: 0.000098 Step: 101550 Total Loss: 4.9351 Recon Loss: 4.9242 
[12/23 15:51:56 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000098 Step: 101600 Total Loss: 4.9264 Recon Loss: 4.9156 
[12/23 15:52:37 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000098 Step: 101650 Total Loss: 4.8646 Recon Loss: 4.8538 
[12/23 15:53:17 TiTok]: Data (t): 0.0010, 39.42/s/gpu Batch (t): 0.8118 LR: 0.000098 Step: 101700 Total Loss: 5.5578 Recon Loss: 5.5470 
[12/23 15:53:58 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8162 LR: 0.000098 Step: 101750 Total Loss: 4.2013 Recon Loss: 4.1905 
[12/23 15:54:39 TiTok]: Data (t): 0.0011, 38.72/s/gpu Batch (t): 0.8265 LR: 0.000098 Step: 101800 Total Loss: 4.2594 Recon Loss: 4.2485 
[12/23 15:55:20 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000098 Step: 101850 Total Loss: 4.2099 Recon Loss: 4.1991 
[12/23 15:56:01 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000098 Step: 101900 Total Loss: 4.2074 Recon Loss: 4.1966 
[12/23 15:56:42 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000098 Step: 101950 Total Loss: 5.5227 Recon Loss: 5.5119 
[12/23 15:57:23 TiTok]: Data (t): 0.0010, 35.24/s/gpu Batch (t): 0.9081 LR: 0.000098 Step: 102000 Total Loss: 6.1435 Recon Loss: 6.1327 
[12/23 15:58:04 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000098 Step: 102050 Total Loss: 4.2384 Recon Loss: 4.2275 
[12/23 15:58:44 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000098 Step: 102100 Total Loss: 4.2870 Recon Loss: 4.2761 
[12/23 15:59:25 TiTok]: Data (t): 0.0033, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000098 Step: 102150 Total Loss: 4.9072 Recon Loss: 4.8964 
[12/23 16:00:06 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000098 Step: 102200 Total Loss: 4.9280 Recon Loss: 4.9172 
[12/23 16:00:47 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000098 Step: 102250 Total Loss: 4.8741 Recon Loss: 4.8633 
[12/23 16:01:28 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000098 Step: 102300 Total Loss: 5.4998 Recon Loss: 5.4889 
[12/23 16:02:09 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000098 Step: 102350 Total Loss: 4.8750 Recon Loss: 4.8642 
[12/23 16:02:50 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000098 Step: 102400 Total Loss: 4.7801 Recon Loss: 4.7693 
[12/23 16:03:31 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000098 Step: 102450 Total Loss: 4.1824 Recon Loss: 4.1716 
[12/23 16:04:11 TiTok]: Data (t): 0.0010, 39.37/s/gpu Batch (t): 0.8127 LR: 0.000098 Step: 102500 Total Loss: 5.5261 Recon Loss: 5.5153 
[12/23 16:04:52 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000098 Step: 102550 Total Loss: 5.5358 Recon Loss: 5.5250 
[12/23 16:05:33 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000098 Step: 102600 Total Loss: 4.2053 Recon Loss: 4.1944 
[12/23 16:06:14 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000098 Step: 102650 Total Loss: 4.2367 Recon Loss: 4.2258 
[12/23 16:06:55 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000098 Step: 102700 Total Loss: 4.2525 Recon Loss: 4.2417 
[12/23 16:07:36 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8206 LR: 0.000098 Step: 102750 Total Loss: 4.2316 Recon Loss: 4.2208 
[12/23 16:08:17 TiTok]: Data (t): 0.0011, 39.37/s/gpu Batch (t): 0.8127 LR: 0.000098 Step: 102800 Total Loss: 4.8190 Recon Loss: 4.8082 
[12/23 16:08:58 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000098 Step: 102850 Total Loss: 4.9306 Recon Loss: 4.9197 
[12/23 16:09:38 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000098 Step: 102900 Total Loss: 4.8240 Recon Loss: 4.8131 
[12/23 16:10:19 TiTok]: Data (t): 0.0013, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000098 Step: 102950 Total Loss: 5.5391 Recon Loss: 5.5282 
[12/23 16:11:00 TiTok]: Data (t): 0.0011, 35.42/s/gpu Batch (t): 0.9036 LR: 0.000098 Step: 103000 Total Loss: 4.8428 Recon Loss: 4.8320 
[12/23 16:11:41 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8219 LR: 0.000098 Step: 103050 Total Loss: 4.2478 Recon Loss: 4.2370 
[12/23 16:12:22 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000098 Step: 103100 Total Loss: 5.4722 Recon Loss: 5.4614 
[12/23 16:13:03 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000098 Step: 103150 Total Loss: 5.5219 Recon Loss: 5.5110 
[12/23 16:13:44 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8217 LR: 0.000098 Step: 103200 Total Loss: 4.2669 Recon Loss: 4.2561 
[12/23 16:14:25 TiTok]: Data (t): 0.0011, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000098 Step: 103250 Total Loss: 4.8889 Recon Loss: 4.8780 
[12/23 16:15:06 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000098 Step: 103300 Total Loss: 5.5728 Recon Loss: 5.5619 
[12/23 16:15:46 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000098 Step: 103350 Total Loss: 5.5201 Recon Loss: 5.5093 
[12/23 16:16:27 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000098 Step: 103400 Total Loss: 4.2413 Recon Loss: 4.2306 
[12/23 16:17:08 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000098 Step: 103450 Total Loss: 4.9567 Recon Loss: 4.9459 
[12/23 16:17:49 TiTok]: Data (t): 0.0011, 39.38/s/gpu Batch (t): 0.8127 LR: 0.000098 Step: 103500 Total Loss: 4.9153 Recon Loss: 4.9044 
[12/23 16:18:30 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000098 Step: 103550 Total Loss: 4.2596 Recon Loss: 4.2488 
[12/23 16:19:11 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000098 Step: 103600 Total Loss: 4.9044 Recon Loss: 4.8936 
[12/23 16:19:52 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000098 Step: 103650 Total Loss: 4.2846 Recon Loss: 4.2737 
[12/23 16:20:32 TiTok]: Data (t): 0.0011, 39.48/s/gpu Batch (t): 0.8106 LR: 0.000098 Step: 103700 Total Loss: 4.8885 Recon Loss: 4.8776 
[12/23 16:21:13 TiTok]: Data (t): 0.0011, 39.45/s/gpu Batch (t): 0.8111 LR: 0.000098 Step: 103750 Total Loss: 5.5398 Recon Loss: 5.5290 
[12/23 16:21:54 TiTok]: Data (t): 0.0011, 38.66/s/gpu Batch (t): 0.8276 LR: 0.000098 Step: 103800 Total Loss: 4.8897 Recon Loss: 4.8789 
[12/23 16:22:35 TiTok]: Data (t): 0.0010, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000098 Step: 103850 Total Loss: 4.9324 Recon Loss: 4.9215 
[12/23 16:23:16 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000098 Step: 103900 Total Loss: 6.1994 Recon Loss: 6.1885 
[12/23 16:23:57 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000098 Step: 103950 Total Loss: 4.1529 Recon Loss: 4.1421 
[12/23 16:24:38 TiTok]: Data (t): 0.0012, 35.05/s/gpu Batch (t): 0.9129 LR: 0.000098 Step: 104000 Total Loss: 4.2337 Recon Loss: 4.2228 
[12/23 16:25:19 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000098 Step: 104050 Total Loss: 5.4659 Recon Loss: 5.4550 
[12/23 16:25:59 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000098 Step: 104100 Total Loss: 4.8926 Recon Loss: 4.8818 
[12/23 16:26:40 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8217 LR: 0.000098 Step: 104150 Total Loss: 4.8978 Recon Loss: 4.8870 
[12/23 16:27:21 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8185 LR: 0.000098 Step: 104200 Total Loss: 4.2630 Recon Loss: 4.2521 
[12/23 16:28:02 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000098 Step: 104250 Total Loss: 4.2895 Recon Loss: 4.2785 
[12/23 16:28:43 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000098 Step: 104300 Total Loss: 4.2835 Recon Loss: 4.2726 
[12/23 16:29:24 TiTok]: Data (t): 0.0011, 39.53/s/gpu Batch (t): 0.8095 LR: 0.000098 Step: 104350 Total Loss: 5.5867 Recon Loss: 5.5758 
[12/23 16:30:05 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000098 Step: 104400 Total Loss: 4.2338 Recon Loss: 4.2229 
[12/23 16:30:46 TiTok]: Data (t): 0.0011, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000098 Step: 104450 Total Loss: 4.9130 Recon Loss: 4.9022 
[12/23 16:31:26 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000098 Step: 104500 Total Loss: 4.2027 Recon Loss: 4.1919 
[12/23 16:32:07 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000098 Step: 104550 Total Loss: 4.8296 Recon Loss: 4.8187 
[12/23 16:32:48 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000098 Step: 104600 Total Loss: 4.9549 Recon Loss: 4.9440 
[12/23 16:33:29 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000098 Step: 104650 Total Loss: 4.2328 Recon Loss: 4.2219 
[12/23 16:34:10 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000098 Step: 104700 Total Loss: 4.9194 Recon Loss: 4.9086 
[12/23 16:34:51 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000098 Step: 104750 Total Loss: 4.8836 Recon Loss: 4.8726 
[12/23 16:35:32 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8162 LR: 0.000098 Step: 104800 Total Loss: 4.1799 Recon Loss: 4.1691 
[12/23 16:36:13 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000098 Step: 104850 Total Loss: 5.5456 Recon Loss: 5.5347 
[12/23 16:36:53 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000098 Step: 104900 Total Loss: 4.8918 Recon Loss: 4.8809 
[12/23 16:37:34 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8225 LR: 0.000098 Step: 104950 Total Loss: 5.5546 Recon Loss: 5.5437 
[12/23 16:38:15 TiTok]: Data (t): 0.0011, 35.39/s/gpu Batch (t): 0.9041 LR: 0.000098 Step: 105000 Total Loss: 5.5726 Recon Loss: 5.5616 
[12/23 16:38:16 TiTok]: Reconstructing images...
[12/23 16:38:58 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8162 LR: 0.000098 Step: 105050 Total Loss: 4.1659 Recon Loss: 4.1551 
[12/23 16:39:39 TiTok]: Data (t): 0.0012, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000098 Step: 105100 Total Loss: 4.9163 Recon Loss: 4.9054 
[12/23 16:40:20 TiTok]: Data (t): 0.0009, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000098 Step: 105150 Total Loss: 4.8916 Recon Loss: 4.8807 
[12/23 16:41:00 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8177 LR: 0.000098 Step: 105200 Total Loss: 4.8341 Recon Loss: 4.8232 
[12/23 16:41:41 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000098 Step: 105250 Total Loss: 4.8510 Recon Loss: 4.8402 
[12/23 16:42:22 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000098 Step: 105300 Total Loss: 4.2489 Recon Loss: 4.2380 
[12/23 16:43:03 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000098 Step: 105350 Total Loss: 4.8639 Recon Loss: 4.8530 
[12/23 16:43:44 TiTok]: Data (t): 0.0011, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000098 Step: 105400 Total Loss: 4.8974 Recon Loss: 4.8865 
[12/23 16:44:25 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8154 LR: 0.000098 Step: 105450 Total Loss: 4.9008 Recon Loss: 4.8899 
[12/23 16:45:06 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8202 LR: 0.000098 Step: 105500 Total Loss: 4.8964 Recon Loss: 4.8855 
[12/23 16:45:47 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000098 Step: 105550 Total Loss: 4.2942 Recon Loss: 4.2833 
[12/23 16:46:28 TiTok]: Data (t): 0.0011, 38.85/s/gpu Batch (t): 0.8237 LR: 0.000098 Step: 105600 Total Loss: 4.1848 Recon Loss: 4.1740 
[12/23 16:47:08 TiTok]: Data (t): 0.0011, 39.57/s/gpu Batch (t): 0.8087 LR: 0.000098 Step: 105650 Total Loss: 5.4928 Recon Loss: 5.4820 
[12/23 16:47:49 TiTok]: Data (t): 0.0011, 39.40/s/gpu Batch (t): 0.8121 LR: 0.000098 Step: 105700 Total Loss: 4.9163 Recon Loss: 4.9054 
[12/23 16:48:30 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000098 Step: 105750 Total Loss: 4.3168 Recon Loss: 4.3059 
[12/23 16:49:11 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8162 LR: 0.000098 Step: 105800 Total Loss: 6.1380 Recon Loss: 6.1272 
[12/23 16:49:52 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000098 Step: 105850 Total Loss: 4.1878 Recon Loss: 4.1769 
[12/23 16:50:33 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000098 Step: 105900 Total Loss: 4.1856 Recon Loss: 4.1748 
[12/23 16:51:13 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000098 Step: 105950 Total Loss: 5.5523 Recon Loss: 5.5415 
[12/23 16:51:54 TiTok]: Data (t): 0.0010, 34.85/s/gpu Batch (t): 0.9182 LR: 0.000098 Step: 106000 Total Loss: 4.8275 Recon Loss: 4.8166 
[12/23 16:52:35 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000098 Step: 106050 Total Loss: 4.2013 Recon Loss: 4.1904 
[12/23 16:53:16 TiTok]: Data (t): 0.0011, 39.48/s/gpu Batch (t): 0.8105 LR: 0.000098 Step: 106100 Total Loss: 4.2559 Recon Loss: 4.2450 
[12/23 16:53:57 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000098 Step: 106150 Total Loss: 4.8564 Recon Loss: 4.8455 
[12/23 16:54:38 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000098 Step: 106200 Total Loss: 5.5486 Recon Loss: 5.5377 
[12/23 16:55:19 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000098 Step: 106250 Total Loss: 5.5030 Recon Loss: 5.4921 
[12/23 16:56:00 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000098 Step: 106300 Total Loss: 4.9092 Recon Loss: 4.8983 
[12/23 16:56:40 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000098 Step: 106350 Total Loss: 4.9135 Recon Loss: 4.9026 
[12/23 16:57:21 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000098 Step: 106400 Total Loss: 4.2387 Recon Loss: 4.2278 
[12/23 16:58:02 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8181 LR: 0.000098 Step: 106450 Total Loss: 4.9479 Recon Loss: 4.9370 
[12/23 16:58:43 TiTok]: Data (t): 0.0011, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000098 Step: 106500 Total Loss: 5.5688 Recon Loss: 5.5579 
[12/23 16:59:24 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000098 Step: 106550 Total Loss: 5.5335 Recon Loss: 5.5226 
[12/23 17:00:05 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000098 Step: 106600 Total Loss: 4.2520 Recon Loss: 4.2411 
[12/23 17:00:45 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000098 Step: 106650 Total Loss: 5.5179 Recon Loss: 5.5069 
[12/23 17:01:26 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000098 Step: 106700 Total Loss: 5.5493 Recon Loss: 5.5384 
[12/23 17:02:07 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000098 Step: 106750 Total Loss: 4.2202 Recon Loss: 4.2092 
[12/23 17:02:48 TiTok]: Data (t): 0.0010, 39.49/s/gpu Batch (t): 0.8104 LR: 0.000098 Step: 106800 Total Loss: 4.8642 Recon Loss: 4.8533 
[12/23 17:03:29 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000098 Step: 106850 Total Loss: 5.5185 Recon Loss: 5.5076 
[12/23 17:04:10 TiTok]: Data (t): 0.0035, 39.39/s/gpu Batch (t): 0.8125 LR: 0.000098 Step: 106900 Total Loss: 4.8980 Recon Loss: 4.8871 
[12/23 17:04:50 TiTok]: Data (t): 0.0015, 39.74/s/gpu Batch (t): 0.8052 LR: 0.000098 Step: 106950 Total Loss: 4.2372 Recon Loss: 4.2262 
[12/23 17:05:31 TiTok]: Data (t): 0.0010, 35.09/s/gpu Batch (t): 0.9120 LR: 0.000098 Step: 107000 Total Loss: 4.8722 Recon Loss: 4.8613 
[12/23 17:06:12 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000098 Step: 107050 Total Loss: 4.8654 Recon Loss: 4.8544 
[12/23 17:06:53 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000098 Step: 107100 Total Loss: 4.8827 Recon Loss: 4.8717 
[12/23 17:07:34 TiTok]: Data (t): 0.0010, 38.57/s/gpu Batch (t): 0.8297 LR: 0.000098 Step: 107150 Total Loss: 4.9026 Recon Loss: 4.8918 
[12/23 17:08:15 TiTok]: Data (t): 0.0011, 38.98/s/gpu Batch (t): 0.8209 LR: 0.000098 Step: 107200 Total Loss: 4.8433 Recon Loss: 4.8324 
[12/23 17:08:56 TiTok]: Data (t): 0.0021, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000098 Step: 107250 Total Loss: 4.8510 Recon Loss: 4.8401 
[12/23 17:09:36 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000098 Step: 107300 Total Loss: 4.1906 Recon Loss: 4.1797 
[12/23 17:10:17 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000098 Step: 107350 Total Loss: 5.5286 Recon Loss: 5.5177 
[12/23 17:10:58 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000098 Step: 107400 Total Loss: 4.2733 Recon Loss: 4.2623 
[12/23 17:11:39 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000098 Step: 107450 Total Loss: 5.5166 Recon Loss: 5.5057 
[12/23 17:12:20 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000098 Step: 107500 Total Loss: 4.2024 Recon Loss: 4.1914 
[12/23 17:13:01 TiTok]: Data (t): 0.0011, 38.86/s/gpu Batch (t): 0.8234 LR: 0.000098 Step: 107550 Total Loss: 5.4991 Recon Loss: 5.4881 
[12/23 17:13:42 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8177 LR: 0.000098 Step: 107600 Total Loss: 4.2405 Recon Loss: 4.2295 
[12/23 17:14:22 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000098 Step: 107650 Total Loss: 4.1370 Recon Loss: 4.1261 
[12/23 17:15:03 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000098 Step: 107700 Total Loss: 4.8529 Recon Loss: 4.8419 
[12/23 17:15:44 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000098 Step: 107750 Total Loss: 4.9080 Recon Loss: 4.8969 
[12/23 17:16:25 TiTok]: Data (t): 0.0011, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000098 Step: 107800 Total Loss: 5.4838 Recon Loss: 5.4728 
[12/23 17:17:06 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000098 Step: 107850 Total Loss: 4.8638 Recon Loss: 4.8528 
[12/23 17:17:47 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000098 Step: 107900 Total Loss: 4.9368 Recon Loss: 4.9258 
[12/23 17:18:28 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8156 LR: 0.000098 Step: 107950 Total Loss: 5.5529 Recon Loss: 5.5419 
[12/23 17:19:09 TiTok]: Data (t): 0.0011, 35.52/s/gpu Batch (t): 0.9008 LR: 0.000098 Step: 108000 Total Loss: 6.1900 Recon Loss: 6.1790 
[12/23 17:19:49 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8183 LR: 0.000098 Step: 108050 Total Loss: 4.8254 Recon Loss: 4.8144 
[12/23 17:20:30 TiTok]: Data (t): 0.0010, 38.87/s/gpu Batch (t): 0.8232 LR: 0.000098 Step: 108100 Total Loss: 5.5211 Recon Loss: 5.5101 
[12/23 17:21:11 TiTok]: Data (t): 0.0010, 39.47/s/gpu Batch (t): 0.8108 LR: 0.000098 Step: 108150 Total Loss: 4.8620 Recon Loss: 4.8509 
[12/23 17:21:52 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000098 Step: 108200 Total Loss: 4.3072 Recon Loss: 4.2962 
[12/23 17:22:33 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8227 LR: 0.000098 Step: 108250 Total Loss: 4.1938 Recon Loss: 4.1827 
[12/23 17:23:14 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000098 Step: 108300 Total Loss: 4.7823 Recon Loss: 4.7712 
[12/23 17:23:55 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000098 Step: 108350 Total Loss: 4.8305 Recon Loss: 4.8195 
[12/23 17:24:35 TiTok]: Data (t): 0.0017, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000098 Step: 108400 Total Loss: 5.5617 Recon Loss: 5.5505 
[12/23 17:25:16 TiTok]: Data (t): 0.0033, 38.80/s/gpu Batch (t): 0.8247 LR: 0.000098 Step: 108450 Total Loss: 4.2240 Recon Loss: 4.2128 
[12/23 17:25:57 TiTok]: Data (t): 0.0012, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000098 Step: 108500 Total Loss: 4.9074 Recon Loss: 4.8963 
[12/23 17:26:38 TiTok]: Data (t): 0.0011, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000098 Step: 108550 Total Loss: 4.8310 Recon Loss: 4.8199 
[12/23 17:27:19 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8204 LR: 0.000098 Step: 108600 Total Loss: 5.5523 Recon Loss: 5.5411 
[12/23 17:28:00 TiTok]: Data (t): 0.0011, 39.73/s/gpu Batch (t): 0.8054 LR: 0.000098 Step: 108650 Total Loss: 6.1898 Recon Loss: 6.1786 
[12/23 17:28:41 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000098 Step: 108700 Total Loss: 4.8890 Recon Loss: 4.8779 
[12/23 17:29:21 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000098 Step: 108750 Total Loss: 4.8475 Recon Loss: 4.8362 
[12/23 17:30:02 TiTok]: Data (t): 0.0011, 38.98/s/gpu Batch (t): 0.8208 LR: 0.000098 Step: 108800 Total Loss: 4.2389 Recon Loss: 4.2276 
[12/23 17:30:43 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8137 LR: 0.000098 Step: 108850 Total Loss: 4.9240 Recon Loss: 4.9127 
[12/23 17:31:24 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000098 Step: 108900 Total Loss: 4.2456 Recon Loss: 4.2343 
[12/23 17:32:05 TiTok]: Data (t): 0.0011, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000098 Step: 108950 Total Loss: 4.1805 Recon Loss: 4.1692 
[12/23 17:32:46 TiTok]: Data (t): 0.0011, 35.03/s/gpu Batch (t): 0.9134 LR: 0.000098 Step: 109000 Total Loss: 6.1838 Recon Loss: 6.1723 
[12/23 17:33:27 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000098 Step: 109050 Total Loss: 4.9063 Recon Loss: 4.8947 
[12/23 17:34:07 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000098 Step: 109100 Total Loss: 4.8852 Recon Loss: 4.8735 
[12/23 17:34:48 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000098 Step: 109150 Total Loss: 5.5781 Recon Loss: 5.5662 
[12/23 17:35:29 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000098 Step: 109200 Total Loss: 5.5070 Recon Loss: 5.4950 
[12/23 17:36:10 TiTok]: Data (t): 0.0011, 39.37/s/gpu Batch (t): 0.8129 LR: 0.000098 Step: 109250 Total Loss: 5.5459 Recon Loss: 5.5334 
[12/23 17:36:51 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000098 Step: 109300 Total Loss: 4.2947 Recon Loss: 4.2816 
[12/23 17:37:32 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8187 LR: 0.000098 Step: 109350 Total Loss: 4.8825 Recon Loss: 4.8687 
[12/23 17:38:13 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8221 LR: 0.000098 Step: 109400 Total Loss: 4.2271 Recon Loss: 4.2130 
[12/23 17:38:53 TiTok]: Data (t): 0.0011, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000098 Step: 109450 Total Loss: 4.2447 Recon Loss: 4.2303 
[12/23 17:39:34 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000098 Step: 109500 Total Loss: 4.8648 Recon Loss: 4.8501 
[12/23 17:40:15 TiTok]: Data (t): 0.0019, 39.43/s/gpu Batch (t): 0.8116 LR: 0.000098 Step: 109550 Total Loss: 4.9023 Recon Loss: 4.8876 
[12/23 17:40:56 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8209 LR: 0.000098 Step: 109600 Total Loss: 4.2125 Recon Loss: 4.1978 
[12/23 17:41:37 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000098 Step: 109650 Total Loss: 4.8864 Recon Loss: 4.8718 
[12/23 17:42:18 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8162 LR: 0.000098 Step: 109700 Total Loss: 4.8945 Recon Loss: 4.8799 
[12/23 17:42:59 TiTok]: Data (t): 0.0011, 38.95/s/gpu Batch (t): 0.8217 LR: 0.000098 Step: 109750 Total Loss: 4.2690 Recon Loss: 4.2545 
[12/23 17:43:39 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000098 Step: 109800 Total Loss: 4.9067 Recon Loss: 4.8923 
[12/23 17:44:20 TiTok]: Data (t): 0.0014, 39.65/s/gpu Batch (t): 0.8070 LR: 0.000098 Step: 109850 Total Loss: 5.5126 Recon Loss: 5.4982 
[12/23 17:45:01 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8181 LR: 0.000098 Step: 109900 Total Loss: 4.2429 Recon Loss: 4.2285 
[12/23 17:45:42 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000098 Step: 109950 Total Loss: 4.2483 Recon Loss: 4.2340 
[12/23 17:46:23 TiTok]: Data (t): 0.0011, 35.29/s/gpu Batch (t): 0.9069 LR: 0.000098 Step: 110000 Total Loss: 6.1847 Recon Loss: 6.1703 
[12/23 17:46:24 TiTok]: Reconstructing images...
[12/23 17:47:05 TiTok]: Data (t): 0.0010, 39.40/s/gpu Batch (t): 0.8121 LR: 0.000098 Step: 110050 Total Loss: 5.5415 Recon Loss: 5.5271 
[12/23 17:47:46 TiTok]: Data (t): 0.0011, 39.37/s/gpu Batch (t): 0.8129 LR: 0.000098 Step: 110100 Total Loss: 4.9538 Recon Loss: 4.9395 
[12/23 17:48:27 TiTok]: Data (t): 0.0009, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000098 Step: 110150 Total Loss: 4.8973 Recon Loss: 4.8829 
[12/23 17:49:08 TiTok]: Data (t): 0.0010, 39.47/s/gpu Batch (t): 0.8107 LR: 0.000098 Step: 110200 Total Loss: 5.5677 Recon Loss: 5.5533 
Epoch 11/99 started.
[12/23 17:49:50 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000098 Step: 110250 Total Loss: 6.1650 Recon Loss: 6.1507 
[12/23 17:50:30 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000098 Step: 110300 Total Loss: 4.8477 Recon Loss: 4.8334 
[12/23 17:51:11 TiTok]: Data (t): 0.0012, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000098 Step: 110350 Total Loss: 5.4995 Recon Loss: 5.4852 
[12/23 17:51:52 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8209 LR: 0.000098 Step: 110400 Total Loss: 4.2542 Recon Loss: 4.2399 
[12/23 17:52:33 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000098 Step: 110450 Total Loss: 4.8376 Recon Loss: 4.8232 
[12/23 17:53:14 TiTok]: Data (t): 0.0011, 39.36/s/gpu Batch (t): 0.8131 LR: 0.000098 Step: 110500 Total Loss: 6.1718 Recon Loss: 6.1575 
[12/23 17:53:55 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000098 Step: 110550 Total Loss: 4.8746 Recon Loss: 4.8602 
[12/23 17:54:36 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000098 Step: 110600 Total Loss: 4.8395 Recon Loss: 4.8252 
[12/23 17:55:16 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8204 LR: 0.000098 Step: 110650 Total Loss: 4.2564 Recon Loss: 4.2420 
[12/23 17:55:57 TiTok]: Data (t): 0.0015, 38.85/s/gpu Batch (t): 0.8237 LR: 0.000098 Step: 110700 Total Loss: 5.5165 Recon Loss: 5.5021 
[12/23 17:56:38 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000098 Step: 110750 Total Loss: 4.8993 Recon Loss: 4.8848 
[12/23 17:57:19 TiTok]: Data (t): 0.0010, 39.50/s/gpu Batch (t): 0.8102 LR: 0.000098 Step: 110800 Total Loss: 4.2571 Recon Loss: 4.2427 
[12/23 17:58:00 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000098 Step: 110850 Total Loss: 4.9208 Recon Loss: 4.9064 
[12/23 17:58:41 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000098 Step: 110900 Total Loss: 4.8359 Recon Loss: 4.8215 
[12/23 17:59:22 TiTok]: Data (t): 0.0013, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000098 Step: 110950 Total Loss: 5.5123 Recon Loss: 5.4980 
[12/23 18:00:03 TiTok]: Data (t): 0.0011, 34.12/s/gpu Batch (t): 0.9380 LR: 0.000098 Step: 111000 Total Loss: 4.8737 Recon Loss: 4.8593 
[12/23 18:00:43 TiTok]: Data (t): 0.0011, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000098 Step: 111050 Total Loss: 4.9852 Recon Loss: 4.9707 
[12/23 18:01:24 TiTok]: Data (t): 0.0020, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000098 Step: 111100 Total Loss: 4.1999 Recon Loss: 4.1854 
[12/23 18:02:05 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000098 Step: 111150 Total Loss: 5.5492 Recon Loss: 5.5348 
[12/23 18:02:46 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000098 Step: 111200 Total Loss: 4.9130 Recon Loss: 4.8985 
[12/23 18:03:27 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000098 Step: 111250 Total Loss: 4.8623 Recon Loss: 4.8479 
[12/23 18:04:08 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000098 Step: 111300 Total Loss: 4.8700 Recon Loss: 4.8555 
[12/23 18:04:49 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000098 Step: 111350 Total Loss: 4.9138 Recon Loss: 4.8994 
[12/23 18:05:30 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8154 LR: 0.000098 Step: 111400 Total Loss: 4.8986 Recon Loss: 4.8842 
[12/23 18:06:11 TiTok]: Data (t): 0.0017, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000098 Step: 111450 Total Loss: 5.4874 Recon Loss: 5.4729 
[12/23 18:06:51 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000098 Step: 111500 Total Loss: 5.5433 Recon Loss: 5.5289 
[12/23 18:07:32 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000098 Step: 111550 Total Loss: 5.4503 Recon Loss: 5.4358 
[12/23 18:08:13 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000098 Step: 111600 Total Loss: 4.2416 Recon Loss: 4.2271 
[12/23 18:08:54 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000098 Step: 111650 Total Loss: 4.9037 Recon Loss: 4.8892 
[12/23 18:09:35 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000098 Step: 111700 Total Loss: 4.1792 Recon Loss: 4.1647 
[12/23 18:10:16 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000098 Step: 111750 Total Loss: 4.2620 Recon Loss: 4.2473 
[12/23 18:10:57 TiTok]: Data (t): 0.0010, 38.78/s/gpu Batch (t): 0.8252 LR: 0.000098 Step: 111800 Total Loss: 4.2893 Recon Loss: 4.2747 
[12/23 18:11:38 TiTok]: Data (t): 0.0010, 38.81/s/gpu Batch (t): 0.8246 LR: 0.000098 Step: 111850 Total Loss: 4.8882 Recon Loss: 4.8737 
[12/23 18:12:19 TiTok]: Data (t): 0.0011, 38.87/s/gpu Batch (t): 0.8232 LR: 0.000098 Step: 111900 Total Loss: 4.2493 Recon Loss: 4.2348 
[12/23 18:12:59 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000098 Step: 111950 Total Loss: 4.8734 Recon Loss: 4.8588 
[12/23 18:13:40 TiTok]: Data (t): 0.0010, 35.39/s/gpu Batch (t): 0.9042 LR: 0.000098 Step: 112000 Total Loss: 4.2350 Recon Loss: 4.2204 
[12/23 18:14:21 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000098 Step: 112050 Total Loss: 4.7708 Recon Loss: 4.7561 
[12/23 18:15:02 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000098 Step: 112100 Total Loss: 4.3195 Recon Loss: 4.3048 
[12/23 18:15:43 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000098 Step: 112150 Total Loss: 6.1403 Recon Loss: 6.1259 
[12/23 18:16:24 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8152 LR: 0.000098 Step: 112200 Total Loss: 4.2399 Recon Loss: 4.2254 
[12/23 18:17:05 TiTok]: Data (t): 0.0010, 39.43/s/gpu Batch (t): 0.8116 LR: 0.000098 Step: 112250 Total Loss: 4.8381 Recon Loss: 4.8236 
[12/23 18:17:46 TiTok]: Data (t): 0.0018, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000098 Step: 112300 Total Loss: 4.2123 Recon Loss: 4.1977 
[12/23 18:18:27 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8206 LR: 0.000098 Step: 112350 Total Loss: 4.9071 Recon Loss: 4.8926 
[12/23 18:19:07 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000098 Step: 112400 Total Loss: 5.4955 Recon Loss: 5.4810 
[12/23 18:19:48 TiTok]: Data (t): 0.0011, 39.38/s/gpu Batch (t): 0.8125 LR: 0.000098 Step: 112450 Total Loss: 4.2889 Recon Loss: 4.2742 
[12/23 18:20:29 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000098 Step: 112500 Total Loss: 5.5269 Recon Loss: 5.5123 
[12/23 18:21:10 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000098 Step: 112550 Total Loss: 4.1853 Recon Loss: 4.1708 
[12/23 18:21:51 TiTok]: Data (t): 0.0022, 40.06/s/gpu Batch (t): 0.7987 LR: 0.000098 Step: 112600 Total Loss: 4.8933 Recon Loss: 4.8787 
[12/23 18:22:32 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8215 LR: 0.000098 Step: 112650 Total Loss: 4.1748 Recon Loss: 4.1603 
[12/23 18:23:13 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000098 Step: 112700 Total Loss: 4.2312 Recon Loss: 4.2166 
[12/23 18:23:54 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000098 Step: 112750 Total Loss: 5.5199 Recon Loss: 5.5054 
[12/23 18:24:34 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000098 Step: 112800 Total Loss: 4.8696 Recon Loss: 4.8550 
[12/23 18:25:15 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000098 Step: 112850 Total Loss: 4.9014 Recon Loss: 4.8869 
[12/23 18:25:56 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8125 LR: 0.000098 Step: 112900 Total Loss: 4.9011 Recon Loss: 4.8865 
[12/23 18:26:37 TiTok]: Data (t): 0.0011, 38.87/s/gpu Batch (t): 0.8232 LR: 0.000098 Step: 112950 Total Loss: 4.2968 Recon Loss: 4.2823 
[12/23 18:27:18 TiTok]: Data (t): 0.0011, 34.58/s/gpu Batch (t): 0.9254 LR: 0.000098 Step: 113000 Total Loss: 5.5248 Recon Loss: 5.5101 
[12/23 18:27:59 TiTok]: Data (t): 0.0011, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000098 Step: 113050 Total Loss: 6.1622 Recon Loss: 6.1476 
[12/23 18:28:40 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000098 Step: 113100 Total Loss: 4.8352 Recon Loss: 4.8206 
[12/23 18:29:21 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000098 Step: 113150 Total Loss: 5.5470 Recon Loss: 5.5325 
[12/23 18:30:01 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8215 LR: 0.000098 Step: 113200 Total Loss: 4.9321 Recon Loss: 4.9175 
[12/23 18:30:42 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000098 Step: 113250 Total Loss: 4.8400 Recon Loss: 4.8255 
[12/23 18:31:23 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000098 Step: 113300 Total Loss: 4.8652 Recon Loss: 4.8507 
[12/23 18:32:04 TiTok]: Data (t): 0.0010, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000098 Step: 113350 Total Loss: 5.4902 Recon Loss: 5.4757 
[12/23 18:32:45 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000098 Step: 113400 Total Loss: 4.8953 Recon Loss: 4.8807 
[12/23 18:33:26 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000098 Step: 113450 Total Loss: 4.8843 Recon Loss: 4.8699 
[12/23 18:34:07 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000098 Step: 113500 Total Loss: 5.5005 Recon Loss: 5.4859 
[12/23 18:34:48 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000098 Step: 113550 Total Loss: 4.8900 Recon Loss: 4.8755 
[12/23 18:35:28 TiTok]: Data (t): 0.0011, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000098 Step: 113600 Total Loss: 4.3029 Recon Loss: 4.2882 
[12/23 18:36:09 TiTok]: Data (t): 0.0010, 39.45/s/gpu Batch (t): 0.8112 LR: 0.000098 Step: 113650 Total Loss: 4.9298 Recon Loss: 4.9152 
[12/23 18:36:50 TiTok]: Data (t): 0.0013, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000098 Step: 113700 Total Loss: 4.2046 Recon Loss: 4.1901 
[12/23 18:37:31 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8223 LR: 0.000098 Step: 113750 Total Loss: 4.1835 Recon Loss: 4.1690 
[12/23 18:38:12 TiTok]: Data (t): 0.0010, 38.75/s/gpu Batch (t): 0.8258 LR: 0.000098 Step: 113800 Total Loss: 5.5355 Recon Loss: 5.5208 
[12/23 18:38:53 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8181 LR: 0.000098 Step: 113850 Total Loss: 4.8681 Recon Loss: 4.8536 
[12/23 18:39:34 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000098 Step: 113900 Total Loss: 4.2543 Recon Loss: 4.2397 
[12/23 18:40:14 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8137 LR: 0.000098 Step: 113950 Total Loss: 4.2286 Recon Loss: 4.2140 
[12/23 18:40:56 TiTok]: Data (t): 0.0010, 34.63/s/gpu Batch (t): 0.9239 LR: 0.000098 Step: 114000 Total Loss: 4.9054 Recon Loss: 4.8908 
[12/23 18:41:36 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8223 LR: 0.000098 Step: 114050 Total Loss: 4.8076 Recon Loss: 4.7930 
[12/23 18:42:17 TiTok]: Data (t): 0.0010, 38.84/s/gpu Batch (t): 0.8239 LR: 0.000098 Step: 114100 Total Loss: 4.1964 Recon Loss: 4.1818 
[12/23 18:42:58 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000098 Step: 114150 Total Loss: 6.1863 Recon Loss: 6.1717 
[12/23 18:43:39 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000098 Step: 114200 Total Loss: 4.2851 Recon Loss: 4.2707 
[12/23 18:44:20 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000098 Step: 114250 Total Loss: 4.2097 Recon Loss: 4.1950 
[12/23 18:45:01 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8202 LR: 0.000098 Step: 114300 Total Loss: 4.2137 Recon Loss: 4.1990 
[12/23 18:45:42 TiTok]: Data (t): 0.0011, 39.37/s/gpu Batch (t): 0.8127 LR: 0.000098 Step: 114350 Total Loss: 4.2493 Recon Loss: 4.2346 
[12/23 18:46:22 TiTok]: Data (t): 0.0010, 38.79/s/gpu Batch (t): 0.8250 LR: 0.000098 Step: 114400 Total Loss: 4.2033 Recon Loss: 4.1886 
[12/23 18:47:03 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000098 Step: 114450 Total Loss: 5.5478 Recon Loss: 5.5331 
[12/23 18:47:44 TiTok]: Data (t): 0.0015, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000098 Step: 114500 Total Loss: 5.5644 Recon Loss: 5.5498 
[12/23 18:48:25 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000098 Step: 114550 Total Loss: 4.9330 Recon Loss: 4.9184 
[12/23 18:49:06 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000098 Step: 114600 Total Loss: 4.1808 Recon Loss: 4.1662 
[12/23 18:49:47 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000098 Step: 114650 Total Loss: 4.8451 Recon Loss: 4.8304 
[12/23 18:50:28 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000098 Step: 114700 Total Loss: 4.2049 Recon Loss: 4.1904 
[12/23 18:51:09 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000098 Step: 114750 Total Loss: 4.1940 Recon Loss: 4.1793 
[12/23 18:51:49 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8235 LR: 0.000098 Step: 114800 Total Loss: 4.1947 Recon Loss: 4.1801 
[12/23 18:52:30 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000098 Step: 114850 Total Loss: 4.8416 Recon Loss: 4.8270 
[12/23 18:53:11 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000098 Step: 114900 Total Loss: 4.2191 Recon Loss: 4.2045 
[12/23 18:53:52 TiTok]: Data (t): 0.0012, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000098 Step: 114950 Total Loss: 4.2703 Recon Loss: 4.2556 
[12/23 18:54:33 TiTok]: Data (t): 0.0010, 35.04/s/gpu Batch (t): 0.9133 LR: 0.000098 Step: 115000 Total Loss: 4.8548 Recon Loss: 4.8402 
[12/23 18:54:34 TiTok]: Reconstructing images...
[12/23 18:55:16 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000098 Step: 115050 Total Loss: 4.2655 Recon Loss: 4.2508 
[12/23 18:55:56 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000098 Step: 115100 Total Loss: 4.8316 Recon Loss: 4.8168 
[12/23 18:56:37 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8183 LR: 0.000098 Step: 115150 Total Loss: 4.9093 Recon Loss: 4.8947 
[12/23 18:57:18 TiTok]: Data (t): 0.0011, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000098 Step: 115200 Total Loss: 5.5239 Recon Loss: 5.5093 
[12/23 18:57:59 TiTok]: Data (t): 0.0010, 39.37/s/gpu Batch (t): 0.8127 LR: 0.000098 Step: 115250 Total Loss: 4.9088 Recon Loss: 4.8942 
[12/23 18:58:40 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000098 Step: 115300 Total Loss: 4.1816 Recon Loss: 4.1670 
[12/23 18:59:21 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000098 Step: 115350 Total Loss: 4.2136 Recon Loss: 4.1990 
[12/23 19:00:02 TiTok]: Data (t): 0.0017, 39.67/s/gpu Batch (t): 0.8066 LR: 0.000098 Step: 115400 Total Loss: 6.1433 Recon Loss: 6.1287 
[12/23 19:00:43 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000098 Step: 115450 Total Loss: 4.8397 Recon Loss: 4.8251 
[12/23 19:01:23 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000098 Step: 115500 Total Loss: 4.2836 Recon Loss: 4.2690 
[12/23 19:02:04 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000098 Step: 115550 Total Loss: 4.8927 Recon Loss: 4.8780 
[12/23 19:02:45 TiTok]: Data (t): 0.0016, 40.08/s/gpu Batch (t): 0.7983 LR: 0.000097 Step: 115600 Total Loss: 5.5131 Recon Loss: 5.4984 
[12/23 19:03:26 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8177 LR: 0.000097 Step: 115650 Total Loss: 4.2645 Recon Loss: 4.2498 
[12/23 19:04:07 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000097 Step: 115700 Total Loss: 4.8757 Recon Loss: 4.8611 
[12/23 19:04:48 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000097 Step: 115750 Total Loss: 4.1425 Recon Loss: 4.1279 
[12/23 19:05:29 TiTok]: Data (t): 0.0011, 39.49/s/gpu Batch (t): 0.8104 LR: 0.000097 Step: 115800 Total Loss: 5.5361 Recon Loss: 5.5215 
[12/23 19:06:10 TiTok]: Data (t): 0.0016, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000097 Step: 115850 Total Loss: 4.1631 Recon Loss: 4.1485 
[12/23 19:06:51 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8200 LR: 0.000097 Step: 115900 Total Loss: 4.8310 Recon Loss: 4.8163 
[12/23 19:07:31 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000097 Step: 115950 Total Loss: 5.5452 Recon Loss: 5.5307 
[12/23 19:08:12 TiTok]: Data (t): 0.0010, 35.33/s/gpu Batch (t): 0.9057 LR: 0.000097 Step: 116000 Total Loss: 4.7928 Recon Loss: 4.7782 
[12/23 19:08:53 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000097 Step: 116050 Total Loss: 4.2288 Recon Loss: 4.2142 
[12/23 19:09:34 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000097 Step: 116100 Total Loss: 4.2190 Recon Loss: 4.2044 
[12/23 19:10:15 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8154 LR: 0.000097 Step: 116150 Total Loss: 5.5473 Recon Loss: 5.5327 
[12/23 19:10:56 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8200 LR: 0.000097 Step: 116200 Total Loss: 4.2019 Recon Loss: 4.1873 
[12/23 19:11:37 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000097 Step: 116250 Total Loss: 4.8718 Recon Loss: 4.8572 
[12/23 19:12:18 TiTok]: Data (t): 0.0010, 38.68/s/gpu Batch (t): 0.8272 LR: 0.000097 Step: 116300 Total Loss: 4.2021 Recon Loss: 4.1874 
[12/23 19:12:59 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000097 Step: 116350 Total Loss: 4.2081 Recon Loss: 4.1935 
[12/23 19:13:40 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000097 Step: 116400 Total Loss: 4.8541 Recon Loss: 4.8395 
[12/23 19:14:20 TiTok]: Data (t): 0.0010, 38.80/s/gpu Batch (t): 0.8246 LR: 0.000097 Step: 116450 Total Loss: 4.1676 Recon Loss: 4.1530 
[12/23 19:15:01 TiTok]: Data (t): 0.0011, 39.40/s/gpu Batch (t): 0.8123 LR: 0.000097 Step: 116500 Total Loss: 5.5625 Recon Loss: 5.5479 
[12/23 19:15:42 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8181 LR: 0.000097 Step: 116550 Total Loss: 4.8061 Recon Loss: 4.7914 
[12/23 19:16:23 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8223 LR: 0.000097 Step: 116600 Total Loss: 4.8604 Recon Loss: 4.8457 
[12/23 19:17:04 TiTok]: Data (t): 0.0033, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000097 Step: 116650 Total Loss: 5.5095 Recon Loss: 5.4948 
[12/23 19:17:45 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000097 Step: 116700 Total Loss: 4.1816 Recon Loss: 4.1669 
[12/23 19:18:26 TiTok]: Data (t): 0.0010, 39.40/s/gpu Batch (t): 0.8121 LR: 0.000097 Step: 116750 Total Loss: 5.4820 Recon Loss: 5.4674 
[12/23 19:19:07 TiTok]: Data (t): 0.0017, 39.45/s/gpu Batch (t): 0.8111 LR: 0.000097 Step: 116800 Total Loss: 4.8930 Recon Loss: 4.8784 
[12/23 19:19:47 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000097 Step: 116850 Total Loss: 4.8814 Recon Loss: 4.8667 
[12/23 19:20:28 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000097 Step: 116900 Total Loss: 4.2144 Recon Loss: 4.1998 
[12/23 19:21:09 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000097 Step: 116950 Total Loss: 4.8783 Recon Loss: 4.8637 
[12/23 19:21:50 TiTok]: Data (t): 0.0011, 34.70/s/gpu Batch (t): 0.9222 LR: 0.000097 Step: 117000 Total Loss: 4.1691 Recon Loss: 4.1545 
[12/23 19:22:31 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000097 Step: 117050 Total Loss: 4.2709 Recon Loss: 4.2563 
[12/23 19:23:12 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000097 Step: 117100 Total Loss: 4.8262 Recon Loss: 4.8117 
[12/23 19:23:53 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000097 Step: 117150 Total Loss: 5.4945 Recon Loss: 5.4798 
[12/23 19:24:34 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000097 Step: 117200 Total Loss: 5.4514 Recon Loss: 5.4367 
[12/23 19:25:15 TiTok]: Data (t): 0.0016, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000097 Step: 117250 Total Loss: 4.2242 Recon Loss: 4.2095 
[12/23 19:25:55 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000097 Step: 117300 Total Loss: 4.8256 Recon Loss: 4.8109 
[12/23 19:26:36 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000097 Step: 117350 Total Loss: 5.5300 Recon Loss: 5.5153 
[12/23 19:27:17 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8166 LR: 0.000097 Step: 117400 Total Loss: 4.8883 Recon Loss: 4.8736 
[12/23 19:27:58 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000097 Step: 117450 Total Loss: 4.9066 Recon Loss: 4.8920 
[12/23 19:28:39 TiTok]: Data (t): 0.0011, 39.40/s/gpu Batch (t): 0.8122 LR: 0.000097 Step: 117500 Total Loss: 4.8407 Recon Loss: 4.8260 
[12/23 19:29:20 TiTok]: Data (t): 0.0012, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000097 Step: 117550 Total Loss: 4.8748 Recon Loss: 4.8602 
[12/23 19:30:01 TiTok]: Data (t): 0.0012, 39.50/s/gpu Batch (t): 0.8101 LR: 0.000097 Step: 117600 Total Loss: 4.8224 Recon Loss: 4.8077 
[12/23 19:30:42 TiTok]: Data (t): 0.0012, 39.55/s/gpu Batch (t): 0.8091 LR: 0.000097 Step: 117650 Total Loss: 4.1794 Recon Loss: 4.1647 
[12/23 19:31:22 TiTok]: Data (t): 0.0018, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000097 Step: 117700 Total Loss: 4.1888 Recon Loss: 4.1741 
[12/23 19:32:03 TiTok]: Data (t): 0.0010, 39.39/s/gpu Batch (t): 0.8124 LR: 0.000097 Step: 117750 Total Loss: 5.5120 Recon Loss: 5.4974 
[12/23 19:32:44 TiTok]: Data (t): 0.0011, 38.91/s/gpu Batch (t): 0.8223 LR: 0.000097 Step: 117800 Total Loss: 4.1547 Recon Loss: 4.1400 
[12/23 19:33:25 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000097 Step: 117850 Total Loss: 4.2000 Recon Loss: 4.1854 
[12/23 19:34:06 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000097 Step: 117900 Total Loss: 4.1852 Recon Loss: 4.1706 
[12/23 19:34:47 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000097 Step: 117950 Total Loss: 4.8424 Recon Loss: 4.8278 
[12/23 19:35:28 TiTok]: Data (t): 0.0011, 35.46/s/gpu Batch (t): 0.9024 LR: 0.000097 Step: 118000 Total Loss: 4.1934 Recon Loss: 4.1788 
[12/23 19:36:09 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000097 Step: 118050 Total Loss: 4.8800 Recon Loss: 4.8654 
[12/23 19:36:50 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000097 Step: 118100 Total Loss: 4.8959 Recon Loss: 4.8813 
[12/23 19:37:30 TiTok]: Data (t): 0.0011, 38.55/s/gpu Batch (t): 0.8302 LR: 0.000097 Step: 118150 Total Loss: 4.8617 Recon Loss: 4.8471 
[12/23 19:38:11 TiTok]: Data (t): 0.0011, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000097 Step: 118200 Total Loss: 4.8426 Recon Loss: 4.8278 
[12/23 19:38:52 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000097 Step: 118250 Total Loss: 4.7905 Recon Loss: 4.7758 
[12/23 19:39:33 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000097 Step: 118300 Total Loss: 4.1521 Recon Loss: 4.1375 
[12/23 19:40:14 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000097 Step: 118350 Total Loss: 4.2311 Recon Loss: 4.2164 
[12/23 19:40:55 TiTok]: Data (t): 0.0010, 38.78/s/gpu Batch (t): 0.8251 LR: 0.000097 Step: 118400 Total Loss: 4.8119 Recon Loss: 4.7973 
[12/23 19:41:36 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000097 Step: 118450 Total Loss: 5.5019 Recon Loss: 5.4872 
[12/23 19:42:17 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000097 Step: 118500 Total Loss: 4.8684 Recon Loss: 4.8538 
[12/23 19:42:57 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000097 Step: 118550 Total Loss: 5.5440 Recon Loss: 5.5294 
[12/23 19:43:38 TiTok]: Data (t): 0.0023, 39.82/s/gpu Batch (t): 0.8036 LR: 0.000097 Step: 118600 Total Loss: 4.9206 Recon Loss: 4.9060 
[12/23 19:44:19 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000097 Step: 118650 Total Loss: 4.1852 Recon Loss: 4.1706 
[12/23 19:45:00 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000097 Step: 118700 Total Loss: 4.2547 Recon Loss: 4.2401 
[12/23 19:45:41 TiTok]: Data (t): 0.0010, 38.77/s/gpu Batch (t): 0.8253 LR: 0.000097 Step: 118750 Total Loss: 5.5103 Recon Loss: 5.4956 
[12/23 19:46:22 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000097 Step: 118800 Total Loss: 4.8788 Recon Loss: 4.8641 
[12/23 19:47:03 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8198 LR: 0.000097 Step: 118850 Total Loss: 4.8481 Recon Loss: 4.8335 
[12/23 19:47:44 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000097 Step: 118900 Total Loss: 4.8054 Recon Loss: 4.7908 
[12/23 19:48:25 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000097 Step: 118950 Total Loss: 4.8519 Recon Loss: 4.8371 
[12/23 19:49:06 TiTok]: Data (t): 0.0010, 34.66/s/gpu Batch (t): 0.9232 LR: 0.000097 Step: 119000 Total Loss: 4.8418 Recon Loss: 4.8272 
[12/23 19:49:46 TiTok]: Data (t): 0.0010, 39.39/s/gpu Batch (t): 0.8123 LR: 0.000097 Step: 119050 Total Loss: 4.8186 Recon Loss: 4.8039 
[12/23 19:50:27 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000097 Step: 119100 Total Loss: 4.8637 Recon Loss: 4.8491 
[12/23 19:51:08 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000097 Step: 119150 Total Loss: 4.7973 Recon Loss: 4.7827 
[12/23 19:51:49 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000097 Step: 119200 Total Loss: 4.8270 Recon Loss: 4.8123 
[12/23 19:52:30 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000097 Step: 119250 Total Loss: 4.2192 Recon Loss: 4.2046 
[12/23 19:53:11 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8154 LR: 0.000097 Step: 119300 Total Loss: 5.4735 Recon Loss: 5.4588 
[12/23 19:53:52 TiTok]: Data (t): 0.0010, 38.54/s/gpu Batch (t): 0.8303 LR: 0.000097 Step: 119350 Total Loss: 4.8966 Recon Loss: 4.8819 
[12/23 19:54:32 TiTok]: Data (t): 0.0011, 39.34/s/gpu Batch (t): 0.8133 LR: 0.000097 Step: 119400 Total Loss: 4.9163 Recon Loss: 4.9017 
[12/23 19:55:13 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000097 Step: 119450 Total Loss: 4.2667 Recon Loss: 4.2521 
[12/23 19:55:54 TiTok]: Data (t): 0.0010, 39.53/s/gpu Batch (t): 0.8094 LR: 0.000097 Step: 119500 Total Loss: 5.4982 Recon Loss: 5.4835 
[12/23 19:56:35 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000097 Step: 119550 Total Loss: 4.8467 Recon Loss: 4.8319 
[12/23 19:57:16 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000097 Step: 119600 Total Loss: 4.2494 Recon Loss: 4.2347 
[12/23 19:57:57 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000097 Step: 119650 Total Loss: 4.8408 Recon Loss: 4.8261 
[12/23 19:58:38 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000097 Step: 119700 Total Loss: 4.8559 Recon Loss: 4.8412 
[12/23 19:59:18 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000097 Step: 119750 Total Loss: 4.8278 Recon Loss: 4.8131 
[12/23 19:59:59 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8175 LR: 0.000097 Step: 119800 Total Loss: 5.5294 Recon Loss: 5.5147 
[12/23 20:00:40 TiTok]: Data (t): 0.0010, 39.44/s/gpu Batch (t): 0.8113 LR: 0.000097 Step: 119850 Total Loss: 5.5180 Recon Loss: 5.5034 
[12/23 20:01:21 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000097 Step: 119900 Total Loss: 6.2100 Recon Loss: 6.1953 
[12/23 20:02:02 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000097 Step: 119950 Total Loss: 4.8389 Recon Loss: 4.8242 
[12/23 20:02:43 TiTok]: Data (t): 0.0018, 34.53/s/gpu Batch (t): 0.9266 LR: 0.000097 Step: 120000 Total Loss: 4.8846 Recon Loss: 4.8699 
[12/23 20:02:45 TiTok]: Reconstructing images...
[12/23 20:03:26 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000097 Step: 120050 Total Loss: 4.8067 Recon Loss: 4.7920 
[12/23 20:04:07 TiTok]: Data (t): 0.0010, 39.39/s/gpu Batch (t): 0.8123 LR: 0.000097 Step: 120100 Total Loss: 4.8899 Recon Loss: 4.8753 
[12/23 20:04:48 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000097 Step: 120150 Total Loss: 4.1954 Recon Loss: 4.1808 
[12/23 20:05:28 TiTok]: Data (t): 0.0011, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000097 Step: 120200 Total Loss: 4.2543 Recon Loss: 4.2397 
Epoch 12/99 started.
[12/23 20:06:10 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000097 Step: 120250 Total Loss: 4.1542 Recon Loss: 4.1395 
[12/23 20:06:51 TiTok]: Data (t): 0.0010, 38.80/s/gpu Batch (t): 0.8247 LR: 0.000097 Step: 120300 Total Loss: 4.8700 Recon Loss: 4.8554 
[12/23 20:07:32 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8160 LR: 0.000097 Step: 120350 Total Loss: 4.2208 Recon Loss: 4.2061 
[12/23 20:08:13 TiTok]: Data (t): 0.0011, 38.84/s/gpu Batch (t): 0.8238 LR: 0.000097 Step: 120400 Total Loss: 4.8305 Recon Loss: 4.8159 
[12/23 20:08:54 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000097 Step: 120450 Total Loss: 5.5201 Recon Loss: 5.5055 
[12/23 20:09:35 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000097 Step: 120500 Total Loss: 4.8714 Recon Loss: 4.8566 
[12/23 20:10:15 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000097 Step: 120550 Total Loss: 4.2286 Recon Loss: 4.2140 
[12/23 20:10:56 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000097 Step: 120600 Total Loss: 4.1938 Recon Loss: 4.1790 
[12/23 20:11:37 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000097 Step: 120650 Total Loss: 4.7360 Recon Loss: 4.7214 
[12/23 20:12:18 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8158 LR: 0.000097 Step: 120700 Total Loss: 4.8262 Recon Loss: 4.8115 
[12/23 20:12:59 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000097 Step: 120750 Total Loss: 4.7855 Recon Loss: 4.7708 
[12/23 20:13:40 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000097 Step: 120800 Total Loss: 4.1181 Recon Loss: 4.1034 
[12/23 20:14:21 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000097 Step: 120850 Total Loss: 5.5251 Recon Loss: 5.5104 
[12/23 20:15:02 TiTok]: Data (t): 0.0017, 39.45/s/gpu Batch (t): 0.8111 LR: 0.000097 Step: 120900 Total Loss: 4.2029 Recon Loss: 4.1882 
[12/23 20:15:42 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000097 Step: 120950 Total Loss: 5.4946 Recon Loss: 5.4800 
[12/23 20:16:23 TiTok]: Data (t): 0.0010, 34.46/s/gpu Batch (t): 0.9287 LR: 0.000097 Step: 121000 Total Loss: 4.1437 Recon Loss: 4.1290 
[12/23 20:17:04 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000097 Step: 121050 Total Loss: 4.8231 Recon Loss: 4.8085 
[12/23 20:17:45 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000097 Step: 121100 Total Loss: 4.8663 Recon Loss: 4.8517 
[12/23 20:18:26 TiTok]: Data (t): 0.0010, 38.82/s/gpu Batch (t): 0.8243 LR: 0.000097 Step: 121150 Total Loss: 4.8855 Recon Loss: 4.8708 
[12/23 20:19:07 TiTok]: Data (t): 0.0011, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000097 Step: 121200 Total Loss: 4.1545 Recon Loss: 4.1398 
[12/23 20:19:48 TiTok]: Data (t): 0.0011, 38.89/s/gpu Batch (t): 0.8229 LR: 0.000097 Step: 121250 Total Loss: 4.1699 Recon Loss: 4.1551 
[12/23 20:20:29 TiTok]: Data (t): 0.0011, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000097 Step: 121300 Total Loss: 4.1762 Recon Loss: 4.1615 
[12/23 20:21:10 TiTok]: Data (t): 0.0016, 39.23/s/gpu Batch (t): 0.8158 LR: 0.000097 Step: 121350 Total Loss: 4.2865 Recon Loss: 4.2718 
[12/23 20:21:50 TiTok]: Data (t): 0.0011, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000097 Step: 121400 Total Loss: 4.1907 Recon Loss: 4.1760 
[12/23 20:22:31 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8154 LR: 0.000097 Step: 121450 Total Loss: 4.1792 Recon Loss: 4.1644 
[12/23 20:23:12 TiTok]: Data (t): 0.0011, 39.39/s/gpu Batch (t): 0.8123 LR: 0.000097 Step: 121500 Total Loss: 6.1939 Recon Loss: 6.1792 
[12/23 20:23:53 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8200 LR: 0.000097 Step: 121550 Total Loss: 4.8483 Recon Loss: 4.8336 
[12/23 20:24:34 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000097 Step: 121600 Total Loss: 5.5317 Recon Loss: 5.5170 
[12/23 20:25:15 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000097 Step: 121650 Total Loss: 5.5040 Recon Loss: 5.4893 
[12/23 20:25:56 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000097 Step: 121700 Total Loss: 6.1819 Recon Loss: 6.1672 
[12/23 20:26:36 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000097 Step: 121750 Total Loss: 6.1571 Recon Loss: 6.1424 
[12/23 20:27:17 TiTok]: Data (t): 0.0010, 38.63/s/gpu Batch (t): 0.8285 LR: 0.000097 Step: 121800 Total Loss: 4.1927 Recon Loss: 4.1781 
[12/23 20:27:58 TiTok]: Data (t): 0.0013, 39.41/s/gpu Batch (t): 0.8120 LR: 0.000097 Step: 121850 Total Loss: 4.8855 Recon Loss: 4.8708 
[12/23 20:28:39 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8202 LR: 0.000097 Step: 121900 Total Loss: 4.8503 Recon Loss: 4.8356 
[12/23 20:29:20 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000097 Step: 121950 Total Loss: 4.2015 Recon Loss: 4.1868 
[12/23 20:30:01 TiTok]: Data (t): 0.0010, 35.57/s/gpu Batch (t): 0.8996 LR: 0.000097 Step: 122000 Total Loss: 6.1865 Recon Loss: 6.1719 
[12/23 20:30:42 TiTok]: Data (t): 0.0010, 39.44/s/gpu Batch (t): 0.8114 LR: 0.000097 Step: 122050 Total Loss: 5.5227 Recon Loss: 5.5080 
[12/23 20:31:23 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8189 LR: 0.000097 Step: 122100 Total Loss: 5.5632 Recon Loss: 5.5485 
[12/23 20:32:04 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000097 Step: 122150 Total Loss: 4.8537 Recon Loss: 4.8390 
[12/23 20:32:44 TiTok]: Data (t): 0.0011, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000097 Step: 122200 Total Loss: 6.8281 Recon Loss: 6.8135 
[12/23 20:33:25 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000097 Step: 122250 Total Loss: 5.4886 Recon Loss: 5.4738 
[12/23 20:34:06 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000097 Step: 122300 Total Loss: 5.5039 Recon Loss: 5.4891 
[12/23 20:34:47 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000097 Step: 122350 Total Loss: 5.5264 Recon Loss: 5.5116 
[12/23 20:35:28 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000097 Step: 122400 Total Loss: 4.8326 Recon Loss: 4.8179 
[12/23 20:36:09 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000097 Step: 122450 Total Loss: 4.1276 Recon Loss: 4.1130 
[12/23 20:36:50 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000097 Step: 122500 Total Loss: 4.2181 Recon Loss: 4.2034 
[12/23 20:37:30 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8158 LR: 0.000097 Step: 122550 Total Loss: 4.9155 Recon Loss: 4.9007 
[12/23 20:38:11 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8141 LR: 0.000097 Step: 122600 Total Loss: 4.2382 Recon Loss: 4.2235 
[12/23 20:38:52 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000097 Step: 122650 Total Loss: 4.8688 Recon Loss: 4.8542 
[12/23 20:39:33 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000097 Step: 122700 Total Loss: 4.2311 Recon Loss: 4.2164 
[12/23 20:40:14 TiTok]: Data (t): 0.0019, 39.41/s/gpu Batch (t): 0.8120 LR: 0.000097 Step: 122750 Total Loss: 5.4689 Recon Loss: 5.4543 
[12/23 20:40:55 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8175 LR: 0.000097 Step: 122800 Total Loss: 4.8482 Recon Loss: 4.8335 
[12/23 20:41:36 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000097 Step: 122850 Total Loss: 4.2768 Recon Loss: 4.2621 
[12/23 20:42:17 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000097 Step: 122900 Total Loss: 4.8897 Recon Loss: 4.8750 
[12/23 20:42:57 TiTok]: Data (t): 0.0010, 39.60/s/gpu Batch (t): 0.8082 LR: 0.000097 Step: 122950 Total Loss: 4.8383 Recon Loss: 4.8235 
[12/23 20:43:38 TiTok]: Data (t): 0.0011, 35.41/s/gpu Batch (t): 0.9036 LR: 0.000097 Step: 123000 Total Loss: 5.5120 Recon Loss: 5.4972 
[12/23 20:44:19 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000097 Step: 123050 Total Loss: 4.2107 Recon Loss: 4.1960 
[12/23 20:45:00 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000097 Step: 123100 Total Loss: 5.5657 Recon Loss: 5.5509 
[12/23 20:45:41 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000097 Step: 123150 Total Loss: 4.1059 Recon Loss: 4.0913 
[12/23 20:46:22 TiTok]: Data (t): 0.0013, 39.46/s/gpu Batch (t): 0.8109 LR: 0.000097 Step: 123200 Total Loss: 4.8158 Recon Loss: 4.8011 
[12/23 20:47:03 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000097 Step: 123250 Total Loss: 4.8215 Recon Loss: 4.8069 
[12/23 20:47:44 TiTok]: Data (t): 0.0010, 38.85/s/gpu Batch (t): 0.8237 LR: 0.000097 Step: 123300 Total Loss: 4.1699 Recon Loss: 4.1552 
[12/23 20:48:25 TiTok]: Data (t): 0.0011, 38.89/s/gpu Batch (t): 0.8228 LR: 0.000097 Step: 123350 Total Loss: 4.2552 Recon Loss: 4.2406 
[12/23 20:49:05 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000097 Step: 123400 Total Loss: 4.8910 Recon Loss: 4.8762 
[12/23 20:49:46 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000097 Step: 123450 Total Loss: 4.1168 Recon Loss: 4.1021 
[12/23 20:50:27 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000097 Step: 123500 Total Loss: 4.8393 Recon Loss: 4.8246 
[12/23 20:51:08 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000097 Step: 123550 Total Loss: 4.8451 Recon Loss: 4.8305 
[12/23 20:51:49 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000097 Step: 123600 Total Loss: 4.2121 Recon Loss: 4.1975 
[12/23 20:52:30 TiTok]: Data (t): 0.0016, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000097 Step: 123650 Total Loss: 6.1608 Recon Loss: 6.1461 
[12/23 20:53:11 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000097 Step: 123700 Total Loss: 5.4843 Recon Loss: 5.4696 
[12/23 20:53:52 TiTok]: Data (t): 0.0011, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000097 Step: 123750 Total Loss: 4.8492 Recon Loss: 4.8345 
[12/23 20:54:32 TiTok]: Data (t): 0.0011, 39.44/s/gpu Batch (t): 0.8113 LR: 0.000097 Step: 123800 Total Loss: 4.8473 Recon Loss: 4.8326 
[12/23 20:55:13 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000097 Step: 123850 Total Loss: 4.8860 Recon Loss: 4.8712 
[12/23 20:55:54 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000097 Step: 123900 Total Loss: 5.4927 Recon Loss: 5.4779 
[12/23 20:56:35 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000097 Step: 123950 Total Loss: 4.2305 Recon Loss: 4.2159 
[12/23 20:57:16 TiTok]: Data (t): 0.0010, 35.30/s/gpu Batch (t): 0.9066 LR: 0.000097 Step: 124000 Total Loss: 4.8270 Recon Loss: 4.8122 
[12/23 20:57:57 TiTok]: Data (t): 0.0013, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000097 Step: 124050 Total Loss: 5.4946 Recon Loss: 5.4798 
[12/23 20:58:38 TiTok]: Data (t): 0.0011, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000097 Step: 124100 Total Loss: 6.1473 Recon Loss: 6.1326 
[12/23 20:59:19 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000097 Step: 124150 Total Loss: 5.4550 Recon Loss: 5.4404 
[12/23 20:59:59 TiTok]: Data (t): 0.0010, 38.83/s/gpu Batch (t): 0.8241 LR: 0.000097 Step: 124200 Total Loss: 4.2735 Recon Loss: 4.2588 
[12/23 21:00:40 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000097 Step: 124250 Total Loss: 4.7774 Recon Loss: 4.7628 
[12/23 21:01:21 TiTok]: Data (t): 0.0011, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000097 Step: 124300 Total Loss: 6.8194 Recon Loss: 6.8047 
[12/23 21:02:02 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000097 Step: 124350 Total Loss: 5.5018 Recon Loss: 5.4870 
[12/23 21:02:43 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000097 Step: 124400 Total Loss: 4.1981 Recon Loss: 4.1834 
[12/23 21:03:24 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8225 LR: 0.000097 Step: 124450 Total Loss: 4.8519 Recon Loss: 4.8373 
[12/23 21:04:05 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000097 Step: 124500 Total Loss: 4.8222 Recon Loss: 4.8075 
[12/23 21:04:46 TiTok]: Data (t): 0.0011, 38.55/s/gpu Batch (t): 0.8300 LR: 0.000097 Step: 124550 Total Loss: 4.8799 Recon Loss: 4.8651 
[12/23 21:05:26 TiTok]: Data (t): 0.0011, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000097 Step: 124600 Total Loss: 4.7817 Recon Loss: 4.7671 
[12/23 21:06:07 TiTok]: Data (t): 0.0011, 39.38/s/gpu Batch (t): 0.8125 LR: 0.000097 Step: 124650 Total Loss: 6.8356 Recon Loss: 6.8209 
[12/23 21:06:48 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000097 Step: 124700 Total Loss: 4.2189 Recon Loss: 4.2042 
[12/23 21:07:29 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000097 Step: 124750 Total Loss: 4.1914 Recon Loss: 4.1768 
[12/23 21:08:10 TiTok]: Data (t): 0.0010, 39.48/s/gpu Batch (t): 0.8106 LR: 0.000097 Step: 124800 Total Loss: 4.8716 Recon Loss: 4.8570 
[12/23 21:08:51 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8204 LR: 0.000097 Step: 124850 Total Loss: 4.1605 Recon Loss: 4.1458 
[12/23 21:09:32 TiTok]: Data (t): 0.0011, 38.82/s/gpu Batch (t): 0.8242 LR: 0.000097 Step: 124900 Total Loss: 4.8246 Recon Loss: 4.8100 
[12/23 21:10:13 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8154 LR: 0.000097 Step: 124950 Total Loss: 6.1678 Recon Loss: 6.1531 
[12/23 21:10:54 TiTok]: Data (t): 0.0010, 35.53/s/gpu Batch (t): 0.9007 LR: 0.000097 Step: 125000 Total Loss: 4.8627 Recon Loss: 4.8479 
[12/23 21:10:55 TiTok]: Reconstructing images...
[12/23 21:11:36 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000097 Step: 125050 Total Loss: 4.8534 Recon Loss: 4.8387 
[12/23 21:12:17 TiTok]: Data (t): 0.0011, 39.41/s/gpu Batch (t): 0.8119 LR: 0.000097 Step: 125100 Total Loss: 5.4827 Recon Loss: 5.4680 
[12/23 21:12:58 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000097 Step: 125150 Total Loss: 6.8225 Recon Loss: 6.8077 
[12/23 21:13:39 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8148 LR: 0.000097 Step: 125200 Total Loss: 6.1972 Recon Loss: 6.1825 
[12/23 21:14:20 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000097 Step: 125250 Total Loss: 5.4774 Recon Loss: 5.4627 
[12/23 21:15:01 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000097 Step: 125300 Total Loss: 4.7978 Recon Loss: 4.7831 
[12/23 21:15:41 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8177 LR: 0.000097 Step: 125350 Total Loss: 4.1859 Recon Loss: 4.1712 
[12/23 21:16:22 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8200 LR: 0.000097 Step: 125400 Total Loss: 4.8628 Recon Loss: 4.8482 
[12/23 21:17:03 TiTok]: Data (t): 0.0010, 38.71/s/gpu Batch (t): 0.8267 LR: 0.000097 Step: 125450 Total Loss: 4.8455 Recon Loss: 4.8308 
[12/23 21:17:44 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000097 Step: 125500 Total Loss: 4.8518 Recon Loss: 4.8371 
[12/23 21:18:25 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000097 Step: 125550 Total Loss: 4.1970 Recon Loss: 4.1822 
[12/23 21:19:06 TiTok]: Data (t): 0.0011, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000097 Step: 125600 Total Loss: 5.5131 Recon Loss: 5.4983 
[12/23 21:19:47 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000097 Step: 125650 Total Loss: 4.1217 Recon Loss: 4.1070 
[12/23 21:20:28 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000097 Step: 125700 Total Loss: 4.2031 Recon Loss: 4.1884 
[12/23 21:21:08 TiTok]: Data (t): 0.0010, 38.75/s/gpu Batch (t): 0.8259 LR: 0.000097 Step: 125750 Total Loss: 4.1956 Recon Loss: 4.1809 
[12/23 21:21:49 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000097 Step: 125800 Total Loss: 4.7588 Recon Loss: 4.7441 
[12/23 21:22:30 TiTok]: Data (t): 0.0011, 39.41/s/gpu Batch (t): 0.8120 LR: 0.000097 Step: 125850 Total Loss: 6.1644 Recon Loss: 6.1497 
[12/23 21:23:11 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000097 Step: 125900 Total Loss: 4.9001 Recon Loss: 4.8854 
[12/23 21:23:52 TiTok]: Data (t): 0.0014, 39.63/s/gpu Batch (t): 0.8075 LR: 0.000097 Step: 125950 Total Loss: 4.1746 Recon Loss: 4.1598 
[12/23 21:24:33 TiTok]: Data (t): 0.0010, 34.93/s/gpu Batch (t): 0.9161 LR: 0.000097 Step: 126000 Total Loss: 4.1774 Recon Loss: 4.1627 
[12/23 21:25:14 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000097 Step: 126050 Total Loss: 4.2231 Recon Loss: 4.2084 
[12/23 21:25:55 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000097 Step: 126100 Total Loss: 4.1605 Recon Loss: 4.1457 
[12/23 21:26:35 TiTok]: Data (t): 0.0010, 38.82/s/gpu Batch (t): 0.8243 LR: 0.000097 Step: 126150 Total Loss: 4.8479 Recon Loss: 4.8331 
[12/23 21:27:16 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000097 Step: 126200 Total Loss: 5.4672 Recon Loss: 5.4525 
[12/23 21:27:57 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8206 LR: 0.000097 Step: 126250 Total Loss: 4.8636 Recon Loss: 4.8488 
[12/23 21:28:38 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000097 Step: 126300 Total Loss: 4.1719 Recon Loss: 4.1572 
[12/23 21:29:19 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000097 Step: 126350 Total Loss: 4.8457 Recon Loss: 4.8309 
[12/23 21:30:00 TiTok]: Data (t): 0.0014, 39.09/s/gpu Batch (t): 0.8185 LR: 0.000097 Step: 126400 Total Loss: 4.8481 Recon Loss: 4.8334 
[12/23 21:30:41 TiTok]: Data (t): 0.0010, 39.39/s/gpu Batch (t): 0.8124 LR: 0.000097 Step: 126450 Total Loss: 4.8472 Recon Loss: 4.8325 
[12/23 21:31:21 TiTok]: Data (t): 0.0010, 38.87/s/gpu Batch (t): 0.8232 LR: 0.000097 Step: 126500 Total Loss: 4.8735 Recon Loss: 4.8588 
[12/23 21:32:02 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000097 Step: 126550 Total Loss: 4.8868 Recon Loss: 4.8720 
[12/23 21:32:43 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8146 LR: 0.000097 Step: 126600 Total Loss: 4.2257 Recon Loss: 4.2111 
[12/23 21:33:24 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000097 Step: 126650 Total Loss: 5.5268 Recon Loss: 5.5120 
[12/23 21:34:05 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8217 LR: 0.000097 Step: 126700 Total Loss: 4.1850 Recon Loss: 4.1703 
[12/23 21:34:46 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000097 Step: 126750 Total Loss: 4.8557 Recon Loss: 4.8410 
[12/23 21:35:27 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000097 Step: 126800 Total Loss: 5.4956 Recon Loss: 5.4809 
[12/23 21:36:08 TiTok]: Data (t): 0.0012, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000097 Step: 126850 Total Loss: 4.1347 Recon Loss: 4.1200 
[12/23 21:36:49 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000097 Step: 126900 Total Loss: 4.8452 Recon Loss: 4.8305 
[12/23 21:37:29 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000097 Step: 126950 Total Loss: 4.2126 Recon Loss: 4.1978 
[12/23 21:38:10 TiTok]: Data (t): 0.0011, 35.56/s/gpu Batch (t): 0.9000 LR: 0.000097 Step: 127000 Total Loss: 4.1774 Recon Loss: 4.1627 
[12/23 21:38:51 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000097 Step: 127050 Total Loss: 5.5476 Recon Loss: 5.5327 
[12/23 21:39:32 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000097 Step: 127100 Total Loss: 4.1339 Recon Loss: 4.1192 
[12/23 21:40:13 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8223 LR: 0.000097 Step: 127150 Total Loss: 4.1637 Recon Loss: 4.1491 
[12/23 21:40:54 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000097 Step: 127200 Total Loss: 5.5369 Recon Loss: 5.5221 
[12/23 21:41:35 TiTok]: Data (t): 0.0010, 39.50/s/gpu Batch (t): 0.8102 LR: 0.000097 Step: 127250 Total Loss: 5.4963 Recon Loss: 5.4816 
[12/23 21:42:16 TiTok]: Data (t): 0.0011, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000097 Step: 127300 Total Loss: 4.2097 Recon Loss: 4.1950 
[12/23 21:42:56 TiTok]: Data (t): 0.0021, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000097 Step: 127350 Total Loss: 4.1721 Recon Loss: 4.1574 
[12/23 21:43:37 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000097 Step: 127400 Total Loss: 4.8409 Recon Loss: 4.8261 
[12/23 21:44:18 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000097 Step: 127450 Total Loss: 5.4512 Recon Loss: 5.4365 
[12/23 21:44:59 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8162 LR: 0.000097 Step: 127500 Total Loss: 4.2163 Recon Loss: 4.2016 
[12/23 21:45:40 TiTok]: Data (t): 0.0010, 39.47/s/gpu Batch (t): 0.8108 LR: 0.000097 Step: 127550 Total Loss: 4.8057 Recon Loss: 4.7910 
[12/23 21:46:21 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000097 Step: 127600 Total Loss: 4.8390 Recon Loss: 4.8243 
[12/23 21:47:02 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000097 Step: 127650 Total Loss: 4.8212 Recon Loss: 4.8065 
[12/23 21:47:42 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8185 LR: 0.000097 Step: 127700 Total Loss: 4.8671 Recon Loss: 4.8523 
[12/23 21:48:23 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000097 Step: 127750 Total Loss: 4.1373 Recon Loss: 4.1226 
[12/23 21:49:04 TiTok]: Data (t): 0.0016, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000097 Step: 127800 Total Loss: 4.8011 Recon Loss: 4.7863 
[12/23 21:49:45 TiTok]: Data (t): 0.0010, 39.50/s/gpu Batch (t): 0.8102 LR: 0.000097 Step: 127850 Total Loss: 4.1554 Recon Loss: 4.1407 
[12/23 21:50:26 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000097 Step: 127900 Total Loss: 5.4845 Recon Loss: 5.4697 
[12/23 21:51:07 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8185 LR: 0.000097 Step: 127950 Total Loss: 4.8095 Recon Loss: 4.7949 
[12/23 21:51:48 TiTok]: Data (t): 0.0010, 35.20/s/gpu Batch (t): 0.9090 LR: 0.000097 Step: 128000 Total Loss: 5.4695 Recon Loss: 5.4548 
[12/23 21:52:29 TiTok]: Data (t): 0.0011, 39.50/s/gpu Batch (t): 0.8100 LR: 0.000097 Step: 128050 Total Loss: 5.5381 Recon Loss: 5.5233 
[12/23 21:53:09 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000097 Step: 128100 Total Loss: 5.5179 Recon Loss: 5.5032 
[12/23 21:53:50 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000097 Step: 128150 Total Loss: 4.7639 Recon Loss: 4.7492 
[12/23 21:54:31 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000097 Step: 128200 Total Loss: 5.4839 Recon Loss: 5.4691 
[12/23 21:55:12 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000097 Step: 128250 Total Loss: 4.2321 Recon Loss: 4.2174 
[12/23 21:55:53 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000097 Step: 128300 Total Loss: 4.1146 Recon Loss: 4.0998 
[12/23 21:56:34 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000097 Step: 128350 Total Loss: 5.4994 Recon Loss: 5.4846 
[12/23 21:57:15 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8221 LR: 0.000097 Step: 128400 Total Loss: 4.7793 Recon Loss: 4.7646 
[12/23 21:57:56 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000097 Step: 128450 Total Loss: 4.7992 Recon Loss: 4.7844 
[12/23 21:58:36 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000097 Step: 128500 Total Loss: 4.2112 Recon Loss: 4.1965 
[12/23 21:59:17 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000097 Step: 128550 Total Loss: 4.0532 Recon Loss: 4.0385 
[12/23 21:59:58 TiTok]: Data (t): 0.0009, 39.01/s/gpu Batch (t): 0.8204 LR: 0.000097 Step: 128600 Total Loss: 4.1811 Recon Loss: 4.1664 
[12/23 22:00:39 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000097 Step: 128650 Total Loss: 4.7883 Recon Loss: 4.7737 
[12/23 22:01:20 TiTok]: Data (t): 0.0010, 38.79/s/gpu Batch (t): 0.8249 LR: 0.000097 Step: 128700 Total Loss: 4.8410 Recon Loss: 4.8263 
[12/23 22:02:01 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8177 LR: 0.000097 Step: 128750 Total Loss: 6.1601 Recon Loss: 6.1454 
[12/23 22:02:42 TiTok]: Data (t): 0.0010, 39.37/s/gpu Batch (t): 0.8129 LR: 0.000097 Step: 128800 Total Loss: 5.5521 Recon Loss: 5.5374 
[12/23 22:03:22 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000097 Step: 128850 Total Loss: 4.1645 Recon Loss: 4.1498 
[12/23 22:04:03 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000097 Step: 128900 Total Loss: 4.8331 Recon Loss: 4.8183 
[12/23 22:04:44 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8181 LR: 0.000097 Step: 128950 Total Loss: 4.7715 Recon Loss: 4.7568 
[12/23 22:05:25 TiTok]: Data (t): 0.0010, 35.02/s/gpu Batch (t): 0.9139 LR: 0.000097 Step: 129000 Total Loss: 6.1528 Recon Loss: 6.1381 
[12/23 22:06:06 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000097 Step: 129050 Total Loss: 4.8787 Recon Loss: 4.8639 
[12/23 22:06:47 TiTok]: Data (t): 0.0010, 38.52/s/gpu Batch (t): 0.8307 LR: 0.000097 Step: 129100 Total Loss: 4.8684 Recon Loss: 4.8537 
[12/23 22:07:28 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000097 Step: 129150 Total Loss: 4.0923 Recon Loss: 4.0776 
[12/23 22:08:08 TiTok]: Data (t): 0.0015, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000097 Step: 129200 Total Loss: 5.4631 Recon Loss: 5.4485 
[12/23 22:08:49 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000097 Step: 129250 Total Loss: 4.7932 Recon Loss: 4.7785 
[12/23 22:09:30 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000097 Step: 129300 Total Loss: 4.8357 Recon Loss: 4.8209 
[12/23 22:10:11 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000097 Step: 129350 Total Loss: 5.4710 Recon Loss: 5.4562 
[12/23 22:10:52 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000097 Step: 129400 Total Loss: 4.8134 Recon Loss: 4.7987 
[12/23 22:11:33 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000097 Step: 129450 Total Loss: 4.8149 Recon Loss: 4.8001 
[12/23 22:12:14 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000097 Step: 129500 Total Loss: 4.8129 Recon Loss: 4.7983 
[12/23 22:12:55 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000097 Step: 129550 Total Loss: 4.8255 Recon Loss: 4.8107 
[12/23 22:13:35 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000097 Step: 129600 Total Loss: 6.1493 Recon Loss: 6.1346 
[12/23 22:14:16 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000097 Step: 129650 Total Loss: 6.2023 Recon Loss: 6.1876 
[12/23 22:14:57 TiTok]: Data (t): 0.0010, 39.45/s/gpu Batch (t): 0.8111 LR: 0.000097 Step: 129700 Total Loss: 5.4669 Recon Loss: 5.4521 
[12/23 22:15:38 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000097 Step: 129750 Total Loss: 4.1482 Recon Loss: 4.1335 
[12/23 22:16:19 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000097 Step: 129800 Total Loss: 4.8814 Recon Loss: 4.8667 
[12/23 22:17:00 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000097 Step: 129850 Total Loss: 4.8492 Recon Loss: 4.8344 
[12/23 22:17:41 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000097 Step: 129900 Total Loss: 4.1560 Recon Loss: 4.1413 
[12/23 22:18:21 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000097 Step: 129950 Total Loss: 4.7907 Recon Loss: 4.7760 
[12/23 22:19:02 TiTok]: Data (t): 0.0010, 35.48/s/gpu Batch (t): 0.9018 LR: 0.000097 Step: 130000 Total Loss: 4.7755 Recon Loss: 4.7607 
[12/23 22:19:04 TiTok]: Reconstructing images...
[12/23 22:19:45 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000097 Step: 130050 Total Loss: 5.5292 Recon Loss: 5.5144 
[12/23 22:20:26 TiTok]: Data (t): 0.0011, 39.44/s/gpu Batch (t): 0.8113 LR: 0.000097 Step: 130100 Total Loss: 4.8273 Recon Loss: 4.8126 
[12/23 22:21:06 TiTok]: Data (t): 0.0011, 39.49/s/gpu Batch (t): 0.8104 LR: 0.000097 Step: 130150 Total Loss: 6.2028 Recon Loss: 6.1882 
[12/23 22:21:47 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000097 Step: 130200 Total Loss: 4.1586 Recon Loss: 4.1439 
[12/23 22:22:28 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000097 Step: 130250 Total Loss: 4.9241 Recon Loss: 4.9093 
Epoch 13/99 started.
[12/23 22:23:10 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000097 Step: 130300 Total Loss: 4.8047 Recon Loss: 4.7898 
[12/23 22:23:51 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000097 Step: 130350 Total Loss: 4.8268 Recon Loss: 4.8120 
[12/23 22:24:32 TiTok]: Data (t): 0.0012, 38.94/s/gpu Batch (t): 0.8217 LR: 0.000097 Step: 130400 Total Loss: 5.4963 Recon Loss: 5.4816 
[12/23 22:25:13 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000097 Step: 130450 Total Loss: 4.7936 Recon Loss: 4.7790 
[12/23 22:25:53 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8150 LR: 0.000097 Step: 130500 Total Loss: 4.1421 Recon Loss: 4.1273 
[12/23 22:26:34 TiTok]: Data (t): 0.0017, 39.64/s/gpu Batch (t): 0.8073 LR: 0.000097 Step: 130550 Total Loss: 5.5028 Recon Loss: 5.4882 
[12/23 22:27:15 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000097 Step: 130600 Total Loss: 4.1852 Recon Loss: 4.1704 
[12/23 22:27:56 TiTok]: Data (t): 0.0010, 39.45/s/gpu Batch (t): 0.8112 LR: 0.000097 Step: 130650 Total Loss: 4.7772 Recon Loss: 4.7625 
[12/23 22:28:37 TiTok]: Data (t): 0.0011, 38.79/s/gpu Batch (t): 0.8250 LR: 0.000097 Step: 130700 Total Loss: 5.4397 Recon Loss: 5.4250 
[12/23 22:29:18 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8202 LR: 0.000097 Step: 130750 Total Loss: 4.8123 Recon Loss: 4.7975 
[12/23 22:29:59 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000097 Step: 130800 Total Loss: 4.8434 Recon Loss: 4.8287 
[12/23 22:30:40 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000097 Step: 130850 Total Loss: 4.8395 Recon Loss: 4.8248 
[12/23 22:31:20 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000097 Step: 130900 Total Loss: 4.8297 Recon Loss: 4.8149 
[12/23 22:32:01 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000097 Step: 130950 Total Loss: 4.1312 Recon Loss: 4.1166 
[12/23 22:32:42 TiTok]: Data (t): 0.0010, 34.02/s/gpu Batch (t): 0.9407 LR: 0.000097 Step: 131000 Total Loss: 4.8590 Recon Loss: 4.8443 
[12/23 22:33:23 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000097 Step: 131050 Total Loss: 4.1202 Recon Loss: 4.1054 
[12/23 22:34:04 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000097 Step: 131100 Total Loss: 4.8367 Recon Loss: 4.8219 
[12/23 22:34:45 TiTok]: Data (t): 0.0011, 38.89/s/gpu Batch (t): 0.8229 LR: 0.000097 Step: 131150 Total Loss: 4.8757 Recon Loss: 4.8609 
[12/23 22:35:26 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000097 Step: 131200 Total Loss: 4.8944 Recon Loss: 4.8797 
[12/23 22:36:07 TiTok]: Data (t): 0.0011, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000097 Step: 131250 Total Loss: 5.4677 Recon Loss: 5.4530 
[12/23 22:36:47 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000097 Step: 131300 Total Loss: 4.2058 Recon Loss: 4.1911 
[12/23 22:37:28 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000097 Step: 131350 Total Loss: 6.1356 Recon Loss: 6.1209 
[12/23 22:38:09 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8133 LR: 0.000097 Step: 131400 Total Loss: 6.1543 Recon Loss: 6.1395 
[12/23 22:38:50 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000097 Step: 131450 Total Loss: 4.8344 Recon Loss: 4.8197 
[12/23 22:39:31 TiTok]: Data (t): 0.0015, 39.02/s/gpu Batch (t): 0.8202 LR: 0.000097 Step: 131500 Total Loss: 6.1728 Recon Loss: 6.1581 
[12/23 22:40:12 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000097 Step: 131550 Total Loss: 4.2027 Recon Loss: 4.1880 
[12/23 22:40:53 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8204 LR: 0.000097 Step: 131600 Total Loss: 4.1631 Recon Loss: 4.1484 
[12/23 22:41:33 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000097 Step: 131650 Total Loss: 4.1669 Recon Loss: 4.1522 
[12/23 22:42:14 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000097 Step: 131700 Total Loss: 4.1576 Recon Loss: 4.1429 
[12/23 22:42:55 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000097 Step: 131750 Total Loss: 4.7759 Recon Loss: 4.7612 
[12/23 22:43:36 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000097 Step: 131800 Total Loss: 4.8226 Recon Loss: 4.8079 
[12/23 22:44:17 TiTok]: Data (t): 0.0010, 39.40/s/gpu Batch (t): 0.8123 LR: 0.000097 Step: 131850 Total Loss: 5.4929 Recon Loss: 5.4782 
[12/23 22:44:58 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000097 Step: 131900 Total Loss: 5.5366 Recon Loss: 5.5218 
[12/23 22:45:39 TiTok]: Data (t): 0.0016, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000097 Step: 131950 Total Loss: 4.8036 Recon Loss: 4.7888 
[12/23 22:46:20 TiTok]: Data (t): 0.0011, 34.98/s/gpu Batch (t): 0.9147 LR: 0.000097 Step: 132000 Total Loss: 5.4765 Recon Loss: 5.4618 
[12/23 22:47:00 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000097 Step: 132050 Total Loss: 4.8279 Recon Loss: 4.8132 
[12/23 22:47:41 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000097 Step: 132100 Total Loss: 4.1972 Recon Loss: 4.1826 
[12/23 22:48:22 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8183 LR: 0.000097 Step: 132150 Total Loss: 4.8861 Recon Loss: 4.8714 
[12/23 22:49:03 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000097 Step: 132200 Total Loss: 4.1187 Recon Loss: 4.1039 
[12/23 22:49:44 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000097 Step: 132250 Total Loss: 4.1712 Recon Loss: 4.1565 
[12/23 22:50:25 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8204 LR: 0.000097 Step: 132300 Total Loss: 5.4587 Recon Loss: 5.4440 
[12/23 22:51:06 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8131 LR: 0.000097 Step: 132350 Total Loss: 5.5050 Recon Loss: 5.4902 
[12/23 22:51:46 TiTok]: Data (t): 0.0011, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000097 Step: 132400 Total Loss: 4.7893 Recon Loss: 4.7746 
[12/23 22:52:27 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8129 LR: 0.000097 Step: 132450 Total Loss: 6.1679 Recon Loss: 6.1532 
[12/23 22:53:08 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000097 Step: 132500 Total Loss: 4.1462 Recon Loss: 4.1315 
[12/23 22:53:49 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000097 Step: 132550 Total Loss: 4.8049 Recon Loss: 4.7901 
[12/23 22:54:30 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000097 Step: 132600 Total Loss: 4.8267 Recon Loss: 4.8119 
[12/23 22:55:11 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000097 Step: 132650 Total Loss: 4.1795 Recon Loss: 4.1648 
[12/23 22:55:52 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000097 Step: 132700 Total Loss: 4.1952 Recon Loss: 4.1804 
[12/23 22:56:33 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000097 Step: 132750 Total Loss: 4.8397 Recon Loss: 4.8250 
[12/23 22:57:13 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000097 Step: 132800 Total Loss: 6.8389 Recon Loss: 6.8241 
[12/23 22:57:54 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000097 Step: 132850 Total Loss: 5.5406 Recon Loss: 5.5258 
[12/23 22:58:35 TiTok]: Data (t): 0.0015, 39.48/s/gpu Batch (t): 0.8105 LR: 0.000097 Step: 132900 Total Loss: 4.8542 Recon Loss: 4.8394 
[12/23 22:59:16 TiTok]: Data (t): 0.0010, 39.42/s/gpu Batch (t): 0.8118 LR: 0.000097 Step: 132950 Total Loss: 5.4144 Recon Loss: 5.3997 
[12/23 22:59:57 TiTok]: Data (t): 0.0010, 35.25/s/gpu Batch (t): 0.9077 LR: 0.000097 Step: 133000 Total Loss: 4.2026 Recon Loss: 4.1879 
[12/23 23:00:38 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000097 Step: 133050 Total Loss: 4.1267 Recon Loss: 4.1119 
[12/23 23:01:19 TiTok]: Data (t): 0.0011, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000097 Step: 133100 Total Loss: 5.5121 Recon Loss: 5.4974 
[12/23 23:02:00 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8162 LR: 0.000097 Step: 133150 Total Loss: 6.1747 Recon Loss: 6.1600 
[12/23 23:02:41 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000097 Step: 133200 Total Loss: 4.1725 Recon Loss: 4.1578 
[12/23 23:03:21 TiTok]: Data (t): 0.0010, 38.69/s/gpu Batch (t): 0.8271 LR: 0.000097 Step: 133250 Total Loss: 5.4902 Recon Loss: 5.4754 
[12/23 23:04:02 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000097 Step: 133300 Total Loss: 4.1875 Recon Loss: 4.1727 
[12/23 23:04:43 TiTok]: Data (t): 0.0010, 38.76/s/gpu Batch (t): 0.8256 LR: 0.000097 Step: 133350 Total Loss: 5.4716 Recon Loss: 5.4569 
[12/23 23:05:24 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000097 Step: 133400 Total Loss: 5.4854 Recon Loss: 5.4707 
[12/23 23:06:05 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000097 Step: 133450 Total Loss: 4.1542 Recon Loss: 4.1394 
[12/23 23:06:46 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000097 Step: 133500 Total Loss: 4.1948 Recon Loss: 4.1800 
[12/23 23:07:27 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000097 Step: 133550 Total Loss: 4.8620 Recon Loss: 4.8472 
[12/23 23:08:08 TiTok]: Data (t): 0.0010, 38.84/s/gpu Batch (t): 0.8238 LR: 0.000097 Step: 133600 Total Loss: 4.7752 Recon Loss: 4.7604 
[12/23 23:08:48 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8131 LR: 0.000097 Step: 133650 Total Loss: 4.7734 Recon Loss: 4.7587 
[12/23 23:09:29 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000097 Step: 133700 Total Loss: 4.2087 Recon Loss: 4.1939 
[12/23 23:10:10 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000097 Step: 133750 Total Loss: 4.1450 Recon Loss: 4.1302 
[12/23 23:10:51 TiTok]: Data (t): 0.0016, 39.19/s/gpu Batch (t): 0.8164 LR: 0.000097 Step: 133800 Total Loss: 4.0824 Recon Loss: 4.0677 
[12/23 23:11:32 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000097 Step: 133850 Total Loss: 5.4970 Recon Loss: 5.4822 
[12/23 23:12:13 TiTok]: Data (t): 0.0010, 38.43/s/gpu Batch (t): 0.8327 LR: 0.000097 Step: 133900 Total Loss: 5.5068 Recon Loss: 5.4920 
[12/23 23:12:54 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000097 Step: 133950 Total Loss: 4.1513 Recon Loss: 4.1366 
[12/23 23:13:35 TiTok]: Data (t): 0.0011, 35.26/s/gpu Batch (t): 0.9076 LR: 0.000097 Step: 134000 Total Loss: 4.1445 Recon Loss: 4.1298 
[12/23 23:14:15 TiTok]: Data (t): 0.0011, 39.61/s/gpu Batch (t): 0.8078 LR: 0.000097 Step: 134050 Total Loss: 5.5123 Recon Loss: 5.4975 
[12/23 23:14:56 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000097 Step: 134100 Total Loss: 4.7832 Recon Loss: 4.7685 
[12/23 23:15:37 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000097 Step: 134150 Total Loss: 4.8295 Recon Loss: 4.8147 
[12/23 23:16:18 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000097 Step: 134200 Total Loss: 5.4987 Recon Loss: 5.4841 
[12/23 23:16:59 TiTok]: Data (t): 0.0015, 39.38/s/gpu Batch (t): 0.8125 LR: 0.000097 Step: 134250 Total Loss: 4.1402 Recon Loss: 4.1254 
[12/23 23:17:40 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8137 LR: 0.000097 Step: 134300 Total Loss: 5.4923 Recon Loss: 5.4775 
[12/23 23:18:21 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000097 Step: 134350 Total Loss: 5.4471 Recon Loss: 5.4323 
[12/23 23:19:02 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000097 Step: 134400 Total Loss: 5.4901 Recon Loss: 5.4754 
[12/23 23:19:42 TiTok]: Data (t): 0.0011, 38.98/s/gpu Batch (t): 0.8209 LR: 0.000097 Step: 134450 Total Loss: 4.1465 Recon Loss: 4.1318 
[12/23 23:20:23 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8206 LR: 0.000097 Step: 134500 Total Loss: 4.8592 Recon Loss: 4.8444 
[12/23 23:21:04 TiTok]: Data (t): 0.0010, 38.84/s/gpu Batch (t): 0.8240 LR: 0.000097 Step: 134550 Total Loss: 4.1482 Recon Loss: 4.1334 
[12/23 23:21:45 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8162 LR: 0.000097 Step: 134600 Total Loss: 4.7811 Recon Loss: 4.7664 
[12/23 23:22:26 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000097 Step: 134650 Total Loss: 4.8003 Recon Loss: 4.7857 
[12/23 23:23:07 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000097 Step: 134700 Total Loss: 4.7771 Recon Loss: 4.7623 
[12/23 23:23:48 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000097 Step: 134750 Total Loss: 4.1712 Recon Loss: 4.1565 
[12/23 23:24:29 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000097 Step: 134800 Total Loss: 4.8309 Recon Loss: 4.8161 
[12/23 23:25:09 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000097 Step: 134850 Total Loss: 5.5400 Recon Loss: 5.5252 
[12/23 23:25:50 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8137 LR: 0.000097 Step: 134900 Total Loss: 4.8308 Recon Loss: 4.8160 
[12/23 23:26:31 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8156 LR: 0.000097 Step: 134950 Total Loss: 4.2006 Recon Loss: 4.1858 
[12/23 23:27:12 TiTok]: Data (t): 0.0010, 35.38/s/gpu Batch (t): 0.9045 LR: 0.000097 Step: 135000 Total Loss: 4.8045 Recon Loss: 4.7898 
[12/23 23:27:13 TiTok]: Reconstructing images...
[12/23 23:27:55 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000097 Step: 135050 Total Loss: 5.5276 Recon Loss: 5.5129 
[12/23 23:28:35 TiTok]: Data (t): 0.0010, 38.84/s/gpu Batch (t): 0.8240 LR: 0.000097 Step: 135100 Total Loss: 5.4755 Recon Loss: 5.4608 
[12/23 23:29:16 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000097 Step: 135150 Total Loss: 4.8036 Recon Loss: 4.7888 
[12/23 23:29:57 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8217 LR: 0.000096 Step: 135200 Total Loss: 4.0702 Recon Loss: 4.0555 
[12/23 23:30:38 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8150 LR: 0.000096 Step: 135250 Total Loss: 5.4931 Recon Loss: 5.4784 
[12/23 23:31:19 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000096 Step: 135300 Total Loss: 4.1277 Recon Loss: 4.1129 
[12/23 23:32:00 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8179 LR: 0.000096 Step: 135350 Total Loss: 4.7979 Recon Loss: 4.7832 
[12/23 23:32:41 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000096 Step: 135400 Total Loss: 6.0988 Recon Loss: 6.0841 
[12/23 23:33:22 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000096 Step: 135450 Total Loss: 4.7548 Recon Loss: 4.7401 
[12/23 23:34:03 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000096 Step: 135500 Total Loss: 5.5241 Recon Loss: 5.5094 
[12/23 23:34:43 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000096 Step: 135550 Total Loss: 6.1595 Recon Loss: 6.1447 
[12/23 23:35:24 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000096 Step: 135600 Total Loss: 4.0833 Recon Loss: 4.0686 
[12/23 23:36:05 TiTok]: Data (t): 0.0012, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000096 Step: 135650 Total Loss: 5.4687 Recon Loss: 5.4539 
[12/23 23:36:46 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000096 Step: 135700 Total Loss: 4.8323 Recon Loss: 4.8175 
[12/23 23:37:27 TiTok]: Data (t): 0.0011, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000096 Step: 135750 Total Loss: 5.5295 Recon Loss: 5.5148 
[12/23 23:38:08 TiTok]: Data (t): 0.0011, 39.35/s/gpu Batch (t): 0.8133 LR: 0.000096 Step: 135800 Total Loss: 4.8157 Recon Loss: 4.8009 
[12/23 23:38:49 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000096 Step: 135850 Total Loss: 4.8608 Recon Loss: 4.8461 
[12/23 23:39:30 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8217 LR: 0.000096 Step: 135900 Total Loss: 4.0823 Recon Loss: 4.0675 
[12/23 23:40:11 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8204 LR: 0.000096 Step: 135950 Total Loss: 5.4890 Recon Loss: 5.4743 
[12/23 23:40:51 TiTok]: Data (t): 0.0010, 35.13/s/gpu Batch (t): 0.9109 LR: 0.000096 Step: 136000 Total Loss: 4.1255 Recon Loss: 4.1107 
[12/23 23:41:32 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000096 Step: 136050 Total Loss: 4.7883 Recon Loss: 4.7735 
[12/23 23:42:13 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000096 Step: 136100 Total Loss: 4.1345 Recon Loss: 4.1198 
[12/23 23:42:54 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000096 Step: 136150 Total Loss: 5.5074 Recon Loss: 5.4926 
[12/23 23:43:35 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000096 Step: 136200 Total Loss: 4.7627 Recon Loss: 4.7479 
[12/23 23:44:16 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000096 Step: 136250 Total Loss: 4.8431 Recon Loss: 4.8284 
[12/23 23:44:57 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000096 Step: 136300 Total Loss: 4.8189 Recon Loss: 4.8042 
[12/23 23:45:38 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000096 Step: 136350 Total Loss: 4.1831 Recon Loss: 4.1684 
[12/23 23:46:19 TiTok]: Data (t): 0.0010, 39.44/s/gpu Batch (t): 0.8114 LR: 0.000096 Step: 136400 Total Loss: 4.8891 Recon Loss: 4.8744 
[12/23 23:46:59 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8221 LR: 0.000096 Step: 136450 Total Loss: 4.7961 Recon Loss: 4.7813 
[12/23 23:47:40 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000096 Step: 136500 Total Loss: 4.1774 Recon Loss: 4.1626 
[12/23 23:48:21 TiTok]: Data (t): 0.0017, 39.55/s/gpu Batch (t): 0.8091 LR: 0.000096 Step: 136550 Total Loss: 6.1532 Recon Loss: 6.1385 
[12/23 23:49:02 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000096 Step: 136600 Total Loss: 4.1663 Recon Loss: 4.1515 
[12/23 23:49:43 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000096 Step: 136650 Total Loss: 4.1591 Recon Loss: 4.1444 
[12/23 23:50:24 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000096 Step: 136700 Total Loss: 4.8518 Recon Loss: 4.8371 
[12/23 23:51:05 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000096 Step: 136750 Total Loss: 4.1638 Recon Loss: 4.1490 
[12/23 23:51:46 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000096 Step: 136800 Total Loss: 5.5418 Recon Loss: 5.5271 
[12/23 23:52:27 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000096 Step: 136850 Total Loss: 4.1417 Recon Loss: 4.1269 
[12/23 23:53:08 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8177 LR: 0.000096 Step: 136900 Total Loss: 5.5126 Recon Loss: 5.4979 
[12/23 23:53:49 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000096 Step: 136950 Total Loss: 4.1327 Recon Loss: 4.1180 
[12/23 23:54:30 TiTok]: Data (t): 0.0013, 34.26/s/gpu Batch (t): 0.9342 LR: 0.000096 Step: 137000 Total Loss: 5.5021 Recon Loss: 5.4873 
[12/23 23:55:10 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000096 Step: 137050 Total Loss: 4.8266 Recon Loss: 4.8118 
[12/23 23:55:51 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8137 LR: 0.000096 Step: 137100 Total Loss: 4.8701 Recon Loss: 4.8553 
[12/23 23:56:32 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000096 Step: 137150 Total Loss: 5.4581 Recon Loss: 5.4433 
[12/23 23:57:13 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8225 LR: 0.000096 Step: 137200 Total Loss: 4.0661 Recon Loss: 4.0513 
[12/23 23:57:54 TiTok]: Data (t): 0.0010, 39.41/s/gpu Batch (t): 0.8119 LR: 0.000096 Step: 137250 Total Loss: 5.4867 Recon Loss: 5.4720 
[12/23 23:58:35 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8181 LR: 0.000096 Step: 137300 Total Loss: 4.8216 Recon Loss: 4.8069 
[12/23 23:59:16 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000096 Step: 137350 Total Loss: 4.7785 Recon Loss: 4.7636 
[12/23 23:59:56 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000096 Step: 137400 Total Loss: 4.0848 Recon Loss: 4.0700 
[12/24 00:00:37 TiTok]: Data (t): 0.0013, 38.71/s/gpu Batch (t): 0.8267 LR: 0.000096 Step: 137450 Total Loss: 4.7801 Recon Loss: 4.7654 
[12/24 00:01:18 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000096 Step: 137500 Total Loss: 4.1446 Recon Loss: 4.1299 
[12/24 00:01:59 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8150 LR: 0.000096 Step: 137550 Total Loss: 4.8160 Recon Loss: 4.8012 
[12/24 00:02:40 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000096 Step: 137600 Total Loss: 4.8146 Recon Loss: 4.7999 
[12/24 00:03:21 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000096 Step: 137650 Total Loss: 4.7577 Recon Loss: 4.7429 
[12/24 00:04:02 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000096 Step: 137700 Total Loss: 5.4500 Recon Loss: 5.4353 
[12/24 00:04:43 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8137 LR: 0.000096 Step: 137750 Total Loss: 4.1761 Recon Loss: 4.1613 
[12/24 00:05:23 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000096 Step: 137800 Total Loss: 5.5033 Recon Loss: 5.4886 
[12/24 00:06:04 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000096 Step: 137850 Total Loss: 4.1255 Recon Loss: 4.1107 
[12/24 00:06:45 TiTok]: Data (t): 0.0011, 38.75/s/gpu Batch (t): 0.8258 LR: 0.000096 Step: 137900 Total Loss: 5.4956 Recon Loss: 5.4808 
[12/24 00:07:26 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8162 LR: 0.000096 Step: 137950 Total Loss: 4.8186 Recon Loss: 4.8038 
[12/24 00:08:07 TiTok]: Data (t): 0.0010, 35.33/s/gpu Batch (t): 0.9058 LR: 0.000096 Step: 138000 Total Loss: 4.1853 Recon Loss: 4.1705 
[12/24 00:08:48 TiTok]: Data (t): 0.0010, 38.83/s/gpu Batch (t): 0.8240 LR: 0.000096 Step: 138050 Total Loss: 4.7721 Recon Loss: 4.7574 
[12/24 00:09:29 TiTok]: Data (t): 0.0011, 39.37/s/gpu Batch (t): 0.8129 LR: 0.000096 Step: 138100 Total Loss: 5.4305 Recon Loss: 5.4157 
[12/24 00:10:10 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000096 Step: 138150 Total Loss: 4.1399 Recon Loss: 4.1251 
[12/24 00:10:50 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000096 Step: 138200 Total Loss: 4.7648 Recon Loss: 4.7501 
[12/24 00:11:31 TiTok]: Data (t): 0.0010, 39.43/s/gpu Batch (t): 0.8115 LR: 0.000096 Step: 138250 Total Loss: 4.1878 Recon Loss: 4.1731 
[12/24 00:12:12 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000096 Step: 138300 Total Loss: 4.7997 Recon Loss: 4.7848 
[12/24 00:12:53 TiTok]: Data (t): 0.0020, 39.66/s/gpu Batch (t): 0.8069 LR: 0.000096 Step: 138350 Total Loss: 4.9113 Recon Loss: 4.8966 
[12/24 00:13:34 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8234 LR: 0.000096 Step: 138400 Total Loss: 4.0954 Recon Loss: 4.0807 
[12/24 00:14:15 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000096 Step: 138450 Total Loss: 4.1780 Recon Loss: 4.1632 
[12/24 00:14:56 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000096 Step: 138500 Total Loss: 4.0628 Recon Loss: 4.0481 
[12/24 00:15:37 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8200 LR: 0.000096 Step: 138550 Total Loss: 4.8392 Recon Loss: 4.8244 
[12/24 00:16:17 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000096 Step: 138600 Total Loss: 4.7187 Recon Loss: 4.7040 
[12/24 00:16:58 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8158 LR: 0.000096 Step: 138650 Total Loss: 4.1320 Recon Loss: 4.1173 
[12/24 00:17:39 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000096 Step: 138700 Total Loss: 4.1428 Recon Loss: 4.1280 
[12/24 00:18:20 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000096 Step: 138750 Total Loss: 4.8196 Recon Loss: 4.8048 
[12/24 00:19:01 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000096 Step: 138800 Total Loss: 4.0657 Recon Loss: 4.0509 
[12/24 00:19:42 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000096 Step: 138850 Total Loss: 4.1313 Recon Loss: 4.1165 
[12/24 00:20:23 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000096 Step: 138900 Total Loss: 4.8075 Recon Loss: 4.7927 
[12/24 00:21:04 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000096 Step: 138950 Total Loss: 4.7815 Recon Loss: 4.7667 
[12/24 00:21:45 TiTok]: Data (t): 0.0010, 35.24/s/gpu Batch (t): 0.9081 LR: 0.000096 Step: 139000 Total Loss: 4.8584 Recon Loss: 4.8436 
[12/24 00:22:25 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8133 LR: 0.000096 Step: 139050 Total Loss: 5.4723 Recon Loss: 5.4576 
[12/24 00:23:06 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000096 Step: 139100 Total Loss: 4.8158 Recon Loss: 4.8010 
[12/24 00:23:47 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000096 Step: 139150 Total Loss: 4.7947 Recon Loss: 4.7800 
[12/24 00:24:28 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000096 Step: 139200 Total Loss: 5.4895 Recon Loss: 5.4748 
[12/24 00:25:09 TiTok]: Data (t): 0.0011, 39.43/s/gpu Batch (t): 0.8116 LR: 0.000096 Step: 139250 Total Loss: 5.4551 Recon Loss: 5.4403 
[12/24 00:25:50 TiTok]: Data (t): 0.0018, 39.39/s/gpu Batch (t): 0.8123 LR: 0.000096 Step: 139300 Total Loss: 4.8296 Recon Loss: 4.8148 
[12/24 00:26:31 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000096 Step: 139350 Total Loss: 4.8158 Recon Loss: 4.8010 
[12/24 00:27:11 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000096 Step: 139400 Total Loss: 4.1551 Recon Loss: 4.1403 
[12/24 00:27:52 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8162 LR: 0.000096 Step: 139450 Total Loss: 4.7904 Recon Loss: 4.7757 
[12/24 00:28:33 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000096 Step: 139500 Total Loss: 4.1094 Recon Loss: 4.0947 
[12/24 00:29:14 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000096 Step: 139550 Total Loss: 4.7485 Recon Loss: 4.7338 
[12/24 00:29:55 TiTok]: Data (t): 0.0010, 39.42/s/gpu Batch (t): 0.8118 LR: 0.000096 Step: 139600 Total Loss: 5.4697 Recon Loss: 5.4549 
[12/24 00:30:36 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8158 LR: 0.000096 Step: 139650 Total Loss: 4.1595 Recon Loss: 4.1448 
[12/24 00:31:17 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000096 Step: 139700 Total Loss: 4.7898 Recon Loss: 4.7750 
[12/24 00:31:58 TiTok]: Data (t): 0.0014, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000096 Step: 139750 Total Loss: 4.7533 Recon Loss: 4.7386 
[12/24 00:32:38 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000096 Step: 139800 Total Loss: 5.5067 Recon Loss: 5.4919 
[12/24 00:33:19 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000096 Step: 139850 Total Loss: 4.8030 Recon Loss: 4.7883 
[12/24 00:34:00 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000096 Step: 139900 Total Loss: 5.5142 Recon Loss: 5.4995 
[12/24 00:34:41 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8133 LR: 0.000096 Step: 139950 Total Loss: 4.7465 Recon Loss: 4.7318 
[12/24 00:35:22 TiTok]: Data (t): 0.0010, 35.31/s/gpu Batch (t): 0.9063 LR: 0.000096 Step: 140000 Total Loss: 4.1634 Recon Loss: 4.1487 
[12/24 00:35:23 TiTok]: Reconstructing images...
[12/24 00:36:04 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000096 Step: 140050 Total Loss: 4.8720 Recon Loss: 4.8571 
[12/24 00:36:45 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000096 Step: 140100 Total Loss: 4.7687 Recon Loss: 4.7539 
[12/24 00:37:26 TiTok]: Data (t): 0.0013, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000096 Step: 140150 Total Loss: 4.8333 Recon Loss: 4.8185 
[12/24 00:38:07 TiTok]: Data (t): 0.0017, 39.40/s/gpu Batch (t): 0.8121 LR: 0.000096 Step: 140200 Total Loss: 5.5041 Recon Loss: 5.4894 
[12/24 00:38:48 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8177 LR: 0.000096 Step: 140250 Total Loss: 4.8676 Recon Loss: 4.8528 
Epoch 14/99 started.
[12/24 00:39:30 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000096 Step: 140300 Total Loss: 4.1318 Recon Loss: 4.1170 
[12/24 00:40:10 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000096 Step: 140350 Total Loss: 5.4990 Recon Loss: 5.4842 
[12/24 00:40:51 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000096 Step: 140400 Total Loss: 4.8258 Recon Loss: 4.8110 
[12/24 00:41:32 TiTok]: Data (t): 0.0011, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000096 Step: 140450 Total Loss: 4.0951 Recon Loss: 4.0803 
[12/24 00:42:13 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000096 Step: 140500 Total Loss: 4.8147 Recon Loss: 4.7999 
[12/24 00:42:54 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000096 Step: 140550 Total Loss: 4.1595 Recon Loss: 4.1447 
[12/24 00:43:35 TiTok]: Data (t): 0.0010, 39.42/s/gpu Batch (t): 0.8117 LR: 0.000096 Step: 140600 Total Loss: 6.1727 Recon Loss: 6.1580 
[12/24 00:44:16 TiTok]: Data (t): 0.0017, 39.47/s/gpu Batch (t): 0.8108 LR: 0.000096 Step: 140650 Total Loss: 5.4489 Recon Loss: 5.4342 
[12/24 00:44:57 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000096 Step: 140700 Total Loss: 4.1896 Recon Loss: 4.1748 
[12/24 00:45:37 TiTok]: Data (t): 0.0011, 38.69/s/gpu Batch (t): 0.8272 LR: 0.000096 Step: 140750 Total Loss: 6.1937 Recon Loss: 6.1790 
[12/24 00:46:18 TiTok]: Data (t): 0.0011, 39.48/s/gpu Batch (t): 0.8105 LR: 0.000096 Step: 140800 Total Loss: 4.8311 Recon Loss: 4.8163 
[12/24 00:46:59 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000096 Step: 140850 Total Loss: 4.7479 Recon Loss: 4.7331 
[12/24 00:47:40 TiTok]: Data (t): 0.0010, 39.49/s/gpu Batch (t): 0.8104 LR: 0.000096 Step: 140900 Total Loss: 4.8573 Recon Loss: 4.8424 
[12/24 00:48:21 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000096 Step: 140950 Total Loss: 4.7812 Recon Loss: 4.7665 
[12/24 00:49:02 TiTok]: Data (t): 0.0010, 34.31/s/gpu Batch (t): 0.9327 LR: 0.000096 Step: 141000 Total Loss: 4.1230 Recon Loss: 4.1082 
[12/24 00:49:43 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000096 Step: 141050 Total Loss: 4.1200 Recon Loss: 4.1052 
[12/24 00:50:24 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000096 Step: 141100 Total Loss: 5.4747 Recon Loss: 5.4600 
[12/24 00:51:04 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000096 Step: 141150 Total Loss: 5.4684 Recon Loss: 5.4537 
[12/24 00:51:45 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8223 LR: 0.000096 Step: 141200 Total Loss: 4.1040 Recon Loss: 4.0894 
[12/24 00:52:26 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8129 LR: 0.000096 Step: 141250 Total Loss: 4.8430 Recon Loss: 4.8283 
[12/24 00:53:07 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000096 Step: 141300 Total Loss: 6.1253 Recon Loss: 6.1106 
[12/24 00:53:48 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000096 Step: 141350 Total Loss: 5.4334 Recon Loss: 5.4187 
[12/24 00:54:29 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8208 LR: 0.000096 Step: 141400 Total Loss: 4.1063 Recon Loss: 4.0915 
[12/24 00:55:10 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000096 Step: 141450 Total Loss: 4.0953 Recon Loss: 4.0805 
[12/24 00:55:50 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8206 LR: 0.000096 Step: 141500 Total Loss: 4.1301 Recon Loss: 4.1154 
[12/24 00:56:31 TiTok]: Data (t): 0.0012, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000096 Step: 141550 Total Loss: 4.7539 Recon Loss: 4.7392 
[12/24 00:57:12 TiTok]: Data (t): 0.0014, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000096 Step: 141600 Total Loss: 4.1468 Recon Loss: 4.1320 
[12/24 00:57:53 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000096 Step: 141650 Total Loss: 4.7853 Recon Loss: 4.7705 
[12/24 00:58:34 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000096 Step: 141700 Total Loss: 5.5008 Recon Loss: 5.4860 
[12/24 00:59:15 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000096 Step: 141750 Total Loss: 4.1312 Recon Loss: 4.1165 
[12/24 00:59:56 TiTok]: Data (t): 0.0015, 40.03/s/gpu Batch (t): 0.7994 LR: 0.000096 Step: 141800 Total Loss: 4.1957 Recon Loss: 4.1809 
[12/24 01:00:37 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000096 Step: 141850 Total Loss: 4.1113 Recon Loss: 4.0966 
[12/24 01:01:18 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000096 Step: 141900 Total Loss: 5.4792 Recon Loss: 5.4644 
[12/24 01:01:58 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000096 Step: 141950 Total Loss: 5.4825 Recon Loss: 5.4677 
[12/24 01:02:39 TiTok]: Data (t): 0.0017, 34.93/s/gpu Batch (t): 0.9161 LR: 0.000096 Step: 142000 Total Loss: 4.8217 Recon Loss: 4.8070 
[12/24 01:03:20 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000096 Step: 142050 Total Loss: 4.1745 Recon Loss: 4.1597 
[12/24 01:04:01 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000096 Step: 142100 Total Loss: 4.1335 Recon Loss: 4.1187 
[12/24 01:04:42 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000096 Step: 142150 Total Loss: 5.4798 Recon Loss: 5.4651 
[12/24 01:05:23 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000096 Step: 142200 Total Loss: 4.1654 Recon Loss: 4.1507 
[12/24 01:06:04 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8200 LR: 0.000096 Step: 142250 Total Loss: 4.1304 Recon Loss: 4.1156 
[12/24 01:06:45 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000096 Step: 142300 Total Loss: 5.5015 Recon Loss: 5.4867 
[12/24 01:07:26 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8158 LR: 0.000096 Step: 142350 Total Loss: 4.7505 Recon Loss: 4.7357 
[12/24 01:08:06 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8125 LR: 0.000096 Step: 142400 Total Loss: 4.1268 Recon Loss: 4.1120 
[12/24 01:08:47 TiTok]: Data (t): 0.0011, 38.66/s/gpu Batch (t): 0.8277 LR: 0.000096 Step: 142450 Total Loss: 5.4649 Recon Loss: 5.4501 
[12/24 01:09:28 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000096 Step: 142500 Total Loss: 4.8811 Recon Loss: 4.8663 
[12/24 01:10:09 TiTok]: Data (t): 0.0010, 38.64/s/gpu Batch (t): 0.8282 LR: 0.000096 Step: 142550 Total Loss: 4.0896 Recon Loss: 4.0749 
[12/24 01:10:50 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8160 LR: 0.000096 Step: 142600 Total Loss: 4.7905 Recon Loss: 4.7758 
[12/24 01:11:31 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000096 Step: 142650 Total Loss: 4.1115 Recon Loss: 4.0967 
[12/24 01:12:12 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000096 Step: 142700 Total Loss: 4.8409 Recon Loss: 4.8261 
[12/24 01:12:52 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000096 Step: 142750 Total Loss: 4.8332 Recon Loss: 4.8183 
[12/24 01:13:33 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8129 LR: 0.000096 Step: 142800 Total Loss: 5.4862 Recon Loss: 5.4715 
[12/24 01:14:14 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000096 Step: 142850 Total Loss: 4.1594 Recon Loss: 4.1447 
[12/24 01:14:55 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8179 LR: 0.000096 Step: 142900 Total Loss: 4.8311 Recon Loss: 4.8163 
[12/24 01:15:36 TiTok]: Data (t): 0.0015, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000096 Step: 142950 Total Loss: 4.8221 Recon Loss: 4.8074 
[12/24 01:16:17 TiTok]: Data (t): 0.0011, 35.51/s/gpu Batch (t): 0.9012 LR: 0.000096 Step: 143000 Total Loss: 4.0961 Recon Loss: 4.0813 
[12/24 01:16:58 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000096 Step: 143050 Total Loss: 4.7961 Recon Loss: 4.7813 
[12/24 01:17:39 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8162 LR: 0.000096 Step: 143100 Total Loss: 4.8099 Recon Loss: 4.7952 
[12/24 01:18:20 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000096 Step: 143150 Total Loss: 4.8072 Recon Loss: 4.7924 
[12/24 01:19:00 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000096 Step: 143200 Total Loss: 4.7740 Recon Loss: 4.7592 
[12/24 01:19:41 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000096 Step: 143250 Total Loss: 5.5427 Recon Loss: 5.5279 
[12/24 01:20:22 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000096 Step: 143300 Total Loss: 4.7734 Recon Loss: 4.7587 
[12/24 01:21:03 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000096 Step: 143350 Total Loss: 4.8484 Recon Loss: 4.8336 
[12/24 01:21:44 TiTok]: Data (t): 0.0013, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000096 Step: 143400 Total Loss: 5.5375 Recon Loss: 5.5228 
[12/24 01:22:25 TiTok]: Data (t): 0.0012, 39.10/s/gpu Batch (t): 0.8183 LR: 0.000096 Step: 143450 Total Loss: 4.7850 Recon Loss: 4.7701 
[12/24 01:23:06 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8160 LR: 0.000096 Step: 143500 Total Loss: 4.8605 Recon Loss: 4.8458 
[12/24 01:23:47 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000096 Step: 143550 Total Loss: 4.8176 Recon Loss: 4.8028 
[12/24 01:24:27 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000096 Step: 143600 Total Loss: 5.5350 Recon Loss: 5.5203 
[12/24 01:25:08 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000096 Step: 143650 Total Loss: 4.8109 Recon Loss: 4.7961 
[12/24 01:25:49 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000096 Step: 143700 Total Loss: 4.0970 Recon Loss: 4.0823 
[12/24 01:26:30 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000096 Step: 143750 Total Loss: 4.1295 Recon Loss: 4.1147 
[12/24 01:27:11 TiTok]: Data (t): 0.0010, 39.43/s/gpu Batch (t): 0.8115 LR: 0.000096 Step: 143800 Total Loss: 4.7594 Recon Loss: 4.7447 
[12/24 01:27:52 TiTok]: Data (t): 0.0014, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000096 Step: 143850 Total Loss: 6.1759 Recon Loss: 6.1611 
[12/24 01:28:33 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8150 LR: 0.000096 Step: 143900 Total Loss: 5.4722 Recon Loss: 5.4575 
[12/24 01:29:14 TiTok]: Data (t): 0.0010, 39.46/s/gpu Batch (t): 0.8110 LR: 0.000096 Step: 143950 Total Loss: 5.4782 Recon Loss: 5.4634 
[12/24 01:29:55 TiTok]: Data (t): 0.0010, 34.58/s/gpu Batch (t): 0.9253 LR: 0.000096 Step: 144000 Total Loss: 4.1290 Recon Loss: 4.1142 
[12/24 01:30:35 TiTok]: Data (t): 0.0010, 38.80/s/gpu Batch (t): 0.8247 LR: 0.000096 Step: 144050 Total Loss: 4.7922 Recon Loss: 4.7774 
[12/24 01:31:16 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000096 Step: 144100 Total Loss: 4.7961 Recon Loss: 4.7814 
[12/24 01:31:57 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000096 Step: 144150 Total Loss: 5.4643 Recon Loss: 5.4496 
[12/24 01:32:38 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000096 Step: 144200 Total Loss: 4.7964 Recon Loss: 4.7816 
[12/24 01:33:19 TiTok]: Data (t): 0.0011, 38.89/s/gpu Batch (t): 0.8229 LR: 0.000096 Step: 144250 Total Loss: 4.7404 Recon Loss: 4.7257 
[12/24 01:34:00 TiTok]: Data (t): 0.0011, 38.80/s/gpu Batch (t): 0.8248 LR: 0.000096 Step: 144300 Total Loss: 4.0751 Recon Loss: 4.0604 
[12/24 01:34:41 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000096 Step: 144350 Total Loss: 4.1506 Recon Loss: 4.1358 
[12/24 01:35:21 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000096 Step: 144400 Total Loss: 4.1428 Recon Loss: 4.1280 
[12/24 01:36:02 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000096 Step: 144450 Total Loss: 4.1393 Recon Loss: 4.1245 
[12/24 01:36:43 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8206 LR: 0.000096 Step: 144500 Total Loss: 5.4528 Recon Loss: 5.4379 
[12/24 01:37:24 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000096 Step: 144550 Total Loss: 4.7788 Recon Loss: 4.7640 
[12/24 01:38:05 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8183 LR: 0.000096 Step: 144600 Total Loss: 4.1612 Recon Loss: 4.1465 
[12/24 01:38:46 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000096 Step: 144650 Total Loss: 5.4865 Recon Loss: 5.4718 
[12/24 01:39:27 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000096 Step: 144700 Total Loss: 5.4735 Recon Loss: 5.4588 
[12/24 01:40:08 TiTok]: Data (t): 0.0021, 39.24/s/gpu Batch (t): 0.8156 LR: 0.000096 Step: 144750 Total Loss: 4.8222 Recon Loss: 4.8075 
[12/24 01:40:48 TiTok]: Data (t): 0.0010, 39.40/s/gpu Batch (t): 0.8122 LR: 0.000096 Step: 144800 Total Loss: 5.4351 Recon Loss: 5.4203 
[12/24 01:41:29 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000096 Step: 144850 Total Loss: 4.7993 Recon Loss: 4.7844 
[12/24 01:42:10 TiTok]: Data (t): 0.0011, 39.46/s/gpu Batch (t): 0.8109 LR: 0.000096 Step: 144900 Total Loss: 5.5248 Recon Loss: 5.5099 
[12/24 01:42:51 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000096 Step: 144950 Total Loss: 5.5133 Recon Loss: 5.4986 
[12/24 01:43:32 TiTok]: Data (t): 0.0010, 35.47/s/gpu Batch (t): 0.9022 LR: 0.000096 Step: 145000 Total Loss: 4.8439 Recon Loss: 4.8291 
[12/24 01:43:33 TiTok]: Reconstructing images...
[12/24 01:44:14 TiTok]: Data (t): 0.0010, 39.44/s/gpu Batch (t): 0.8113 LR: 0.000096 Step: 145050 Total Loss: 6.1721 Recon Loss: 6.1573 
[12/24 01:44:55 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8150 LR: 0.000096 Step: 145100 Total Loss: 4.7771 Recon Loss: 4.7623 
[12/24 01:45:36 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000096 Step: 145150 Total Loss: 4.8210 Recon Loss: 4.8063 
[12/24 01:46:17 TiTok]: Data (t): 0.0012, 38.97/s/gpu Batch (t): 0.8210 LR: 0.000096 Step: 145200 Total Loss: 4.7780 Recon Loss: 4.7632 
[12/24 01:46:58 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8177 LR: 0.000096 Step: 145250 Total Loss: 4.8501 Recon Loss: 4.8353 
[12/24 01:47:39 TiTok]: Data (t): 0.0011, 38.91/s/gpu Batch (t): 0.8223 LR: 0.000096 Step: 145300 Total Loss: 4.2417 Recon Loss: 4.2270 
[12/24 01:48:20 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000096 Step: 145350 Total Loss: 4.1505 Recon Loss: 4.1358 
[12/24 01:49:01 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8154 LR: 0.000096 Step: 145400 Total Loss: 4.1392 Recon Loss: 4.1244 
[12/24 01:49:41 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000096 Step: 145450 Total Loss: 6.1941 Recon Loss: 6.1793 
[12/24 01:50:22 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000096 Step: 145500 Total Loss: 4.7925 Recon Loss: 4.7778 
[12/24 01:51:03 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000096 Step: 145550 Total Loss: 4.7904 Recon Loss: 4.7756 
[12/24 01:51:44 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000096 Step: 145600 Total Loss: 5.4761 Recon Loss: 5.4613 
[12/24 01:52:25 TiTok]: Data (t): 0.0023, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000096 Step: 145650 Total Loss: 4.8360 Recon Loss: 4.8213 
[12/24 01:53:06 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000096 Step: 145700 Total Loss: 6.1308 Recon Loss: 6.1161 
[12/24 01:53:47 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000096 Step: 145750 Total Loss: 5.5006 Recon Loss: 5.4859 
[12/24 01:54:27 TiTok]: Data (t): 0.0010, 39.40/s/gpu Batch (t): 0.8121 LR: 0.000096 Step: 145800 Total Loss: 5.4411 Recon Loss: 5.4264 
[12/24 01:55:08 TiTok]: Data (t): 0.0010, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000096 Step: 145850 Total Loss: 4.1273 Recon Loss: 4.1126 
[12/24 01:55:49 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000096 Step: 145900 Total Loss: 5.5247 Recon Loss: 5.5099 
[12/24 01:56:30 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000096 Step: 145950 Total Loss: 4.8975 Recon Loss: 4.8827 
[12/24 01:57:11 TiTok]: Data (t): 0.0010, 34.98/s/gpu Batch (t): 0.9148 LR: 0.000096 Step: 146000 Total Loss: 4.8202 Recon Loss: 4.8055 
[12/24 01:57:52 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8221 LR: 0.000096 Step: 146050 Total Loss: 4.7883 Recon Loss: 4.7735 
[12/24 01:58:33 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000096 Step: 146100 Total Loss: 5.4474 Recon Loss: 5.4326 
[12/24 01:59:14 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000096 Step: 146150 Total Loss: 4.1274 Recon Loss: 4.1127 
[12/24 01:59:54 TiTok]: Data (t): 0.0011, 38.85/s/gpu Batch (t): 0.8237 LR: 0.000096 Step: 146200 Total Loss: 4.1801 Recon Loss: 4.1653 
[12/24 02:00:35 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8158 LR: 0.000096 Step: 146250 Total Loss: 4.1209 Recon Loss: 4.1061 
[12/24 02:01:16 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000096 Step: 146300 Total Loss: 5.4838 Recon Loss: 5.4690 
[12/24 02:01:57 TiTok]: Data (t): 0.0010, 39.54/s/gpu Batch (t): 0.8094 LR: 0.000096 Step: 146350 Total Loss: 5.5189 Recon Loss: 5.5041 
[12/24 02:02:38 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000096 Step: 146400 Total Loss: 4.7866 Recon Loss: 4.7719 
[12/24 02:03:19 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000096 Step: 146450 Total Loss: 6.1294 Recon Loss: 6.1146 
[12/24 02:04:00 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000096 Step: 146500 Total Loss: 4.7950 Recon Loss: 4.7802 
[12/24 02:04:41 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000096 Step: 146550 Total Loss: 4.7869 Recon Loss: 4.7721 
[12/24 02:05:21 TiTok]: Data (t): 0.0011, 38.77/s/gpu Batch (t): 0.8254 LR: 0.000096 Step: 146600 Total Loss: 5.5211 Recon Loss: 5.5063 
[12/24 02:06:02 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000096 Step: 146650 Total Loss: 4.8173 Recon Loss: 4.8025 
[12/24 02:06:43 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000096 Step: 146700 Total Loss: 4.8122 Recon Loss: 4.7974 
[12/24 02:07:24 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000096 Step: 146750 Total Loss: 4.1424 Recon Loss: 4.1276 
[12/24 02:08:05 TiTok]: Data (t): 0.0011, 39.42/s/gpu Batch (t): 0.8118 LR: 0.000096 Step: 146800 Total Loss: 4.8384 Recon Loss: 4.8237 
[12/24 02:08:46 TiTok]: Data (t): 0.0011, 38.84/s/gpu Batch (t): 0.8238 LR: 0.000096 Step: 146850 Total Loss: 4.8680 Recon Loss: 4.8532 
[12/24 02:09:27 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8158 LR: 0.000096 Step: 146900 Total Loss: 4.1339 Recon Loss: 4.1192 
[12/24 02:10:08 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000096 Step: 146950 Total Loss: 4.8010 Recon Loss: 4.7863 
[12/24 02:10:49 TiTok]: Data (t): 0.0011, 34.56/s/gpu Batch (t): 0.9260 LR: 0.000096 Step: 147000 Total Loss: 4.7543 Recon Loss: 4.7395 
[12/24 02:11:29 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000096 Step: 147050 Total Loss: 4.8300 Recon Loss: 4.8152 
[12/24 02:12:10 TiTok]: Data (t): 0.0011, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000096 Step: 147100 Total Loss: 4.1468 Recon Loss: 4.1320 
[12/24 02:12:51 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000096 Step: 147150 Total Loss: 5.5069 Recon Loss: 5.4921 
[12/24 02:13:32 TiTok]: Data (t): 0.0013, 38.86/s/gpu Batch (t): 0.8234 LR: 0.000096 Step: 147200 Total Loss: 4.7584 Recon Loss: 4.7436 
[12/24 02:14:13 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000096 Step: 147250 Total Loss: 5.4712 Recon Loss: 5.4564 
[12/24 02:14:54 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000096 Step: 147300 Total Loss: 5.4527 Recon Loss: 5.4380 
[12/24 02:15:35 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000096 Step: 147350 Total Loss: 4.7966 Recon Loss: 4.7819 
[12/24 02:16:16 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000096 Step: 147400 Total Loss: 4.8254 Recon Loss: 4.8107 
[12/24 02:16:56 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000096 Step: 147450 Total Loss: 5.4280 Recon Loss: 5.4133 
[12/24 02:17:37 TiTok]: Data (t): 0.0012, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000096 Step: 147500 Total Loss: 4.8042 Recon Loss: 4.7894 
[12/24 02:18:18 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000096 Step: 147550 Total Loss: 4.8334 Recon Loss: 4.8187 
[12/24 02:18:59 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8162 LR: 0.000096 Step: 147600 Total Loss: 5.5205 Recon Loss: 5.5058 
[12/24 02:19:40 TiTok]: Data (t): 0.0011, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000096 Step: 147650 Total Loss: 4.8253 Recon Loss: 4.8105 
[12/24 02:20:21 TiTok]: Data (t): 0.0012, 39.38/s/gpu Batch (t): 0.8127 LR: 0.000096 Step: 147700 Total Loss: 4.7720 Recon Loss: 4.7573 
[12/24 02:21:02 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8160 LR: 0.000096 Step: 147750 Total Loss: 4.8327 Recon Loss: 4.8179 
[12/24 02:21:43 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000096 Step: 147800 Total Loss: 4.7929 Recon Loss: 4.7781 
[12/24 02:22:24 TiTok]: Data (t): 0.0011, 38.81/s/gpu Batch (t): 0.8244 LR: 0.000096 Step: 147850 Total Loss: 5.5004 Recon Loss: 5.4856 
[12/24 02:23:04 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000096 Step: 147900 Total Loss: 5.4598 Recon Loss: 5.4451 
[12/24 02:23:45 TiTok]: Data (t): 0.0019, 39.40/s/gpu Batch (t): 0.8123 LR: 0.000096 Step: 147950 Total Loss: 5.4888 Recon Loss: 5.4740 
[12/24 02:24:26 TiTok]: Data (t): 0.0011, 34.68/s/gpu Batch (t): 0.9228 LR: 0.000096 Step: 148000 Total Loss: 4.8322 Recon Loss: 4.8173 
[12/24 02:25:07 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000096 Step: 148050 Total Loss: 4.1346 Recon Loss: 4.1199 
[12/24 02:25:48 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000096 Step: 148100 Total Loss: 4.1087 Recon Loss: 4.0940 
[12/24 02:26:29 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000096 Step: 148150 Total Loss: 4.1053 Recon Loss: 4.0906 
[12/24 02:27:10 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000096 Step: 148200 Total Loss: 5.5307 Recon Loss: 5.5160 
[12/24 02:27:51 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000096 Step: 148250 Total Loss: 4.7976 Recon Loss: 4.7828 
[12/24 02:28:32 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000096 Step: 148300 Total Loss: 4.7608 Recon Loss: 4.7460 
[12/24 02:29:12 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000096 Step: 148350 Total Loss: 4.8558 Recon Loss: 4.8410 
[12/24 02:29:53 TiTok]: Data (t): 0.0010, 38.69/s/gpu Batch (t): 0.8271 LR: 0.000096 Step: 148400 Total Loss: 5.4182 Recon Loss: 5.4034 
[12/24 02:30:34 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000096 Step: 148450 Total Loss: 4.1191 Recon Loss: 4.1043 
[12/24 02:31:15 TiTok]: Data (t): 0.0010, 39.42/s/gpu Batch (t): 0.8117 LR: 0.000096 Step: 148500 Total Loss: 4.8195 Recon Loss: 4.8047 
[12/24 02:31:56 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8162 LR: 0.000096 Step: 148550 Total Loss: 4.1260 Recon Loss: 4.1112 
[12/24 02:32:37 TiTok]: Data (t): 0.0011, 39.51/s/gpu Batch (t): 0.8100 LR: 0.000096 Step: 148600 Total Loss: 4.7653 Recon Loss: 4.7506 
[12/24 02:33:18 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8189 LR: 0.000096 Step: 148650 Total Loss: 4.0956 Recon Loss: 4.0808 
[12/24 02:33:59 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8223 LR: 0.000096 Step: 148700 Total Loss: 5.4980 Recon Loss: 5.4833 
[12/24 02:34:39 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000096 Step: 148750 Total Loss: 6.1462 Recon Loss: 6.1314 
[12/24 02:35:20 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000096 Step: 148800 Total Loss: 4.1242 Recon Loss: 4.1094 
[12/24 02:36:01 TiTok]: Data (t): 0.0011, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000096 Step: 148850 Total Loss: 4.1469 Recon Loss: 4.1322 
[12/24 02:36:42 TiTok]: Data (t): 0.0017, 39.63/s/gpu Batch (t): 0.8074 LR: 0.000096 Step: 148900 Total Loss: 4.7705 Recon Loss: 4.7558 
[12/24 02:37:23 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000096 Step: 148950 Total Loss: 4.7860 Recon Loss: 4.7712 
[12/24 02:38:04 TiTok]: Data (t): 0.0011, 35.19/s/gpu Batch (t): 0.9093 LR: 0.000096 Step: 149000 Total Loss: 4.8180 Recon Loss: 4.8033 
[12/24 02:38:45 TiTok]: Data (t): 0.0014, 39.21/s/gpu Batch (t): 0.8162 LR: 0.000096 Step: 149050 Total Loss: 4.7828 Recon Loss: 4.7679 
[12/24 02:39:26 TiTok]: Data (t): 0.0010, 39.44/s/gpu Batch (t): 0.8114 LR: 0.000096 Step: 149100 Total Loss: 4.7594 Recon Loss: 4.7446 
[12/24 02:40:06 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000096 Step: 149150 Total Loss: 4.8104 Recon Loss: 4.7957 
[12/24 02:40:47 TiTok]: Data (t): 0.0011, 38.98/s/gpu Batch (t): 0.8209 LR: 0.000096 Step: 149200 Total Loss: 4.8157 Recon Loss: 4.8010 
[12/24 02:41:28 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000096 Step: 149250 Total Loss: 6.1354 Recon Loss: 6.1206 
[12/24 02:42:09 TiTok]: Data (t): 0.0011, 39.42/s/gpu Batch (t): 0.8118 LR: 0.000096 Step: 149300 Total Loss: 4.7775 Recon Loss: 4.7627 
[12/24 02:42:50 TiTok]: Data (t): 0.0018, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000096 Step: 149350 Total Loss: 5.4484 Recon Loss: 5.4336 
[12/24 02:43:31 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000096 Step: 149400 Total Loss: 4.1737 Recon Loss: 4.1589 
[12/24 02:44:12 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8221 LR: 0.000096 Step: 149450 Total Loss: 4.6948 Recon Loss: 4.6800 
[12/24 02:44:53 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000096 Step: 149500 Total Loss: 4.7869 Recon Loss: 4.7721 
[12/24 02:45:34 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000096 Step: 149550 Total Loss: 4.0926 Recon Loss: 4.0778 
[12/24 02:46:14 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000096 Step: 149600 Total Loss: 4.8185 Recon Loss: 4.8038 
[12/24 02:46:55 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000096 Step: 149650 Total Loss: 4.0679 Recon Loss: 4.0531 
[12/24 02:47:36 TiTok]: Data (t): 0.0010, 39.48/s/gpu Batch (t): 0.8106 LR: 0.000096 Step: 149700 Total Loss: 4.8376 Recon Loss: 4.8228 
[12/24 02:48:17 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000096 Step: 149750 Total Loss: 4.8986 Recon Loss: 4.8839 
[12/24 02:48:58 TiTok]: Data (t): 0.0016, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000096 Step: 149800 Total Loss: 4.8370 Recon Loss: 4.8221 
[12/24 02:49:39 TiTok]: Data (t): 0.0011, 39.45/s/gpu Batch (t): 0.8112 LR: 0.000096 Step: 149850 Total Loss: 6.1596 Recon Loss: 6.1448 
[12/24 02:50:20 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000096 Step: 149900 Total Loss: 4.8072 Recon Loss: 4.7923 
[12/24 02:51:01 TiTok]: Data (t): 0.0011, 38.84/s/gpu Batch (t): 0.8239 LR: 0.000096 Step: 149950 Total Loss: 5.4471 Recon Loss: 5.4324 
[12/24 02:51:42 TiTok]: Data (t): 0.0012, 35.67/s/gpu Batch (t): 0.8970 LR: 0.000096 Step: 150000 Total Loss: 6.1488 Recon Loss: 6.1338 
Model weights saved in titok_b64_stage1_run1/checkpoint-150000/unwrapped_model/pytorch_model.bin
[12/24 02:51:44 TiTok]: Saved state to titok_b64_stage1_run1/checkpoint-150000
Model weights saved in titok_b64_stage1_run1/checkpoint-150000/ema_model/pytorch_model.bin
[12/24 02:51:59 TiTok]: Reconstructing images...
[12/24 02:52:00 TiTok]: Computing metrics on the validation set.
[12/24 03:05:41 TiTok]: EMA EVALUATION Step: 150000 
[12/24 03:05:41 TiTok]: {'CodebookEntropy': tensor(11.6430, device='cuda:0', dtype=torch.float64),
 'CodebookUsage': 0.784423828125,
 'InceptionScore': 37.63217062272948,
 'rFID': 91.23027493997819}
[12/24 03:07:43 TiTok]: Data (t): 0.0011, 39.88/s/gpu Batch (t): 0.8023 LR: 0.000096 Step: 150050 Total Loss: 4.7215 Recon Loss: 4.7067 
[12/24 03:08:23 TiTok]: Data (t): 0.0011, 39.45/s/gpu Batch (t): 0.8111 LR: 0.000096 Step: 150100 Total Loss: 4.8021 Recon Loss: 4.7873 
[12/24 03:09:04 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000096 Step: 150150 Total Loss: 5.5102 Recon Loss: 5.4953 
[12/24 03:09:45 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000096 Step: 150200 Total Loss: 6.1592 Recon Loss: 6.1445 
[12/24 03:10:26 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000096 Step: 150250 Total Loss: 4.7626 Recon Loss: 4.7478 
[12/24 03:11:06 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8200 LR: 0.000096 Step: 150300 Total Loss: 4.7449 Recon Loss: 4.7301 
Epoch 15/99 started.
[12/24 03:11:48 TiTok]: Data (t): 0.0010, 39.37/s/gpu Batch (t): 0.8129 LR: 0.000096 Step: 150350 Total Loss: 5.4775 Recon Loss: 5.4627 
[12/24 03:12:29 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000096 Step: 150400 Total Loss: 5.5063 Recon Loss: 5.4915 
[12/24 03:13:10 TiTok]: Data (t): 0.0010, 38.83/s/gpu Batch (t): 0.8241 LR: 0.000096 Step: 150450 Total Loss: 4.8062 Recon Loss: 4.7915 
[12/24 03:13:51 TiTok]: Data (t): 0.0011, 38.44/s/gpu Batch (t): 0.8325 LR: 0.000096 Step: 150500 Total Loss: 5.4506 Recon Loss: 5.4358 
[12/24 03:14:32 TiTok]: Data (t): 0.0009, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000096 Step: 150550 Total Loss: 4.8251 Recon Loss: 4.8103 
[12/24 03:15:13 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000096 Step: 150600 Total Loss: 5.4416 Recon Loss: 5.4268 
[12/24 03:15:54 TiTok]: Data (t): 0.0011, 39.41/s/gpu Batch (t): 0.8121 LR: 0.000096 Step: 150650 Total Loss: 5.4236 Recon Loss: 5.4089 
[12/24 03:16:35 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000096 Step: 150700 Total Loss: 4.7818 Recon Loss: 4.7670 
[12/24 03:17:16 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000096 Step: 150750 Total Loss: 4.7663 Recon Loss: 4.7514 
[12/24 03:17:56 TiTok]: Data (t): 0.0011, 39.35/s/gpu Batch (t): 0.8133 LR: 0.000096 Step: 150800 Total Loss: 4.8277 Recon Loss: 4.8129 
[12/24 03:18:37 TiTok]: Data (t): 0.0010, 38.43/s/gpu Batch (t): 0.8328 LR: 0.000096 Step: 150850 Total Loss: 4.7112 Recon Loss: 4.6965 
[12/24 03:19:18 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000096 Step: 150900 Total Loss: 4.8856 Recon Loss: 4.8708 
[12/24 03:19:59 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8217 LR: 0.000096 Step: 150950 Total Loss: 5.5583 Recon Loss: 5.5433 
[12/24 03:20:40 TiTok]: Data (t): 0.0010, 33.05/s/gpu Batch (t): 0.9682 LR: 0.000096 Step: 151000 Total Loss: 5.4507 Recon Loss: 5.4360 
[12/24 03:21:21 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8158 LR: 0.000096 Step: 151050 Total Loss: 5.4413 Recon Loss: 5.4264 
[12/24 03:22:02 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000096 Step: 151100 Total Loss: 4.1433 Recon Loss: 4.1285 
[12/24 03:22:43 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8210 LR: 0.000096 Step: 151150 Total Loss: 4.7865 Recon Loss: 4.7718 
[12/24 03:23:24 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000096 Step: 151200 Total Loss: 4.8033 Recon Loss: 4.7886 
[12/24 03:24:05 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8204 LR: 0.000096 Step: 151250 Total Loss: 5.4660 Recon Loss: 5.4512 
[12/24 03:24:46 TiTok]: Data (t): 0.0011, 38.83/s/gpu Batch (t): 0.8241 LR: 0.000096 Step: 151300 Total Loss: 4.0738 Recon Loss: 4.0591 
[12/24 03:25:27 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8158 LR: 0.000096 Step: 151350 Total Loss: 4.7839 Recon Loss: 4.7691 
[12/24 03:26:08 TiTok]: Data (t): 0.0010, 38.83/s/gpu Batch (t): 0.8241 LR: 0.000096 Step: 151400 Total Loss: 4.1441 Recon Loss: 4.1294 
[12/24 03:26:48 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8160 LR: 0.000096 Step: 151450 Total Loss: 4.8317 Recon Loss: 4.8170 
[12/24 03:27:29 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8231 LR: 0.000096 Step: 151500 Total Loss: 4.1351 Recon Loss: 4.1204 
[12/24 03:28:10 TiTok]: Data (t): 0.0011, 38.32/s/gpu Batch (t): 0.8351 LR: 0.000096 Step: 151550 Total Loss: 4.8331 Recon Loss: 4.8183 
[12/24 03:28:51 TiTok]: Data (t): 0.0011, 39.34/s/gpu Batch (t): 0.8133 LR: 0.000096 Step: 151600 Total Loss: 4.8430 Recon Loss: 4.8282 
[12/24 03:29:32 TiTok]: Data (t): 0.0009, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000096 Step: 151650 Total Loss: 4.8358 Recon Loss: 4.8211 
[12/24 03:30:13 TiTok]: Data (t): 0.0014, 39.51/s/gpu Batch (t): 0.8100 LR: 0.000096 Step: 151700 Total Loss: 4.8525 Recon Loss: 4.8377 
[12/24 03:30:54 TiTok]: Data (t): 0.0021, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000096 Step: 151750 Total Loss: 5.5060 Recon Loss: 5.4912 
[12/24 03:31:35 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000096 Step: 151800 Total Loss: 4.7957 Recon Loss: 4.7810 
[12/24 03:32:16 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8179 LR: 0.000096 Step: 151850 Total Loss: 4.7079 Recon Loss: 4.6930 
[12/24 03:32:57 TiTok]: Data (t): 0.0011, 38.75/s/gpu Batch (t): 0.8258 LR: 0.000096 Step: 151900 Total Loss: 4.1548 Recon Loss: 4.1401 
[12/24 03:33:38 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000096 Step: 151950 Total Loss: 4.7582 Recon Loss: 4.7434 
[12/24 03:34:19 TiTok]: Data (t): 0.0010, 34.60/s/gpu Batch (t): 0.9248 LR: 0.000096 Step: 152000 Total Loss: 5.4116 Recon Loss: 5.3969 
[12/24 03:34:59 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000096 Step: 152050 Total Loss: 5.4650 Recon Loss: 5.4502 
[12/24 03:35:40 TiTok]: Data (t): 0.0010, 39.39/s/gpu Batch (t): 0.8124 LR: 0.000096 Step: 152100 Total Loss: 4.8753 Recon Loss: 4.8605 
[12/24 03:36:21 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000096 Step: 152150 Total Loss: 4.8007 Recon Loss: 4.7859 
[12/24 03:37:02 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000095 Step: 152200 Total Loss: 5.4419 Recon Loss: 5.4271 
[12/24 03:37:43 TiTok]: Data (t): 0.0010, 39.42/s/gpu Batch (t): 0.8118 LR: 0.000095 Step: 152250 Total Loss: 5.4888 Recon Loss: 5.4740 
[12/24 03:38:24 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000095 Step: 152300 Total Loss: 4.8254 Recon Loss: 4.8105 
[12/24 03:39:05 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000095 Step: 152350 Total Loss: 4.1001 Recon Loss: 4.0853 
[12/24 03:39:46 TiTok]: Data (t): 0.0012, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000095 Step: 152400 Total Loss: 5.4432 Recon Loss: 5.4284 
[12/24 03:40:27 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8185 LR: 0.000095 Step: 152450 Total Loss: 5.4536 Recon Loss: 5.4388 
[12/24 03:41:07 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000095 Step: 152500 Total Loss: 4.7541 Recon Loss: 4.7393 
[12/24 03:41:48 TiTok]: Data (t): 0.0010, 38.34/s/gpu Batch (t): 0.8347 LR: 0.000095 Step: 152550 Total Loss: 4.1586 Recon Loss: 4.1438 
[12/24 03:42:29 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000095 Step: 152600 Total Loss: 4.7788 Recon Loss: 4.7640 
[12/24 03:43:10 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000095 Step: 152650 Total Loss: 4.1213 Recon Loss: 4.1065 
[12/24 03:43:51 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000095 Step: 152700 Total Loss: 4.7750 Recon Loss: 4.7604 
[12/24 03:44:32 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000095 Step: 152750 Total Loss: 6.1306 Recon Loss: 6.1157 
[12/24 03:45:13 TiTok]: Data (t): 0.0010, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000095 Step: 152800 Total Loss: 4.0454 Recon Loss: 4.0307 
[12/24 03:45:54 TiTok]: Data (t): 0.0011, 38.83/s/gpu Batch (t): 0.8241 LR: 0.000095 Step: 152850 Total Loss: 4.7952 Recon Loss: 4.7804 
[12/24 03:46:35 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000095 Step: 152900 Total Loss: 4.8785 Recon Loss: 4.8636 
[12/24 03:47:16 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000095 Step: 152950 Total Loss: 4.7868 Recon Loss: 4.7719 
[12/24 03:47:57 TiTok]: Data (t): 0.0011, 35.05/s/gpu Batch (t): 0.9129 LR: 0.000095 Step: 153000 Total Loss: 4.8243 Recon Loss: 4.8096 
[12/24 03:48:38 TiTok]: Data (t): 0.0011, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000095 Step: 153050 Total Loss: 4.7800 Recon Loss: 4.7652 
[12/24 03:49:19 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000095 Step: 153100 Total Loss: 4.8091 Recon Loss: 4.7943 
[12/24 03:49:59 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000095 Step: 153150 Total Loss: 4.1425 Recon Loss: 4.1277 
[12/24 03:50:40 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000095 Step: 153200 Total Loss: 4.8193 Recon Loss: 4.8045 
[12/24 03:51:21 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000095 Step: 153250 Total Loss: 4.1403 Recon Loss: 4.1256 
[12/24 03:52:02 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000095 Step: 153300 Total Loss: 4.1368 Recon Loss: 4.1220 
[12/24 03:52:43 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000095 Step: 153350 Total Loss: 5.5045 Recon Loss: 5.4897 
[12/24 03:53:24 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8202 LR: 0.000095 Step: 153400 Total Loss: 4.7915 Recon Loss: 4.7768 
[12/24 03:54:05 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000095 Step: 153450 Total Loss: 4.7829 Recon Loss: 4.7681 
[12/24 03:54:46 TiTok]: Data (t): 0.0010, 39.42/s/gpu Batch (t): 0.8117 LR: 0.000095 Step: 153500 Total Loss: 4.8107 Recon Loss: 4.7960 
[12/24 03:55:27 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8219 LR: 0.000095 Step: 153550 Total Loss: 4.7490 Recon Loss: 4.7343 
[12/24 03:56:08 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000095 Step: 153600 Total Loss: 6.1289 Recon Loss: 6.1141 
[12/24 03:56:48 TiTok]: Data (t): 0.0012, 39.45/s/gpu Batch (t): 0.8111 LR: 0.000095 Step: 153650 Total Loss: 5.4855 Recon Loss: 5.4708 
[12/24 03:57:29 TiTok]: Data (t): 0.0011, 39.50/s/gpu Batch (t): 0.8100 LR: 0.000095 Step: 153700 Total Loss: 4.0623 Recon Loss: 4.0476 
[12/24 03:58:10 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000095 Step: 153750 Total Loss: 4.1066 Recon Loss: 4.0918 
[12/24 03:58:51 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8158 LR: 0.000095 Step: 153800 Total Loss: 5.4404 Recon Loss: 5.4256 
[12/24 03:59:32 TiTok]: Data (t): 0.0013, 39.25/s/gpu Batch (t): 0.8154 LR: 0.000095 Step: 153850 Total Loss: 5.4858 Recon Loss: 5.4709 
[12/24 04:00:13 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000095 Step: 153900 Total Loss: 5.4738 Recon Loss: 5.4590 
[12/24 04:00:54 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000095 Step: 153950 Total Loss: 5.5105 Recon Loss: 5.4957 
[12/24 04:01:35 TiTok]: Data (t): 0.0011, 34.70/s/gpu Batch (t): 0.9222 LR: 0.000095 Step: 154000 Total Loss: 4.0591 Recon Loss: 4.0442 
[12/24 04:02:16 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000095 Step: 154050 Total Loss: 6.1514 Recon Loss: 6.1366 
[12/24 04:02:56 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000095 Step: 154100 Total Loss: 5.5028 Recon Loss: 5.4880 
[12/24 04:03:37 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000095 Step: 154150 Total Loss: 4.7646 Recon Loss: 4.7498 
[12/24 04:04:18 TiTok]: Data (t): 0.0014, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000095 Step: 154200 Total Loss: 5.4652 Recon Loss: 5.4504 
[12/24 04:04:59 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000095 Step: 154250 Total Loss: 5.5186 Recon Loss: 5.5037 
[12/24 04:05:40 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000095 Step: 154300 Total Loss: 4.1350 Recon Loss: 4.1202 
[12/24 04:06:21 TiTok]: Data (t): 0.0011, 39.48/s/gpu Batch (t): 0.8105 LR: 0.000095 Step: 154350 Total Loss: 4.0875 Recon Loss: 4.0728 
[12/24 04:07:02 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000095 Step: 154400 Total Loss: 4.0898 Recon Loss: 4.0750 
[12/24 04:07:43 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000095 Step: 154450 Total Loss: 5.5524 Recon Loss: 5.5376 
[12/24 04:08:24 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8181 LR: 0.000095 Step: 154500 Total Loss: 4.7844 Recon Loss: 4.7696 
[12/24 04:09:05 TiTok]: Data (t): 0.0011, 39.35/s/gpu Batch (t): 0.8133 LR: 0.000095 Step: 154550 Total Loss: 4.8255 Recon Loss: 4.8107 
[12/24 04:09:45 TiTok]: Data (t): 0.0011, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000095 Step: 154600 Total Loss: 4.0585 Recon Loss: 4.0437 
[12/24 04:10:26 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000095 Step: 154650 Total Loss: 4.0800 Recon Loss: 4.0653 
[12/24 04:11:07 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000095 Step: 154700 Total Loss: 6.1330 Recon Loss: 6.1182 
[12/24 04:11:48 TiTok]: Data (t): 0.0010, 38.82/s/gpu Batch (t): 0.8243 LR: 0.000095 Step: 154750 Total Loss: 4.7707 Recon Loss: 4.7560 
[12/24 04:12:29 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000095 Step: 154800 Total Loss: 5.4779 Recon Loss: 5.4632 
[12/24 04:13:10 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8139 LR: 0.000095 Step: 154850 Total Loss: 4.7395 Recon Loss: 4.7247 
[12/24 04:13:51 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8217 LR: 0.000095 Step: 154900 Total Loss: 4.8393 Recon Loss: 4.8245 
[12/24 04:14:32 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000095 Step: 154950 Total Loss: 4.7587 Recon Loss: 4.7439 
[12/24 04:15:13 TiTok]: Data (t): 0.0011, 34.72/s/gpu Batch (t): 0.9217 LR: 0.000095 Step: 155000 Total Loss: 4.8000 Recon Loss: 4.7852 
[12/24 04:15:14 TiTok]: Reconstructing images...
[12/24 04:15:55 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000095 Step: 155050 Total Loss: 4.8082 Recon Loss: 4.7934 
[12/24 04:16:36 TiTok]: Data (t): 0.0010, 38.78/s/gpu Batch (t): 0.8252 LR: 0.000095 Step: 155100 Total Loss: 4.8217 Recon Loss: 4.8069 
[12/24 04:17:17 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000095 Step: 155150 Total Loss: 4.7923 Recon Loss: 4.7775 
[12/24 04:17:58 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8229 LR: 0.000095 Step: 155200 Total Loss: 4.1298 Recon Loss: 4.1150 
[12/24 04:18:39 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000095 Step: 155250 Total Loss: 4.8005 Recon Loss: 4.7857 
[12/24 04:19:20 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8183 LR: 0.000095 Step: 155300 Total Loss: 4.0558 Recon Loss: 4.0410 
[12/24 04:20:01 TiTok]: Data (t): 0.0015, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000095 Step: 155350 Total Loss: 5.4817 Recon Loss: 5.4669 
[12/24 04:20:42 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8150 LR: 0.000095 Step: 155400 Total Loss: 5.5146 Recon Loss: 5.4997 
[12/24 04:21:22 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8219 LR: 0.000095 Step: 155450 Total Loss: 4.8230 Recon Loss: 4.8082 
[12/24 04:22:03 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000095 Step: 155500 Total Loss: 5.4825 Recon Loss: 5.4677 
[12/24 04:22:44 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000095 Step: 155550 Total Loss: 4.7242 Recon Loss: 4.7095 
[12/24 04:23:25 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000095 Step: 155600 Total Loss: 4.0955 Recon Loss: 4.0807 
[12/24 04:24:06 TiTok]: Data (t): 0.0011, 38.67/s/gpu Batch (t): 0.8274 LR: 0.000095 Step: 155650 Total Loss: 4.0326 Recon Loss: 4.0178 
[12/24 04:24:47 TiTok]: Data (t): 0.0010, 38.83/s/gpu Batch (t): 0.8241 LR: 0.000095 Step: 155700 Total Loss: 4.1329 Recon Loss: 4.1181 
[12/24 04:25:28 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8187 LR: 0.000095 Step: 155750 Total Loss: 5.4872 Recon Loss: 5.4724 
[12/24 04:26:09 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000095 Step: 155800 Total Loss: 4.7646 Recon Loss: 4.7498 
[12/24 04:26:50 TiTok]: Data (t): 0.0011, 39.39/s/gpu Batch (t): 0.8123 LR: 0.000095 Step: 155850 Total Loss: 4.1536 Recon Loss: 4.1389 
[12/24 04:27:31 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8154 LR: 0.000095 Step: 155900 Total Loss: 4.7371 Recon Loss: 4.7223 
[12/24 04:28:11 TiTok]: Data (t): 0.0016, 39.19/s/gpu Batch (t): 0.8164 LR: 0.000095 Step: 155950 Total Loss: 5.4579 Recon Loss: 5.4431 
[12/24 04:28:53 TiTok]: Data (t): 0.0010, 33.47/s/gpu Batch (t): 0.9561 LR: 0.000095 Step: 156000 Total Loss: 4.8289 Recon Loss: 4.8142 
[12/24 04:29:33 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000095 Step: 156050 Total Loss: 4.7238 Recon Loss: 4.7090 
[12/24 04:30:14 TiTok]: Data (t): 0.0011, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000095 Step: 156100 Total Loss: 4.8220 Recon Loss: 4.8072 
[12/24 04:30:55 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000095 Step: 156150 Total Loss: 5.4846 Recon Loss: 5.4697 
[12/24 04:31:36 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000095 Step: 156200 Total Loss: 4.8589 Recon Loss: 4.8441 
[12/24 04:32:17 TiTok]: Data (t): 0.0010, 38.80/s/gpu Batch (t): 0.8247 LR: 0.000095 Step: 156250 Total Loss: 4.0375 Recon Loss: 4.0228 
[12/24 04:32:58 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000095 Step: 156300 Total Loss: 4.0957 Recon Loss: 4.0810 
[12/24 04:33:39 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000095 Step: 156350 Total Loss: 5.5101 Recon Loss: 5.4954 
[12/24 04:34:20 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000095 Step: 156400 Total Loss: 4.0513 Recon Loss: 4.0366 
[12/24 04:35:01 TiTok]: Data (t): 0.0011, 38.78/s/gpu Batch (t): 0.8252 LR: 0.000095 Step: 156450 Total Loss: 4.1395 Recon Loss: 4.1247 
[12/24 04:35:42 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000095 Step: 156500 Total Loss: 4.8134 Recon Loss: 4.7986 
[12/24 04:36:22 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000095 Step: 156550 Total Loss: 5.4391 Recon Loss: 5.4242 
[12/24 04:37:03 TiTok]: Data (t): 0.0011, 38.66/s/gpu Batch (t): 0.8278 LR: 0.000095 Step: 156600 Total Loss: 5.5087 Recon Loss: 5.4939 
[12/24 04:37:44 TiTok]: Data (t): 0.0010, 39.41/s/gpu Batch (t): 0.8119 LR: 0.000095 Step: 156650 Total Loss: 4.8046 Recon Loss: 4.7899 
[12/24 04:38:25 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000095 Step: 156700 Total Loss: 4.7314 Recon Loss: 4.7167 
[12/24 04:39:06 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000095 Step: 156750 Total Loss: 4.1847 Recon Loss: 4.1699 
[12/24 04:39:47 TiTok]: Data (t): 0.0010, 38.75/s/gpu Batch (t): 0.8259 LR: 0.000095 Step: 156800 Total Loss: 5.4819 Recon Loss: 5.4672 
[12/24 04:40:28 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8127 LR: 0.000095 Step: 156850 Total Loss: 4.8367 Recon Loss: 4.8219 
[12/24 04:41:09 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000095 Step: 156900 Total Loss: 5.5169 Recon Loss: 5.5022 
[12/24 04:41:50 TiTok]: Data (t): 0.0011, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000095 Step: 156950 Total Loss: 4.0474 Recon Loss: 4.0326 
[12/24 04:42:31 TiTok]: Data (t): 0.0012, 33.99/s/gpu Batch (t): 0.9415 LR: 0.000095 Step: 157000 Total Loss: 5.4405 Recon Loss: 5.4257 
[12/24 04:43:12 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8166 LR: 0.000095 Step: 157050 Total Loss: 4.1705 Recon Loss: 4.1558 
[12/24 04:43:53 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000095 Step: 157100 Total Loss: 4.8103 Recon Loss: 4.7956 
[12/24 04:44:34 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000095 Step: 157150 Total Loss: 5.4532 Recon Loss: 5.4384 
[12/24 04:45:14 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8131 LR: 0.000095 Step: 157200 Total Loss: 5.4901 Recon Loss: 5.4753 
[12/24 04:45:55 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000095 Step: 157250 Total Loss: 4.1038 Recon Loss: 4.0891 
[12/24 04:46:36 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000095 Step: 157300 Total Loss: 4.8212 Recon Loss: 4.8064 
[12/24 04:47:17 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000095 Step: 157350 Total Loss: 5.4753 Recon Loss: 5.4605 
[12/24 04:47:58 TiTok]: Data (t): 0.0011, 38.86/s/gpu Batch (t): 0.8234 LR: 0.000095 Step: 157400 Total Loss: 4.0777 Recon Loss: 4.0630 
[12/24 04:48:39 TiTok]: Data (t): 0.0014, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000095 Step: 157450 Total Loss: 4.8094 Recon Loss: 4.7947 
[12/24 04:49:20 TiTok]: Data (t): 0.0011, 38.64/s/gpu Batch (t): 0.8281 LR: 0.000095 Step: 157500 Total Loss: 4.8272 Recon Loss: 4.8124 
[12/24 04:50:01 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000095 Step: 157550 Total Loss: 4.7693 Recon Loss: 4.7544 
[12/24 04:50:42 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000095 Step: 157600 Total Loss: 4.7896 Recon Loss: 4.7748 
[12/24 04:51:22 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8231 LR: 0.000095 Step: 157650 Total Loss: 4.1399 Recon Loss: 4.1251 
[12/24 04:52:03 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000095 Step: 157700 Total Loss: 4.8361 Recon Loss: 4.8213 
[12/24 04:52:44 TiTok]: Data (t): 0.0010, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000095 Step: 157750 Total Loss: 4.7746 Recon Loss: 4.7598 
[12/24 04:53:25 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000095 Step: 157800 Total Loss: 5.4348 Recon Loss: 5.4199 
[12/24 04:54:06 TiTok]: Data (t): 0.0010, 39.41/s/gpu Batch (t): 0.8120 LR: 0.000095 Step: 157850 Total Loss: 4.7805 Recon Loss: 4.7657 
[12/24 04:54:47 TiTok]: Data (t): 0.0010, 38.87/s/gpu Batch (t): 0.8233 LR: 0.000095 Step: 157900 Total Loss: 4.7954 Recon Loss: 4.7806 
[12/24 04:55:28 TiTok]: Data (t): 0.0010, 38.54/s/gpu Batch (t): 0.8303 LR: 0.000095 Step: 157950 Total Loss: 4.8615 Recon Loss: 4.8467 
[12/24 04:56:09 TiTok]: Data (t): 0.0011, 35.54/s/gpu Batch (t): 0.9004 LR: 0.000095 Step: 158000 Total Loss: 6.1957 Recon Loss: 6.1808 
[12/24 04:56:50 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000095 Step: 158050 Total Loss: 5.4838 Recon Loss: 5.4691 
[12/24 04:57:31 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000095 Step: 158100 Total Loss: 4.8332 Recon Loss: 4.8184 
[12/24 04:58:12 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000095 Step: 158150 Total Loss: 4.1115 Recon Loss: 4.0966 
[12/24 04:58:52 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000095 Step: 158200 Total Loss: 4.7533 Recon Loss: 4.7386 
[12/24 04:59:33 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000095 Step: 158250 Total Loss: 4.1712 Recon Loss: 4.1565 
[12/24 05:00:14 TiTok]: Data (t): 0.0010, 38.74/s/gpu Batch (t): 0.8260 LR: 0.000095 Step: 158300 Total Loss: 4.8449 Recon Loss: 4.8300 
[12/24 05:00:55 TiTok]: Data (t): 0.0012, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000095 Step: 158350 Total Loss: 6.1680 Recon Loss: 6.1531 
[12/24 05:01:36 TiTok]: Data (t): 0.0033, 38.94/s/gpu Batch (t): 0.8217 LR: 0.000095 Step: 158400 Total Loss: 5.4134 Recon Loss: 5.3987 
[12/24 05:02:17 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000095 Step: 158450 Total Loss: 4.1269 Recon Loss: 4.1121 
[12/24 05:02:58 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000095 Step: 158500 Total Loss: 4.7618 Recon Loss: 4.7470 
[12/24 05:03:39 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000095 Step: 158550 Total Loss: 4.0920 Recon Loss: 4.0771 
[12/24 05:04:20 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000095 Step: 158600 Total Loss: 4.8181 Recon Loss: 4.8032 
[12/24 05:05:00 TiTok]: Data (t): 0.0010, 39.56/s/gpu Batch (t): 0.8088 LR: 0.000095 Step: 158650 Total Loss: 4.7965 Recon Loss: 4.7816 
[12/24 05:05:41 TiTok]: Data (t): 0.0015, 39.41/s/gpu Batch (t): 0.8119 LR: 0.000095 Step: 158700 Total Loss: 5.4763 Recon Loss: 5.4615 
[12/24 05:06:22 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8181 LR: 0.000095 Step: 158750 Total Loss: 4.0694 Recon Loss: 4.0547 
[12/24 05:07:03 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000095 Step: 158800 Total Loss: 4.7695 Recon Loss: 4.7548 
[12/24 05:07:44 TiTok]: Data (t): 0.0012, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000095 Step: 158850 Total Loss: 4.8013 Recon Loss: 4.7865 
[12/24 05:08:25 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000095 Step: 158900 Total Loss: 4.0479 Recon Loss: 4.0331 
[12/24 05:09:06 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000095 Step: 158950 Total Loss: 4.0813 Recon Loss: 4.0664 
[12/24 05:09:47 TiTok]: Data (t): 0.0011, 35.46/s/gpu Batch (t): 0.9024 LR: 0.000095 Step: 159000 Total Loss: 4.6998 Recon Loss: 4.6849 
[12/24 05:10:27 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000095 Step: 159050 Total Loss: 4.1574 Recon Loss: 4.1426 
[12/24 05:11:08 TiTok]: Data (t): 0.0011, 38.95/s/gpu Batch (t): 0.8217 LR: 0.000095 Step: 159100 Total Loss: 4.7819 Recon Loss: 4.7671 
[12/24 05:11:49 TiTok]: Data (t): 0.0010, 38.87/s/gpu Batch (t): 0.8232 LR: 0.000095 Step: 159150 Total Loss: 4.7568 Recon Loss: 4.7420 
[12/24 05:12:30 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000095 Step: 159200 Total Loss: 4.8162 Recon Loss: 4.8014 
[12/24 05:13:11 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000095 Step: 159250 Total Loss: 4.8371 Recon Loss: 4.8223 
[12/24 05:13:52 TiTok]: Data (t): 0.0011, 39.43/s/gpu Batch (t): 0.8115 LR: 0.000095 Step: 159300 Total Loss: 4.7348 Recon Loss: 4.7198 
[12/24 05:14:33 TiTok]: Data (t): 0.0010, 38.83/s/gpu Batch (t): 0.8241 LR: 0.000095 Step: 159350 Total Loss: 4.7802 Recon Loss: 4.7655 
[12/24 05:15:14 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000095 Step: 159400 Total Loss: 4.7602 Recon Loss: 4.7454 
[12/24 05:15:54 TiTok]: Data (t): 0.0010, 38.85/s/gpu Batch (t): 0.8236 LR: 0.000095 Step: 159450 Total Loss: 4.0696 Recon Loss: 4.0548 
[12/24 05:16:35 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000095 Step: 159500 Total Loss: 5.4578 Recon Loss: 5.4430 
[12/24 05:17:16 TiTok]: Data (t): 0.0011, 39.51/s/gpu Batch (t): 0.8099 LR: 0.000095 Step: 159550 Total Loss: 4.7994 Recon Loss: 4.7846 
[12/24 05:17:57 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000095 Step: 159600 Total Loss: 6.1298 Recon Loss: 6.1150 
[12/24 05:18:38 TiTok]: Data (t): 0.0011, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000095 Step: 159650 Total Loss: 4.8494 Recon Loss: 4.8345 
[12/24 05:19:19 TiTok]: Data (t): 0.0011, 39.35/s/gpu Batch (t): 0.8133 LR: 0.000095 Step: 159700 Total Loss: 4.8175 Recon Loss: 4.8028 
[12/24 05:20:00 TiTok]: Data (t): 0.0011, 39.39/s/gpu Batch (t): 0.8125 LR: 0.000095 Step: 159750 Total Loss: 4.0312 Recon Loss: 4.0164 
[12/24 05:20:41 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8181 LR: 0.000095 Step: 159800 Total Loss: 4.1801 Recon Loss: 4.1653 
[12/24 05:21:22 TiTok]: Data (t): 0.0010, 39.54/s/gpu Batch (t): 0.8093 LR: 0.000095 Step: 159850 Total Loss: 6.8272 Recon Loss: 6.8125 
[12/24 05:22:03 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000095 Step: 159900 Total Loss: 5.4520 Recon Loss: 5.4371 
[12/24 05:22:43 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000095 Step: 159950 Total Loss: 4.8018 Recon Loss: 4.7871 
[12/24 05:23:24 TiTok]: Data (t): 0.0018, 34.49/s/gpu Batch (t): 0.9279 LR: 0.000095 Step: 160000 Total Loss: 4.7694 Recon Loss: 4.7546 
[12/24 05:23:26 TiTok]: Reconstructing images...
[12/24 05:24:07 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000095 Step: 160050 Total Loss: 4.7981 Recon Loss: 4.7833 
[12/24 05:24:48 TiTok]: Data (t): 0.0011, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000095 Step: 160100 Total Loss: 4.1055 Recon Loss: 4.0907 
[12/24 05:25:29 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000095 Step: 160150 Total Loss: 4.7394 Recon Loss: 4.7246 
[12/24 05:26:10 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000095 Step: 160200 Total Loss: 4.7813 Recon Loss: 4.7665 
[12/24 05:26:51 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000095 Step: 160250 Total Loss: 4.1159 Recon Loss: 4.1012 
[12/24 05:27:31 TiTok]: Data (t): 0.0014, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000095 Step: 160300 Total Loss: 4.7890 Recon Loss: 4.7741 
Epoch 16/99 started.
[12/24 05:28:13 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000095 Step: 160350 Total Loss: 5.5246 Recon Loss: 5.5097 
[12/24 05:28:54 TiTok]: Data (t): 0.0011, 39.45/s/gpu Batch (t): 0.8112 LR: 0.000095 Step: 160400 Total Loss: 4.8241 Recon Loss: 4.8093 
[12/24 05:29:35 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8202 LR: 0.000095 Step: 160450 Total Loss: 4.8214 Recon Loss: 4.8066 
[12/24 05:30:16 TiTok]: Data (t): 0.0011, 38.89/s/gpu Batch (t): 0.8228 LR: 0.000095 Step: 160500 Total Loss: 4.1592 Recon Loss: 4.1445 
[12/24 05:30:57 TiTok]: Data (t): 0.0010, 39.43/s/gpu Batch (t): 0.8115 LR: 0.000095 Step: 160550 Total Loss: 4.2001 Recon Loss: 4.1853 
[12/24 05:31:38 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000095 Step: 160600 Total Loss: 4.7544 Recon Loss: 4.7396 
[12/24 05:32:19 TiTok]: Data (t): 0.0011, 38.85/s/gpu Batch (t): 0.8236 LR: 0.000095 Step: 160650 Total Loss: 4.1362 Recon Loss: 4.1214 
[12/24 05:32:59 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000095 Step: 160700 Total Loss: 4.7995 Recon Loss: 4.7846 
[12/24 05:33:40 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000095 Step: 160750 Total Loss: 4.1842 Recon Loss: 4.1693 
[12/24 05:34:21 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000095 Step: 160800 Total Loss: 4.0769 Recon Loss: 4.0621 
[12/24 05:35:02 TiTok]: Data (t): 0.0024, 39.47/s/gpu Batch (t): 0.8107 LR: 0.000095 Step: 160850 Total Loss: 4.0665 Recon Loss: 4.0518 
[12/24 05:35:43 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000095 Step: 160900 Total Loss: 4.1216 Recon Loss: 4.1067 
[12/24 05:36:24 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000095 Step: 160950 Total Loss: 4.1033 Recon Loss: 4.0885 
[12/24 05:37:05 TiTok]: Data (t): 0.0010, 33.66/s/gpu Batch (t): 0.9506 LR: 0.000095 Step: 161000 Total Loss: 4.1104 Recon Loss: 4.0956 
[12/24 05:37:46 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000095 Step: 161050 Total Loss: 4.0893 Recon Loss: 4.0744 
[12/24 05:38:27 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000095 Step: 161100 Total Loss: 4.7480 Recon Loss: 4.7332 
[12/24 05:39:07 TiTok]: Data (t): 0.0020, 39.40/s/gpu Batch (t): 0.8123 LR: 0.000095 Step: 161150 Total Loss: 4.7267 Recon Loss: 4.7119 
[12/24 05:39:48 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000095 Step: 161200 Total Loss: 5.4997 Recon Loss: 5.4850 
[12/24 05:40:29 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8181 LR: 0.000095 Step: 161250 Total Loss: 4.7988 Recon Loss: 4.7840 
[12/24 05:41:10 TiTok]: Data (t): 0.0011, 39.35/s/gpu Batch (t): 0.8131 LR: 0.000095 Step: 161300 Total Loss: 4.1490 Recon Loss: 4.1342 
[12/24 05:41:51 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000095 Step: 161350 Total Loss: 5.4487 Recon Loss: 5.4338 
[12/24 05:42:32 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000095 Step: 161400 Total Loss: 4.0577 Recon Loss: 4.0429 
[12/24 05:43:13 TiTok]: Data (t): 0.0011, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000095 Step: 161450 Total Loss: 4.1683 Recon Loss: 4.1535 
[12/24 05:43:54 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000095 Step: 161500 Total Loss: 6.1673 Recon Loss: 6.1524 
[12/24 05:44:35 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8221 LR: 0.000095 Step: 161550 Total Loss: 4.0835 Recon Loss: 4.0688 
[12/24 05:45:15 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000095 Step: 161600 Total Loss: 4.0972 Recon Loss: 4.0824 
[12/24 05:45:56 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000095 Step: 161650 Total Loss: 4.1105 Recon Loss: 4.0957 
[12/24 05:46:37 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000095 Step: 161700 Total Loss: 4.7313 Recon Loss: 4.7165 
[12/24 05:47:18 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000095 Step: 161750 Total Loss: 4.7930 Recon Loss: 4.7782 
[12/24 05:47:59 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000095 Step: 161800 Total Loss: 4.7985 Recon Loss: 4.7837 
[12/24 05:48:40 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000095 Step: 161850 Total Loss: 4.7344 Recon Loss: 4.7197 
[12/24 05:49:21 TiTok]: Data (t): 0.0011, 39.41/s/gpu Batch (t): 0.8120 LR: 0.000095 Step: 161900 Total Loss: 6.1843 Recon Loss: 6.1694 
[12/24 05:50:02 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000095 Step: 161950 Total Loss: 4.0874 Recon Loss: 4.0726 
[12/24 05:50:43 TiTok]: Data (t): 0.0011, 33.65/s/gpu Batch (t): 0.9510 LR: 0.000095 Step: 162000 Total Loss: 4.7868 Recon Loss: 4.7720 
[12/24 05:51:24 TiTok]: Data (t): 0.0011, 38.55/s/gpu Batch (t): 0.8300 LR: 0.000095 Step: 162050 Total Loss: 4.0836 Recon Loss: 4.0688 
[12/24 05:52:04 TiTok]: Data (t): 0.0031, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000095 Step: 162100 Total Loss: 4.1485 Recon Loss: 4.1337 
[12/24 05:52:45 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000095 Step: 162150 Total Loss: 5.4486 Recon Loss: 5.4338 
[12/24 05:53:26 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000095 Step: 162200 Total Loss: 5.4810 Recon Loss: 5.4662 
[12/24 05:54:07 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000095 Step: 162250 Total Loss: 3.9939 Recon Loss: 3.9791 
[12/24 05:54:48 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000095 Step: 162300 Total Loss: 6.1417 Recon Loss: 6.1270 
[12/24 05:55:29 TiTok]: Data (t): 0.0011, 39.41/s/gpu Batch (t): 0.8119 LR: 0.000095 Step: 162350 Total Loss: 5.4821 Recon Loss: 5.4673 
[12/24 05:56:10 TiTok]: Data (t): 0.0014, 39.36/s/gpu Batch (t): 0.8131 LR: 0.000095 Step: 162400 Total Loss: 4.7651 Recon Loss: 4.7503 
[12/24 05:56:51 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000095 Step: 162450 Total Loss: 4.7992 Recon Loss: 4.7844 
[12/24 05:57:32 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000095 Step: 162500 Total Loss: 4.7934 Recon Loss: 4.7786 
[12/24 05:58:13 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000095 Step: 162550 Total Loss: 4.1081 Recon Loss: 4.0933 
[12/24 05:58:53 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000095 Step: 162600 Total Loss: 4.7605 Recon Loss: 4.7458 
[12/24 05:59:34 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000095 Step: 162650 Total Loss: 4.0869 Recon Loss: 4.0720 
[12/24 06:00:15 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8200 LR: 0.000095 Step: 162700 Total Loss: 4.7853 Recon Loss: 4.7705 
[12/24 06:00:56 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000095 Step: 162750 Total Loss: 5.4701 Recon Loss: 5.4553 
[12/24 06:01:37 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000095 Step: 162800 Total Loss: 4.7731 Recon Loss: 4.7583 
[12/24 06:02:18 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000095 Step: 162850 Total Loss: 4.0706 Recon Loss: 4.0558 
[12/24 06:02:59 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000095 Step: 162900 Total Loss: 4.0013 Recon Loss: 3.9865 
[12/24 06:03:40 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000095 Step: 162950 Total Loss: 4.8534 Recon Loss: 4.8385 
[12/24 06:04:21 TiTok]: Data (t): 0.0011, 34.89/s/gpu Batch (t): 0.9170 LR: 0.000095 Step: 163000 Total Loss: 4.0703 Recon Loss: 4.0554 
[12/24 06:05:02 TiTok]: Data (t): 0.0010, 39.46/s/gpu Batch (t): 0.8110 LR: 0.000095 Step: 163050 Total Loss: 4.8011 Recon Loss: 4.7862 
[12/24 06:05:42 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8200 LR: 0.000095 Step: 163100 Total Loss: 4.7690 Recon Loss: 4.7542 
[12/24 06:06:23 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8154 LR: 0.000095 Step: 163150 Total Loss: 4.7778 Recon Loss: 4.7630 
[12/24 06:07:04 TiTok]: Data (t): 0.0010, 38.74/s/gpu Batch (t): 0.8261 LR: 0.000095 Step: 163200 Total Loss: 4.7800 Recon Loss: 4.7651 
[12/24 06:07:45 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000095 Step: 163250 Total Loss: 4.0424 Recon Loss: 4.0276 
[12/24 06:08:26 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8125 LR: 0.000095 Step: 163300 Total Loss: 4.0423 Recon Loss: 4.0275 
[12/24 06:09:07 TiTok]: Data (t): 0.0011, 38.85/s/gpu Batch (t): 0.8238 LR: 0.000095 Step: 163350 Total Loss: 4.7360 Recon Loss: 4.7212 
[12/24 06:09:48 TiTok]: Data (t): 0.0011, 39.41/s/gpu Batch (t): 0.8120 LR: 0.000095 Step: 163400 Total Loss: 4.1136 Recon Loss: 4.0987 
[12/24 06:10:29 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000095 Step: 163450 Total Loss: 4.7275 Recon Loss: 4.7127 
[12/24 06:11:10 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8228 LR: 0.000095 Step: 163500 Total Loss: 4.8195 Recon Loss: 4.8047 
[12/24 06:11:50 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000095 Step: 163550 Total Loss: 4.0785 Recon Loss: 4.0637 
[12/24 06:12:31 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000095 Step: 163600 Total Loss: 4.7918 Recon Loss: 4.7770 
[12/24 06:13:12 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000095 Step: 163650 Total Loss: 4.7474 Recon Loss: 4.7326 
[12/24 06:13:53 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000095 Step: 163700 Total Loss: 4.0538 Recon Loss: 4.0390 
[12/24 06:14:34 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000095 Step: 163750 Total Loss: 5.4572 Recon Loss: 5.4424 
[12/24 06:15:15 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000095 Step: 163800 Total Loss: 4.0993 Recon Loss: 4.0846 
[12/24 06:15:56 TiTok]: Data (t): 0.0011, 39.38/s/gpu Batch (t): 0.8125 LR: 0.000095 Step: 163850 Total Loss: 5.4563 Recon Loss: 5.4414 
[12/24 06:16:37 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000095 Step: 163900 Total Loss: 4.1261 Recon Loss: 4.1113 
[12/24 06:17:18 TiTok]: Data (t): 0.0011, 38.86/s/gpu Batch (t): 0.8234 LR: 0.000095 Step: 163950 Total Loss: 4.0602 Recon Loss: 4.0455 
[12/24 06:17:59 TiTok]: Data (t): 0.0019, 34.98/s/gpu Batch (t): 0.9148 LR: 0.000095 Step: 164000 Total Loss: 4.0738 Recon Loss: 4.0590 
[12/24 06:18:39 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000095 Step: 164050 Total Loss: 4.7665 Recon Loss: 4.7517 
[12/24 06:19:20 TiTok]: Data (t): 0.0010, 38.76/s/gpu Batch (t): 0.8257 LR: 0.000095 Step: 164100 Total Loss: 5.4549 Recon Loss: 5.4401 
[12/24 06:20:01 TiTok]: Data (t): 0.0010, 38.84/s/gpu Batch (t): 0.8238 LR: 0.000095 Step: 164150 Total Loss: 3.9934 Recon Loss: 3.9787 
[12/24 06:20:42 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000095 Step: 164200 Total Loss: 5.4044 Recon Loss: 5.3897 
[12/24 06:21:23 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000095 Step: 164250 Total Loss: 4.7445 Recon Loss: 4.7297 
[12/24 06:22:04 TiTok]: Data (t): 0.0014, 39.62/s/gpu Batch (t): 0.8077 LR: 0.000095 Step: 164300 Total Loss: 4.7904 Recon Loss: 4.7756 
[12/24 06:22:45 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8223 LR: 0.000095 Step: 164350 Total Loss: 4.7333 Recon Loss: 4.7185 
[12/24 06:23:26 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000095 Step: 164400 Total Loss: 6.1615 Recon Loss: 6.1466 
[12/24 06:24:07 TiTok]: Data (t): 0.0012, 39.09/s/gpu Batch (t): 0.8185 LR: 0.000095 Step: 164450 Total Loss: 4.0608 Recon Loss: 4.0459 
[12/24 06:24:47 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8179 LR: 0.000095 Step: 164500 Total Loss: 4.1824 Recon Loss: 4.1676 
[12/24 06:25:28 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000095 Step: 164550 Total Loss: 4.1054 Recon Loss: 4.0905 
[12/24 06:26:09 TiTok]: Data (t): 0.0011, 38.56/s/gpu Batch (t): 0.8299 LR: 0.000095 Step: 164600 Total Loss: 4.1005 Recon Loss: 4.0857 
[12/24 06:26:50 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8234 LR: 0.000095 Step: 164650 Total Loss: 4.1055 Recon Loss: 4.0906 
[12/24 06:27:31 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000095 Step: 164700 Total Loss: 4.7913 Recon Loss: 4.7766 
[12/24 06:28:12 TiTok]: Data (t): 0.0010, 38.82/s/gpu Batch (t): 0.8244 LR: 0.000095 Step: 164750 Total Loss: 5.4060 Recon Loss: 5.3913 
[12/24 06:28:53 TiTok]: Data (t): 0.0011, 39.53/s/gpu Batch (t): 0.8096 LR: 0.000095 Step: 164800 Total Loss: 4.8544 Recon Loss: 4.8395 
[12/24 06:29:34 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000095 Step: 164850 Total Loss: 5.4932 Recon Loss: 5.4784 
[12/24 06:30:15 TiTok]: Data (t): 0.0011, 38.68/s/gpu Batch (t): 0.8272 LR: 0.000095 Step: 164900 Total Loss: 4.8210 Recon Loss: 4.8062 
[12/24 06:30:56 TiTok]: Data (t): 0.0010, 38.81/s/gpu Batch (t): 0.8246 LR: 0.000095 Step: 164950 Total Loss: 4.8377 Recon Loss: 4.8228 
[12/24 06:31:37 TiTok]: Data (t): 0.0010, 35.05/s/gpu Batch (t): 0.9131 LR: 0.000095 Step: 165000 Total Loss: 4.7451 Recon Loss: 4.7303 
[12/24 06:31:38 TiTok]: Reconstructing images...
[12/24 06:32:19 TiTok]: Data (t): 0.0011, 39.52/s/gpu Batch (t): 0.8097 LR: 0.000095 Step: 165050 Total Loss: 4.7474 Recon Loss: 4.7327 
[12/24 06:33:00 TiTok]: Data (t): 0.0011, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000095 Step: 165100 Total Loss: 4.0552 Recon Loss: 4.0406 
[12/24 06:33:41 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000095 Step: 165150 Total Loss: 5.4650 Recon Loss: 5.4502 
[12/24 06:34:22 TiTok]: Data (t): 0.0010, 38.87/s/gpu Batch (t): 0.8232 LR: 0.000095 Step: 165200 Total Loss: 4.7252 Recon Loss: 4.7105 
[12/24 06:35:03 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8164 LR: 0.000095 Step: 165250 Total Loss: 4.1137 Recon Loss: 4.0989 
[12/24 06:35:44 TiTok]: Data (t): 0.0010, 38.87/s/gpu Batch (t): 0.8233 LR: 0.000095 Step: 165300 Total Loss: 4.7888 Recon Loss: 4.7739 
[12/24 06:36:24 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000095 Step: 165350 Total Loss: 4.8104 Recon Loss: 4.7956 
[12/24 06:37:05 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8204 LR: 0.000095 Step: 165400 Total Loss: 4.8254 Recon Loss: 4.8106 
[12/24 06:37:46 TiTok]: Data (t): 0.0011, 39.54/s/gpu Batch (t): 0.8092 LR: 0.000095 Step: 165450 Total Loss: 5.4779 Recon Loss: 5.4631 
[12/24 06:38:27 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000095 Step: 165500 Total Loss: 4.8339 Recon Loss: 4.8191 
[12/24 06:39:08 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000095 Step: 165550 Total Loss: 4.7346 Recon Loss: 4.7198 
[12/24 06:39:49 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000095 Step: 165600 Total Loss: 4.1233 Recon Loss: 4.1086 
[12/24 06:40:30 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000095 Step: 165650 Total Loss: 4.0575 Recon Loss: 4.0428 
[12/24 06:41:11 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000095 Step: 165700 Total Loss: 4.7845 Recon Loss: 4.7698 
[12/24 06:41:52 TiTok]: Data (t): 0.0013, 39.39/s/gpu Batch (t): 0.8124 LR: 0.000095 Step: 165750 Total Loss: 4.8109 Recon Loss: 4.7962 
[12/24 06:42:33 TiTok]: Data (t): 0.0010, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000095 Step: 165800 Total Loss: 4.7150 Recon Loss: 4.7002 
[12/24 06:43:13 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000095 Step: 165850 Total Loss: 4.1082 Recon Loss: 4.0934 
[12/24 06:43:54 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000095 Step: 165900 Total Loss: 5.4554 Recon Loss: 5.4407 
[12/24 06:44:35 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000095 Step: 165950 Total Loss: 4.1206 Recon Loss: 4.1057 
[12/24 06:45:16 TiTok]: Data (t): 0.0010, 33.10/s/gpu Batch (t): 0.9666 LR: 0.000095 Step: 166000 Total Loss: 4.7277 Recon Loss: 4.7128 
[12/24 06:45:57 TiTok]: Data (t): 0.0011, 38.25/s/gpu Batch (t): 0.8366 LR: 0.000095 Step: 166050 Total Loss: 4.1346 Recon Loss: 4.1198 
[12/24 06:46:38 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000095 Step: 166100 Total Loss: 4.1289 Recon Loss: 4.1141 
[12/24 06:47:19 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000095 Step: 166150 Total Loss: 4.1448 Recon Loss: 4.1300 
[12/24 06:48:00 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000095 Step: 166200 Total Loss: 4.0767 Recon Loss: 4.0619 
[12/24 06:48:41 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000095 Step: 166250 Total Loss: 6.1377 Recon Loss: 6.1229 
[12/24 06:49:22 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000095 Step: 166300 Total Loss: 5.4388 Recon Loss: 5.4240 
[12/24 06:50:02 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000095 Step: 166350 Total Loss: 5.4934 Recon Loss: 5.4786 
[12/24 06:50:43 TiTok]: Data (t): 0.0011, 39.40/s/gpu Batch (t): 0.8123 LR: 0.000095 Step: 166400 Total Loss: 4.7813 Recon Loss: 4.7667 
[12/24 06:51:24 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000095 Step: 166450 Total Loss: 4.7446 Recon Loss: 4.7298 
[12/24 06:52:05 TiTok]: Data (t): 0.0011, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000095 Step: 166500 Total Loss: 4.7402 Recon Loss: 4.7255 
[12/24 06:52:46 TiTok]: Data (t): 0.0011, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000095 Step: 166550 Total Loss: 4.7520 Recon Loss: 4.7372 
[12/24 06:53:27 TiTok]: Data (t): 0.0020, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000095 Step: 166600 Total Loss: 4.0711 Recon Loss: 4.0563 
[12/24 06:54:08 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000095 Step: 166650 Total Loss: 4.0709 Recon Loss: 4.0561 
[12/24 06:54:48 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000095 Step: 166700 Total Loss: 4.1356 Recon Loss: 4.1208 
[12/24 06:55:29 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000095 Step: 166750 Total Loss: 4.7748 Recon Loss: 4.7600 
[12/24 06:56:10 TiTok]: Data (t): 0.0012, 39.42/s/gpu Batch (t): 0.8119 LR: 0.000095 Step: 166800 Total Loss: 5.4896 Recon Loss: 5.4748 
[12/24 06:56:51 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000095 Step: 166850 Total Loss: 4.0865 Recon Loss: 4.0717 
[12/24 06:57:32 TiTok]: Data (t): 0.0017, 39.33/s/gpu Batch (t): 0.8137 LR: 0.000095 Step: 166900 Total Loss: 4.8338 Recon Loss: 4.8190 
[12/24 06:58:13 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8164 LR: 0.000095 Step: 166950 Total Loss: 4.1056 Recon Loss: 4.0907 
[12/24 06:58:54 TiTok]: Data (t): 0.0011, 35.10/s/gpu Batch (t): 0.9116 LR: 0.000095 Step: 167000 Total Loss: 5.4029 Recon Loss: 5.3881 
[12/24 06:59:34 TiTok]: Data (t): 0.0011, 39.60/s/gpu Batch (t): 0.8080 LR: 0.000095 Step: 167050 Total Loss: 4.1116 Recon Loss: 4.0968 
[12/24 07:00:15 TiTok]: Data (t): 0.0011, 38.78/s/gpu Batch (t): 0.8253 LR: 0.000095 Step: 167100 Total Loss: 4.0534 Recon Loss: 4.0387 
[12/24 07:00:56 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000095 Step: 167150 Total Loss: 4.7974 Recon Loss: 4.7826 
[12/24 07:01:37 TiTok]: Data (t): 0.0015, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000095 Step: 167200 Total Loss: 5.5301 Recon Loss: 5.5153 
[12/24 07:02:18 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000095 Step: 167250 Total Loss: 4.7852 Recon Loss: 4.7704 
[12/24 07:02:59 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000095 Step: 167300 Total Loss: 5.4412 Recon Loss: 5.4264 
[12/24 07:03:40 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000095 Step: 167350 Total Loss: 4.7294 Recon Loss: 4.7145 
[12/24 07:04:20 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000095 Step: 167400 Total Loss: 5.4194 Recon Loss: 5.4047 
[12/24 07:05:01 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000095 Step: 167450 Total Loss: 4.7711 Recon Loss: 4.7564 
[12/24 07:05:42 TiTok]: Data (t): 0.0011, 39.39/s/gpu Batch (t): 0.8123 LR: 0.000095 Step: 167500 Total Loss: 5.4349 Recon Loss: 5.4201 
[12/24 07:06:23 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8150 LR: 0.000094 Step: 167550 Total Loss: 4.7945 Recon Loss: 4.7798 
[12/24 07:07:04 TiTok]: Data (t): 0.0012, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000094 Step: 167600 Total Loss: 4.8267 Recon Loss: 4.8120 
[12/24 07:07:45 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000094 Step: 167650 Total Loss: 4.7736 Recon Loss: 4.7589 
[12/24 07:08:26 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000094 Step: 167700 Total Loss: 4.1475 Recon Loss: 4.1328 
[12/24 07:09:06 TiTok]: Data (t): 0.0013, 39.57/s/gpu Batch (t): 0.8087 LR: 0.000094 Step: 167750 Total Loss: 4.7953 Recon Loss: 4.7805 
[12/24 07:09:47 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000094 Step: 167800 Total Loss: 4.7729 Recon Loss: 4.7581 
[12/24 07:10:28 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000094 Step: 167850 Total Loss: 4.0790 Recon Loss: 4.0643 
[12/24 07:11:09 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000094 Step: 167900 Total Loss: 5.4537 Recon Loss: 5.4389 
[12/24 07:11:50 TiTok]: Data (t): 0.0012, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000094 Step: 167950 Total Loss: 5.4673 Recon Loss: 5.4525 
[12/24 07:12:31 TiTok]: Data (t): 0.0011, 35.42/s/gpu Batch (t): 0.9033 LR: 0.000094 Step: 168000 Total Loss: 4.7552 Recon Loss: 4.7404 
[12/24 07:13:12 TiTok]: Data (t): 0.0015, 39.26/s/gpu Batch (t): 0.8150 LR: 0.000094 Step: 168050 Total Loss: 5.4905 Recon Loss: 5.4758 
[12/24 07:13:53 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000094 Step: 168100 Total Loss: 4.7269 Recon Loss: 4.7121 
[12/24 07:14:33 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000094 Step: 168150 Total Loss: 6.1385 Recon Loss: 6.1237 
[12/24 07:15:14 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000094 Step: 168200 Total Loss: 5.4787 Recon Loss: 5.4639 
[12/24 07:15:55 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000094 Step: 168250 Total Loss: 4.6817 Recon Loss: 4.6670 
[12/24 07:16:36 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8228 LR: 0.000094 Step: 168300 Total Loss: 4.0211 Recon Loss: 4.0063 
[12/24 07:17:17 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000094 Step: 168350 Total Loss: 5.4297 Recon Loss: 5.4149 
[12/24 07:17:58 TiTok]: Data (t): 0.0011, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000094 Step: 168400 Total Loss: 6.1297 Recon Loss: 6.1149 
[12/24 07:18:39 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000094 Step: 168450 Total Loss: 4.0420 Recon Loss: 4.0272 
[12/24 07:19:20 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000094 Step: 168500 Total Loss: 4.1434 Recon Loss: 4.1286 
[12/24 07:20:01 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000094 Step: 168550 Total Loss: 4.7371 Recon Loss: 4.7223 
[12/24 07:20:42 TiTok]: Data (t): 0.0010, 38.79/s/gpu Batch (t): 0.8250 LR: 0.000094 Step: 168600 Total Loss: 4.7581 Recon Loss: 4.7433 
[12/24 07:21:23 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000094 Step: 168650 Total Loss: 4.7376 Recon Loss: 4.7229 
[12/24 07:22:04 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000094 Step: 168700 Total Loss: 4.0257 Recon Loss: 4.0109 
[12/24 07:22:44 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8223 LR: 0.000094 Step: 168750 Total Loss: 4.0830 Recon Loss: 4.0682 
[12/24 07:23:25 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000094 Step: 168800 Total Loss: 4.0770 Recon Loss: 4.0622 
[12/24 07:24:06 TiTok]: Data (t): 0.0011, 39.06/s/gpu Batch (t): 0.8194 LR: 0.000094 Step: 168850 Total Loss: 4.7174 Recon Loss: 4.7027 
[12/24 07:24:47 TiTok]: Data (t): 0.0011, 38.69/s/gpu Batch (t): 0.8271 LR: 0.000094 Step: 168900 Total Loss: 4.7794 Recon Loss: 4.7646 
[12/24 07:25:28 TiTok]: Data (t): 0.0011, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000094 Step: 168950 Total Loss: 4.0105 Recon Loss: 3.9957 
[12/24 07:26:09 TiTok]: Data (t): 0.0011, 34.98/s/gpu Batch (t): 0.9149 LR: 0.000094 Step: 169000 Total Loss: 4.1265 Recon Loss: 4.1118 
[12/24 07:26:50 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8202 LR: 0.000094 Step: 169050 Total Loss: 4.1704 Recon Loss: 4.1556 
[12/24 07:27:31 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8235 LR: 0.000094 Step: 169100 Total Loss: 4.7794 Recon Loss: 4.7646 
[12/24 07:28:12 TiTok]: Data (t): 0.0011, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000094 Step: 169150 Total Loss: 4.7891 Recon Loss: 4.7743 
[12/24 07:28:52 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8156 LR: 0.000094 Step: 169200 Total Loss: 4.0723 Recon Loss: 4.0575 
[12/24 07:29:33 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8187 LR: 0.000094 Step: 169250 Total Loss: 4.8001 Recon Loss: 4.7853 
[12/24 07:30:14 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000094 Step: 169300 Total Loss: 5.4988 Recon Loss: 5.4840 
[12/24 07:30:55 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000094 Step: 169350 Total Loss: 4.0729 Recon Loss: 4.0581 
[12/24 07:31:36 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000094 Step: 169400 Total Loss: 5.4866 Recon Loss: 5.4718 
[12/24 07:32:17 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8144 LR: 0.000094 Step: 169450 Total Loss: 4.7728 Recon Loss: 4.7580 
[12/24 07:32:58 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000094 Step: 169500 Total Loss: 4.8074 Recon Loss: 4.7926 
[12/24 07:33:39 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000094 Step: 169550 Total Loss: 5.4521 Recon Loss: 5.4373 
[12/24 07:34:20 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000094 Step: 169600 Total Loss: 4.0831 Recon Loss: 4.0683 
[12/24 07:35:00 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000094 Step: 169650 Total Loss: 5.4866 Recon Loss: 5.4718 
[12/24 07:35:41 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000094 Step: 169700 Total Loss: 4.1572 Recon Loss: 4.1424 
[12/24 07:36:22 TiTok]: Data (t): 0.0011, 38.84/s/gpu Batch (t): 0.8239 LR: 0.000094 Step: 169750 Total Loss: 5.3615 Recon Loss: 5.3467 
[12/24 07:37:03 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000094 Step: 169800 Total Loss: 4.8372 Recon Loss: 4.8224 
[12/24 07:37:44 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000094 Step: 169850 Total Loss: 4.0970 Recon Loss: 4.0822 
[12/24 07:38:25 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000094 Step: 169900 Total Loss: 4.7962 Recon Loss: 4.7814 
[12/24 07:39:06 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000094 Step: 169950 Total Loss: 4.1012 Recon Loss: 4.0864 
[12/24 07:39:47 TiTok]: Data (t): 0.0010, 35.25/s/gpu Batch (t): 0.9078 LR: 0.000094 Step: 170000 Total Loss: 4.7722 Recon Loss: 4.7574 
[12/24 07:39:48 TiTok]: Reconstructing images...
[12/24 07:40:29 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000094 Step: 170050 Total Loss: 4.8114 Recon Loss: 4.7966 
[12/24 07:41:10 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000094 Step: 170100 Total Loss: 4.8155 Recon Loss: 4.8007 
[12/24 07:41:51 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000094 Step: 170150 Total Loss: 4.0934 Recon Loss: 4.0787 
[12/24 07:42:32 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000094 Step: 170200 Total Loss: 4.7508 Recon Loss: 4.7360 
[12/24 07:43:13 TiTok]: Data (t): 0.0022, 38.84/s/gpu Batch (t): 0.8240 LR: 0.000094 Step: 170250 Total Loss: 5.4757 Recon Loss: 5.4609 
[12/24 07:43:54 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000094 Step: 170300 Total Loss: 4.7379 Recon Loss: 4.7231 
Epoch 17/99 started.
[12/24 07:44:35 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000094 Step: 170350 Total Loss: 4.7615 Recon Loss: 4.7467 
[12/24 07:45:16 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8162 LR: 0.000094 Step: 170400 Total Loss: 5.4624 Recon Loss: 5.4476 
[12/24 07:45:57 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000094 Step: 170450 Total Loss: 5.4081 Recon Loss: 5.3934 
[12/24 07:46:38 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000094 Step: 170500 Total Loss: 4.7033 Recon Loss: 4.6885 
[12/24 07:47:19 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000094 Step: 170550 Total Loss: 4.0402 Recon Loss: 4.0255 
[12/24 07:48:00 TiTok]: Data (t): 0.0012, 38.71/s/gpu Batch (t): 0.8266 LR: 0.000094 Step: 170600 Total Loss: 3.9884 Recon Loss: 3.9736 
[12/24 07:48:41 TiTok]: Data (t): 0.0012, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000094 Step: 170650 Total Loss: 4.7860 Recon Loss: 4.7712 
[12/24 07:49:22 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000094 Step: 170700 Total Loss: 4.7312 Recon Loss: 4.7165 
[12/24 07:50:02 TiTok]: Data (t): 0.0011, 38.75/s/gpu Batch (t): 0.8259 LR: 0.000094 Step: 170750 Total Loss: 4.7645 Recon Loss: 4.7498 
[12/24 07:50:43 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000094 Step: 170800 Total Loss: 4.7664 Recon Loss: 4.7516 
[12/24 07:51:24 TiTok]: Data (t): 0.0010, 39.39/s/gpu Batch (t): 0.8125 LR: 0.000094 Step: 170850 Total Loss: 5.4114 Recon Loss: 5.3966 
[12/24 07:52:05 TiTok]: Data (t): 0.0011, 39.39/s/gpu Batch (t): 0.8123 LR: 0.000094 Step: 170900 Total Loss: 5.4265 Recon Loss: 5.4118 
[12/24 07:52:46 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000094 Step: 170950 Total Loss: 4.0809 Recon Loss: 4.0661 
[12/24 07:53:27 TiTok]: Data (t): 0.0010, 33.33/s/gpu Batch (t): 0.9601 LR: 0.000094 Step: 171000 Total Loss: 4.0844 Recon Loss: 4.0696 
[12/24 07:54:08 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8137 LR: 0.000094 Step: 171050 Total Loss: 4.7820 Recon Loss: 4.7672 
[12/24 07:54:49 TiTok]: Data (t): 0.0011, 39.45/s/gpu Batch (t): 0.8112 LR: 0.000094 Step: 171100 Total Loss: 4.7666 Recon Loss: 4.7518 
[12/24 07:55:29 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000094 Step: 171150 Total Loss: 4.7492 Recon Loss: 4.7344 
[12/24 07:56:10 TiTok]: Data (t): 0.0011, 38.95/s/gpu Batch (t): 0.8217 LR: 0.000094 Step: 171200 Total Loss: 4.1335 Recon Loss: 4.1187 
[12/24 07:56:51 TiTok]: Data (t): 0.0010, 39.44/s/gpu Batch (t): 0.8114 LR: 0.000094 Step: 171250 Total Loss: 5.4917 Recon Loss: 5.4769 
[12/24 07:57:32 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000094 Step: 171300 Total Loss: 5.3936 Recon Loss: 5.3788 
[12/24 07:58:13 TiTok]: Data (t): 0.0011, 39.51/s/gpu Batch (t): 0.8100 LR: 0.000094 Step: 171350 Total Loss: 5.4260 Recon Loss: 5.4111 
[12/24 07:58:54 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000094 Step: 171400 Total Loss: 4.7975 Recon Loss: 4.7826 
[12/24 07:59:34 TiTok]: Data (t): 0.0011, 38.82/s/gpu Batch (t): 0.8243 LR: 0.000094 Step: 171450 Total Loss: 4.0810 Recon Loss: 4.0662 
[12/24 08:00:15 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000094 Step: 171500 Total Loss: 4.1013 Recon Loss: 4.0865 
[12/24 08:00:56 TiTok]: Data (t): 0.0010, 39.39/s/gpu Batch (t): 0.8124 LR: 0.000094 Step: 171550 Total Loss: 4.1063 Recon Loss: 4.0916 
[12/24 08:01:37 TiTok]: Data (t): 0.0010, 38.82/s/gpu Batch (t): 0.8243 LR: 0.000094 Step: 171600 Total Loss: 4.1311 Recon Loss: 4.1163 
[12/24 08:02:18 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000094 Step: 171650 Total Loss: 4.7414 Recon Loss: 4.7266 
[12/24 08:02:59 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8221 LR: 0.000094 Step: 171700 Total Loss: 4.0182 Recon Loss: 4.0035 
[12/24 08:03:40 TiTok]: Data (t): 0.0012, 38.79/s/gpu Batch (t): 0.8250 LR: 0.000094 Step: 171750 Total Loss: 5.4176 Recon Loss: 5.4029 
[12/24 08:04:21 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000094 Step: 171800 Total Loss: 4.7662 Recon Loss: 4.7515 
[12/24 08:05:02 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000094 Step: 171850 Total Loss: 4.7892 Recon Loss: 4.7745 
[12/24 08:05:42 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000094 Step: 171900 Total Loss: 5.4621 Recon Loss: 5.4472 
[12/24 08:06:23 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000094 Step: 171950 Total Loss: 5.4855 Recon Loss: 5.4707 
[12/24 08:07:04 TiTok]: Data (t): 0.0010, 34.95/s/gpu Batch (t): 0.9157 LR: 0.000094 Step: 172000 Total Loss: 4.8025 Recon Loss: 4.7877 
[12/24 08:07:45 TiTok]: Data (t): 0.0011, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000094 Step: 172050 Total Loss: 4.0483 Recon Loss: 4.0335 
[12/24 08:08:26 TiTok]: Data (t): 0.0012, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000094 Step: 172100 Total Loss: 4.7640 Recon Loss: 4.7492 
[12/24 08:09:07 TiTok]: Data (t): 0.0011, 38.91/s/gpu Batch (t): 0.8223 LR: 0.000094 Step: 172150 Total Loss: 4.1231 Recon Loss: 4.1083 
[12/24 08:09:48 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000094 Step: 172200 Total Loss: 4.7272 Recon Loss: 4.7124 
[12/24 08:10:29 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000094 Step: 172250 Total Loss: 4.7448 Recon Loss: 4.7300 
[12/24 08:11:10 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8156 LR: 0.000094 Step: 172300 Total Loss: 5.4367 Recon Loss: 5.4219 
[12/24 08:11:50 TiTok]: Data (t): 0.0010, 38.80/s/gpu Batch (t): 0.8247 LR: 0.000094 Step: 172350 Total Loss: 4.0738 Recon Loss: 4.0591 
[12/24 08:12:31 TiTok]: Data (t): 0.0010, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000094 Step: 172400 Total Loss: 5.4721 Recon Loss: 5.4573 
[12/24 08:13:12 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000094 Step: 172450 Total Loss: 4.7643 Recon Loss: 4.7495 
[12/24 08:13:53 TiTok]: Data (t): 0.0011, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000094 Step: 172500 Total Loss: 4.0897 Recon Loss: 4.0749 
[12/24 08:14:34 TiTok]: Data (t): 0.0010, 39.64/s/gpu Batch (t): 0.8073 LR: 0.000094 Step: 172550 Total Loss: 4.7432 Recon Loss: 4.7284 
[12/24 08:15:15 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000094 Step: 172600 Total Loss: 4.7534 Recon Loss: 4.7387 
[12/24 08:15:56 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8225 LR: 0.000094 Step: 172650 Total Loss: 4.0709 Recon Loss: 4.0561 
[12/24 08:16:37 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000094 Step: 172700 Total Loss: 4.7645 Recon Loss: 4.7497 
[12/24 08:17:18 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000094 Step: 172750 Total Loss: 5.4442 Recon Loss: 5.4294 
[12/24 08:17:59 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000094 Step: 172800 Total Loss: 6.1554 Recon Loss: 6.1406 
[12/24 08:18:39 TiTok]: Data (t): 0.0010, 39.41/s/gpu Batch (t): 0.8120 LR: 0.000094 Step: 172850 Total Loss: 6.1707 Recon Loss: 6.1559 
[12/24 08:19:20 TiTok]: Data (t): 0.0018, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000094 Step: 172900 Total Loss: 5.4595 Recon Loss: 5.4446 
[12/24 08:20:01 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8185 LR: 0.000094 Step: 172950 Total Loss: 4.1100 Recon Loss: 4.0952 
[12/24 08:20:42 TiTok]: Data (t): 0.0011, 35.05/s/gpu Batch (t): 0.9129 LR: 0.000094 Step: 173000 Total Loss: 4.0824 Recon Loss: 4.0676 
[12/24 08:21:23 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000094 Step: 173050 Total Loss: 4.7504 Recon Loss: 4.7355 
[12/24 08:22:04 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000094 Step: 173100 Total Loss: 5.4540 Recon Loss: 5.4392 
[12/24 08:22:45 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8160 LR: 0.000094 Step: 173150 Total Loss: 4.7330 Recon Loss: 4.7182 
[12/24 08:23:26 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000094 Step: 173200 Total Loss: 4.7790 Recon Loss: 4.7643 
[12/24 08:24:07 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8234 LR: 0.000094 Step: 173250 Total Loss: 4.0982 Recon Loss: 4.0834 
[12/24 08:24:48 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000094 Step: 173300 Total Loss: 4.7637 Recon Loss: 4.7490 
[12/24 08:25:28 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000094 Step: 173350 Total Loss: 4.8189 Recon Loss: 4.8041 
[12/24 08:26:09 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000094 Step: 173400 Total Loss: 4.7761 Recon Loss: 4.7613 
[12/24 08:26:50 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000094 Step: 173450 Total Loss: 4.7910 Recon Loss: 4.7763 
[12/24 08:27:31 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000094 Step: 173500 Total Loss: 5.4352 Recon Loss: 5.4204 
[12/24 08:28:12 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000094 Step: 173550 Total Loss: 5.4530 Recon Loss: 5.4381 
[12/24 08:28:53 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000094 Step: 173600 Total Loss: 4.1125 Recon Loss: 4.0977 
[12/24 08:29:34 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000094 Step: 173650 Total Loss: 4.0796 Recon Loss: 4.0648 
[12/24 08:30:15 TiTok]: Data (t): 0.0011, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000094 Step: 173700 Total Loss: 5.4301 Recon Loss: 5.4153 
[12/24 08:30:56 TiTok]: Data (t): 0.0016, 39.49/s/gpu Batch (t): 0.8103 LR: 0.000094 Step: 173750 Total Loss: 5.5161 Recon Loss: 5.5012 
[12/24 08:31:37 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000094 Step: 173800 Total Loss: 4.0658 Recon Loss: 4.0510 
[12/24 08:32:18 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000094 Step: 173850 Total Loss: 4.7711 Recon Loss: 4.7563 
[12/24 08:32:58 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000094 Step: 173900 Total Loss: 4.1314 Recon Loss: 4.1167 
[12/24 08:33:39 TiTok]: Data (t): 0.0012, 39.43/s/gpu Batch (t): 0.8115 LR: 0.000094 Step: 173950 Total Loss: 4.0451 Recon Loss: 4.0303 
[12/24 08:34:20 TiTok]: Data (t): 0.0011, 35.25/s/gpu Batch (t): 0.9079 LR: 0.000094 Step: 174000 Total Loss: 4.6880 Recon Loss: 4.6732 
[12/24 08:35:01 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000094 Step: 174050 Total Loss: 5.4619 Recon Loss: 5.4471 
[12/24 08:35:42 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000094 Step: 174100 Total Loss: 4.7325 Recon Loss: 4.7177 
[12/24 08:36:23 TiTok]: Data (t): 0.0019, 39.53/s/gpu Batch (t): 0.8095 LR: 0.000094 Step: 174150 Total Loss: 4.7643 Recon Loss: 4.7496 
[12/24 08:37:04 TiTok]: Data (t): 0.0010, 38.84/s/gpu Batch (t): 0.8240 LR: 0.000094 Step: 174200 Total Loss: 4.7761 Recon Loss: 4.7613 
[12/24 08:37:45 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000094 Step: 174250 Total Loss: 4.7898 Recon Loss: 4.7749 
[12/24 08:38:26 TiTok]: Data (t): 0.0010, 38.87/s/gpu Batch (t): 0.8233 LR: 0.000094 Step: 174300 Total Loss: 4.1427 Recon Loss: 4.1279 
[12/24 08:39:07 TiTok]: Data (t): 0.0011, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000094 Step: 174350 Total Loss: 5.4225 Recon Loss: 5.4077 
[12/24 08:39:47 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000094 Step: 174400 Total Loss: 4.7492 Recon Loss: 4.7344 
[12/24 08:40:28 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000094 Step: 174450 Total Loss: 4.7987 Recon Loss: 4.7839 
[12/24 08:41:09 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000094 Step: 174500 Total Loss: 4.7758 Recon Loss: 4.7610 
[12/24 08:41:50 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000094 Step: 174550 Total Loss: 4.0295 Recon Loss: 4.0149 
[12/24 08:42:31 TiTok]: Data (t): 0.0010, 38.29/s/gpu Batch (t): 0.8357 LR: 0.000094 Step: 174600 Total Loss: 4.0173 Recon Loss: 4.0025 
[12/24 08:43:12 TiTok]: Data (t): 0.0012, 39.23/s/gpu Batch (t): 0.8158 LR: 0.000094 Step: 174650 Total Loss: 6.1272 Recon Loss: 6.1123 
[12/24 08:43:53 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000094 Step: 174700 Total Loss: 5.4815 Recon Loss: 5.4667 
[12/24 08:44:34 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000094 Step: 174750 Total Loss: 4.7989 Recon Loss: 4.7840 
[12/24 08:45:15 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000094 Step: 174800 Total Loss: 4.7690 Recon Loss: 4.7543 
[12/24 08:45:56 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000094 Step: 174850 Total Loss: 5.4830 Recon Loss: 5.4683 
[12/24 08:46:37 TiTok]: Data (t): 0.0012, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000094 Step: 174900 Total Loss: 4.1343 Recon Loss: 4.1195 
[12/24 08:47:17 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000094 Step: 174950 Total Loss: 4.7944 Recon Loss: 4.7796 
[12/24 08:47:58 TiTok]: Data (t): 0.0010, 35.06/s/gpu Batch (t): 0.9126 LR: 0.000094 Step: 175000 Total Loss: 4.7373 Recon Loss: 4.7225 
[12/24 08:48:00 TiTok]: Reconstructing images...
[12/24 08:48:41 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000094 Step: 175050 Total Loss: 4.7665 Recon Loss: 4.7516 
[12/24 08:49:22 TiTok]: Data (t): 0.0011, 38.63/s/gpu Batch (t): 0.8284 LR: 0.000094 Step: 175100 Total Loss: 4.7707 Recon Loss: 4.7559 
[12/24 08:50:03 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000094 Step: 175150 Total Loss: 4.1215 Recon Loss: 4.1067 
[12/24 08:50:44 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000094 Step: 175200 Total Loss: 5.4080 Recon Loss: 5.3931 
[12/24 08:51:24 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000094 Step: 175250 Total Loss: 6.1611 Recon Loss: 6.1464 
[12/24 08:52:05 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000094 Step: 175300 Total Loss: 4.0312 Recon Loss: 4.0163 
[12/24 08:52:46 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000094 Step: 175350 Total Loss: 4.0369 Recon Loss: 4.0220 
[12/24 08:53:27 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000094 Step: 175400 Total Loss: 4.7772 Recon Loss: 4.7624 
[12/24 08:54:08 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000094 Step: 175450 Total Loss: 4.1056 Recon Loss: 4.0908 
[12/24 08:54:49 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000094 Step: 175500 Total Loss: 5.4894 Recon Loss: 5.4745 
[12/24 08:55:30 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000094 Step: 175550 Total Loss: 6.1228 Recon Loss: 6.1079 
[12/24 08:56:11 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000094 Step: 175600 Total Loss: 4.7284 Recon Loss: 4.7136 
[12/24 08:56:52 TiTok]: Data (t): 0.0012, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000094 Step: 175650 Total Loss: 4.7687 Recon Loss: 4.7540 
[12/24 08:57:33 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000094 Step: 175700 Total Loss: 4.7514 Recon Loss: 4.7366 
[12/24 08:58:14 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8206 LR: 0.000094 Step: 175750 Total Loss: 4.7442 Recon Loss: 4.7294 
[12/24 08:58:55 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000094 Step: 175800 Total Loss: 4.0697 Recon Loss: 4.0549 
[12/24 08:59:35 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8223 LR: 0.000094 Step: 175850 Total Loss: 5.4265 Recon Loss: 5.4117 
[12/24 09:00:16 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8181 LR: 0.000094 Step: 175900 Total Loss: 4.7840 Recon Loss: 4.7693 
[12/24 09:00:57 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000094 Step: 175950 Total Loss: 5.5095 Recon Loss: 5.4947 
[12/24 09:01:38 TiTok]: Data (t): 0.0011, 35.15/s/gpu Batch (t): 0.9104 LR: 0.000094 Step: 176000 Total Loss: 4.1023 Recon Loss: 4.0875 
[12/24 09:02:19 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000094 Step: 176050 Total Loss: 4.0801 Recon Loss: 4.0654 
[12/24 09:03:00 TiTok]: Data (t): 0.0010, 38.78/s/gpu Batch (t): 0.8252 LR: 0.000094 Step: 176100 Total Loss: 4.0598 Recon Loss: 4.0451 
[12/24 09:03:41 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000094 Step: 176150 Total Loss: 4.0404 Recon Loss: 4.0256 
[12/24 09:04:22 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000094 Step: 176200 Total Loss: 5.4722 Recon Loss: 5.4575 
[12/24 09:05:03 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000094 Step: 176250 Total Loss: 4.7837 Recon Loss: 4.7690 
[12/24 09:05:44 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000094 Step: 176300 Total Loss: 4.0880 Recon Loss: 4.0731 
[12/24 09:06:25 TiTok]: Data (t): 0.0010, 38.81/s/gpu Batch (t): 0.8244 LR: 0.000094 Step: 176350 Total Loss: 5.4721 Recon Loss: 5.4573 
[12/24 09:07:05 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000094 Step: 176400 Total Loss: 5.4804 Recon Loss: 5.4656 
[12/24 09:07:46 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000094 Step: 176450 Total Loss: 4.0733 Recon Loss: 4.0585 
[12/24 09:08:27 TiTok]: Data (t): 0.0011, 38.87/s/gpu Batch (t): 0.8233 LR: 0.000094 Step: 176500 Total Loss: 3.9708 Recon Loss: 3.9560 
[12/24 09:09:08 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8215 LR: 0.000094 Step: 176550 Total Loss: 4.7589 Recon Loss: 4.7441 
[12/24 09:09:49 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000094 Step: 176600 Total Loss: 4.7231 Recon Loss: 4.7084 
[12/24 09:10:30 TiTok]: Data (t): 0.0035, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000094 Step: 176650 Total Loss: 4.7675 Recon Loss: 4.7528 
[12/24 09:11:11 TiTok]: Data (t): 0.0010, 39.37/s/gpu Batch (t): 0.8127 LR: 0.000094 Step: 176700 Total Loss: 4.0591 Recon Loss: 4.0442 
[12/24 09:11:52 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8133 LR: 0.000094 Step: 176750 Total Loss: 4.0989 Recon Loss: 4.0841 
[12/24 09:12:32 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000094 Step: 176800 Total Loss: 6.8216 Recon Loss: 6.8068 
[12/24 09:13:13 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000094 Step: 176850 Total Loss: 4.1040 Recon Loss: 4.0891 
[12/24 09:13:54 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000094 Step: 176900 Total Loss: 4.0221 Recon Loss: 4.0073 
[12/24 09:14:35 TiTok]: Data (t): 0.0015, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000094 Step: 176950 Total Loss: 4.7652 Recon Loss: 4.7504 
[12/24 09:15:16 TiTok]: Data (t): 0.0011, 35.20/s/gpu Batch (t): 0.9092 LR: 0.000094 Step: 177000 Total Loss: 5.4371 Recon Loss: 5.4224 
[12/24 09:15:57 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8158 LR: 0.000094 Step: 177050 Total Loss: 5.4606 Recon Loss: 5.4458 
[12/24 09:16:38 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8206 LR: 0.000094 Step: 177100 Total Loss: 4.7468 Recon Loss: 4.7319 
[12/24 09:17:19 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000094 Step: 177150 Total Loss: 4.1103 Recon Loss: 4.0955 
[12/24 09:18:00 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000094 Step: 177200 Total Loss: 4.7815 Recon Loss: 4.7667 
[12/24 09:18:41 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8177 LR: 0.000094 Step: 177250 Total Loss: 4.7154 Recon Loss: 4.7005 
[12/24 09:19:22 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000094 Step: 177300 Total Loss: 4.7613 Recon Loss: 4.7465 
[12/24 09:20:02 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000094 Step: 177350 Total Loss: 4.0930 Recon Loss: 4.0781 
[12/24 09:20:43 TiTok]: Data (t): 0.0011, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000094 Step: 177400 Total Loss: 4.7341 Recon Loss: 4.7193 
[12/24 09:21:24 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000094 Step: 177450 Total Loss: 4.1150 Recon Loss: 4.1001 
[12/24 09:22:05 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000094 Step: 177500 Total Loss: 5.4400 Recon Loss: 5.4252 
[12/24 09:22:46 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000094 Step: 177550 Total Loss: 4.7994 Recon Loss: 4.7846 
[12/24 09:23:27 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000094 Step: 177600 Total Loss: 6.1154 Recon Loss: 6.1006 
[12/24 09:24:08 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000094 Step: 177650 Total Loss: 4.7678 Recon Loss: 4.7530 
[12/24 09:24:49 TiTok]: Data (t): 0.0012, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000094 Step: 177700 Total Loss: 5.4046 Recon Loss: 5.3898 
[12/24 09:25:30 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8208 LR: 0.000094 Step: 177750 Total Loss: 5.5075 Recon Loss: 5.4927 
[12/24 09:26:11 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000094 Step: 177800 Total Loss: 4.7780 Recon Loss: 4.7632 
[12/24 09:26:52 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000094 Step: 177850 Total Loss: 5.4327 Recon Loss: 5.4179 
[12/24 09:27:32 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000094 Step: 177900 Total Loss: 5.5028 Recon Loss: 5.4879 
[12/24 09:28:13 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8217 LR: 0.000094 Step: 177950 Total Loss: 4.7832 Recon Loss: 4.7685 
[12/24 09:28:54 TiTok]: Data (t): 0.0010, 35.31/s/gpu Batch (t): 0.9063 LR: 0.000094 Step: 178000 Total Loss: 4.7846 Recon Loss: 4.7699 
[12/24 09:29:35 TiTok]: Data (t): 0.0011, 39.56/s/gpu Batch (t): 0.8090 LR: 0.000094 Step: 178050 Total Loss: 5.4303 Recon Loss: 5.4155 
[12/24 09:30:16 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000094 Step: 178100 Total Loss: 4.1167 Recon Loss: 4.1019 
[12/24 09:30:57 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000094 Step: 178150 Total Loss: 4.0101 Recon Loss: 3.9953 
[12/24 09:31:38 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8202 LR: 0.000094 Step: 178200 Total Loss: 6.1307 Recon Loss: 6.1160 
[12/24 09:32:19 TiTok]: Data (t): 0.0011, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000094 Step: 178250 Total Loss: 5.4353 Recon Loss: 5.4205 
[12/24 09:33:00 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000094 Step: 178300 Total Loss: 4.0395 Recon Loss: 4.0248 
[12/24 09:33:40 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000094 Step: 178350 Total Loss: 5.3913 Recon Loss: 5.3764 
[12/24 09:34:21 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8223 LR: 0.000094 Step: 178400 Total Loss: 4.8039 Recon Loss: 4.7890 
[12/24 09:35:02 TiTok]: Data (t): 0.0010, 39.48/s/gpu Batch (t): 0.8105 LR: 0.000094 Step: 178450 Total Loss: 4.0194 Recon Loss: 4.0047 
[12/24 09:35:43 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000094 Step: 178500 Total Loss: 4.0260 Recon Loss: 4.0112 
[12/24 09:36:24 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000094 Step: 178550 Total Loss: 4.0902 Recon Loss: 4.0756 
[12/24 09:37:05 TiTok]: Data (t): 0.0011, 38.77/s/gpu Batch (t): 0.8254 LR: 0.000094 Step: 178600 Total Loss: 4.0868 Recon Loss: 4.0719 
[12/24 09:37:46 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000094 Step: 178650 Total Loss: 4.6932 Recon Loss: 4.6784 
[12/24 09:38:27 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000094 Step: 178700 Total Loss: 4.7443 Recon Loss: 4.7294 
[12/24 09:39:08 TiTok]: Data (t): 0.0010, 39.41/s/gpu Batch (t): 0.8119 LR: 0.000094 Step: 178750 Total Loss: 4.0926 Recon Loss: 4.0777 
[12/24 09:39:48 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000094 Step: 178800 Total Loss: 5.4226 Recon Loss: 5.4078 
[12/24 09:40:29 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8179 LR: 0.000094 Step: 178850 Total Loss: 5.4591 Recon Loss: 5.4442 
[12/24 09:41:10 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000094 Step: 178900 Total Loss: 4.6943 Recon Loss: 4.6794 
[12/24 09:41:51 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000094 Step: 178950 Total Loss: 4.7674 Recon Loss: 4.7526 
[12/24 09:42:32 TiTok]: Data (t): 0.0011, 35.03/s/gpu Batch (t): 0.9135 LR: 0.000094 Step: 179000 Total Loss: 4.7594 Recon Loss: 4.7447 
[12/24 09:43:13 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000094 Step: 179050 Total Loss: 4.7708 Recon Loss: 4.7561 
[12/24 09:43:54 TiTok]: Data (t): 0.0010, 39.50/s/gpu Batch (t): 0.8101 LR: 0.000094 Step: 179100 Total Loss: 4.1043 Recon Loss: 4.0894 
[12/24 09:44:35 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000094 Step: 179150 Total Loss: 4.7579 Recon Loss: 4.7431 
[12/24 09:45:16 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8200 LR: 0.000094 Step: 179200 Total Loss: 4.7922 Recon Loss: 4.7774 
[12/24 09:45:57 TiTok]: Data (t): 0.0011, 38.87/s/gpu Batch (t): 0.8232 LR: 0.000094 Step: 179250 Total Loss: 5.4811 Recon Loss: 5.4663 
[12/24 09:46:38 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000094 Step: 179300 Total Loss: 4.7437 Recon Loss: 4.7289 
[12/24 09:47:19 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000094 Step: 179350 Total Loss: 4.7842 Recon Loss: 4.7693 
[12/24 09:47:59 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000094 Step: 179400 Total Loss: 5.4049 Recon Loss: 5.3901 
[12/24 09:48:40 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000094 Step: 179450 Total Loss: 5.4544 Recon Loss: 5.4396 
[12/24 09:49:21 TiTok]: Data (t): 0.0015, 40.18/s/gpu Batch (t): 0.7964 LR: 0.000094 Step: 179500 Total Loss: 5.4853 Recon Loss: 5.4705 
[12/24 09:50:02 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8158 LR: 0.000094 Step: 179550 Total Loss: 4.0578 Recon Loss: 4.0430 
[12/24 09:50:43 TiTok]: Data (t): 0.0010, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000094 Step: 179600 Total Loss: 4.8172 Recon Loss: 4.8025 
[12/24 09:51:24 TiTok]: Data (t): 0.0012, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000094 Step: 179650 Total Loss: 5.4746 Recon Loss: 5.4598 
[12/24 09:52:05 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000094 Step: 179700 Total Loss: 5.3912 Recon Loss: 5.3764 
[12/24 09:52:46 TiTok]: Data (t): 0.0010, 38.82/s/gpu Batch (t): 0.8244 LR: 0.000094 Step: 179750 Total Loss: 4.7705 Recon Loss: 4.7557 
[12/24 09:53:27 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000094 Step: 179800 Total Loss: 4.0919 Recon Loss: 4.0771 
[12/24 09:54:08 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000094 Step: 179850 Total Loss: 4.0235 Recon Loss: 4.0088 
[12/24 09:54:48 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000094 Step: 179900 Total Loss: 4.7564 Recon Loss: 4.7416 
[12/24 09:55:29 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000094 Step: 179950 Total Loss: 4.7654 Recon Loss: 4.7506 
[12/24 09:56:10 TiTok]: Data (t): 0.0010, 34.95/s/gpu Batch (t): 0.9156 LR: 0.000094 Step: 180000 Total Loss: 4.6885 Recon Loss: 4.6737 
[12/24 09:56:11 TiTok]: Reconstructing images...
[12/24 09:56:53 TiTok]: Data (t): 0.0010, 38.77/s/gpu Batch (t): 0.8254 LR: 0.000094 Step: 180050 Total Loss: 4.0678 Recon Loss: 4.0530 
[12/24 09:57:34 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8227 LR: 0.000094 Step: 180100 Total Loss: 5.4465 Recon Loss: 5.4317 
[12/24 09:58:15 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000094 Step: 180150 Total Loss: 4.7789 Recon Loss: 4.7640 
[12/24 09:58:55 TiTok]: Data (t): 0.0011, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000094 Step: 180200 Total Loss: 4.6560 Recon Loss: 4.6412 
[12/24 09:59:36 TiTok]: Data (t): 0.0011, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000094 Step: 180250 Total Loss: 4.1006 Recon Loss: 4.0858 
[12/24 10:00:17 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8179 LR: 0.000094 Step: 180300 Total Loss: 4.7896 Recon Loss: 4.7748 
[12/24 10:00:58 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000094 Step: 180350 Total Loss: 4.7381 Recon Loss: 4.7232 
Epoch 18/99 started.
[12/24 10:01:40 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000094 Step: 180400 Total Loss: 5.4448 Recon Loss: 5.4300 
[12/24 10:02:21 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000094 Step: 180450 Total Loss: 4.0766 Recon Loss: 4.0618 
[12/24 10:03:02 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000094 Step: 180500 Total Loss: 4.7456 Recon Loss: 4.7308 
[12/24 10:03:43 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000094 Step: 180550 Total Loss: 4.0956 Recon Loss: 4.0808 
[12/24 10:04:24 TiTok]: Data (t): 0.0010, 39.50/s/gpu Batch (t): 0.8101 LR: 0.000094 Step: 180600 Total Loss: 4.7617 Recon Loss: 4.7469 
[12/24 10:05:04 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000094 Step: 180650 Total Loss: 5.3822 Recon Loss: 5.3674 
[12/24 10:05:45 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8158 LR: 0.000094 Step: 180700 Total Loss: 4.0394 Recon Loss: 4.0247 
[12/24 10:06:26 TiTok]: Data (t): 0.0018, 39.77/s/gpu Batch (t): 0.8047 LR: 0.000094 Step: 180750 Total Loss: 4.0909 Recon Loss: 4.0761 
[12/24 10:07:07 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8131 LR: 0.000094 Step: 180800 Total Loss: 4.0870 Recon Loss: 4.0723 
[12/24 10:07:48 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000094 Step: 180850 Total Loss: 5.4771 Recon Loss: 5.4622 
[12/24 10:08:29 TiTok]: Data (t): 0.0011, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000094 Step: 180900 Total Loss: 4.8167 Recon Loss: 4.8019 
[12/24 10:09:10 TiTok]: Data (t): 0.0011, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000094 Step: 180950 Total Loss: 4.7609 Recon Loss: 4.7461 
[12/24 10:09:51 TiTok]: Data (t): 0.0011, 33.38/s/gpu Batch (t): 0.9588 LR: 0.000094 Step: 181000 Total Loss: 4.8347 Recon Loss: 4.8199 
[12/24 10:10:31 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000094 Step: 181050 Total Loss: 4.7756 Recon Loss: 4.7608 
[12/24 10:11:12 TiTok]: Data (t): 0.0010, 38.74/s/gpu Batch (t): 0.8260 LR: 0.000094 Step: 181100 Total Loss: 4.7443 Recon Loss: 4.7296 
[12/24 10:11:53 TiTok]: Data (t): 0.0011, 38.70/s/gpu Batch (t): 0.8269 LR: 0.000094 Step: 181150 Total Loss: 6.1208 Recon Loss: 6.1060 
[12/24 10:12:34 TiTok]: Data (t): 0.0010, 39.42/s/gpu Batch (t): 0.8118 LR: 0.000094 Step: 181200 Total Loss: 5.4007 Recon Loss: 5.3860 
[12/24 10:13:15 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8202 LR: 0.000094 Step: 181250 Total Loss: 4.6873 Recon Loss: 4.6725 
[12/24 10:13:56 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000094 Step: 181300 Total Loss: 4.7910 Recon Loss: 4.7762 
[12/24 10:14:37 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000094 Step: 181350 Total Loss: 4.1043 Recon Loss: 4.0896 
[12/24 10:15:18 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000094 Step: 181400 Total Loss: 4.7906 Recon Loss: 4.7758 
[12/24 10:15:58 TiTok]: Data (t): 0.0016, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000094 Step: 181450 Total Loss: 4.7308 Recon Loss: 4.7160 
[12/24 10:16:39 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000094 Step: 181500 Total Loss: 4.0318 Recon Loss: 4.0170 
[12/24 10:17:20 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000094 Step: 181550 Total Loss: 4.6884 Recon Loss: 4.6737 
[12/24 10:18:01 TiTok]: Data (t): 0.0011, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000093 Step: 181600 Total Loss: 4.8248 Recon Loss: 4.8100 
[12/24 10:18:42 TiTok]: Data (t): 0.0010, 39.42/s/gpu Batch (t): 0.8119 LR: 0.000093 Step: 181650 Total Loss: 6.8295 Recon Loss: 6.8148 
[12/24 10:19:23 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000093 Step: 181700 Total Loss: 4.0587 Recon Loss: 4.0440 
[12/24 10:20:04 TiTok]: Data (t): 0.0011, 39.42/s/gpu Batch (t): 0.8118 LR: 0.000093 Step: 181750 Total Loss: 4.7647 Recon Loss: 4.7498 
[12/24 10:20:45 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000093 Step: 181800 Total Loss: 4.0345 Recon Loss: 4.0197 
[12/24 10:21:25 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000093 Step: 181850 Total Loss: 4.0655 Recon Loss: 4.0506 
[12/24 10:22:06 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8202 LR: 0.000093 Step: 181900 Total Loss: 4.7794 Recon Loss: 4.7646 
[12/24 10:22:47 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000093 Step: 181950 Total Loss: 3.9303 Recon Loss: 3.9155 
[12/24 10:23:28 TiTok]: Data (t): 0.0010, 34.74/s/gpu Batch (t): 0.9211 LR: 0.000093 Step: 182000 Total Loss: 4.1417 Recon Loss: 4.1268 
[12/24 10:24:09 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000093 Step: 182050 Total Loss: 4.8390 Recon Loss: 4.8242 
[12/24 10:24:50 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000093 Step: 182100 Total Loss: 4.0993 Recon Loss: 4.0845 
[12/24 10:25:31 TiTok]: Data (t): 0.0012, 39.40/s/gpu Batch (t): 0.8122 LR: 0.000093 Step: 182150 Total Loss: 4.1004 Recon Loss: 4.0856 
[12/24 10:26:11 TiTok]: Data (t): 0.0011, 39.39/s/gpu Batch (t): 0.8124 LR: 0.000093 Step: 182200 Total Loss: 5.4213 Recon Loss: 5.4064 
[12/24 10:26:52 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000093 Step: 182250 Total Loss: 5.4845 Recon Loss: 5.4697 
[12/24 10:27:33 TiTok]: Data (t): 0.0011, 38.85/s/gpu Batch (t): 0.8236 LR: 0.000093 Step: 182300 Total Loss: 6.1084 Recon Loss: 6.0936 
[12/24 10:28:14 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000093 Step: 182350 Total Loss: 4.7395 Recon Loss: 4.7247 
[12/24 10:28:55 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000093 Step: 182400 Total Loss: 4.0970 Recon Loss: 4.0821 
[12/24 10:29:36 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000093 Step: 182450 Total Loss: 4.7583 Recon Loss: 4.7434 
[12/24 10:30:17 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8177 LR: 0.000093 Step: 182500 Total Loss: 4.8024 Recon Loss: 4.7877 
[12/24 10:30:57 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000093 Step: 182550 Total Loss: 4.7690 Recon Loss: 4.7540 
[12/24 10:31:38 TiTok]: Data (t): 0.0012, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000093 Step: 182600 Total Loss: 4.7475 Recon Loss: 4.7327 
[12/24 10:32:19 TiTok]: Data (t): 0.0010, 39.42/s/gpu Batch (t): 0.8119 LR: 0.000093 Step: 182650 Total Loss: 5.4935 Recon Loss: 5.4786 
[12/24 10:33:00 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000093 Step: 182700 Total Loss: 4.7579 Recon Loss: 4.7431 
[12/24 10:33:41 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000093 Step: 182750 Total Loss: 5.4886 Recon Loss: 5.4738 
[12/24 10:34:22 TiTok]: Data (t): 0.0010, 38.87/s/gpu Batch (t): 0.8233 LR: 0.000093 Step: 182800 Total Loss: 5.4443 Recon Loss: 5.4296 
[12/24 10:35:03 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000093 Step: 182850 Total Loss: 5.4808 Recon Loss: 5.4660 
[12/24 10:35:43 TiTok]: Data (t): 0.0033, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000093 Step: 182900 Total Loss: 6.1839 Recon Loss: 6.1691 
[12/24 10:36:24 TiTok]: Data (t): 0.0035, 38.95/s/gpu Batch (t): 0.8217 LR: 0.000093 Step: 182950 Total Loss: 5.4392 Recon Loss: 5.4244 
[12/24 10:37:05 TiTok]: Data (t): 0.0010, 35.00/s/gpu Batch (t): 0.9143 LR: 0.000093 Step: 183000 Total Loss: 4.6727 Recon Loss: 4.6579 
[12/24 10:37:46 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000093 Step: 183050 Total Loss: 4.7756 Recon Loss: 4.7608 
[12/24 10:38:27 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000093 Step: 183100 Total Loss: 5.4497 Recon Loss: 5.4348 
[12/24 10:39:08 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000093 Step: 183150 Total Loss: 4.7815 Recon Loss: 4.7667 
[12/24 10:39:49 TiTok]: Data (t): 0.0011, 38.74/s/gpu Batch (t): 0.8259 LR: 0.000093 Step: 183200 Total Loss: 4.0539 Recon Loss: 4.0391 
[12/24 10:40:30 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000093 Step: 183250 Total Loss: 5.4728 Recon Loss: 5.4580 
[12/24 10:41:11 TiTok]: Data (t): 0.0016, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000093 Step: 183300 Total Loss: 4.7381 Recon Loss: 4.7234 
[12/24 10:41:52 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8206 LR: 0.000093 Step: 183350 Total Loss: 4.7621 Recon Loss: 4.7473 
[12/24 10:42:32 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000093 Step: 183400 Total Loss: 4.0619 Recon Loss: 4.0472 
[12/24 10:43:13 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000093 Step: 183450 Total Loss: 4.7409 Recon Loss: 4.7262 
[12/24 10:43:54 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8150 LR: 0.000093 Step: 183500 Total Loss: 4.0793 Recon Loss: 4.0645 
[12/24 10:44:35 TiTok]: Data (t): 0.0010, 38.81/s/gpu Batch (t): 0.8246 LR: 0.000093 Step: 183550 Total Loss: 4.0620 Recon Loss: 4.0473 
[12/24 10:45:16 TiTok]: Data (t): 0.0012, 38.91/s/gpu Batch (t): 0.8225 LR: 0.000093 Step: 183600 Total Loss: 4.0457 Recon Loss: 4.0309 
[12/24 10:45:57 TiTok]: Data (t): 0.0010, 39.55/s/gpu Batch (t): 0.8091 LR: 0.000093 Step: 183650 Total Loss: 6.1256 Recon Loss: 6.1108 
[12/24 10:46:38 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000093 Step: 183700 Total Loss: 4.7540 Recon Loss: 4.7392 
[12/24 10:47:19 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000093 Step: 183750 Total Loss: 4.7617 Recon Loss: 4.7468 
[12/24 10:48:00 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000093 Step: 183800 Total Loss: 4.7392 Recon Loss: 4.7245 
[12/24 10:48:40 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000093 Step: 183850 Total Loss: 4.0470 Recon Loss: 4.0322 
[12/24 10:49:21 TiTok]: Data (t): 0.0010, 38.82/s/gpu Batch (t): 0.8242 LR: 0.000093 Step: 183900 Total Loss: 5.4404 Recon Loss: 5.4256 
[12/24 10:50:02 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000093 Step: 183950 Total Loss: 4.0509 Recon Loss: 4.0361 
[12/24 10:50:43 TiTok]: Data (t): 0.0010, 34.55/s/gpu Batch (t): 0.9262 LR: 0.000093 Step: 184000 Total Loss: 4.0877 Recon Loss: 4.0729 
[12/24 10:51:24 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000093 Step: 184050 Total Loss: 4.7606 Recon Loss: 4.7458 
[12/24 10:52:05 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000093 Step: 184100 Total Loss: 4.0581 Recon Loss: 4.0433 
[12/24 10:52:46 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000093 Step: 184150 Total Loss: 4.7848 Recon Loss: 4.7699 
[12/24 10:53:27 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000093 Step: 184200 Total Loss: 4.7177 Recon Loss: 4.7030 
[12/24 10:54:08 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000093 Step: 184250 Total Loss: 4.7766 Recon Loss: 4.7618 
[12/24 10:54:48 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000093 Step: 184300 Total Loss: 4.7372 Recon Loss: 4.7224 
[12/24 10:55:29 TiTok]: Data (t): 0.0012, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000093 Step: 184350 Total Loss: 5.4627 Recon Loss: 5.4479 
[12/24 10:56:10 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000093 Step: 184400 Total Loss: 4.7624 Recon Loss: 4.7476 
[12/24 10:56:51 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000093 Step: 184450 Total Loss: 4.6579 Recon Loss: 4.6432 
[12/24 10:57:32 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000093 Step: 184500 Total Loss: 4.1283 Recon Loss: 4.1135 
[12/24 10:58:13 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000093 Step: 184550 Total Loss: 4.7926 Recon Loss: 4.7777 
[12/24 10:58:54 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000093 Step: 184600 Total Loss: 4.0044 Recon Loss: 3.9897 
[12/24 10:59:35 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000093 Step: 184650 Total Loss: 4.0695 Recon Loss: 4.0547 
[12/24 11:00:16 TiTok]: Data (t): 0.0011, 38.69/s/gpu Batch (t): 0.8270 LR: 0.000093 Step: 184700 Total Loss: 4.1690 Recon Loss: 4.1542 
[12/24 11:00:57 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000093 Step: 184750 Total Loss: 5.4694 Recon Loss: 5.4546 
[12/24 11:01:38 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8158 LR: 0.000093 Step: 184800 Total Loss: 4.0762 Recon Loss: 4.0614 
[12/24 11:02:18 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8185 LR: 0.000093 Step: 184850 Total Loss: 4.7945 Recon Loss: 4.7797 
[12/24 11:02:59 TiTok]: Data (t): 0.0011, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000093 Step: 184900 Total Loss: 5.4456 Recon Loss: 5.4308 
[12/24 11:03:40 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8154 LR: 0.000093 Step: 184950 Total Loss: 4.1040 Recon Loss: 4.0891 
[12/24 11:04:21 TiTok]: Data (t): 0.0010, 34.64/s/gpu Batch (t): 0.9237 LR: 0.000093 Step: 185000 Total Loss: 4.0187 Recon Loss: 4.0038 
[12/24 11:04:22 TiTok]: Reconstructing images...
[12/24 11:05:04 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000093 Step: 185050 Total Loss: 4.6950 Recon Loss: 4.6802 
[12/24 11:05:45 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000093 Step: 185100 Total Loss: 5.4254 Recon Loss: 5.4106 
[12/24 11:06:26 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000093 Step: 185150 Total Loss: 4.0867 Recon Loss: 4.0719 
[12/24 11:07:06 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000093 Step: 185200 Total Loss: 5.4580 Recon Loss: 5.4432 
[12/24 11:07:47 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000093 Step: 185250 Total Loss: 4.7840 Recon Loss: 4.7692 
[12/24 11:08:28 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000093 Step: 185300 Total Loss: 4.7416 Recon Loss: 4.7269 
[12/24 11:09:09 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000093 Step: 185350 Total Loss: 4.0497 Recon Loss: 4.0349 
[12/24 11:09:50 TiTok]: Data (t): 0.0010, 38.85/s/gpu Batch (t): 0.8237 LR: 0.000093 Step: 185400 Total Loss: 4.7418 Recon Loss: 4.7270 
[12/24 11:10:31 TiTok]: Data (t): 0.0034, 39.22/s/gpu Batch (t): 0.8158 LR: 0.000093 Step: 185450 Total Loss: 4.7956 Recon Loss: 4.7809 
[12/24 11:11:12 TiTok]: Data (t): 0.0011, 39.45/s/gpu Batch (t): 0.8112 LR: 0.000093 Step: 185500 Total Loss: 6.1379 Recon Loss: 6.1231 
[12/24 11:11:53 TiTok]: Data (t): 0.0010, 39.44/s/gpu Batch (t): 0.8113 LR: 0.000093 Step: 185550 Total Loss: 4.7784 Recon Loss: 4.7636 
[12/24 11:12:34 TiTok]: Data (t): 0.0010, 38.36/s/gpu Batch (t): 0.8343 LR: 0.000093 Step: 185600 Total Loss: 4.0305 Recon Loss: 4.0157 
[12/24 11:13:14 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000093 Step: 185650 Total Loss: 5.4759 Recon Loss: 5.4611 
[12/24 11:13:55 TiTok]: Data (t): 0.0010, 39.39/s/gpu Batch (t): 0.8123 LR: 0.000093 Step: 185700 Total Loss: 3.9854 Recon Loss: 3.9706 
[12/24 11:14:36 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000093 Step: 185750 Total Loss: 4.7490 Recon Loss: 4.7342 
[12/24 11:15:17 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000093 Step: 185800 Total Loss: 4.7492 Recon Loss: 4.7344 
[12/24 11:15:58 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000093 Step: 185850 Total Loss: 4.0212 Recon Loss: 4.0064 
[12/24 11:16:39 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8229 LR: 0.000093 Step: 185900 Total Loss: 5.4587 Recon Loss: 5.4439 
[12/24 11:17:20 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000093 Step: 185950 Total Loss: 5.4864 Recon Loss: 5.4716 
[12/24 11:18:01 TiTok]: Data (t): 0.0011, 35.21/s/gpu Batch (t): 0.9089 LR: 0.000093 Step: 186000 Total Loss: 4.1267 Recon Loss: 4.1120 
[12/24 11:18:42 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000093 Step: 186050 Total Loss: 4.7349 Recon Loss: 4.7201 
[12/24 11:19:23 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000093 Step: 186100 Total Loss: 4.7587 Recon Loss: 4.7440 
[12/24 11:20:03 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000093 Step: 186150 Total Loss: 5.4424 Recon Loss: 5.4276 
[12/24 11:20:44 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000093 Step: 186200 Total Loss: 6.1495 Recon Loss: 6.1347 
[12/24 11:21:25 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000093 Step: 186250 Total Loss: 4.0535 Recon Loss: 4.0386 
[12/24 11:22:06 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8204 LR: 0.000093 Step: 186300 Total Loss: 4.7051 Recon Loss: 4.6903 
[12/24 11:22:47 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8200 LR: 0.000093 Step: 186350 Total Loss: 5.4691 Recon Loss: 5.4543 
[12/24 11:23:28 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000093 Step: 186400 Total Loss: 4.6873 Recon Loss: 4.6725 
[12/24 11:24:09 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000093 Step: 186450 Total Loss: 4.0105 Recon Loss: 3.9956 
[12/24 11:24:50 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000093 Step: 186500 Total Loss: 5.4950 Recon Loss: 5.4802 
[12/24 11:25:31 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8146 LR: 0.000093 Step: 186550 Total Loss: 5.4503 Recon Loss: 5.4355 
[12/24 11:26:12 TiTok]: Data (t): 0.0011, 38.51/s/gpu Batch (t): 0.8310 LR: 0.000093 Step: 186600 Total Loss: 3.9929 Recon Loss: 3.9782 
[12/24 11:26:53 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8217 LR: 0.000093 Step: 186650 Total Loss: 5.4305 Recon Loss: 5.4157 
[12/24 11:27:34 TiTok]: Data (t): 0.0011, 39.49/s/gpu Batch (t): 0.8103 LR: 0.000093 Step: 186700 Total Loss: 5.4444 Recon Loss: 5.4295 
[12/24 11:28:15 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000093 Step: 186750 Total Loss: 5.4932 Recon Loss: 5.4784 
[12/24 11:28:55 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8234 LR: 0.000093 Step: 186800 Total Loss: 4.6931 Recon Loss: 4.6783 
[12/24 11:29:36 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000093 Step: 186850 Total Loss: 4.7731 Recon Loss: 4.7583 
[12/24 11:30:17 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000093 Step: 186900 Total Loss: 4.7168 Recon Loss: 4.7020 
[12/24 11:30:58 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000093 Step: 186950 Total Loss: 4.1184 Recon Loss: 4.1036 
[12/24 11:31:39 TiTok]: Data (t): 0.0010, 35.20/s/gpu Batch (t): 0.9091 LR: 0.000093 Step: 187000 Total Loss: 4.7818 Recon Loss: 4.7671 
[12/24 11:32:20 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000093 Step: 187050 Total Loss: 4.1442 Recon Loss: 4.1294 
[12/24 11:33:01 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000093 Step: 187100 Total Loss: 4.1798 Recon Loss: 4.1651 
[12/24 11:33:42 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000093 Step: 187150 Total Loss: 4.0508 Recon Loss: 4.0361 
[12/24 11:34:23 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8223 LR: 0.000093 Step: 187200 Total Loss: 4.8037 Recon Loss: 4.7889 
[12/24 11:35:04 TiTok]: Data (t): 0.0012, 39.40/s/gpu Batch (t): 0.8122 LR: 0.000093 Step: 187250 Total Loss: 4.7192 Recon Loss: 4.7045 
[12/24 11:35:45 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000093 Step: 187300 Total Loss: 6.1258 Recon Loss: 6.1111 
[12/24 11:36:25 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000093 Step: 187350 Total Loss: 5.4710 Recon Loss: 5.4562 
[12/24 11:37:06 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000093 Step: 187400 Total Loss: 4.0120 Recon Loss: 3.9972 
[12/24 11:37:47 TiTok]: Data (t): 0.0011, 38.44/s/gpu Batch (t): 0.8326 LR: 0.000093 Step: 187450 Total Loss: 4.7915 Recon Loss: 4.7767 
[12/24 11:38:28 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000093 Step: 187500 Total Loss: 4.0907 Recon Loss: 4.0759 
[12/24 11:39:09 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000093 Step: 187550 Total Loss: 4.7320 Recon Loss: 4.7172 
[12/24 11:39:50 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000093 Step: 187600 Total Loss: 4.0198 Recon Loss: 4.0050 
[12/24 11:40:31 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000093 Step: 187650 Total Loss: 4.0597 Recon Loss: 4.0450 
[12/24 11:41:12 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000093 Step: 187700 Total Loss: 4.0504 Recon Loss: 4.0357 
[12/24 11:41:53 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000093 Step: 187750 Total Loss: 4.0193 Recon Loss: 4.0045 
[12/24 11:42:34 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8223 LR: 0.000093 Step: 187800 Total Loss: 5.4034 Recon Loss: 5.3887 
[12/24 11:43:15 TiTok]: Data (t): 0.0010, 38.78/s/gpu Batch (t): 0.8251 LR: 0.000093 Step: 187850 Total Loss: 4.7458 Recon Loss: 4.7310 
[12/24 11:43:56 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000093 Step: 187900 Total Loss: 4.0654 Recon Loss: 4.0506 
[12/24 11:44:37 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000093 Step: 187950 Total Loss: 4.7909 Recon Loss: 4.7762 
[12/24 11:45:18 TiTok]: Data (t): 0.0011, 35.47/s/gpu Batch (t): 0.9022 LR: 0.000093 Step: 188000 Total Loss: 4.7175 Recon Loss: 4.7027 
[12/24 11:45:58 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000093 Step: 188050 Total Loss: 4.0677 Recon Loss: 4.0528 
[12/24 11:46:39 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000093 Step: 188100 Total Loss: 5.4055 Recon Loss: 5.3907 
[12/24 11:47:20 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8231 LR: 0.000093 Step: 188150 Total Loss: 5.4170 Recon Loss: 5.4022 
[12/24 11:48:01 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000093 Step: 188200 Total Loss: 5.4624 Recon Loss: 5.4476 
[12/24 11:48:42 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8158 LR: 0.000093 Step: 188250 Total Loss: 5.4449 Recon Loss: 5.4301 
[12/24 11:49:23 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000093 Step: 188300 Total Loss: 4.7148 Recon Loss: 4.7000 
[12/24 11:50:04 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000093 Step: 188350 Total Loss: 4.7237 Recon Loss: 4.7089 
[12/24 11:50:45 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000093 Step: 188400 Total Loss: 4.7808 Recon Loss: 4.7659 
[12/24 11:51:26 TiTok]: Data (t): 0.0010, 38.56/s/gpu Batch (t): 0.8298 LR: 0.000093 Step: 188450 Total Loss: 4.7330 Recon Loss: 4.7182 
[12/24 11:52:07 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8181 LR: 0.000093 Step: 188500 Total Loss: 4.7926 Recon Loss: 4.7778 
[12/24 11:52:47 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000093 Step: 188550 Total Loss: 5.4223 Recon Loss: 5.4075 
[12/24 11:53:28 TiTok]: Data (t): 0.0013, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000093 Step: 188600 Total Loss: 4.8023 Recon Loss: 4.7875 
[12/24 11:54:09 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8150 LR: 0.000093 Step: 188650 Total Loss: 4.0887 Recon Loss: 4.0739 
[12/24 11:54:50 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000093 Step: 188700 Total Loss: 4.7440 Recon Loss: 4.7292 
[12/24 11:55:31 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000093 Step: 188750 Total Loss: 5.4139 Recon Loss: 5.3991 
[12/24 11:56:12 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000093 Step: 188800 Total Loss: 4.6963 Recon Loss: 4.6815 
[12/24 11:56:53 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000093 Step: 188850 Total Loss: 4.7057 Recon Loss: 4.6909 
[12/24 11:57:34 TiTok]: Data (t): 0.0011, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000093 Step: 188900 Total Loss: 5.4646 Recon Loss: 5.4498 
[12/24 11:58:15 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000093 Step: 188950 Total Loss: 4.0360 Recon Loss: 4.0212 
[12/24 11:58:56 TiTok]: Data (t): 0.0011, 35.25/s/gpu Batch (t): 0.9079 LR: 0.000093 Step: 189000 Total Loss: 4.0504 Recon Loss: 4.0355 
[12/24 11:59:36 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000093 Step: 189050 Total Loss: 4.7224 Recon Loss: 4.7076 
[12/24 12:00:17 TiTok]: Data (t): 0.0011, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000093 Step: 189100 Total Loss: 5.4927 Recon Loss: 5.4779 
[12/24 12:00:58 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000093 Step: 189150 Total Loss: 4.8162 Recon Loss: 4.8015 
[12/24 12:01:39 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000093 Step: 189200 Total Loss: 4.7096 Recon Loss: 4.6947 
[12/24 12:02:20 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000093 Step: 189250 Total Loss: 4.1230 Recon Loss: 4.1082 
[12/24 12:03:01 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000093 Step: 189300 Total Loss: 4.0990 Recon Loss: 4.0841 
[12/24 12:03:42 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000093 Step: 189350 Total Loss: 4.7865 Recon Loss: 4.7717 
[12/24 12:04:23 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000093 Step: 189400 Total Loss: 4.6370 Recon Loss: 4.6223 
[12/24 12:05:04 TiTok]: Data (t): 0.0011, 38.82/s/gpu Batch (t): 0.8242 LR: 0.000093 Step: 189450 Total Loss: 4.7779 Recon Loss: 4.7630 
[12/24 12:05:45 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8154 LR: 0.000093 Step: 189500 Total Loss: 5.4248 Recon Loss: 5.4100 
[12/24 12:06:26 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8156 LR: 0.000093 Step: 189550 Total Loss: 4.7374 Recon Loss: 4.7226 
[12/24 12:07:06 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000093 Step: 189600 Total Loss: 4.7563 Recon Loss: 4.7414 
[12/24 12:07:47 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000093 Step: 189650 Total Loss: 6.1462 Recon Loss: 6.1314 
[12/24 12:08:28 TiTok]: Data (t): 0.0011, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000093 Step: 189700 Total Loss: 4.7407 Recon Loss: 4.7258 
[12/24 12:09:09 TiTok]: Data (t): 0.0011, 38.79/s/gpu Batch (t): 0.8250 LR: 0.000093 Step: 189750 Total Loss: 6.1504 Recon Loss: 6.1357 
[12/24 12:09:50 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8225 LR: 0.000093 Step: 189800 Total Loss: 6.8281 Recon Loss: 6.8132 
[12/24 12:10:31 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000093 Step: 189850 Total Loss: 6.1417 Recon Loss: 6.1270 
[12/24 12:11:12 TiTok]: Data (t): 0.0010, 38.40/s/gpu Batch (t): 0.8333 LR: 0.000093 Step: 189900 Total Loss: 4.0660 Recon Loss: 4.0512 
[12/24 12:11:53 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8131 LR: 0.000093 Step: 189950 Total Loss: 4.0690 Recon Loss: 4.0542 
[12/24 12:12:34 TiTok]: Data (t): 0.0010, 34.59/s/gpu Batch (t): 0.9252 LR: 0.000093 Step: 190000 Total Loss: 5.4234 Recon Loss: 5.4085 
[12/24 12:12:35 TiTok]: Reconstructing images...
[12/24 12:13:16 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000093 Step: 190050 Total Loss: 4.7993 Recon Loss: 4.7845 
[12/24 12:13:57 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000093 Step: 190100 Total Loss: 4.0786 Recon Loss: 4.0637 
[12/24 12:14:38 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000093 Step: 190150 Total Loss: 5.4297 Recon Loss: 5.4149 
[12/24 12:15:19 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000093 Step: 190200 Total Loss: 4.8376 Recon Loss: 4.8227 
[12/24 12:16:00 TiTok]: Data (t): 0.0010, 39.51/s/gpu Batch (t): 0.8099 LR: 0.000093 Step: 190250 Total Loss: 6.1448 Recon Loss: 6.1300 
[12/24 12:16:41 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000093 Step: 190300 Total Loss: 4.0331 Recon Loss: 4.0184 
[12/24 12:17:22 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000093 Step: 190350 Total Loss: 4.0634 Recon Loss: 4.0485 
Epoch 19/99 started.
[12/24 12:18:04 TiTok]: Data (t): 0.0011, 39.44/s/gpu Batch (t): 0.8113 LR: 0.000093 Step: 190400 Total Loss: 5.4229 Recon Loss: 5.4081 
[12/24 12:18:44 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000093 Step: 190450 Total Loss: 4.7611 Recon Loss: 4.7464 
[12/24 12:19:25 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000093 Step: 190500 Total Loss: 4.7655 Recon Loss: 4.7507 
[12/24 12:20:06 TiTok]: Data (t): 0.0009, 39.34/s/gpu Batch (t): 0.8133 LR: 0.000093 Step: 190550 Total Loss: 4.7784 Recon Loss: 4.7636 
[12/24 12:20:47 TiTok]: Data (t): 0.0010, 38.78/s/gpu Batch (t): 0.8252 LR: 0.000093 Step: 190600 Total Loss: 4.8055 Recon Loss: 4.7907 
[12/24 12:21:28 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000093 Step: 190650 Total Loss: 4.7140 Recon Loss: 4.6991 
[12/24 12:22:09 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8231 LR: 0.000093 Step: 190700 Total Loss: 4.7494 Recon Loss: 4.7345 
[12/24 12:22:50 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000093 Step: 190750 Total Loss: 5.4433 Recon Loss: 5.4285 
[12/24 12:23:31 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000093 Step: 190800 Total Loss: 4.7469 Recon Loss: 4.7321 
[12/24 12:24:12 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000093 Step: 190850 Total Loss: 4.7453 Recon Loss: 4.7305 
[12/24 12:24:52 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000093 Step: 190900 Total Loss: 5.4635 Recon Loss: 5.4487 
[12/24 12:25:33 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000093 Step: 190950 Total Loss: 4.7262 Recon Loss: 4.7113 
[12/24 12:26:14 TiTok]: Data (t): 0.0010, 33.61/s/gpu Batch (t): 0.9522 LR: 0.000093 Step: 191000 Total Loss: 6.1176 Recon Loss: 6.1028 
[12/24 12:26:55 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000093 Step: 191050 Total Loss: 4.7909 Recon Loss: 4.7761 
[12/24 12:27:36 TiTok]: Data (t): 0.0010, 39.41/s/gpu Batch (t): 0.8120 LR: 0.000093 Step: 191100 Total Loss: 4.0480 Recon Loss: 4.0332 
[12/24 12:28:17 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000093 Step: 191150 Total Loss: 5.3950 Recon Loss: 5.3800 
[12/24 12:28:58 TiTok]: Data (t): 0.0010, 39.39/s/gpu Batch (t): 0.8124 LR: 0.000093 Step: 191200 Total Loss: 4.7356 Recon Loss: 4.7208 
[12/24 12:29:39 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8154 LR: 0.000093 Step: 191250 Total Loss: 5.4419 Recon Loss: 5.4271 
[12/24 12:30:20 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000093 Step: 191300 Total Loss: 3.9927 Recon Loss: 3.9779 
[12/24 12:31:01 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000093 Step: 191350 Total Loss: 4.1115 Recon Loss: 4.0966 
[12/24 12:31:42 TiTok]: Data (t): 0.0011, 39.41/s/gpu Batch (t): 0.8121 LR: 0.000093 Step: 191400 Total Loss: 4.6904 Recon Loss: 4.6755 
[12/24 12:32:22 TiTok]: Data (t): 0.0010, 39.45/s/gpu Batch (t): 0.8112 LR: 0.000093 Step: 191450 Total Loss: 5.3602 Recon Loss: 5.3454 
[12/24 12:33:03 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000093 Step: 191500 Total Loss: 4.6993 Recon Loss: 4.6845 
[12/24 12:33:44 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000093 Step: 191550 Total Loss: 4.6990 Recon Loss: 4.6842 
[12/24 12:34:25 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000093 Step: 191600 Total Loss: 4.7472 Recon Loss: 4.7324 
[12/24 12:35:06 TiTok]: Data (t): 0.0011, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000093 Step: 191650 Total Loss: 4.7826 Recon Loss: 4.7677 
[12/24 12:35:47 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000093 Step: 191700 Total Loss: 5.4441 Recon Loss: 5.4293 
[12/24 12:36:28 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000093 Step: 191750 Total Loss: 4.0473 Recon Loss: 4.0325 
[12/24 12:37:09 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000093 Step: 191800 Total Loss: 4.0625 Recon Loss: 4.0478 
[12/24 12:37:50 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000093 Step: 191850 Total Loss: 4.7667 Recon Loss: 4.7519 
[12/24 12:38:30 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000093 Step: 191900 Total Loss: 4.7430 Recon Loss: 4.7281 
[12/24 12:39:11 TiTok]: Data (t): 0.0011, 38.87/s/gpu Batch (t): 0.8233 LR: 0.000093 Step: 191950 Total Loss: 5.4860 Recon Loss: 5.4712 
[12/24 12:39:52 TiTok]: Data (t): 0.0011, 34.80/s/gpu Batch (t): 0.9196 LR: 0.000093 Step: 192000 Total Loss: 4.7298 Recon Loss: 4.7150 
[12/24 12:40:33 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000093 Step: 192050 Total Loss: 4.0236 Recon Loss: 4.0088 
[12/24 12:41:14 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000093 Step: 192100 Total Loss: 4.8211 Recon Loss: 4.8063 
[12/24 12:41:55 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000093 Step: 192150 Total Loss: 4.7858 Recon Loss: 4.7710 
[12/24 12:42:36 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000093 Step: 192200 Total Loss: 5.4500 Recon Loss: 5.4352 
[12/24 12:43:17 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000093 Step: 192250 Total Loss: 4.0958 Recon Loss: 4.0810 
[12/24 12:43:58 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8202 LR: 0.000093 Step: 192300 Total Loss: 5.4514 Recon Loss: 5.4366 
[12/24 12:44:39 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000093 Step: 192350 Total Loss: 4.7430 Recon Loss: 4.7282 
[12/24 12:45:20 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8223 LR: 0.000093 Step: 192400 Total Loss: 4.0954 Recon Loss: 4.0806 
[12/24 12:46:01 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000093 Step: 192450 Total Loss: 5.4294 Recon Loss: 5.4146 
[12/24 12:46:42 TiTok]: Data (t): 0.0010, 38.79/s/gpu Batch (t): 0.8250 LR: 0.000093 Step: 192500 Total Loss: 4.7016 Recon Loss: 4.6867 
[12/24 12:47:23 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000093 Step: 192550 Total Loss: 5.4341 Recon Loss: 5.4192 
[12/24 12:48:04 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000093 Step: 192600 Total Loss: 4.0563 Recon Loss: 4.0415 
[12/24 12:48:45 TiTok]: Data (t): 0.0011, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000093 Step: 192650 Total Loss: 5.4203 Recon Loss: 5.4054 
[12/24 12:49:25 TiTok]: Data (t): 0.0011, 38.80/s/gpu Batch (t): 0.8248 LR: 0.000093 Step: 192700 Total Loss: 5.4697 Recon Loss: 5.4548 
[12/24 12:50:06 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8183 LR: 0.000093 Step: 192750 Total Loss: 5.4426 Recon Loss: 5.4278 
[12/24 12:50:47 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000093 Step: 192800 Total Loss: 4.0571 Recon Loss: 4.0423 
[12/24 12:51:28 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000093 Step: 192850 Total Loss: 4.7048 Recon Loss: 4.6900 
[12/24 12:52:09 TiTok]: Data (t): 0.0010, 38.79/s/gpu Batch (t): 0.8250 LR: 0.000093 Step: 192900 Total Loss: 4.0592 Recon Loss: 4.0444 
[12/24 12:52:50 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8148 LR: 0.000093 Step: 192950 Total Loss: 4.7471 Recon Loss: 4.7323 
[12/24 12:53:31 TiTok]: Data (t): 0.0011, 35.01/s/gpu Batch (t): 0.9140 LR: 0.000093 Step: 193000 Total Loss: 4.0902 Recon Loss: 4.0752 
[12/24 12:54:12 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8150 LR: 0.000093 Step: 193050 Total Loss: 4.0910 Recon Loss: 4.0762 
[12/24 12:54:53 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000093 Step: 193100 Total Loss: 4.0641 Recon Loss: 4.0493 
[12/24 12:55:34 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000093 Step: 193150 Total Loss: 3.9599 Recon Loss: 3.9451 
[12/24 12:56:15 TiTok]: Data (t): 0.0011, 38.84/s/gpu Batch (t): 0.8240 LR: 0.000093 Step: 193200 Total Loss: 4.7248 Recon Loss: 4.7100 
[12/24 12:56:56 TiTok]: Data (t): 0.0010, 38.82/s/gpu Batch (t): 0.8244 LR: 0.000093 Step: 193250 Total Loss: 4.1116 Recon Loss: 4.0968 
[12/24 12:57:36 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8156 LR: 0.000093 Step: 193300 Total Loss: 4.0939 Recon Loss: 4.0792 
[12/24 12:58:17 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000093 Step: 193350 Total Loss: 4.7247 Recon Loss: 4.7099 
[12/24 12:58:58 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000093 Step: 193400 Total Loss: 4.7706 Recon Loss: 4.7558 
[12/24 12:59:39 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000093 Step: 193450 Total Loss: 4.0960 Recon Loss: 4.0813 
[12/24 13:00:20 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8200 LR: 0.000093 Step: 193500 Total Loss: 4.7716 Recon Loss: 4.7568 
[12/24 13:01:01 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000093 Step: 193550 Total Loss: 4.7186 Recon Loss: 4.7037 
[12/24 13:01:42 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000093 Step: 193600 Total Loss: 5.4311 Recon Loss: 5.4162 
[12/24 13:02:23 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8158 LR: 0.000093 Step: 193650 Total Loss: 6.1278 Recon Loss: 6.1130 
[12/24 13:03:04 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000093 Step: 193700 Total Loss: 6.1400 Recon Loss: 6.1253 
[12/24 13:03:45 TiTok]: Data (t): 0.0011, 38.11/s/gpu Batch (t): 0.8396 LR: 0.000093 Step: 193750 Total Loss: 4.7389 Recon Loss: 4.7241 
[12/24 13:04:26 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8231 LR: 0.000093 Step: 193800 Total Loss: 4.0490 Recon Loss: 4.0342 
[12/24 13:05:06 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000093 Step: 193850 Total Loss: 4.0416 Recon Loss: 4.0269 
[12/24 13:05:47 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000093 Step: 193900 Total Loss: 4.0684 Recon Loss: 4.0536 
[12/24 13:06:28 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000093 Step: 193950 Total Loss: 4.7481 Recon Loss: 4.7334 
[12/24 13:07:09 TiTok]: Data (t): 0.0011, 34.46/s/gpu Batch (t): 0.9285 LR: 0.000093 Step: 194000 Total Loss: 5.4176 Recon Loss: 5.4027 
[12/24 13:07:50 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000093 Step: 194050 Total Loss: 4.7367 Recon Loss: 4.7219 
[12/24 13:08:31 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000093 Step: 194100 Total Loss: 4.7211 Recon Loss: 4.7063 
[12/24 13:09:12 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000093 Step: 194150 Total Loss: 4.7235 Recon Loss: 4.7087 
[12/24 13:09:53 TiTok]: Data (t): 0.0010, 38.79/s/gpu Batch (t): 0.8250 LR: 0.000093 Step: 194200 Total Loss: 4.7240 Recon Loss: 4.7091 
[12/24 13:10:34 TiTok]: Data (t): 0.0010, 39.41/s/gpu Batch (t): 0.8121 LR: 0.000093 Step: 194250 Total Loss: 4.7523 Recon Loss: 4.7376 
[12/24 13:11:15 TiTok]: Data (t): 0.0040, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000093 Step: 194300 Total Loss: 4.8311 Recon Loss: 4.8164 
[12/24 13:11:56 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000093 Step: 194350 Total Loss: 4.0883 Recon Loss: 4.0735 
[12/24 13:12:36 TiTok]: Data (t): 0.0009, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000093 Step: 194400 Total Loss: 4.7653 Recon Loss: 4.7505 
[12/24 13:13:17 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000093 Step: 194450 Total Loss: 5.4715 Recon Loss: 5.4568 
[12/24 13:13:58 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000093 Step: 194500 Total Loss: 4.0676 Recon Loss: 4.0528 
[12/24 13:14:39 TiTok]: Data (t): 0.0010, 40.29/s/gpu Batch (t): 0.7943 LR: 0.000093 Step: 194550 Total Loss: 5.4769 Recon Loss: 5.4621 
[12/24 13:15:20 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8202 LR: 0.000093 Step: 194600 Total Loss: 3.9856 Recon Loss: 3.9707 
[12/24 13:16:01 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000092 Step: 194650 Total Loss: 4.7501 Recon Loss: 4.7352 
[12/24 13:16:42 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8227 LR: 0.000092 Step: 194700 Total Loss: 4.0301 Recon Loss: 4.0153 
[12/24 13:17:23 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8148 LR: 0.000092 Step: 194750 Total Loss: 4.1187 Recon Loss: 4.1038 
[12/24 13:18:04 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000092 Step: 194800 Total Loss: 4.0289 Recon Loss: 4.0140 
[12/24 13:18:44 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000092 Step: 194850 Total Loss: 4.0286 Recon Loss: 4.0138 
[12/24 13:19:25 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8204 LR: 0.000092 Step: 194900 Total Loss: 4.0708 Recon Loss: 4.0560 
[12/24 13:20:06 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8175 LR: 0.000092 Step: 194950 Total Loss: 5.4580 Recon Loss: 5.4432 
[12/24 13:20:47 TiTok]: Data (t): 0.0011, 35.14/s/gpu Batch (t): 0.9106 LR: 0.000092 Step: 195000 Total Loss: 4.7005 Recon Loss: 4.6857 
[12/24 13:20:48 TiTok]: Reconstructing images...
[12/24 13:21:30 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000092 Step: 195050 Total Loss: 4.6980 Recon Loss: 4.6832 
[12/24 13:22:11 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000092 Step: 195100 Total Loss: 4.7257 Recon Loss: 4.7109 
[12/24 13:22:52 TiTok]: Data (t): 0.0016, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000092 Step: 195150 Total Loss: 4.1157 Recon Loss: 4.1009 
[12/24 13:23:32 TiTok]: Data (t): 0.0011, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000092 Step: 195200 Total Loss: 4.1148 Recon Loss: 4.1001 
[12/24 13:24:13 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000092 Step: 195250 Total Loss: 4.7250 Recon Loss: 4.7102 
[12/24 13:24:54 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8160 LR: 0.000092 Step: 195300 Total Loss: 4.0518 Recon Loss: 4.0370 
[12/24 13:25:35 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000092 Step: 195350 Total Loss: 4.0655 Recon Loss: 4.0508 
[12/24 13:26:16 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000092 Step: 195400 Total Loss: 4.0114 Recon Loss: 3.9966 
[12/24 13:26:57 TiTok]: Data (t): 0.0010, 38.77/s/gpu Batch (t): 0.8255 LR: 0.000092 Step: 195450 Total Loss: 4.7157 Recon Loss: 4.7010 
[12/24 13:27:38 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000092 Step: 195500 Total Loss: 3.9429 Recon Loss: 3.9280 
[12/24 13:28:19 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000092 Step: 195550 Total Loss: 5.3947 Recon Loss: 5.3799 
[12/24 13:29:00 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000092 Step: 195600 Total Loss: 4.0968 Recon Loss: 4.0820 
[12/24 13:29:41 TiTok]: Data (t): 0.0011, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000092 Step: 195650 Total Loss: 4.6832 Recon Loss: 4.6683 
[12/24 13:30:21 TiTok]: Data (t): 0.0011, 39.49/s/gpu Batch (t): 0.8104 LR: 0.000092 Step: 195700 Total Loss: 5.4729 Recon Loss: 5.4581 
[12/24 13:31:02 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000092 Step: 195750 Total Loss: 4.7650 Recon Loss: 4.7501 
[12/24 13:31:43 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000092 Step: 195800 Total Loss: 5.4447 Recon Loss: 5.4298 
[12/24 13:32:24 TiTok]: Data (t): 0.0011, 38.88/s/gpu Batch (t): 0.8231 LR: 0.000092 Step: 195850 Total Loss: 4.7313 Recon Loss: 4.7163 
[12/24 13:33:05 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8185 LR: 0.000092 Step: 195900 Total Loss: 4.8462 Recon Loss: 4.8314 
[12/24 13:33:46 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000092 Step: 195950 Total Loss: 4.7266 Recon Loss: 4.7119 
[12/24 13:34:27 TiTok]: Data (t): 0.0012, 33.88/s/gpu Batch (t): 0.9446 LR: 0.000092 Step: 196000 Total Loss: 5.4404 Recon Loss: 5.4256 
[12/24 13:35:08 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000092 Step: 196050 Total Loss: 4.7737 Recon Loss: 4.7589 
[12/24 13:35:49 TiTok]: Data (t): 0.0011, 39.49/s/gpu Batch (t): 0.8104 LR: 0.000092 Step: 196100 Total Loss: 4.0675 Recon Loss: 4.0526 
[12/24 13:36:30 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8154 LR: 0.000092 Step: 196150 Total Loss: 4.6819 Recon Loss: 4.6670 
[12/24 13:37:11 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8225 LR: 0.000092 Step: 196200 Total Loss: 4.7295 Recon Loss: 4.7147 
[12/24 13:37:51 TiTok]: Data (t): 0.0011, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000092 Step: 196250 Total Loss: 4.7742 Recon Loss: 4.7594 
[12/24 13:38:32 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000092 Step: 196300 Total Loss: 5.4132 Recon Loss: 5.3984 
[12/24 13:39:13 TiTok]: Data (t): 0.0010, 38.87/s/gpu Batch (t): 0.8233 LR: 0.000092 Step: 196350 Total Loss: 3.9801 Recon Loss: 3.9653 
[12/24 13:39:54 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8221 LR: 0.000092 Step: 196400 Total Loss: 4.7885 Recon Loss: 4.7737 
[12/24 13:40:35 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000092 Step: 196450 Total Loss: 4.0861 Recon Loss: 4.0713 
[12/24 13:41:16 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8221 LR: 0.000092 Step: 196500 Total Loss: 4.0145 Recon Loss: 3.9997 
[12/24 13:41:57 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000092 Step: 196550 Total Loss: 4.0128 Recon Loss: 3.9981 
[12/24 13:42:38 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000092 Step: 196600 Total Loss: 3.9646 Recon Loss: 3.9498 
[12/24 13:43:19 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000092 Step: 196650 Total Loss: 4.7690 Recon Loss: 4.7543 
[12/24 13:44:00 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000092 Step: 196700 Total Loss: 4.7820 Recon Loss: 4.7673 
[12/24 13:44:41 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000092 Step: 196750 Total Loss: 4.0299 Recon Loss: 4.0152 
[12/24 13:45:22 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000092 Step: 196800 Total Loss: 4.6853 Recon Loss: 4.6705 
[12/24 13:46:03 TiTok]: Data (t): 0.0017, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000092 Step: 196850 Total Loss: 5.4480 Recon Loss: 5.4332 
[12/24 13:46:43 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000092 Step: 196900 Total Loss: 5.4502 Recon Loss: 5.4354 
[12/24 13:47:24 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8204 LR: 0.000092 Step: 196950 Total Loss: 4.0295 Recon Loss: 4.0147 
[12/24 13:48:05 TiTok]: Data (t): 0.0011, 35.28/s/gpu Batch (t): 0.9069 LR: 0.000092 Step: 197000 Total Loss: 5.4371 Recon Loss: 5.4223 
[12/24 13:48:46 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000092 Step: 197050 Total Loss: 4.7887 Recon Loss: 4.7739 
[12/24 13:49:27 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000092 Step: 197100 Total Loss: 5.4335 Recon Loss: 5.4186 
[12/24 13:50:08 TiTok]: Data (t): 0.0016, 39.33/s/gpu Batch (t): 0.8137 LR: 0.000092 Step: 197150 Total Loss: 4.6886 Recon Loss: 4.6738 
[12/24 13:50:49 TiTok]: Data (t): 0.0011, 38.81/s/gpu Batch (t): 0.8246 LR: 0.000092 Step: 197200 Total Loss: 4.7551 Recon Loss: 4.7403 
[12/24 13:51:30 TiTok]: Data (t): 0.0011, 38.88/s/gpu Batch (t): 0.8231 LR: 0.000092 Step: 197250 Total Loss: 4.0812 Recon Loss: 4.0664 
[12/24 13:52:11 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000092 Step: 197300 Total Loss: 3.9493 Recon Loss: 3.9345 
[12/24 13:52:52 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000092 Step: 197350 Total Loss: 5.4129 Recon Loss: 5.3981 
[12/24 13:53:33 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000092 Step: 197400 Total Loss: 4.0929 Recon Loss: 4.0781 
[12/24 13:54:13 TiTok]: Data (t): 0.0011, 38.86/s/gpu Batch (t): 0.8235 LR: 0.000092 Step: 197450 Total Loss: 4.0580 Recon Loss: 4.0432 
[12/24 13:54:54 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000092 Step: 197500 Total Loss: 6.1405 Recon Loss: 6.1257 
[12/24 13:55:35 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8202 LR: 0.000092 Step: 197550 Total Loss: 4.0667 Recon Loss: 4.0519 
[12/24 13:56:16 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000092 Step: 197600 Total Loss: 4.6903 Recon Loss: 4.6755 
[12/24 13:56:57 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8196 LR: 0.000092 Step: 197650 Total Loss: 5.4480 Recon Loss: 5.4331 
[12/24 13:57:38 TiTok]: Data (t): 0.0011, 39.50/s/gpu Batch (t): 0.8101 LR: 0.000092 Step: 197700 Total Loss: 4.7407 Recon Loss: 4.7259 
[12/24 13:58:19 TiTok]: Data (t): 0.0012, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000092 Step: 197750 Total Loss: 4.0401 Recon Loss: 4.0253 
[12/24 13:59:00 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000092 Step: 197800 Total Loss: 4.7781 Recon Loss: 4.7633 
[12/24 13:59:40 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8223 LR: 0.000092 Step: 197850 Total Loss: 4.6754 Recon Loss: 4.6606 
[12/24 14:00:21 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000092 Step: 197900 Total Loss: 4.7379 Recon Loss: 4.7233 
[12/24 14:01:02 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000092 Step: 197950 Total Loss: 5.4104 Recon Loss: 5.3956 
[12/24 14:01:43 TiTok]: Data (t): 0.0011, 35.06/s/gpu Batch (t): 0.9128 LR: 0.000092 Step: 198000 Total Loss: 3.9898 Recon Loss: 3.9750 
[12/24 14:02:24 TiTok]: Data (t): 0.0011, 38.81/s/gpu Batch (t): 0.8244 LR: 0.000092 Step: 198050 Total Loss: 4.0379 Recon Loss: 4.0231 
[12/24 14:03:05 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000092 Step: 198100 Total Loss: 4.8176 Recon Loss: 4.8028 
[12/24 14:03:46 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8209 LR: 0.000092 Step: 198150 Total Loss: 4.0528 Recon Loss: 4.0379 
[12/24 14:04:27 TiTok]: Data (t): 0.0010, 38.84/s/gpu Batch (t): 0.8239 LR: 0.000092 Step: 198200 Total Loss: 4.0237 Recon Loss: 4.0089 
[12/24 14:05:08 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000092 Step: 198250 Total Loss: 4.7756 Recon Loss: 4.7609 
[12/24 14:05:49 TiTok]: Data (t): 0.0015, 39.35/s/gpu Batch (t): 0.8131 LR: 0.000092 Step: 198300 Total Loss: 5.4264 Recon Loss: 5.4116 
[12/24 14:06:30 TiTok]: Data (t): 0.0011, 39.58/s/gpu Batch (t): 0.8086 LR: 0.000092 Step: 198350 Total Loss: 5.4556 Recon Loss: 5.4407 
[12/24 14:07:11 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000092 Step: 198400 Total Loss: 4.0974 Recon Loss: 4.0826 
[12/24 14:07:51 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000092 Step: 198450 Total Loss: 3.9954 Recon Loss: 3.9805 
[12/24 14:08:32 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000092 Step: 198500 Total Loss: 4.7763 Recon Loss: 4.7615 
[12/24 14:09:13 TiTok]: Data (t): 0.0010, 38.79/s/gpu Batch (t): 0.8250 LR: 0.000092 Step: 198550 Total Loss: 6.8230 Recon Loss: 6.8083 
[12/24 14:09:54 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000092 Step: 198600 Total Loss: 5.4390 Recon Loss: 5.4241 
[12/24 14:10:35 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000092 Step: 198650 Total Loss: 4.0550 Recon Loss: 4.0402 
[12/24 14:11:16 TiTok]: Data (t): 0.0014, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000092 Step: 198700 Total Loss: 4.7715 Recon Loss: 4.7566 
[12/24 14:11:57 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000092 Step: 198750 Total Loss: 4.0084 Recon Loss: 3.9936 
[12/24 14:12:38 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8125 LR: 0.000092 Step: 198800 Total Loss: 5.4114 Recon Loss: 5.3967 
[12/24 14:13:19 TiTok]: Data (t): 0.0010, 38.87/s/gpu Batch (t): 0.8232 LR: 0.000092 Step: 198850 Total Loss: 4.0067 Recon Loss: 3.9919 
[12/24 14:14:00 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000092 Step: 198900 Total Loss: 5.4724 Recon Loss: 5.4577 
[12/24 14:14:41 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000092 Step: 198950 Total Loss: 4.0700 Recon Loss: 4.0552 
[12/24 14:15:22 TiTok]: Data (t): 0.0011, 35.29/s/gpu Batch (t): 0.9068 LR: 0.000092 Step: 199000 Total Loss: 4.7092 Recon Loss: 4.6944 
[12/24 14:16:02 TiTok]: Data (t): 0.0011, 38.68/s/gpu Batch (t): 0.8273 LR: 0.000092 Step: 199050 Total Loss: 4.0476 Recon Loss: 4.0329 
[12/24 14:16:43 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8131 LR: 0.000092 Step: 199100 Total Loss: 4.0000 Recon Loss: 3.9852 
[12/24 14:17:24 TiTok]: Data (t): 0.0011, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000092 Step: 199150 Total Loss: 5.4429 Recon Loss: 5.4281 
[12/24 14:18:05 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000092 Step: 199200 Total Loss: 4.7265 Recon Loss: 4.7117 
[12/24 14:18:46 TiTok]: Data (t): 0.0011, 38.88/s/gpu Batch (t): 0.8231 LR: 0.000092 Step: 199250 Total Loss: 4.7123 Recon Loss: 4.6974 
[12/24 14:19:27 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000092 Step: 199300 Total Loss: 4.7555 Recon Loss: 4.7407 
[12/24 14:20:08 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000092 Step: 199350 Total Loss: 4.7291 Recon Loss: 4.7144 
[12/24 14:20:49 TiTok]: Data (t): 0.0010, 38.82/s/gpu Batch (t): 0.8243 LR: 0.000092 Step: 199400 Total Loss: 4.1160 Recon Loss: 4.1012 
[12/24 14:21:30 TiTok]: Data (t): 0.0011, 38.51/s/gpu Batch (t): 0.8309 LR: 0.000092 Step: 199450 Total Loss: 5.4677 Recon Loss: 5.4529 
[12/24 14:22:11 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000092 Step: 199500 Total Loss: 4.7503 Recon Loss: 4.7354 
[12/24 14:22:51 TiTok]: Data (t): 0.0011, 38.83/s/gpu Batch (t): 0.8241 LR: 0.000092 Step: 199550 Total Loss: 4.7423 Recon Loss: 4.7276 
[12/24 14:23:32 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000092 Step: 199600 Total Loss: 6.1266 Recon Loss: 6.1117 
[12/24 14:24:13 TiTok]: Data (t): 0.0010, 39.39/s/gpu Batch (t): 0.8123 LR: 0.000092 Step: 199650 Total Loss: 4.0229 Recon Loss: 4.0082 
[12/24 14:24:54 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000092 Step: 199700 Total Loss: 4.7142 Recon Loss: 4.6994 
[12/24 14:25:35 TiTok]: Data (t): 0.0012, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000092 Step: 199750 Total Loss: 4.6409 Recon Loss: 4.6260 
[12/24 14:26:16 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8200 LR: 0.000092 Step: 199800 Total Loss: 6.1065 Recon Loss: 6.0917 
[12/24 14:26:57 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000092 Step: 199850 Total Loss: 5.4318 Recon Loss: 5.4171 
[12/24 14:27:38 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000092 Step: 199900 Total Loss: 4.0658 Recon Loss: 4.0510 
[12/24 14:28:19 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8127 LR: 0.000092 Step: 199950 Total Loss: 4.7398 Recon Loss: 4.7251 
[12/24 14:29:00 TiTok]: Data (t): 0.0033, 35.50/s/gpu Batch (t): 0.9015 LR: 0.000092 Step: 200000 Total Loss: 4.7159 Recon Loss: 4.7011 
Model weights saved in titok_b64_stage1_run1/checkpoint-200000/unwrapped_model/pytorch_model.bin
[12/24 14:29:01 TiTok]: Saved state to titok_b64_stage1_run1/checkpoint-200000
Model weights saved in titok_b64_stage1_run1/checkpoint-200000/ema_model/pytorch_model.bin
[12/24 14:29:18 TiTok]: Reconstructing images...
[12/24 14:29:18 TiTok]: Computing metrics on the validation set.
[12/24 14:43:53 TiTok]: EMA EVALUATION Step: 200000 
[12/24 14:43:53 TiTok]: {'CodebookEntropy': tensor(11.6434, device='cuda:0', dtype=torch.float64),
 'CodebookUsage': 0.784423828125,
 'InceptionScore': 43.5508734491121,
 'rFID': 63.20302054520175}
[12/24 14:45:04 TiTok]: Data (t): 0.0010, 39.66/s/gpu Batch (t): 0.8069 LR: 0.000092 Step: 200050 Total Loss: 4.0379 Recon Loss: 4.0231 
[12/24 14:45:44 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000092 Step: 200100 Total Loss: 4.7345 Recon Loss: 4.7197 
[12/24 14:46:25 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000092 Step: 200150 Total Loss: 4.7250 Recon Loss: 4.7102 
[12/24 14:47:06 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000092 Step: 200200 Total Loss: 4.0157 Recon Loss: 4.0009 
[12/24 14:47:46 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000092 Step: 200250 Total Loss: 4.0259 Recon Loss: 4.0111 
[12/24 14:48:27 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8148 LR: 0.000092 Step: 200300 Total Loss: 4.7245 Recon Loss: 4.7098 
[12/24 14:49:08 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000092 Step: 200350 Total Loss: 3.9610 Recon Loss: 3.9462 
[12/24 14:49:49 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000092 Step: 200400 Total Loss: 4.7400 Recon Loss: 4.7252 
Epoch 20/99 started.
[12/24 14:50:31 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000092 Step: 200450 Total Loss: 3.9831 Recon Loss: 3.9684 
[12/24 14:51:12 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000092 Step: 200500 Total Loss: 5.4455 Recon Loss: 5.4307 
[12/24 14:51:53 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000092 Step: 200550 Total Loss: 5.4305 Recon Loss: 5.4157 
[12/24 14:52:33 TiTok]: Data (t): 0.0012, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000092 Step: 200600 Total Loss: 4.7259 Recon Loss: 4.7111 
[12/24 14:53:15 TiTok]: Data (t): 0.0011, 38.98/s/gpu Batch (t): 0.8209 LR: 0.000092 Step: 200650 Total Loss: 4.0194 Recon Loss: 4.0046 
[12/24 14:53:55 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000092 Step: 200700 Total Loss: 5.4220 Recon Loss: 5.4072 
[12/24 14:54:36 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8225 LR: 0.000092 Step: 200750 Total Loss: 4.0761 Recon Loss: 4.0612 
[12/24 14:55:17 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000092 Step: 200800 Total Loss: 6.1262 Recon Loss: 6.1114 
[12/24 14:55:58 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8137 LR: 0.000092 Step: 200850 Total Loss: 4.7052 Recon Loss: 4.6905 
[12/24 14:56:39 TiTok]: Data (t): 0.0011, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000092 Step: 200900 Total Loss: 4.0218 Recon Loss: 4.0069 
[12/24 14:57:20 TiTok]: Data (t): 0.0010, 39.46/s/gpu Batch (t): 0.8110 LR: 0.000092 Step: 200950 Total Loss: 5.4234 Recon Loss: 5.4085 
[12/24 14:58:01 TiTok]: Data (t): 0.0010, 35.11/s/gpu Batch (t): 0.9114 LR: 0.000092 Step: 201000 Total Loss: 4.8249 Recon Loss: 4.8101 
[12/24 14:58:42 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000092 Step: 201050 Total Loss: 4.7277 Recon Loss: 4.7129 
[12/24 14:59:23 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000092 Step: 201100 Total Loss: 4.0655 Recon Loss: 4.0507 
[12/24 15:00:04 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000092 Step: 201150 Total Loss: 5.4481 Recon Loss: 5.4333 
[12/24 15:00:45 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000092 Step: 201200 Total Loss: 4.7158 Recon Loss: 4.7011 
[12/24 15:01:26 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000092 Step: 201250 Total Loss: 5.4469 Recon Loss: 5.4321 
[12/24 15:02:06 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000092 Step: 201300 Total Loss: 5.4069 Recon Loss: 5.3920 
[12/24 15:02:47 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000092 Step: 201350 Total Loss: 5.4610 Recon Loss: 5.4461 
[12/24 15:03:28 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000092 Step: 201400 Total Loss: 4.7534 Recon Loss: 4.7385 
[12/24 15:04:09 TiTok]: Data (t): 0.0011, 38.83/s/gpu Batch (t): 0.8240 LR: 0.000092 Step: 201450 Total Loss: 4.7079 Recon Loss: 4.6932 
[12/24 15:04:50 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000092 Step: 201500 Total Loss: 4.7095 Recon Loss: 4.6947 
[12/24 15:05:31 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000092 Step: 201550 Total Loss: 4.6829 Recon Loss: 4.6681 
[12/24 15:06:12 TiTok]: Data (t): 0.0012, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000092 Step: 201600 Total Loss: 3.9942 Recon Loss: 3.9795 
[12/24 15:06:53 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8158 LR: 0.000092 Step: 201650 Total Loss: 4.0076 Recon Loss: 3.9928 
[12/24 15:07:34 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000092 Step: 201700 Total Loss: 5.4547 Recon Loss: 5.4399 
[12/24 15:08:15 TiTok]: Data (t): 0.0010, 39.45/s/gpu Batch (t): 0.8112 LR: 0.000092 Step: 201750 Total Loss: 5.4876 Recon Loss: 5.4728 
[12/24 15:08:55 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000092 Step: 201800 Total Loss: 4.7546 Recon Loss: 4.7398 
[12/24 15:09:36 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8200 LR: 0.000092 Step: 201850 Total Loss: 4.7810 Recon Loss: 4.7663 
[12/24 15:10:17 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000092 Step: 201900 Total Loss: 3.9766 Recon Loss: 3.9618 
[12/24 15:10:58 TiTok]: Data (t): 0.0010, 38.79/s/gpu Batch (t): 0.8250 LR: 0.000092 Step: 201950 Total Loss: 3.9687 Recon Loss: 3.9538 
[12/24 15:11:39 TiTok]: Data (t): 0.0011, 34.87/s/gpu Batch (t): 0.9178 LR: 0.000092 Step: 202000 Total Loss: 4.7768 Recon Loss: 4.7620 
[12/24 15:12:20 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000092 Step: 202050 Total Loss: 4.0860 Recon Loss: 4.0712 
[12/24 15:13:01 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000092 Step: 202100 Total Loss: 4.0620 Recon Loss: 4.0472 
[12/24 15:13:42 TiTok]: Data (t): 0.0012, 39.34/s/gpu Batch (t): 0.8133 LR: 0.000092 Step: 202150 Total Loss: 4.7665 Recon Loss: 4.7516 
[12/24 15:14:23 TiTok]: Data (t): 0.0010, 38.87/s/gpu Batch (t): 0.8233 LR: 0.000092 Step: 202200 Total Loss: 4.0355 Recon Loss: 4.0207 
[12/24 15:15:04 TiTok]: Data (t): 0.0015, 40.00/s/gpu Batch (t): 0.8000 LR: 0.000092 Step: 202250 Total Loss: 4.7476 Recon Loss: 4.7329 
[12/24 15:15:45 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000092 Step: 202300 Total Loss: 4.0212 Recon Loss: 4.0064 
[12/24 15:16:26 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8200 LR: 0.000092 Step: 202350 Total Loss: 4.0422 Recon Loss: 4.0273 
[12/24 15:17:07 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8204 LR: 0.000092 Step: 202400 Total Loss: 4.7325 Recon Loss: 4.7177 
[12/24 15:17:47 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8185 LR: 0.000092 Step: 202450 Total Loss: 4.0521 Recon Loss: 4.0373 
[12/24 15:18:28 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000092 Step: 202500 Total Loss: 4.7428 Recon Loss: 4.7280 
[12/24 15:19:09 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8234 LR: 0.000092 Step: 202550 Total Loss: 4.0329 Recon Loss: 4.0181 
[12/24 15:19:50 TiTok]: Data (t): 0.0011, 38.82/s/gpu Batch (t): 0.8243 LR: 0.000092 Step: 202600 Total Loss: 5.3986 Recon Loss: 5.3838 
[12/24 15:20:31 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000092 Step: 202650 Total Loss: 4.7100 Recon Loss: 4.6952 
[12/24 15:21:12 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000092 Step: 202700 Total Loss: 4.7083 Recon Loss: 4.6935 
[12/24 15:21:53 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000092 Step: 202750 Total Loss: 5.4324 Recon Loss: 5.4176 
[12/24 15:22:34 TiTok]: Data (t): 0.0011, 38.98/s/gpu Batch (t): 0.8208 LR: 0.000092 Step: 202800 Total Loss: 4.1460 Recon Loss: 4.1312 
[12/24 15:23:15 TiTok]: Data (t): 0.0011, 39.35/s/gpu Batch (t): 0.8133 LR: 0.000092 Step: 202850 Total Loss: 5.4132 Recon Loss: 5.3983 
[12/24 15:23:56 TiTok]: Data (t): 0.0011, 38.87/s/gpu Batch (t): 0.8232 LR: 0.000092 Step: 202900 Total Loss: 4.7744 Recon Loss: 4.7596 
[12/24 15:24:37 TiTok]: Data (t): 0.0011, 39.46/s/gpu Batch (t): 0.8110 LR: 0.000092 Step: 202950 Total Loss: 4.7337 Recon Loss: 4.7190 
[12/24 15:25:18 TiTok]: Data (t): 0.0011, 35.16/s/gpu Batch (t): 0.9101 LR: 0.000092 Step: 203000 Total Loss: 4.0822 Recon Loss: 4.0674 
[12/24 15:25:58 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000092 Step: 203050 Total Loss: 5.4419 Recon Loss: 5.4272 
[12/24 15:26:39 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8234 LR: 0.000092 Step: 203100 Total Loss: 4.0489 Recon Loss: 4.0341 
[12/24 15:27:20 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000092 Step: 203150 Total Loss: 6.1296 Recon Loss: 6.1147 
[12/24 15:28:01 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000092 Step: 203200 Total Loss: 5.4855 Recon Loss: 5.4707 
[12/24 15:28:42 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000092 Step: 203250 Total Loss: 4.0193 Recon Loss: 4.0044 
[12/24 15:29:23 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000092 Step: 203300 Total Loss: 6.1344 Recon Loss: 6.1195 
[12/24 15:30:04 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000092 Step: 203350 Total Loss: 5.4254 Recon Loss: 5.4106 
[12/24 15:30:45 TiTok]: Data (t): 0.0022, 40.17/s/gpu Batch (t): 0.7967 LR: 0.000092 Step: 203400 Total Loss: 4.7668 Recon Loss: 4.7520 
[12/24 15:31:26 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000092 Step: 203450 Total Loss: 5.4718 Recon Loss: 5.4570 
[12/24 15:32:07 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8154 LR: 0.000092 Step: 203500 Total Loss: 5.4427 Recon Loss: 5.4279 
[12/24 15:32:47 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8179 LR: 0.000092 Step: 203550 Total Loss: 4.6885 Recon Loss: 4.6737 
[12/24 15:33:28 TiTok]: Data (t): 0.0010, 38.83/s/gpu Batch (t): 0.8242 LR: 0.000092 Step: 203600 Total Loss: 4.0708 Recon Loss: 4.0560 
[12/24 15:34:09 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8177 LR: 0.000092 Step: 203650 Total Loss: 4.7423 Recon Loss: 4.7275 
[12/24 15:34:50 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000092 Step: 203700 Total Loss: 4.0357 Recon Loss: 4.0209 
[12/24 15:35:31 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8185 LR: 0.000092 Step: 203750 Total Loss: 4.7450 Recon Loss: 4.7302 
[12/24 15:36:12 TiTok]: Data (t): 0.0009, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000092 Step: 203800 Total Loss: 5.4752 Recon Loss: 5.4604 
[12/24 15:36:53 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000092 Step: 203850 Total Loss: 4.7578 Recon Loss: 4.7429 
[12/24 15:37:34 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000092 Step: 203900 Total Loss: 4.7911 Recon Loss: 4.7763 
[12/24 15:38:15 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8229 LR: 0.000092 Step: 203950 Total Loss: 4.0757 Recon Loss: 4.0608 
[12/24 15:38:56 TiTok]: Data (t): 0.0010, 35.09/s/gpu Batch (t): 0.9120 LR: 0.000092 Step: 204000 Total Loss: 4.7595 Recon Loss: 4.7446 
[12/24 15:39:37 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8152 LR: 0.000092 Step: 204050 Total Loss: 4.7450 Recon Loss: 4.7301 
[12/24 15:40:17 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000092 Step: 204100 Total Loss: 5.4784 Recon Loss: 5.4636 
[12/24 15:40:58 TiTok]: Data (t): 0.0012, 39.24/s/gpu Batch (t): 0.8156 LR: 0.000092 Step: 204150 Total Loss: 5.4489 Recon Loss: 5.4340 
[12/24 15:41:39 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8200 LR: 0.000092 Step: 204200 Total Loss: 4.7675 Recon Loss: 4.7527 
[12/24 15:42:20 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000092 Step: 204250 Total Loss: 5.4328 Recon Loss: 5.4180 
[12/24 15:43:01 TiTok]: Data (t): 0.0010, 38.81/s/gpu Batch (t): 0.8246 LR: 0.000092 Step: 204300 Total Loss: 5.4217 Recon Loss: 5.4068 
[12/24 15:43:42 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000092 Step: 204350 Total Loss: 4.0026 Recon Loss: 3.9878 
[12/24 15:44:23 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000092 Step: 204400 Total Loss: 4.0843 Recon Loss: 4.0695 
[12/24 15:45:04 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000092 Step: 204450 Total Loss: 4.7869 Recon Loss: 4.7721 
[12/24 15:45:45 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000092 Step: 204500 Total Loss: 4.0434 Recon Loss: 4.0286 
[12/24 15:46:26 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000092 Step: 204550 Total Loss: 4.7691 Recon Loss: 4.7543 
[12/24 15:47:06 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000092 Step: 204600 Total Loss: 4.1086 Recon Loss: 4.0938 
[12/24 15:47:47 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8158 LR: 0.000092 Step: 204650 Total Loss: 3.9691 Recon Loss: 3.9543 
[12/24 15:48:28 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000092 Step: 204700 Total Loss: 4.0249 Recon Loss: 4.0101 
[12/24 15:49:09 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000092 Step: 204750 Total Loss: 4.7257 Recon Loss: 4.7109 
[12/24 15:49:50 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000092 Step: 204800 Total Loss: 4.6307 Recon Loss: 4.6158 
[12/24 15:50:31 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000092 Step: 204850 Total Loss: 4.7749 Recon Loss: 4.7601 
[12/24 15:51:12 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000092 Step: 204900 Total Loss: 5.4014 Recon Loss: 5.3865 
[12/24 15:51:53 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000092 Step: 204950 Total Loss: 5.4120 Recon Loss: 5.3972 
[12/24 15:52:34 TiTok]: Data (t): 0.0011, 34.06/s/gpu Batch (t): 0.9395 LR: 0.000092 Step: 205000 Total Loss: 5.4818 Recon Loss: 5.4670 
[12/24 15:52:35 TiTok]: Reconstructing images...
[12/24 15:53:16 TiTok]: Data (t): 0.0011, 39.41/s/gpu Batch (t): 0.8121 LR: 0.000092 Step: 205050 Total Loss: 4.7249 Recon Loss: 4.7101 
[12/24 15:53:57 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000092 Step: 205100 Total Loss: 4.0001 Recon Loss: 3.9852 
[12/24 15:54:38 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000092 Step: 205150 Total Loss: 4.0020 Recon Loss: 3.9872 
[12/24 15:55:19 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000092 Step: 205200 Total Loss: 6.1143 Recon Loss: 6.0995 
[12/24 15:56:00 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000092 Step: 205250 Total Loss: 4.6946 Recon Loss: 4.6797 
[12/24 15:56:41 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8196 LR: 0.000092 Step: 205300 Total Loss: 4.0621 Recon Loss: 4.0472 
[12/24 15:57:22 TiTok]: Data (t): 0.0011, 38.83/s/gpu Batch (t): 0.8241 LR: 0.000092 Step: 205350 Total Loss: 3.9722 Recon Loss: 3.9573 
[12/24 15:58:03 TiTok]: Data (t): 0.0011, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000092 Step: 205400 Total Loss: 5.4627 Recon Loss: 5.4479 
[12/24 15:58:44 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000092 Step: 205450 Total Loss: 5.4193 Recon Loss: 5.4045 
[12/24 15:59:25 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000092 Step: 205500 Total Loss: 6.1279 Recon Loss: 6.1131 
[12/24 16:00:05 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000092 Step: 205550 Total Loss: 4.7492 Recon Loss: 4.7345 
[12/24 16:00:46 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8175 LR: 0.000092 Step: 205600 Total Loss: 4.7087 Recon Loss: 4.6939 
[12/24 16:01:27 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000092 Step: 205650 Total Loss: 4.6968 Recon Loss: 4.6820 
[12/24 16:02:08 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8150 LR: 0.000092 Step: 205700 Total Loss: 4.7388 Recon Loss: 4.7240 
[12/24 16:02:49 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000092 Step: 205750 Total Loss: 4.0497 Recon Loss: 4.0349 
[12/24 16:03:30 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8223 LR: 0.000092 Step: 205800 Total Loss: 5.4713 Recon Loss: 5.4565 
[12/24 16:04:11 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8177 LR: 0.000092 Step: 205850 Total Loss: 4.0205 Recon Loss: 4.0058 
[12/24 16:04:52 TiTok]: Data (t): 0.0010, 39.40/s/gpu Batch (t): 0.8122 LR: 0.000092 Step: 205900 Total Loss: 5.3385 Recon Loss: 5.3238 
[12/24 16:05:33 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000092 Step: 205950 Total Loss: 4.0220 Recon Loss: 4.0072 
[12/24 16:06:14 TiTok]: Data (t): 0.0011, 34.26/s/gpu Batch (t): 0.9339 LR: 0.000092 Step: 206000 Total Loss: 4.7473 Recon Loss: 4.7324 
[12/24 16:06:54 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000092 Step: 206050 Total Loss: 4.7667 Recon Loss: 4.7519 
[12/24 16:07:35 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000092 Step: 206100 Total Loss: 4.7197 Recon Loss: 4.7049 
[12/24 16:08:16 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000092 Step: 206150 Total Loss: 4.6815 Recon Loss: 4.6667 
[12/24 16:08:57 TiTok]: Data (t): 0.0011, 38.76/s/gpu Batch (t): 0.8256 LR: 0.000092 Step: 206200 Total Loss: 5.4180 Recon Loss: 5.4031 
[12/24 16:09:38 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000092 Step: 206250 Total Loss: 6.0985 Recon Loss: 6.0837 
[12/24 16:10:19 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000092 Step: 206300 Total Loss: 4.0785 Recon Loss: 4.0636 
[12/24 16:11:00 TiTok]: Data (t): 0.0033, 38.85/s/gpu Batch (t): 0.8237 LR: 0.000092 Step: 206350 Total Loss: 4.0014 Recon Loss: 3.9866 
[12/24 16:11:41 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8202 LR: 0.000092 Step: 206400 Total Loss: 4.7779 Recon Loss: 4.7631 
[12/24 16:12:22 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000092 Step: 206450 Total Loss: 5.4708 Recon Loss: 5.4560 
[12/24 16:13:02 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000092 Step: 206500 Total Loss: 5.4352 Recon Loss: 5.4205 
[12/24 16:13:43 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000092 Step: 206550 Total Loss: 4.0864 Recon Loss: 4.0717 
[12/24 16:14:24 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000092 Step: 206600 Total Loss: 4.0125 Recon Loss: 3.9977 
[12/24 16:15:05 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000092 Step: 206650 Total Loss: 4.7560 Recon Loss: 4.7412 
[12/24 16:15:46 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8183 LR: 0.000092 Step: 206700 Total Loss: 3.9868 Recon Loss: 3.9720 
[12/24 16:16:27 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000092 Step: 206750 Total Loss: 4.7231 Recon Loss: 4.7082 
[12/24 16:17:08 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000092 Step: 206800 Total Loss: 4.6634 Recon Loss: 4.6486 
[12/24 16:17:49 TiTok]: Data (t): 0.0011, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000092 Step: 206850 Total Loss: 4.7289 Recon Loss: 4.7141 
[12/24 16:18:30 TiTok]: Data (t): 0.0011, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000092 Step: 206900 Total Loss: 4.0769 Recon Loss: 4.0621 
[12/24 16:19:11 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8202 LR: 0.000092 Step: 206950 Total Loss: 4.0920 Recon Loss: 4.0771 
[12/24 16:19:51 TiTok]: Data (t): 0.0010, 35.01/s/gpu Batch (t): 0.9140 LR: 0.000091 Step: 207000 Total Loss: 4.7346 Recon Loss: 4.7197 
[12/24 16:20:32 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000091 Step: 207050 Total Loss: 4.7324 Recon Loss: 4.7175 
[12/24 16:21:13 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000091 Step: 207100 Total Loss: 4.0233 Recon Loss: 4.0086 
[12/24 16:21:54 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000091 Step: 207150 Total Loss: 5.4663 Recon Loss: 5.4514 
[12/24 16:22:35 TiTok]: Data (t): 0.0010, 38.23/s/gpu Batch (t): 0.8371 LR: 0.000091 Step: 207200 Total Loss: 4.7985 Recon Loss: 4.7836 
[12/24 16:23:16 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000091 Step: 207250 Total Loss: 4.0387 Recon Loss: 4.0240 
[12/24 16:23:57 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8221 LR: 0.000091 Step: 207300 Total Loss: 6.1279 Recon Loss: 6.1131 
[12/24 16:24:38 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8185 LR: 0.000091 Step: 207350 Total Loss: 4.0887 Recon Loss: 4.0739 
[12/24 16:25:19 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000091 Step: 207400 Total Loss: 4.0499 Recon Loss: 4.0351 
[12/24 16:25:59 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000091 Step: 207450 Total Loss: 5.4007 Recon Loss: 5.3858 
[12/24 16:26:40 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000091 Step: 207500 Total Loss: 4.7177 Recon Loss: 4.7029 
[12/24 16:27:21 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000091 Step: 207550 Total Loss: 4.0722 Recon Loss: 4.0574 
[12/24 16:28:02 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000091 Step: 207600 Total Loss: 4.0714 Recon Loss: 4.0566 
[12/24 16:28:43 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000091 Step: 207650 Total Loss: 5.3935 Recon Loss: 5.3787 
[12/24 16:29:24 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000091 Step: 207700 Total Loss: 4.8358 Recon Loss: 4.8210 
[12/24 16:30:05 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000091 Step: 207750 Total Loss: 4.0248 Recon Loss: 4.0100 
[12/24 16:30:46 TiTok]: Data (t): 0.0011, 39.45/s/gpu Batch (t): 0.8111 LR: 0.000091 Step: 207800 Total Loss: 6.1282 Recon Loss: 6.1134 
[12/24 16:31:27 TiTok]: Data (t): 0.0009, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000091 Step: 207850 Total Loss: 4.6793 Recon Loss: 4.6645 
[12/24 16:32:07 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8181 LR: 0.000091 Step: 207900 Total Loss: 4.7312 Recon Loss: 4.7164 
[12/24 16:32:48 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000091 Step: 207950 Total Loss: 4.0165 Recon Loss: 4.0017 
[12/24 16:33:29 TiTok]: Data (t): 0.0010, 34.81/s/gpu Batch (t): 0.9194 LR: 0.000091 Step: 208000 Total Loss: 4.0127 Recon Loss: 3.9979 
[12/24 16:34:10 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000091 Step: 208050 Total Loss: 4.6782 Recon Loss: 4.6635 
[12/24 16:34:51 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000091 Step: 208100 Total Loss: 4.7819 Recon Loss: 4.7671 
[12/24 16:35:32 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8202 LR: 0.000091 Step: 208150 Total Loss: 3.9998 Recon Loss: 3.9849 
[12/24 16:36:13 TiTok]: Data (t): 0.0011, 39.33/s/gpu Batch (t): 0.8137 LR: 0.000091 Step: 208200 Total Loss: 4.0616 Recon Loss: 4.0467 
[12/24 16:36:54 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000091 Step: 208250 Total Loss: 5.4265 Recon Loss: 5.4117 
[12/24 16:37:35 TiTok]: Data (t): 0.0010, 39.40/s/gpu Batch (t): 0.8122 LR: 0.000091 Step: 208300 Total Loss: 6.8347 Recon Loss: 6.8199 
[12/24 16:38:16 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8236 LR: 0.000091 Step: 208350 Total Loss: 5.3929 Recon Loss: 5.3781 
[12/24 16:38:57 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8150 LR: 0.000091 Step: 208400 Total Loss: 4.0917 Recon Loss: 4.0769 
[12/24 16:39:37 TiTok]: Data (t): 0.0013, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000091 Step: 208450 Total Loss: 4.7945 Recon Loss: 4.7796 
[12/24 16:40:18 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000091 Step: 208500 Total Loss: 4.7063 Recon Loss: 4.6915 
[12/24 16:40:59 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000091 Step: 208550 Total Loss: 4.7652 Recon Loss: 4.7503 
[12/24 16:41:40 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000091 Step: 208600 Total Loss: 4.6991 Recon Loss: 4.6844 
[12/24 16:42:21 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8127 LR: 0.000091 Step: 208650 Total Loss: 4.0579 Recon Loss: 4.0430 
[12/24 16:43:02 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000091 Step: 208700 Total Loss: 4.6964 Recon Loss: 4.6815 
[12/24 16:43:43 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000091 Step: 208750 Total Loss: 4.0067 Recon Loss: 3.9918 
[12/24 16:44:24 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000091 Step: 208800 Total Loss: 4.7457 Recon Loss: 4.7310 
[12/24 16:45:05 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000091 Step: 208850 Total Loss: 4.0586 Recon Loss: 4.0439 
[12/24 16:45:46 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000091 Step: 208900 Total Loss: 4.0429 Recon Loss: 4.0281 
[12/24 16:46:27 TiTok]: Data (t): 0.0011, 38.90/s/gpu Batch (t): 0.8225 LR: 0.000091 Step: 208950 Total Loss: 3.9992 Recon Loss: 3.9844 
[12/24 16:47:08 TiTok]: Data (t): 0.0011, 34.84/s/gpu Batch (t): 0.9184 LR: 0.000091 Step: 209000 Total Loss: 4.0279 Recon Loss: 4.0131 
[12/24 16:47:49 TiTok]: Data (t): 0.0010, 38.85/s/gpu Batch (t): 0.8237 LR: 0.000091 Step: 209050 Total Loss: 4.8119 Recon Loss: 4.7972 
[12/24 16:48:30 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000091 Step: 209100 Total Loss: 4.7441 Recon Loss: 4.7293 
[12/24 16:49:10 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000091 Step: 209150 Total Loss: 4.0412 Recon Loss: 4.0264 
[12/24 16:49:51 TiTok]: Data (t): 0.0009, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000091 Step: 209200 Total Loss: 4.6945 Recon Loss: 4.6797 
[12/24 16:50:32 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000091 Step: 209250 Total Loss: 4.0484 Recon Loss: 4.0335 
[12/24 16:51:13 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8219 LR: 0.000091 Step: 209300 Total Loss: 4.7294 Recon Loss: 4.7146 
[12/24 16:51:54 TiTok]: Data (t): 0.0021, 39.69/s/gpu Batch (t): 0.8063 LR: 0.000091 Step: 209350 Total Loss: 5.4160 Recon Loss: 5.4012 
[12/24 16:52:35 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000091 Step: 209400 Total Loss: 4.6732 Recon Loss: 4.6584 
[12/24 16:53:16 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8166 LR: 0.000091 Step: 209450 Total Loss: 4.0371 Recon Loss: 4.0222 
[12/24 16:53:57 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000091 Step: 209500 Total Loss: 5.4325 Recon Loss: 5.4177 
[12/24 16:54:38 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8221 LR: 0.000091 Step: 209550 Total Loss: 4.7663 Recon Loss: 4.7515 
[12/24 16:55:18 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000091 Step: 209600 Total Loss: 5.3900 Recon Loss: 5.3752 
[12/24 16:55:59 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000091 Step: 209650 Total Loss: 4.0782 Recon Loss: 4.0634 
[12/24 16:56:40 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000091 Step: 209700 Total Loss: 4.7424 Recon Loss: 4.7276 
[12/24 16:57:21 TiTok]: Data (t): 0.0011, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000091 Step: 209750 Total Loss: 5.4228 Recon Loss: 5.4079 
[12/24 16:58:02 TiTok]: Data (t): 0.0011, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000091 Step: 209800 Total Loss: 4.0482 Recon Loss: 4.0334 
[12/24 16:58:43 TiTok]: Data (t): 0.0011, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000091 Step: 209850 Total Loss: 5.4289 Recon Loss: 5.4141 
[12/24 16:59:24 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000091 Step: 209900 Total Loss: 4.0710 Recon Loss: 4.0561 
[12/24 17:00:05 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000091 Step: 209950 Total Loss: 3.9921 Recon Loss: 3.9773 
[12/24 17:00:46 TiTok]: Data (t): 0.0011, 35.44/s/gpu Batch (t): 0.9029 LR: 0.000091 Step: 210000 Total Loss: 4.6832 Recon Loss: 4.6683 
[12/24 17:00:47 TiTok]: Reconstructing images...
[12/24 17:01:28 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000091 Step: 210050 Total Loss: 4.7582 Recon Loss: 4.7434 
[12/24 17:02:09 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000091 Step: 210100 Total Loss: 5.4152 Recon Loss: 5.4004 
[12/24 17:02:50 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8158 LR: 0.000091 Step: 210150 Total Loss: 4.6729 Recon Loss: 4.6580 
[12/24 17:03:31 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000091 Step: 210200 Total Loss: 3.9930 Recon Loss: 3.9781 
[12/24 17:04:12 TiTok]: Data (t): 0.0011, 38.89/s/gpu Batch (t): 0.8228 LR: 0.000091 Step: 210250 Total Loss: 4.7033 Recon Loss: 4.6885 
[12/24 17:04:53 TiTok]: Data (t): 0.0011, 39.36/s/gpu Batch (t): 0.8129 LR: 0.000091 Step: 210300 Total Loss: 4.7771 Recon Loss: 4.7622 
[12/24 17:05:33 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000091 Step: 210350 Total Loss: 4.6375 Recon Loss: 4.6227 
[12/24 17:06:14 TiTok]: Data (t): 0.0011, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000091 Step: 210400 Total Loss: 4.0857 Recon Loss: 4.0710 
Epoch 21/99 started.
[12/24 17:06:56 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000091 Step: 210450 Total Loss: 4.0247 Recon Loss: 4.0099 
[12/24 17:07:37 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000091 Step: 210500 Total Loss: 5.4698 Recon Loss: 5.4549 
[12/24 17:08:18 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8148 LR: 0.000091 Step: 210550 Total Loss: 5.4248 Recon Loss: 5.4100 
[12/24 17:08:59 TiTok]: Data (t): 0.0011, 38.87/s/gpu Batch (t): 0.8232 LR: 0.000091 Step: 210600 Total Loss: 4.7041 Recon Loss: 4.6893 
[12/24 17:09:40 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000091 Step: 210650 Total Loss: 4.7445 Recon Loss: 4.7296 
[12/24 17:10:21 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000091 Step: 210700 Total Loss: 4.6608 Recon Loss: 4.6460 
[12/24 17:11:02 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000091 Step: 210750 Total Loss: 4.0425 Recon Loss: 4.0277 
[12/24 17:11:43 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000091 Step: 210800 Total Loss: 4.7517 Recon Loss: 4.7368 
[12/24 17:12:24 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000091 Step: 210850 Total Loss: 5.4620 Recon Loss: 5.4473 
[12/24 17:13:04 TiTok]: Data (t): 0.0032, 38.43/s/gpu Batch (t): 0.8326 LR: 0.000091 Step: 210900 Total Loss: 4.7774 Recon Loss: 4.7625 
[12/24 17:13:45 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000091 Step: 210950 Total Loss: 4.7031 Recon Loss: 4.6884 
[12/24 17:14:26 TiTok]: Data (t): 0.0011, 34.04/s/gpu Batch (t): 0.9401 LR: 0.000091 Step: 211000 Total Loss: 3.9563 Recon Loss: 3.9415 
[12/24 17:15:07 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000091 Step: 211050 Total Loss: 3.9798 Recon Loss: 3.9651 
[12/24 17:15:48 TiTok]: Data (t): 0.0011, 39.39/s/gpu Batch (t): 0.8125 LR: 0.000091 Step: 211100 Total Loss: 3.9951 Recon Loss: 3.9803 
[12/24 17:16:29 TiTok]: Data (t): 0.0010, 39.56/s/gpu Batch (t): 0.8089 LR: 0.000091 Step: 211150 Total Loss: 5.3898 Recon Loss: 5.3750 
[12/24 17:17:10 TiTok]: Data (t): 0.0010, 39.49/s/gpu Batch (t): 0.8103 LR: 0.000091 Step: 211200 Total Loss: 4.8066 Recon Loss: 4.7918 
[12/24 17:17:51 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000091 Step: 211250 Total Loss: 4.6823 Recon Loss: 4.6675 
[12/24 17:18:32 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000091 Step: 211300 Total Loss: 5.4401 Recon Loss: 5.4253 
[12/24 17:19:13 TiTok]: Data (t): 0.0012, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000091 Step: 211350 Total Loss: 5.4176 Recon Loss: 5.4028 
[12/24 17:19:53 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000091 Step: 211400 Total Loss: 4.0227 Recon Loss: 4.0079 
[12/24 17:20:34 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8196 LR: 0.000091 Step: 211450 Total Loss: 4.0238 Recon Loss: 4.0090 
[12/24 17:21:15 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000091 Step: 211500 Total Loss: 4.7100 Recon Loss: 4.6952 
[12/24 17:21:56 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000091 Step: 211550 Total Loss: 5.4544 Recon Loss: 5.4396 
[12/24 17:22:37 TiTok]: Data (t): 0.0012, 39.14/s/gpu Batch (t): 0.8177 LR: 0.000091 Step: 211600 Total Loss: 4.0012 Recon Loss: 3.9864 
[12/24 17:23:18 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000091 Step: 211650 Total Loss: 4.7193 Recon Loss: 4.7044 
[12/24 17:23:59 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000091 Step: 211700 Total Loss: 4.7687 Recon Loss: 4.7538 
[12/24 17:24:40 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000091 Step: 211750 Total Loss: 5.4060 Recon Loss: 5.3912 
[12/24 17:25:21 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000091 Step: 211800 Total Loss: 4.6879 Recon Loss: 4.6731 
[12/24 17:26:02 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8202 LR: 0.000091 Step: 211850 Total Loss: 4.7206 Recon Loss: 4.7058 
[12/24 17:26:43 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8162 LR: 0.000091 Step: 211900 Total Loss: 4.6861 Recon Loss: 4.6713 
[12/24 17:27:24 TiTok]: Data (t): 0.0009, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000091 Step: 211950 Total Loss: 4.7802 Recon Loss: 4.7654 
[12/24 17:28:05 TiTok]: Data (t): 0.0009, 34.67/s/gpu Batch (t): 0.9230 LR: 0.000091 Step: 212000 Total Loss: 5.3839 Recon Loss: 5.3691 
[12/24 17:28:46 TiTok]: Data (t): 0.0011, 38.84/s/gpu Batch (t): 0.8240 LR: 0.000091 Step: 212050 Total Loss: 4.0378 Recon Loss: 4.0230 
[12/24 17:29:26 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000091 Step: 212100 Total Loss: 5.4452 Recon Loss: 5.4303 
[12/24 17:30:07 TiTok]: Data (t): 0.0012, 39.19/s/gpu Batch (t): 0.8164 LR: 0.000091 Step: 212150 Total Loss: 5.4124 Recon Loss: 5.3976 
[12/24 17:30:48 TiTok]: Data (t): 0.0011, 39.54/s/gpu Batch (t): 0.8092 LR: 0.000091 Step: 212200 Total Loss: 4.7157 Recon Loss: 4.7009 
[12/24 17:31:29 TiTok]: Data (t): 0.0011, 38.33/s/gpu Batch (t): 0.8348 LR: 0.000091 Step: 212250 Total Loss: 5.4105 Recon Loss: 5.3958 
[12/24 17:32:10 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8129 LR: 0.000091 Step: 212300 Total Loss: 4.0509 Recon Loss: 4.0360 
[12/24 17:32:51 TiTok]: Data (t): 0.0011, 37.93/s/gpu Batch (t): 0.8437 LR: 0.000091 Step: 212350 Total Loss: 4.7277 Recon Loss: 4.7128 
[12/24 17:33:32 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000091 Step: 212400 Total Loss: 4.7795 Recon Loss: 4.7647 
[12/24 17:34:13 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000091 Step: 212450 Total Loss: 4.7245 Recon Loss: 4.7097 
[12/24 17:34:54 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000091 Step: 212500 Total Loss: 4.7422 Recon Loss: 4.7272 
[12/24 17:35:35 TiTok]: Data (t): 0.0011, 39.37/s/gpu Batch (t): 0.8129 LR: 0.000091 Step: 212550 Total Loss: 4.6934 Recon Loss: 4.6786 
[12/24 17:36:16 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000091 Step: 212600 Total Loss: 5.3589 Recon Loss: 5.3441 
[12/24 17:36:56 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8179 LR: 0.000091 Step: 212650 Total Loss: 4.7307 Recon Loss: 4.7159 
[12/24 17:37:37 TiTok]: Data (t): 0.0011, 38.89/s/gpu Batch (t): 0.8227 LR: 0.000091 Step: 212700 Total Loss: 4.0358 Recon Loss: 4.0209 
[12/24 17:38:18 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000091 Step: 212750 Total Loss: 4.7031 Recon Loss: 4.6884 
[12/24 17:38:59 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000091 Step: 212800 Total Loss: 5.4652 Recon Loss: 5.4504 
[12/24 17:39:40 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000091 Step: 212850 Total Loss: 6.1099 Recon Loss: 6.0951 
[12/24 17:40:21 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000091 Step: 212900 Total Loss: 5.4086 Recon Loss: 5.3937 
[12/24 17:41:02 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8223 LR: 0.000091 Step: 212950 Total Loss: 4.7696 Recon Loss: 4.7547 
[12/24 17:41:43 TiTok]: Data (t): 0.0010, 35.22/s/gpu Batch (t): 0.9085 LR: 0.000091 Step: 213000 Total Loss: 4.0201 Recon Loss: 4.0051 
[12/24 17:42:24 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000091 Step: 213050 Total Loss: 3.9171 Recon Loss: 3.9023 
[12/24 17:43:05 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000091 Step: 213100 Total Loss: 4.0284 Recon Loss: 4.0137 
[12/24 17:43:45 TiTok]: Data (t): 0.0010, 39.53/s/gpu Batch (t): 0.8094 LR: 0.000091 Step: 213150 Total Loss: 6.0926 Recon Loss: 6.0777 
[12/24 17:44:26 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000091 Step: 213200 Total Loss: 4.6711 Recon Loss: 4.6563 
[12/24 17:45:07 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8234 LR: 0.000091 Step: 213250 Total Loss: 3.9500 Recon Loss: 3.9353 
[12/24 17:45:48 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8164 LR: 0.000091 Step: 213300 Total Loss: 4.7135 Recon Loss: 4.6987 
[12/24 17:46:29 TiTok]: Data (t): 0.0011, 38.91/s/gpu Batch (t): 0.8225 LR: 0.000091 Step: 213350 Total Loss: 4.8058 Recon Loss: 4.7910 
[12/24 17:47:10 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000091 Step: 213400 Total Loss: 4.0466 Recon Loss: 4.0317 
[12/24 17:47:51 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000091 Step: 213450 Total Loss: 4.0289 Recon Loss: 4.0141 
[12/24 17:48:32 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000091 Step: 213500 Total Loss: 4.7841 Recon Loss: 4.7693 
[12/24 17:49:13 TiTok]: Data (t): 0.0012, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000091 Step: 213550 Total Loss: 4.6914 Recon Loss: 4.6766 
[12/24 17:49:54 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000091 Step: 213600 Total Loss: 4.7410 Recon Loss: 4.7262 
[12/24 17:50:34 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000091 Step: 213650 Total Loss: 4.7102 Recon Loss: 4.6953 
[12/24 17:51:15 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000091 Step: 213700 Total Loss: 4.7787 Recon Loss: 4.7639 
[12/24 17:51:56 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000091 Step: 213750 Total Loss: 4.0016 Recon Loss: 3.9868 
[12/24 17:52:37 TiTok]: Data (t): 0.0011, 38.91/s/gpu Batch (t): 0.8223 LR: 0.000091 Step: 213800 Total Loss: 4.0283 Recon Loss: 4.0135 
[12/24 17:53:18 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000091 Step: 213850 Total Loss: 3.9782 Recon Loss: 3.9634 
[12/24 17:53:59 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8131 LR: 0.000091 Step: 213900 Total Loss: 5.4536 Recon Loss: 5.4387 
[12/24 17:54:40 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000091 Step: 213950 Total Loss: 4.0665 Recon Loss: 4.0517 
[12/24 17:55:21 TiTok]: Data (t): 0.0011, 35.24/s/gpu Batch (t): 0.9081 LR: 0.000091 Step: 214000 Total Loss: 5.4130 Recon Loss: 5.3981 
[12/24 17:56:02 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000091 Step: 214050 Total Loss: 5.4341 Recon Loss: 5.4193 
[12/24 17:56:43 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000091 Step: 214100 Total Loss: 4.7913 Recon Loss: 4.7764 
[12/24 17:57:24 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8129 LR: 0.000091 Step: 214150 Total Loss: 4.0424 Recon Loss: 4.0276 
[12/24 17:58:04 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8202 LR: 0.000091 Step: 214200 Total Loss: 4.6778 Recon Loss: 4.6631 
[12/24 17:58:45 TiTok]: Data (t): 0.0011, 38.77/s/gpu Batch (t): 0.8255 LR: 0.000091 Step: 214250 Total Loss: 4.0179 Recon Loss: 4.0031 
[12/24 17:59:26 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000091 Step: 214300 Total Loss: 4.6826 Recon Loss: 4.6677 
[12/24 18:00:07 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000091 Step: 214350 Total Loss: 4.0285 Recon Loss: 4.0138 
[12/24 18:00:48 TiTok]: Data (t): 0.0011, 38.85/s/gpu Batch (t): 0.8236 LR: 0.000091 Step: 214400 Total Loss: 4.0362 Recon Loss: 4.0214 
[12/24 18:01:29 TiTok]: Data (t): 0.0011, 39.41/s/gpu Batch (t): 0.8121 LR: 0.000091 Step: 214450 Total Loss: 4.0850 Recon Loss: 4.0701 
[12/24 18:02:10 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8133 LR: 0.000091 Step: 214500 Total Loss: 4.0703 Recon Loss: 4.0556 
[12/24 18:02:51 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000091 Step: 214550 Total Loss: 4.7813 Recon Loss: 4.7665 
[12/24 18:03:32 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000091 Step: 214600 Total Loss: 4.7664 Recon Loss: 4.7517 
[12/24 18:04:12 TiTok]: Data (t): 0.0010, 38.82/s/gpu Batch (t): 0.8242 LR: 0.000091 Step: 214650 Total Loss: 3.9751 Recon Loss: 3.9602 
[12/24 18:04:53 TiTok]: Data (t): 0.0011, 38.85/s/gpu Batch (t): 0.8237 LR: 0.000091 Step: 214700 Total Loss: 4.7729 Recon Loss: 4.7580 
[12/24 18:05:34 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000091 Step: 214750 Total Loss: 3.9763 Recon Loss: 3.9614 
[12/24 18:06:15 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000091 Step: 214800 Total Loss: 4.7589 Recon Loss: 4.7442 
[12/24 18:06:56 TiTok]: Data (t): 0.0010, 38.82/s/gpu Batch (t): 0.8242 LR: 0.000091 Step: 214850 Total Loss: 4.0426 Recon Loss: 4.0278 
[12/24 18:07:37 TiTok]: Data (t): 0.0011, 38.89/s/gpu Batch (t): 0.8228 LR: 0.000091 Step: 214900 Total Loss: 3.9652 Recon Loss: 3.9503 
[12/24 18:08:18 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000091 Step: 214950 Total Loss: 4.6938 Recon Loss: 4.6791 
[12/24 18:08:59 TiTok]: Data (t): 0.0010, 35.62/s/gpu Batch (t): 0.8984 LR: 0.000091 Step: 215000 Total Loss: 4.7562 Recon Loss: 4.7415 
[12/24 18:09:00 TiTok]: Reconstructing images...
[12/24 18:09:41 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000091 Step: 215050 Total Loss: 4.7789 Recon Loss: 4.7642 
[12/24 18:10:22 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000091 Step: 215100 Total Loss: 4.7550 Recon Loss: 4.7402 
[12/24 18:11:03 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000091 Step: 215150 Total Loss: 4.7092 Recon Loss: 4.6944 
[12/24 18:11:44 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000091 Step: 215200 Total Loss: 5.4386 Recon Loss: 5.4238 
[12/24 18:12:25 TiTok]: Data (t): 0.0013, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000091 Step: 215250 Total Loss: 4.7516 Recon Loss: 4.7367 
[12/24 18:13:06 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000091 Step: 215300 Total Loss: 4.0373 Recon Loss: 4.0225 
[12/24 18:13:46 TiTok]: Data (t): 0.0010, 38.82/s/gpu Batch (t): 0.8244 LR: 0.000091 Step: 215350 Total Loss: 4.7085 Recon Loss: 4.6938 
[12/24 18:14:27 TiTok]: Data (t): 0.0010, 39.41/s/gpu Batch (t): 0.8119 LR: 0.000091 Step: 215400 Total Loss: 4.7311 Recon Loss: 4.7162 
[12/24 18:15:08 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000091 Step: 215450 Total Loss: 4.6951 Recon Loss: 4.6803 
[12/24 18:15:49 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000091 Step: 215500 Total Loss: 5.4039 Recon Loss: 5.3891 
[12/24 18:16:30 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000091 Step: 215550 Total Loss: 4.0376 Recon Loss: 4.0228 
[12/24 18:17:11 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000091 Step: 215600 Total Loss: 3.9964 Recon Loss: 3.9816 
[12/24 18:17:52 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000091 Step: 215650 Total Loss: 4.7060 Recon Loss: 4.6912 
[12/24 18:18:33 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000091 Step: 215700 Total Loss: 4.7961 Recon Loss: 4.7812 
[12/24 18:19:14 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000091 Step: 215750 Total Loss: 4.7827 Recon Loss: 4.7678 
[12/24 18:19:54 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000091 Step: 215800 Total Loss: 4.7198 Recon Loss: 4.7050 
[12/24 18:20:35 TiTok]: Data (t): 0.0016, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000091 Step: 215850 Total Loss: 4.6714 Recon Loss: 4.6566 
[12/24 18:21:16 TiTok]: Data (t): 0.0011, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000091 Step: 215900 Total Loss: 5.4128 Recon Loss: 5.3979 
[12/24 18:21:57 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000091 Step: 215950 Total Loss: 5.4111 Recon Loss: 5.3962 
[12/24 18:22:38 TiTok]: Data (t): 0.0010, 33.24/s/gpu Batch (t): 0.9626 LR: 0.000091 Step: 216000 Total Loss: 4.7315 Recon Loss: 4.7169 
[12/24 18:23:19 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000091 Step: 216050 Total Loss: 5.4229 Recon Loss: 5.4080 
[12/24 18:24:00 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000091 Step: 216100 Total Loss: 4.6024 Recon Loss: 4.5876 
[12/24 18:24:41 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000091 Step: 216150 Total Loss: 4.0160 Recon Loss: 4.0012 
[12/24 18:25:21 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000091 Step: 216200 Total Loss: 5.5027 Recon Loss: 5.4879 
[12/24 18:26:02 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000091 Step: 216250 Total Loss: 4.7106 Recon Loss: 4.6958 
[12/24 18:26:43 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000091 Step: 216300 Total Loss: 4.0109 Recon Loss: 3.9961 
[12/24 18:27:24 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000091 Step: 216350 Total Loss: 6.1584 Recon Loss: 6.1435 
[12/24 18:28:05 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000091 Step: 216400 Total Loss: 4.7501 Recon Loss: 4.7353 
[12/24 18:28:46 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8225 LR: 0.000091 Step: 216450 Total Loss: 4.7750 Recon Loss: 4.7602 
[12/24 18:29:27 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000091 Step: 216500 Total Loss: 4.0741 Recon Loss: 4.0594 
[12/24 18:30:08 TiTok]: Data (t): 0.0009, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000091 Step: 216550 Total Loss: 5.3941 Recon Loss: 5.3793 
[12/24 18:30:49 TiTok]: Data (t): 0.0009, 39.01/s/gpu Batch (t): 0.8202 LR: 0.000091 Step: 216600 Total Loss: 4.7267 Recon Loss: 4.7119 
[12/24 18:31:30 TiTok]: Data (t): 0.0012, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000091 Step: 216650 Total Loss: 4.0395 Recon Loss: 4.0247 
[12/24 18:32:11 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000091 Step: 216700 Total Loss: 3.9923 Recon Loss: 3.9775 
[12/24 18:32:51 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000091 Step: 216750 Total Loss: 4.7036 Recon Loss: 4.6889 
[12/24 18:33:32 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000091 Step: 216800 Total Loss: 4.0841 Recon Loss: 4.0693 
[12/24 18:34:13 TiTok]: Data (t): 0.0010, 38.85/s/gpu Batch (t): 0.8237 LR: 0.000091 Step: 216850 Total Loss: 4.7139 Recon Loss: 4.6990 
[12/24 18:34:54 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8221 LR: 0.000091 Step: 216900 Total Loss: 5.4232 Recon Loss: 5.4084 
[12/24 18:35:35 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000091 Step: 216950 Total Loss: 4.7641 Recon Loss: 4.7493 
[12/24 18:36:16 TiTok]: Data (t): 0.0010, 35.20/s/gpu Batch (t): 0.9092 LR: 0.000091 Step: 217000 Total Loss: 5.4169 Recon Loss: 5.4020 
[12/24 18:36:57 TiTok]: Data (t): 0.0011, 39.42/s/gpu Batch (t): 0.8119 LR: 0.000091 Step: 217050 Total Loss: 5.4271 Recon Loss: 5.4123 
[12/24 18:37:38 TiTok]: Data (t): 0.0012, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000091 Step: 217100 Total Loss: 4.0342 Recon Loss: 4.0194 
[12/24 18:38:19 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000091 Step: 217150 Total Loss: 4.0343 Recon Loss: 4.0194 
[12/24 18:39:00 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000091 Step: 217200 Total Loss: 3.9269 Recon Loss: 3.9121 
[12/24 18:39:41 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000091 Step: 217250 Total Loss: 4.0739 Recon Loss: 4.0591 
[12/24 18:40:22 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000091 Step: 217300 Total Loss: 4.1127 Recon Loss: 4.0979 
[12/24 18:41:02 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8154 LR: 0.000091 Step: 217350 Total Loss: 6.1245 Recon Loss: 6.1097 
[12/24 18:41:43 TiTok]: Data (t): 0.0011, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000091 Step: 217400 Total Loss: 5.3963 Recon Loss: 5.3816 
[12/24 18:42:24 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000091 Step: 217450 Total Loss: 5.4359 Recon Loss: 5.4211 
[12/24 18:43:05 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000091 Step: 217500 Total Loss: 4.0567 Recon Loss: 4.0419 
[12/24 18:43:46 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000091 Step: 217550 Total Loss: 4.7754 Recon Loss: 4.7605 
[12/24 18:44:27 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8162 LR: 0.000091 Step: 217600 Total Loss: 5.4704 Recon Loss: 5.4556 
[12/24 18:45:08 TiTok]: Data (t): 0.0012, 38.85/s/gpu Batch (t): 0.8237 LR: 0.000091 Step: 217650 Total Loss: 4.0800 Recon Loss: 4.0652 
[12/24 18:45:49 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000091 Step: 217700 Total Loss: 4.6765 Recon Loss: 4.6616 
[12/24 18:46:30 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8162 LR: 0.000091 Step: 217750 Total Loss: 4.7947 Recon Loss: 4.7799 
[12/24 18:47:10 TiTok]: Data (t): 0.0012, 39.41/s/gpu Batch (t): 0.8119 LR: 0.000091 Step: 217800 Total Loss: 6.1216 Recon Loss: 6.1067 
[12/24 18:47:51 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000091 Step: 217850 Total Loss: 3.9525 Recon Loss: 3.9377 
[12/24 18:48:32 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8158 LR: 0.000091 Step: 217900 Total Loss: 4.6789 Recon Loss: 4.6640 
[12/24 18:49:13 TiTok]: Data (t): 0.0011, 39.36/s/gpu Batch (t): 0.8131 LR: 0.000091 Step: 217950 Total Loss: 4.0004 Recon Loss: 3.9857 
[12/24 18:49:54 TiTok]: Data (t): 0.0010, 35.05/s/gpu Batch (t): 0.9129 LR: 0.000091 Step: 218000 Total Loss: 6.1416 Recon Loss: 6.1268 
[12/24 18:50:35 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000091 Step: 218050 Total Loss: 3.9791 Recon Loss: 3.9643 
[12/24 18:51:16 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000091 Step: 218100 Total Loss: 6.1505 Recon Loss: 6.1357 
[12/24 18:51:57 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000091 Step: 218150 Total Loss: 4.0499 Recon Loss: 4.0350 
[12/24 18:52:38 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000091 Step: 218200 Total Loss: 4.7250 Recon Loss: 4.7101 
[12/24 18:53:18 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8189 LR: 0.000091 Step: 218250 Total Loss: 4.8074 Recon Loss: 4.7925 
[12/24 18:53:59 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000091 Step: 218300 Total Loss: 4.0404 Recon Loss: 4.0255 
[12/24 18:54:40 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000091 Step: 218350 Total Loss: 4.5822 Recon Loss: 4.5675 
[12/24 18:55:21 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000091 Step: 218400 Total Loss: 4.7448 Recon Loss: 4.7301 
[12/24 18:56:02 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000091 Step: 218450 Total Loss: 3.9995 Recon Loss: 3.9847 
[12/24 18:56:43 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000091 Step: 218500 Total Loss: 3.9662 Recon Loss: 3.9514 
[12/24 18:57:24 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000091 Step: 218550 Total Loss: 4.7634 Recon Loss: 4.7485 
[12/24 18:58:05 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8183 LR: 0.000091 Step: 218600 Total Loss: 4.7784 Recon Loss: 4.7636 
[12/24 18:58:46 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000090 Step: 218650 Total Loss: 4.6858 Recon Loss: 4.6709 
[12/24 18:59:26 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8164 LR: 0.000090 Step: 218700 Total Loss: 4.7444 Recon Loss: 4.7296 
[12/24 19:00:07 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000090 Step: 218750 Total Loss: 4.0507 Recon Loss: 4.0360 
[12/24 19:00:48 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000090 Step: 218800 Total Loss: 4.7480 Recon Loss: 4.7333 
[12/24 19:01:29 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8177 LR: 0.000090 Step: 218850 Total Loss: 4.7109 Recon Loss: 4.6961 
[12/24 19:02:10 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8223 LR: 0.000090 Step: 218900 Total Loss: 6.1349 Recon Loss: 6.1201 
[12/24 19:02:51 TiTok]: Data (t): 0.0012, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000090 Step: 218950 Total Loss: 4.7037 Recon Loss: 4.6889 
[12/24 19:03:32 TiTok]: Data (t): 0.0010, 34.42/s/gpu Batch (t): 0.9298 LR: 0.000090 Step: 219000 Total Loss: 4.7446 Recon Loss: 4.7297 
[12/24 19:04:13 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8217 LR: 0.000090 Step: 219050 Total Loss: 4.7695 Recon Loss: 4.7548 
[12/24 19:04:54 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000090 Step: 219100 Total Loss: 4.0531 Recon Loss: 4.0383 
[12/24 19:05:34 TiTok]: Data (t): 0.0016, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000090 Step: 219150 Total Loss: 4.7451 Recon Loss: 4.7303 
[12/24 19:06:15 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000090 Step: 219200 Total Loss: 4.6955 Recon Loss: 4.6807 
[12/24 19:06:56 TiTok]: Data (t): 0.0011, 38.78/s/gpu Batch (t): 0.8252 LR: 0.000090 Step: 219250 Total Loss: 4.7354 Recon Loss: 4.7206 
[12/24 19:07:37 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000090 Step: 219300 Total Loss: 4.0295 Recon Loss: 4.0146 
[12/24 19:08:18 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8219 LR: 0.000090 Step: 219350 Total Loss: 5.4324 Recon Loss: 5.4175 
[12/24 19:08:59 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8204 LR: 0.000090 Step: 219400 Total Loss: 4.7669 Recon Loss: 4.7522 
[12/24 19:09:40 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000090 Step: 219450 Total Loss: 5.4468 Recon Loss: 5.4320 
[12/24 19:10:21 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000090 Step: 219500 Total Loss: 6.1075 Recon Loss: 6.0927 
[12/24 19:11:02 TiTok]: Data (t): 0.0011, 38.85/s/gpu Batch (t): 0.8236 LR: 0.000090 Step: 219550 Total Loss: 4.0607 Recon Loss: 4.0458 
[12/24 19:11:43 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000090 Step: 219600 Total Loss: 5.4219 Recon Loss: 5.4070 
[12/24 19:12:23 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000090 Step: 219650 Total Loss: 4.0136 Recon Loss: 3.9988 
[12/24 19:13:04 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000090 Step: 219700 Total Loss: 4.7432 Recon Loss: 4.7283 
[12/24 19:13:45 TiTok]: Data (t): 0.0011, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000090 Step: 219750 Total Loss: 4.7788 Recon Loss: 4.7640 
[12/24 19:14:26 TiTok]: Data (t): 0.0010, 39.39/s/gpu Batch (t): 0.8124 LR: 0.000090 Step: 219800 Total Loss: 4.7474 Recon Loss: 4.7325 
[12/24 19:15:07 TiTok]: Data (t): 0.0016, 39.98/s/gpu Batch (t): 0.8004 LR: 0.000090 Step: 219850 Total Loss: 3.9996 Recon Loss: 3.9848 
[12/24 19:15:48 TiTok]: Data (t): 0.0011, 38.85/s/gpu Batch (t): 0.8237 LR: 0.000090 Step: 219900 Total Loss: 4.0311 Recon Loss: 4.0163 
[12/24 19:16:29 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000090 Step: 219950 Total Loss: 4.0544 Recon Loss: 4.0396 
[12/24 19:17:10 TiTok]: Data (t): 0.0011, 35.07/s/gpu Batch (t): 0.9126 LR: 0.000090 Step: 220000 Total Loss: 3.9791 Recon Loss: 3.9643 
[12/24 19:17:11 TiTok]: Reconstructing images...
[12/24 19:17:52 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000090 Step: 220050 Total Loss: 5.4189 Recon Loss: 5.4042 
[12/24 19:18:33 TiTok]: Data (t): 0.0010, 38.75/s/gpu Batch (t): 0.8257 LR: 0.000090 Step: 220100 Total Loss: 5.3876 Recon Loss: 5.3728 
[12/24 19:19:14 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000090 Step: 220150 Total Loss: 6.1077 Recon Loss: 6.0929 
[12/24 19:19:55 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8206 LR: 0.000090 Step: 220200 Total Loss: 4.0931 Recon Loss: 4.0784 
[12/24 19:20:36 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000090 Step: 220250 Total Loss: 4.7537 Recon Loss: 4.7390 
[12/24 19:21:17 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000090 Step: 220300 Total Loss: 5.4886 Recon Loss: 5.4738 
[12/24 19:21:58 TiTok]: Data (t): 0.0010, 38.83/s/gpu Batch (t): 0.8242 LR: 0.000090 Step: 220350 Total Loss: 6.1118 Recon Loss: 6.0970 
[12/24 19:22:39 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000090 Step: 220400 Total Loss: 4.7423 Recon Loss: 4.7274 
Epoch 22/99 started.
[12/24 19:23:20 TiTok]: Data (t): 0.0010, 39.39/s/gpu Batch (t): 0.8125 LR: 0.000090 Step: 220450 Total Loss: 3.9895 Recon Loss: 3.9747 
[12/24 19:24:01 TiTok]: Data (t): 0.0011, 38.79/s/gpu Batch (t): 0.8249 LR: 0.000090 Step: 220500 Total Loss: 5.4093 Recon Loss: 5.3945 
[12/24 19:24:42 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8200 LR: 0.000090 Step: 220550 Total Loss: 4.6477 Recon Loss: 4.6329 
[12/24 19:25:23 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000090 Step: 220600 Total Loss: 3.9241 Recon Loss: 3.9092 
[12/24 19:26:04 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000090 Step: 220650 Total Loss: 4.7074 Recon Loss: 4.6926 
[12/24 19:26:45 TiTok]: Data (t): 0.0011, 39.46/s/gpu Batch (t): 0.8110 LR: 0.000090 Step: 220700 Total Loss: 4.6964 Recon Loss: 4.6816 
[12/24 19:27:26 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000090 Step: 220750 Total Loss: 3.9459 Recon Loss: 3.9311 
[12/24 19:28:07 TiTok]: Data (t): 0.0011, 38.66/s/gpu Batch (t): 0.8276 LR: 0.000090 Step: 220800 Total Loss: 5.4746 Recon Loss: 5.4598 
[12/24 19:28:48 TiTok]: Data (t): 0.0011, 39.39/s/gpu Batch (t): 0.8124 LR: 0.000090 Step: 220850 Total Loss: 4.0070 Recon Loss: 3.9921 
[12/24 19:29:29 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8150 LR: 0.000090 Step: 220900 Total Loss: 4.7036 Recon Loss: 4.6888 
[12/24 19:30:10 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000090 Step: 220950 Total Loss: 5.4554 Recon Loss: 5.4406 
[12/24 19:30:51 TiTok]: Data (t): 0.0010, 34.22/s/gpu Batch (t): 0.9350 LR: 0.000090 Step: 221000 Total Loss: 4.7639 Recon Loss: 4.7491 
[12/24 19:31:31 TiTok]: Data (t): 0.0012, 39.02/s/gpu Batch (t): 0.8202 LR: 0.000090 Step: 221050 Total Loss: 6.1322 Recon Loss: 6.1173 
[12/24 19:32:12 TiTok]: Data (t): 0.0011, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000090 Step: 221100 Total Loss: 5.4387 Recon Loss: 5.4239 
[12/24 19:32:53 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000090 Step: 221150 Total Loss: 4.7384 Recon Loss: 4.7236 
[12/24 19:33:34 TiTok]: Data (t): 0.0010, 38.87/s/gpu Batch (t): 0.8233 LR: 0.000090 Step: 221200 Total Loss: 4.0429 Recon Loss: 4.0282 
[12/24 19:34:15 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000090 Step: 221250 Total Loss: 5.4089 Recon Loss: 5.3941 
[12/24 19:34:56 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000090 Step: 221300 Total Loss: 4.0009 Recon Loss: 3.9862 
[12/24 19:35:37 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8200 LR: 0.000090 Step: 221350 Total Loss: 4.0058 Recon Loss: 3.9909 
[12/24 19:36:18 TiTok]: Data (t): 0.0011, 39.38/s/gpu Batch (t): 0.8127 LR: 0.000090 Step: 221400 Total Loss: 5.4694 Recon Loss: 5.4546 
[12/24 19:36:59 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000090 Step: 221450 Total Loss: 4.0324 Recon Loss: 4.0176 
[12/24 19:37:40 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000090 Step: 221500 Total Loss: 5.4323 Recon Loss: 5.4175 
[12/24 19:38:21 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000090 Step: 221550 Total Loss: 5.4517 Recon Loss: 5.4370 
[12/24 19:39:01 TiTok]: Data (t): 0.0011, 39.48/s/gpu Batch (t): 0.8106 LR: 0.000090 Step: 221600 Total Loss: 5.3622 Recon Loss: 5.3473 
[12/24 19:39:42 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000090 Step: 221650 Total Loss: 4.7310 Recon Loss: 4.7163 
[12/24 19:40:23 TiTok]: Data (t): 0.0017, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000090 Step: 221700 Total Loss: 4.6736 Recon Loss: 4.6587 
[12/24 19:41:04 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000090 Step: 221750 Total Loss: 4.7458 Recon Loss: 4.7310 
[12/24 19:41:45 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000090 Step: 221800 Total Loss: 4.7011 Recon Loss: 4.6862 
[12/24 19:42:26 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000090 Step: 221850 Total Loss: 4.0137 Recon Loss: 3.9988 
[12/24 19:43:07 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000090 Step: 221900 Total Loss: 4.0163 Recon Loss: 4.0015 
[12/24 19:43:48 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000090 Step: 221950 Total Loss: 4.0722 Recon Loss: 4.0573 
[12/24 19:44:29 TiTok]: Data (t): 0.0010, 34.97/s/gpu Batch (t): 0.9150 LR: 0.000090 Step: 222000 Total Loss: 4.7390 Recon Loss: 4.7242 
[12/24 19:45:10 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000090 Step: 222050 Total Loss: 4.6893 Recon Loss: 4.6745 
[12/24 19:45:50 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8200 LR: 0.000090 Step: 222100 Total Loss: 4.0245 Recon Loss: 4.0096 
[12/24 19:46:31 TiTok]: Data (t): 0.0010, 38.85/s/gpu Batch (t): 0.8236 LR: 0.000090 Step: 222150 Total Loss: 4.7718 Recon Loss: 4.7570 
[12/24 19:47:12 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000090 Step: 222200 Total Loss: 3.9774 Recon Loss: 3.9625 
[12/24 19:47:53 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8185 LR: 0.000090 Step: 222250 Total Loss: 4.7584 Recon Loss: 4.7435 
[12/24 19:48:34 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8158 LR: 0.000090 Step: 222300 Total Loss: 4.0025 Recon Loss: 3.9877 
[12/24 19:49:15 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000090 Step: 222350 Total Loss: 6.1767 Recon Loss: 6.1619 
[12/24 19:49:56 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000090 Step: 222400 Total Loss: 4.0526 Recon Loss: 4.0377 
[12/24 19:50:37 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8162 LR: 0.000090 Step: 222450 Total Loss: 5.4092 Recon Loss: 5.3945 
[12/24 19:51:18 TiTok]: Data (t): 0.0011, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000090 Step: 222500 Total Loss: 4.7459 Recon Loss: 4.7311 
[12/24 19:51:59 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000090 Step: 222550 Total Loss: 3.9689 Recon Loss: 3.9541 
[12/24 19:52:39 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000090 Step: 222600 Total Loss: 5.4320 Recon Loss: 5.4172 
[12/24 19:53:20 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000090 Step: 222650 Total Loss: 4.0689 Recon Loss: 4.0540 
[12/24 19:54:01 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000090 Step: 222700 Total Loss: 4.7316 Recon Loss: 4.7168 
[12/24 19:54:42 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000090 Step: 222750 Total Loss: 4.7327 Recon Loss: 4.7178 
[12/24 19:55:23 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000090 Step: 222800 Total Loss: 4.7266 Recon Loss: 4.7119 
[12/24 19:56:04 TiTok]: Data (t): 0.0023, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000090 Step: 222850 Total Loss: 3.9723 Recon Loss: 3.9575 
[12/24 19:56:45 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000090 Step: 222900 Total Loss: 4.6749 Recon Loss: 4.6601 
[12/24 19:57:26 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000090 Step: 222950 Total Loss: 4.0540 Recon Loss: 4.0391 
[12/24 19:58:07 TiTok]: Data (t): 0.0011, 35.15/s/gpu Batch (t): 0.9103 LR: 0.000090 Step: 223000 Total Loss: 4.7357 Recon Loss: 4.7209 
[12/24 19:58:48 TiTok]: Data (t): 0.0011, 39.38/s/gpu Batch (t): 0.8127 LR: 0.000090 Step: 223050 Total Loss: 4.7408 Recon Loss: 4.7260 
[12/24 19:59:29 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000090 Step: 223100 Total Loss: 3.9437 Recon Loss: 3.9288 
[12/24 20:00:09 TiTok]: Data (t): 0.0011, 39.48/s/gpu Batch (t): 0.8106 LR: 0.000090 Step: 223150 Total Loss: 5.3672 Recon Loss: 5.3524 
[12/24 20:00:50 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000090 Step: 223200 Total Loss: 4.7060 Recon Loss: 4.6912 
[12/24 20:01:31 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000090 Step: 223250 Total Loss: 4.0417 Recon Loss: 4.0269 
[12/24 20:02:12 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000090 Step: 223300 Total Loss: 6.0941 Recon Loss: 6.0793 
[12/24 20:02:53 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8198 LR: 0.000090 Step: 223350 Total Loss: 4.0231 Recon Loss: 4.0082 
[12/24 20:03:34 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000090 Step: 223400 Total Loss: 4.7513 Recon Loss: 4.7365 
[12/24 20:04:15 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000090 Step: 223450 Total Loss: 4.7433 Recon Loss: 4.7285 
[12/24 20:04:56 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000090 Step: 223500 Total Loss: 4.7241 Recon Loss: 4.7093 
[12/24 20:05:36 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000090 Step: 223550 Total Loss: 4.0603 Recon Loss: 4.0455 
[12/24 20:06:17 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000090 Step: 223600 Total Loss: 4.0160 Recon Loss: 4.0011 
[12/24 20:06:58 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8185 LR: 0.000090 Step: 223650 Total Loss: 4.7425 Recon Loss: 4.7278 
[12/24 20:07:39 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000090 Step: 223700 Total Loss: 5.3957 Recon Loss: 5.3809 
[12/24 20:08:20 TiTok]: Data (t): 0.0012, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000090 Step: 223750 Total Loss: 4.6680 Recon Loss: 4.6531 
[12/24 20:09:01 TiTok]: Data (t): 0.0010, 38.77/s/gpu Batch (t): 0.8253 LR: 0.000090 Step: 223800 Total Loss: 4.7042 Recon Loss: 4.6894 
[12/24 20:09:42 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000090 Step: 223850 Total Loss: 4.0612 Recon Loss: 4.0464 
[12/24 20:10:23 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8181 LR: 0.000090 Step: 223900 Total Loss: 4.7682 Recon Loss: 4.7534 
[12/24 20:11:04 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000090 Step: 223950 Total Loss: 4.0620 Recon Loss: 4.0472 
[12/24 20:11:45 TiTok]: Data (t): 0.0035, 35.16/s/gpu Batch (t): 0.9102 LR: 0.000090 Step: 224000 Total Loss: 4.7441 Recon Loss: 4.7294 
[12/24 20:12:25 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000090 Step: 224050 Total Loss: 4.6633 Recon Loss: 4.6485 
[12/24 20:13:06 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000090 Step: 224100 Total Loss: 3.9878 Recon Loss: 3.9731 
[12/24 20:13:47 TiTok]: Data (t): 0.0010, 38.87/s/gpu Batch (t): 0.8232 LR: 0.000090 Step: 224150 Total Loss: 3.9857 Recon Loss: 3.9708 
[12/24 20:14:28 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000090 Step: 224200 Total Loss: 5.4024 Recon Loss: 5.3875 
[12/24 20:15:09 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000090 Step: 224250 Total Loss: 4.7267 Recon Loss: 4.7118 
[12/24 20:15:50 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000090 Step: 224300 Total Loss: 3.9611 Recon Loss: 3.9463 
[12/24 20:16:31 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000090 Step: 224350 Total Loss: 4.7254 Recon Loss: 4.7106 
[12/24 20:17:12 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000090 Step: 224400 Total Loss: 4.0512 Recon Loss: 4.0365 
[12/24 20:17:52 TiTok]: Data (t): 0.0010, 38.83/s/gpu Batch (t): 0.8242 LR: 0.000090 Step: 224450 Total Loss: 5.4508 Recon Loss: 5.4359 
[12/24 20:18:33 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000090 Step: 224500 Total Loss: 4.0450 Recon Loss: 4.0301 
[12/24 20:19:14 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000090 Step: 224550 Total Loss: 4.7346 Recon Loss: 4.7198 
[12/24 20:19:55 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8156 LR: 0.000090 Step: 224600 Total Loss: 3.9664 Recon Loss: 3.9516 
[12/24 20:20:36 TiTok]: Data (t): 0.0010, 38.79/s/gpu Batch (t): 0.8250 LR: 0.000090 Step: 224650 Total Loss: 5.4155 Recon Loss: 5.4007 
[12/24 20:21:17 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000090 Step: 224700 Total Loss: 4.7179 Recon Loss: 4.7031 
[12/24 20:21:58 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000090 Step: 224750 Total Loss: 6.1307 Recon Loss: 6.1159 
[12/24 20:22:39 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000090 Step: 224800 Total Loss: 4.6912 Recon Loss: 4.6764 
[12/24 20:23:19 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000090 Step: 224850 Total Loss: 3.9412 Recon Loss: 3.9264 
[12/24 20:24:00 TiTok]: Data (t): 0.0014, 39.55/s/gpu Batch (t): 0.8092 LR: 0.000090 Step: 224900 Total Loss: 3.9682 Recon Loss: 3.9534 
[12/24 20:24:41 TiTok]: Data (t): 0.0015, 39.63/s/gpu Batch (t): 0.8074 LR: 0.000090 Step: 224950 Total Loss: 4.7451 Recon Loss: 4.7304 
[12/24 20:25:22 TiTok]: Data (t): 0.0012, 35.55/s/gpu Batch (t): 0.9002 LR: 0.000090 Step: 225000 Total Loss: 4.7259 Recon Loss: 4.7111 
[12/24 20:25:23 TiTok]: Reconstructing images...
[12/24 20:26:05 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000090 Step: 225050 Total Loss: 4.6631 Recon Loss: 4.6484 
[12/24 20:26:46 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8200 LR: 0.000090 Step: 225100 Total Loss: 3.9767 Recon Loss: 3.9619 
[12/24 20:27:27 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000090 Step: 225150 Total Loss: 4.7491 Recon Loss: 4.7342 
[12/24 20:28:07 TiTok]: Data (t): 0.0011, 39.40/s/gpu Batch (t): 0.8123 LR: 0.000090 Step: 225200 Total Loss: 4.0219 Recon Loss: 4.0072 
[12/24 20:28:48 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8227 LR: 0.000090 Step: 225250 Total Loss: 4.6718 Recon Loss: 4.6571 
[12/24 20:29:29 TiTok]: Data (t): 0.0010, 39.43/s/gpu Batch (t): 0.8116 LR: 0.000090 Step: 225300 Total Loss: 5.4031 Recon Loss: 5.3883 
[12/24 20:30:10 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000090 Step: 225350 Total Loss: 4.7717 Recon Loss: 4.7570 
[12/24 20:30:51 TiTok]: Data (t): 0.0011, 38.89/s/gpu Batch (t): 0.8227 LR: 0.000090 Step: 225400 Total Loss: 4.7066 Recon Loss: 4.6918 
[12/24 20:31:32 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000090 Step: 225450 Total Loss: 5.4402 Recon Loss: 5.4253 
[12/24 20:32:13 TiTok]: Data (t): 0.0010, 38.83/s/gpu Batch (t): 0.8242 LR: 0.000090 Step: 225500 Total Loss: 4.6715 Recon Loss: 4.6566 
[12/24 20:32:54 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000090 Step: 225550 Total Loss: 3.9828 Recon Loss: 3.9680 
[12/24 20:33:35 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000090 Step: 225600 Total Loss: 6.1378 Recon Loss: 6.1230 
[12/24 20:34:15 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000090 Step: 225650 Total Loss: 6.1258 Recon Loss: 6.1110 
[12/24 20:34:56 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000090 Step: 225700 Total Loss: 4.7374 Recon Loss: 4.7226 
[12/24 20:35:37 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000090 Step: 225750 Total Loss: 4.0054 Recon Loss: 3.9906 
[12/24 20:36:18 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000090 Step: 225800 Total Loss: 4.0681 Recon Loss: 4.0533 
[12/24 20:36:59 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8234 LR: 0.000090 Step: 225850 Total Loss: 4.6497 Recon Loss: 4.6349 
[12/24 20:37:40 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8185 LR: 0.000090 Step: 225900 Total Loss: 4.7460 Recon Loss: 4.7312 
[12/24 20:38:21 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000090 Step: 225950 Total Loss: 5.4582 Recon Loss: 5.4434 
[12/24 20:39:02 TiTok]: Data (t): 0.0010, 33.60/s/gpu Batch (t): 0.9522 LR: 0.000090 Step: 226000 Total Loss: 4.7081 Recon Loss: 4.6933 
[12/24 20:39:43 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000090 Step: 226050 Total Loss: 3.9925 Recon Loss: 3.9777 
[12/24 20:40:24 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000090 Step: 226100 Total Loss: 4.7424 Recon Loss: 4.7276 
[12/24 20:41:04 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000090 Step: 226150 Total Loss: 4.0028 Recon Loss: 3.9881 
[12/24 20:41:45 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8204 LR: 0.000090 Step: 226200 Total Loss: 4.0841 Recon Loss: 4.0693 
[12/24 20:42:26 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000090 Step: 226250 Total Loss: 3.9532 Recon Loss: 3.9384 
[12/24 20:43:07 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000090 Step: 226300 Total Loss: 4.6959 Recon Loss: 4.6811 
[12/24 20:43:48 TiTok]: Data (t): 0.0011, 38.84/s/gpu Batch (t): 0.8239 LR: 0.000090 Step: 226350 Total Loss: 5.4747 Recon Loss: 5.4599 
[12/24 20:44:29 TiTok]: Data (t): 0.0011, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000090 Step: 226400 Total Loss: 4.7023 Recon Loss: 4.6876 
[12/24 20:45:10 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000090 Step: 226450 Total Loss: 4.7469 Recon Loss: 4.7320 
[12/24 20:45:51 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000090 Step: 226500 Total Loss: 4.0985 Recon Loss: 4.0837 
[12/24 20:46:32 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000090 Step: 226550 Total Loss: 4.7059 Recon Loss: 4.6912 
[12/24 20:47:13 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000090 Step: 226600 Total Loss: 6.1739 Recon Loss: 6.1591 
[12/24 20:47:54 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8152 LR: 0.000090 Step: 226650 Total Loss: 4.7421 Recon Loss: 4.7273 
[12/24 20:48:34 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000090 Step: 226700 Total Loss: 4.7014 Recon Loss: 4.6865 
[12/24 20:49:15 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000090 Step: 226750 Total Loss: 5.3639 Recon Loss: 5.3490 
[12/24 20:49:56 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8181 LR: 0.000090 Step: 226800 Total Loss: 4.0223 Recon Loss: 4.0075 
[12/24 20:50:37 TiTok]: Data (t): 0.0011, 38.87/s/gpu Batch (t): 0.8233 LR: 0.000090 Step: 226850 Total Loss: 5.4513 Recon Loss: 5.4366 
[12/24 20:51:18 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000090 Step: 226900 Total Loss: 4.0714 Recon Loss: 4.0566 
[12/24 20:51:59 TiTok]: Data (t): 0.0011, 39.43/s/gpu Batch (t): 0.8116 LR: 0.000090 Step: 226950 Total Loss: 5.3763 Recon Loss: 5.3615 
[12/24 20:52:40 TiTok]: Data (t): 0.0011, 35.17/s/gpu Batch (t): 0.9099 LR: 0.000090 Step: 227000 Total Loss: 4.7038 Recon Loss: 4.6890 
[12/24 20:53:21 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8217 LR: 0.000090 Step: 227050 Total Loss: 4.7530 Recon Loss: 4.7382 
[12/24 20:54:02 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8217 LR: 0.000090 Step: 227100 Total Loss: 4.0490 Recon Loss: 4.0342 
[12/24 20:54:43 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000090 Step: 227150 Total Loss: 5.3705 Recon Loss: 5.3557 
[12/24 20:55:24 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000090 Step: 227200 Total Loss: 4.7001 Recon Loss: 4.6853 
[12/24 20:56:04 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000090 Step: 227250 Total Loss: 3.9823 Recon Loss: 3.9675 
[12/24 20:56:45 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000090 Step: 227300 Total Loss: 4.7151 Recon Loss: 4.7003 
[12/24 20:57:26 TiTok]: Data (t): 0.0010, 39.39/s/gpu Batch (t): 0.8124 LR: 0.000090 Step: 227350 Total Loss: 6.1191 Recon Loss: 6.1043 
[12/24 20:58:07 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000090 Step: 227400 Total Loss: 5.4208 Recon Loss: 5.4060 
[12/24 20:58:48 TiTok]: Data (t): 0.0011, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000090 Step: 227450 Total Loss: 4.0091 Recon Loss: 3.9943 
[12/24 20:59:29 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8196 LR: 0.000090 Step: 227500 Total Loss: 4.7658 Recon Loss: 4.7510 
[12/24 21:00:10 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000090 Step: 227550 Total Loss: 5.4125 Recon Loss: 5.3977 
[12/24 21:00:51 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000090 Step: 227600 Total Loss: 3.9185 Recon Loss: 3.9037 
[12/24 21:01:32 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000090 Step: 227650 Total Loss: 4.0414 Recon Loss: 4.0265 
[12/24 21:02:13 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000090 Step: 227700 Total Loss: 4.0065 Recon Loss: 3.9916 
[12/24 21:02:53 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000090 Step: 227750 Total Loss: 3.9840 Recon Loss: 3.9692 
[12/24 21:03:34 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000090 Step: 227800 Total Loss: 4.0357 Recon Loss: 4.0208 
[12/24 21:04:15 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000090 Step: 227850 Total Loss: 5.3841 Recon Loss: 5.3692 
[12/24 21:04:56 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000090 Step: 227900 Total Loss: 4.6631 Recon Loss: 4.6482 
[12/24 21:05:37 TiTok]: Data (t): 0.0011, 39.42/s/gpu Batch (t): 0.8117 LR: 0.000090 Step: 227950 Total Loss: 4.0330 Recon Loss: 4.0182 
[12/24 21:06:18 TiTok]: Data (t): 0.0010, 35.27/s/gpu Batch (t): 0.9073 LR: 0.000090 Step: 228000 Total Loss: 5.3939 Recon Loss: 5.3792 
[12/24 21:06:59 TiTok]: Data (t): 0.0011, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000090 Step: 228050 Total Loss: 4.0018 Recon Loss: 3.9870 
[12/24 21:07:40 TiTok]: Data (t): 0.0011, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000090 Step: 228100 Total Loss: 6.1097 Recon Loss: 6.0950 
[12/24 21:08:21 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000090 Step: 228150 Total Loss: 5.4101 Recon Loss: 5.3953 
[12/24 21:09:01 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000090 Step: 228200 Total Loss: 5.4193 Recon Loss: 5.4045 
[12/24 21:09:42 TiTok]: Data (t): 0.0010, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000090 Step: 228250 Total Loss: 5.3913 Recon Loss: 5.3765 
[12/24 21:10:23 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000090 Step: 228300 Total Loss: 4.6832 Recon Loss: 4.6684 
[12/24 21:11:04 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000090 Step: 228350 Total Loss: 3.9705 Recon Loss: 3.9557 
[12/24 21:11:45 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8202 LR: 0.000090 Step: 228400 Total Loss: 4.0785 Recon Loss: 4.0637 
[12/24 21:12:26 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8183 LR: 0.000090 Step: 228450 Total Loss: 4.7586 Recon Loss: 4.7438 
[12/24 21:13:07 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000090 Step: 228500 Total Loss: 4.7076 Recon Loss: 4.6928 
[12/24 21:13:48 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8202 LR: 0.000090 Step: 228550 Total Loss: 4.6662 Recon Loss: 4.6513 
[12/24 21:14:28 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8234 LR: 0.000090 Step: 228600 Total Loss: 4.0520 Recon Loss: 4.0372 
[12/24 21:15:09 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8206 LR: 0.000090 Step: 228650 Total Loss: 3.9081 Recon Loss: 3.8933 
[12/24 21:15:50 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8189 LR: 0.000090 Step: 228700 Total Loss: 5.4460 Recon Loss: 5.4312 
[12/24 21:16:31 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000090 Step: 228750 Total Loss: 5.4018 Recon Loss: 5.3869 
[12/24 21:17:12 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000090 Step: 228800 Total Loss: 4.7189 Recon Loss: 4.7040 
[12/24 21:17:53 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8187 LR: 0.000090 Step: 228850 Total Loss: 4.7064 Recon Loss: 4.6915 
[12/24 21:18:34 TiTok]: Data (t): 0.0011, 39.46/s/gpu Batch (t): 0.8110 LR: 0.000090 Step: 228900 Total Loss: 4.7623 Recon Loss: 4.7474 
[12/24 21:19:15 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000090 Step: 228950 Total Loss: 4.7221 Recon Loss: 4.7073 
[12/24 21:19:56 TiTok]: Data (t): 0.0010, 35.14/s/gpu Batch (t): 0.9106 LR: 0.000090 Step: 229000 Total Loss: 4.7807 Recon Loss: 4.7659 
[12/24 21:20:36 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8156 LR: 0.000090 Step: 229050 Total Loss: 3.9740 Recon Loss: 3.9592 
[12/24 21:21:17 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000090 Step: 229100 Total Loss: 4.6855 Recon Loss: 4.6707 
[12/24 21:21:58 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000090 Step: 229150 Total Loss: 4.7205 Recon Loss: 4.7057 
[12/24 21:22:39 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000090 Step: 229200 Total Loss: 4.6622 Recon Loss: 4.6474 
[12/24 21:23:20 TiTok]: Data (t): 0.0011, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000090 Step: 229250 Total Loss: 4.7125 Recon Loss: 4.6977 
[12/24 21:24:01 TiTok]: Data (t): 0.0011, 39.37/s/gpu Batch (t): 0.8127 LR: 0.000090 Step: 229300 Total Loss: 4.0442 Recon Loss: 4.0293 
[12/24 21:24:42 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8156 LR: 0.000090 Step: 229350 Total Loss: 5.3664 Recon Loss: 5.3514 
[12/24 21:25:23 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000090 Step: 229400 Total Loss: 4.0002 Recon Loss: 3.9854 
[12/24 21:26:04 TiTok]: Data (t): 0.0009, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000090 Step: 229450 Total Loss: 5.4018 Recon Loss: 5.3870 
[12/24 21:26:45 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8200 LR: 0.000090 Step: 229500 Total Loss: 4.6081 Recon Loss: 4.5933 
[12/24 21:27:25 TiTok]: Data (t): 0.0010, 39.40/s/gpu Batch (t): 0.8121 LR: 0.000090 Step: 229550 Total Loss: 6.1396 Recon Loss: 6.1248 
[12/24 21:28:06 TiTok]: Data (t): 0.0011, 38.87/s/gpu Batch (t): 0.8233 LR: 0.000090 Step: 229600 Total Loss: 4.7130 Recon Loss: 4.6982 
[12/24 21:28:47 TiTok]: Data (t): 0.0010, 38.84/s/gpu Batch (t): 0.8239 LR: 0.000090 Step: 229650 Total Loss: 4.0720 Recon Loss: 4.0572 
[12/24 21:29:28 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8221 LR: 0.000090 Step: 229700 Total Loss: 4.0701 Recon Loss: 4.0554 
[12/24 21:30:09 TiTok]: Data (t): 0.0010, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000090 Step: 229750 Total Loss: 4.7534 Recon Loss: 4.7387 
[12/24 21:30:50 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8177 LR: 0.000089 Step: 229800 Total Loss: 5.4671 Recon Loss: 5.4522 
[12/24 21:31:31 TiTok]: Data (t): 0.0011, 38.70/s/gpu Batch (t): 0.8269 LR: 0.000089 Step: 229850 Total Loss: 4.7338 Recon Loss: 4.7190 
[12/24 21:32:12 TiTok]: Data (t): 0.0011, 39.41/s/gpu Batch (t): 0.8121 LR: 0.000089 Step: 229900 Total Loss: 4.7536 Recon Loss: 4.7389 
[12/24 21:32:53 TiTok]: Data (t): 0.0010, 39.42/s/gpu Batch (t): 0.8117 LR: 0.000089 Step: 229950 Total Loss: 4.6662 Recon Loss: 4.6515 
[12/24 21:33:34 TiTok]: Data (t): 0.0011, 34.54/s/gpu Batch (t): 0.9266 LR: 0.000089 Step: 230000 Total Loss: 5.4059 Recon Loss: 5.3911 
[12/24 21:33:35 TiTok]: Reconstructing images...
[12/24 21:34:16 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000089 Step: 230050 Total Loss: 4.6781 Recon Loss: 4.6632 
[12/24 21:34:57 TiTok]: Data (t): 0.0010, 39.40/s/gpu Batch (t): 0.8121 LR: 0.000089 Step: 230100 Total Loss: 3.9091 Recon Loss: 3.8942 
[12/24 21:35:38 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000089 Step: 230150 Total Loss: 4.0106 Recon Loss: 3.9958 
[12/24 21:36:19 TiTok]: Data (t): 0.0011, 39.46/s/gpu Batch (t): 0.8109 LR: 0.000089 Step: 230200 Total Loss: 5.4568 Recon Loss: 5.4420 
[12/24 21:37:00 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000089 Step: 230250 Total Loss: 3.9813 Recon Loss: 3.9664 
[12/24 21:37:40 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000089 Step: 230300 Total Loss: 4.7093 Recon Loss: 4.6945 
[12/24 21:38:21 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000089 Step: 230350 Total Loss: 4.0335 Recon Loss: 4.0187 
[12/24 21:39:02 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000089 Step: 230400 Total Loss: 4.0797 Recon Loss: 4.0649 
[12/24 21:39:43 TiTok]: Data (t): 0.0010, 39.42/s/gpu Batch (t): 0.8118 LR: 0.000089 Step: 230450 Total Loss: 5.4059 Recon Loss: 5.3911 
Epoch 23/99 started.
[12/24 21:40:25 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000089 Step: 230500 Total Loss: 3.9040 Recon Loss: 3.8892 
[12/24 21:41:06 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000089 Step: 230550 Total Loss: 4.7450 Recon Loss: 4.7303 
[12/24 21:41:47 TiTok]: Data (t): 0.0010, 39.42/s/gpu Batch (t): 0.8117 LR: 0.000089 Step: 230600 Total Loss: 4.6588 Recon Loss: 4.6440 
[12/24 21:42:27 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000089 Step: 230650 Total Loss: 5.4372 Recon Loss: 5.4223 
[12/24 21:43:08 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000089 Step: 230700 Total Loss: 4.7488 Recon Loss: 4.7340 
[12/24 21:43:49 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000089 Step: 230750 Total Loss: 5.4001 Recon Loss: 5.3853 
[12/24 21:44:30 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000089 Step: 230800 Total Loss: 4.7080 Recon Loss: 4.6932 
[12/24 21:45:11 TiTok]: Data (t): 0.0011, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000089 Step: 230850 Total Loss: 4.6927 Recon Loss: 4.6780 
[12/24 21:45:52 TiTok]: Data (t): 0.0013, 39.24/s/gpu Batch (t): 0.8154 LR: 0.000089 Step: 230900 Total Loss: 4.7137 Recon Loss: 4.6989 
[12/24 21:46:33 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8127 LR: 0.000089 Step: 230950 Total Loss: 4.6818 Recon Loss: 4.6670 
[12/24 21:47:14 TiTok]: Data (t): 0.0010, 34.39/s/gpu Batch (t): 0.9306 LR: 0.000089 Step: 231000 Total Loss: 4.6805 Recon Loss: 4.6657 
[12/24 21:47:54 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000089 Step: 231050 Total Loss: 6.1221 Recon Loss: 6.1074 
[12/24 21:48:35 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000089 Step: 231100 Total Loss: 5.4245 Recon Loss: 5.4097 
[12/24 21:49:16 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000089 Step: 231150 Total Loss: 4.0291 Recon Loss: 4.0143 
[12/24 21:49:57 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000089 Step: 231200 Total Loss: 4.0085 Recon Loss: 3.9937 
[12/24 21:50:38 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000089 Step: 231250 Total Loss: 3.9790 Recon Loss: 3.9642 
[12/24 21:51:19 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000089 Step: 231300 Total Loss: 4.7076 Recon Loss: 4.6926 
[12/24 21:52:00 TiTok]: Data (t): 0.0011, 38.76/s/gpu Batch (t): 0.8257 LR: 0.000089 Step: 231350 Total Loss: 4.7252 Recon Loss: 4.7104 
[12/24 21:52:41 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000089 Step: 231400 Total Loss: 4.6503 Recon Loss: 4.6354 
[12/24 21:53:21 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000089 Step: 231450 Total Loss: 5.4631 Recon Loss: 5.4483 
[12/24 21:54:02 TiTok]: Data (t): 0.0010, 35.94/s/gpu Batch (t): 0.8904 LR: 0.000089 Step: 231500 Total Loss: 3.9893 Recon Loss: 3.9745 
[12/24 21:54:43 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000089 Step: 231550 Total Loss: 5.4408 Recon Loss: 5.4261 
[12/24 21:55:24 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000089 Step: 231600 Total Loss: 4.7152 Recon Loss: 4.7004 
[12/24 21:56:05 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000089 Step: 231650 Total Loss: 5.4218 Recon Loss: 5.4069 
[12/24 21:56:46 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000089 Step: 231700 Total Loss: 4.0036 Recon Loss: 3.9888 
[12/24 21:57:27 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000089 Step: 231750 Total Loss: 4.7007 Recon Loss: 4.6858 
[12/24 21:58:08 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8150 LR: 0.000089 Step: 231800 Total Loss: 4.7361 Recon Loss: 4.7213 
[12/24 21:58:48 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8125 LR: 0.000089 Step: 231850 Total Loss: 4.7099 Recon Loss: 4.6952 
[12/24 21:59:29 TiTok]: Data (t): 0.0011, 38.73/s/gpu Batch (t): 0.8262 LR: 0.000089 Step: 231900 Total Loss: 4.6361 Recon Loss: 4.6213 
[12/24 22:00:10 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000089 Step: 231950 Total Loss: 4.0531 Recon Loss: 4.0383 
[12/24 22:00:51 TiTok]: Data (t): 0.0013, 34.85/s/gpu Batch (t): 0.9182 LR: 0.000089 Step: 232000 Total Loss: 5.3526 Recon Loss: 5.3377 
[12/24 22:01:32 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000089 Step: 232050 Total Loss: 5.4221 Recon Loss: 5.4073 
[12/24 22:02:13 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000089 Step: 232100 Total Loss: 4.6703 Recon Loss: 4.6554 
[12/24 22:02:54 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000089 Step: 232150 Total Loss: 5.3866 Recon Loss: 5.3718 
[12/24 22:03:35 TiTok]: Data (t): 0.0011, 38.89/s/gpu Batch (t): 0.8228 LR: 0.000089 Step: 232200 Total Loss: 4.0373 Recon Loss: 4.0225 
[12/24 22:04:16 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8164 LR: 0.000089 Step: 232250 Total Loss: 4.7294 Recon Loss: 4.7145 
[12/24 22:04:56 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000089 Step: 232300 Total Loss: 4.7302 Recon Loss: 4.7154 
[12/24 22:05:37 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000089 Step: 232350 Total Loss: 3.9770 Recon Loss: 3.9621 
[12/24 22:06:18 TiTok]: Data (t): 0.0012, 39.10/s/gpu Batch (t): 0.8183 LR: 0.000089 Step: 232400 Total Loss: 4.6973 Recon Loss: 4.6824 
[12/24 22:06:59 TiTok]: Data (t): 0.0016, 39.21/s/gpu Batch (t): 0.8160 LR: 0.000089 Step: 232450 Total Loss: 5.4357 Recon Loss: 5.4209 
[12/24 22:07:40 TiTok]: Data (t): 0.0011, 39.39/s/gpu Batch (t): 0.8124 LR: 0.000089 Step: 232500 Total Loss: 3.9833 Recon Loss: 3.9684 
[12/24 22:08:21 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8200 LR: 0.000089 Step: 232550 Total Loss: 4.7579 Recon Loss: 4.7430 
[12/24 22:09:02 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000089 Step: 232600 Total Loss: 4.0559 Recon Loss: 4.0410 
[12/24 22:09:43 TiTok]: Data (t): 0.0011, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000089 Step: 232650 Total Loss: 4.0391 Recon Loss: 4.0242 
[12/24 22:10:24 TiTok]: Data (t): 0.0011, 39.37/s/gpu Batch (t): 0.8129 LR: 0.000089 Step: 232700 Total Loss: 4.7232 Recon Loss: 4.7084 
[12/24 22:11:04 TiTok]: Data (t): 0.0011, 39.35/s/gpu Batch (t): 0.8131 LR: 0.000089 Step: 232750 Total Loss: 5.4393 Recon Loss: 5.4245 
[12/24 22:11:45 TiTok]: Data (t): 0.0012, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000089 Step: 232800 Total Loss: 5.3741 Recon Loss: 5.3592 
[12/24 22:12:26 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000089 Step: 232850 Total Loss: 4.0198 Recon Loss: 4.0050 
[12/24 22:13:07 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000089 Step: 232900 Total Loss: 3.9863 Recon Loss: 3.9715 
[12/24 22:13:48 TiTok]: Data (t): 0.0023, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000089 Step: 232950 Total Loss: 5.4052 Recon Loss: 5.3904 
[12/24 22:14:29 TiTok]: Data (t): 0.0010, 34.91/s/gpu Batch (t): 0.9166 LR: 0.000089 Step: 233000 Total Loss: 3.9986 Recon Loss: 3.9838 
[12/24 22:15:10 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000089 Step: 233050 Total Loss: 4.6960 Recon Loss: 4.6811 
[12/24 22:15:51 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000089 Step: 233100 Total Loss: 4.6931 Recon Loss: 4.6783 
[12/24 22:16:32 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8204 LR: 0.000089 Step: 233150 Total Loss: 4.6818 Recon Loss: 4.6670 
[12/24 22:17:13 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000089 Step: 233200 Total Loss: 5.4266 Recon Loss: 5.4119 
[12/24 22:17:53 TiTok]: Data (t): 0.0014, 38.83/s/gpu Batch (t): 0.8241 LR: 0.000089 Step: 233250 Total Loss: 3.9912 Recon Loss: 3.9764 
[12/24 22:18:34 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000089 Step: 233300 Total Loss: 3.9838 Recon Loss: 3.9690 
[12/24 22:19:15 TiTok]: Data (t): 0.0011, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000089 Step: 233350 Total Loss: 5.4068 Recon Loss: 5.3920 
[12/24 22:19:56 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000089 Step: 233400 Total Loss: 4.1302 Recon Loss: 4.1154 
[12/24 22:20:37 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000089 Step: 233450 Total Loss: 6.1381 Recon Loss: 6.1232 
[12/24 22:21:18 TiTok]: Data (t): 0.0010, 39.44/s/gpu Batch (t): 0.8113 LR: 0.000089 Step: 233500 Total Loss: 5.4085 Recon Loss: 5.3937 
[12/24 22:21:59 TiTok]: Data (t): 0.0011, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000089 Step: 233550 Total Loss: 5.3270 Recon Loss: 5.3121 
[12/24 22:22:40 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000089 Step: 233600 Total Loss: 4.7266 Recon Loss: 4.7118 
[12/24 22:23:21 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000089 Step: 233650 Total Loss: 4.6869 Recon Loss: 4.6720 
[12/24 22:24:01 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8223 LR: 0.000089 Step: 233700 Total Loss: 4.7329 Recon Loss: 4.7180 
[12/24 22:24:42 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000089 Step: 233750 Total Loss: 4.7731 Recon Loss: 4.7583 
[12/24 22:25:23 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000089 Step: 233800 Total Loss: 5.4284 Recon Loss: 5.4136 
[12/24 22:26:04 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000089 Step: 233850 Total Loss: 4.7575 Recon Loss: 4.7426 
[12/24 22:26:45 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000089 Step: 233900 Total Loss: 4.0062 Recon Loss: 3.9913 
[12/24 22:27:26 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8181 LR: 0.000089 Step: 233950 Total Loss: 5.4472 Recon Loss: 5.4324 
[12/24 22:28:07 TiTok]: Data (t): 0.0010, 35.21/s/gpu Batch (t): 0.9089 LR: 0.000089 Step: 234000 Total Loss: 5.4425 Recon Loss: 5.4277 
[12/24 22:28:48 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000089 Step: 234050 Total Loss: 3.9777 Recon Loss: 3.9629 
[12/24 22:29:28 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000089 Step: 234100 Total Loss: 5.4326 Recon Loss: 5.4178 
[12/24 22:30:09 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8202 LR: 0.000089 Step: 234150 Total Loss: 5.4344 Recon Loss: 5.4196 
[12/24 22:30:50 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000089 Step: 234200 Total Loss: 3.9961 Recon Loss: 3.9813 
[12/24 22:31:31 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000089 Step: 234250 Total Loss: 4.7235 Recon Loss: 4.7087 
[12/24 22:32:12 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8202 LR: 0.000089 Step: 234300 Total Loss: 5.3803 Recon Loss: 5.3655 
[12/24 22:32:53 TiTok]: Data (t): 0.0011, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000089 Step: 234350 Total Loss: 5.3815 Recon Loss: 5.3667 
[12/24 22:33:34 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000089 Step: 234400 Total Loss: 4.0287 Recon Loss: 4.0138 
[12/24 22:34:15 TiTok]: Data (t): 0.0010, 38.75/s/gpu Batch (t): 0.8257 LR: 0.000089 Step: 234450 Total Loss: 4.7166 Recon Loss: 4.7019 
[12/24 22:34:56 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000089 Step: 234500 Total Loss: 3.9892 Recon Loss: 3.9745 
[12/24 22:35:36 TiTok]: Data (t): 0.0011, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000089 Step: 234550 Total Loss: 5.4033 Recon Loss: 5.3884 
[12/24 22:36:17 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000089 Step: 234600 Total Loss: 3.9998 Recon Loss: 3.9849 
[12/24 22:36:58 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000089 Step: 234650 Total Loss: 6.1337 Recon Loss: 6.1189 
[12/24 22:37:39 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000089 Step: 234700 Total Loss: 3.9931 Recon Loss: 3.9783 
[12/24 22:38:20 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000089 Step: 234750 Total Loss: 3.9818 Recon Loss: 3.9670 
[12/24 22:39:01 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000089 Step: 234800 Total Loss: 4.7113 Recon Loss: 4.6965 
[12/24 22:39:42 TiTok]: Data (t): 0.0010, 39.44/s/gpu Batch (t): 0.8114 LR: 0.000089 Step: 234850 Total Loss: 5.4043 Recon Loss: 5.3894 
[12/24 22:40:23 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000089 Step: 234900 Total Loss: 5.4026 Recon Loss: 5.3878 
[12/24 22:41:04 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000089 Step: 234950 Total Loss: 4.0101 Recon Loss: 3.9953 
[12/24 22:41:45 TiTok]: Data (t): 0.0011, 34.53/s/gpu Batch (t): 0.9267 LR: 0.000089 Step: 235000 Total Loss: 5.4156 Recon Loss: 5.4008 
[12/24 22:41:46 TiTok]: Reconstructing images...
[12/24 22:42:27 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000089 Step: 235050 Total Loss: 4.6591 Recon Loss: 4.6444 
[12/24 22:43:08 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000089 Step: 235100 Total Loss: 5.4099 Recon Loss: 5.3951 
[12/24 22:43:49 TiTok]: Data (t): 0.0010, 38.51/s/gpu Batch (t): 0.8309 LR: 0.000089 Step: 235150 Total Loss: 4.0214 Recon Loss: 4.0066 
[12/24 22:44:30 TiTok]: Data (t): 0.0033, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000089 Step: 235200 Total Loss: 5.4238 Recon Loss: 5.4090 
[12/24 22:45:11 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000089 Step: 235250 Total Loss: 3.9817 Recon Loss: 3.9668 
[12/24 22:45:52 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000089 Step: 235300 Total Loss: 4.7075 Recon Loss: 4.6928 
[12/24 22:46:32 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000089 Step: 235350 Total Loss: 4.0047 Recon Loss: 3.9899 
[12/24 22:47:13 TiTok]: Data (t): 0.0011, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000089 Step: 235400 Total Loss: 4.0305 Recon Loss: 4.0156 
[12/24 22:47:54 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8204 LR: 0.000089 Step: 235450 Total Loss: 4.6591 Recon Loss: 4.6442 
[12/24 22:48:35 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000089 Step: 235500 Total Loss: 4.7017 Recon Loss: 4.6868 
[12/24 22:49:16 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000089 Step: 235550 Total Loss: 5.4337 Recon Loss: 5.4188 
[12/24 22:49:57 TiTok]: Data (t): 0.0010, 39.45/s/gpu Batch (t): 0.8112 LR: 0.000089 Step: 235600 Total Loss: 5.4159 Recon Loss: 5.4010 
[12/24 22:50:38 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000089 Step: 235650 Total Loss: 4.0384 Recon Loss: 4.0236 
[12/24 22:51:18 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000089 Step: 235700 Total Loss: 4.7111 Recon Loss: 4.6963 
[12/24 22:51:59 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000089 Step: 235750 Total Loss: 4.6700 Recon Loss: 4.6552 
[12/24 22:52:40 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000089 Step: 235800 Total Loss: 4.7045 Recon Loss: 4.6897 
[12/24 22:53:21 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8221 LR: 0.000089 Step: 235850 Total Loss: 4.7442 Recon Loss: 4.7293 
[12/24 22:54:02 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000089 Step: 235900 Total Loss: 4.7564 Recon Loss: 4.7416 
[12/24 22:54:43 TiTok]: Data (t): 0.0011, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000089 Step: 235950 Total Loss: 4.6878 Recon Loss: 4.6730 
[12/24 22:55:24 TiTok]: Data (t): 0.0011, 34.37/s/gpu Batch (t): 0.9311 LR: 0.000089 Step: 236000 Total Loss: 4.7921 Recon Loss: 4.7772 
[12/24 22:56:05 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8158 LR: 0.000089 Step: 236050 Total Loss: 4.7098 Recon Loss: 4.6949 
[12/24 22:56:46 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000089 Step: 236100 Total Loss: 4.8013 Recon Loss: 4.7865 
[12/24 22:57:27 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000089 Step: 236150 Total Loss: 4.7474 Recon Loss: 4.7326 
[12/24 22:58:08 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000089 Step: 236200 Total Loss: 4.7177 Recon Loss: 4.7029 
[12/24 22:58:48 TiTok]: Data (t): 0.0011, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000089 Step: 236250 Total Loss: 4.7167 Recon Loss: 4.7020 
[12/24 22:59:29 TiTok]: Data (t): 0.0011, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000089 Step: 236300 Total Loss: 5.4052 Recon Loss: 5.3904 
[12/24 23:00:10 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000089 Step: 236350 Total Loss: 3.9531 Recon Loss: 3.9382 
[12/24 23:00:51 TiTok]: Data (t): 0.0010, 38.81/s/gpu Batch (t): 0.8246 LR: 0.000089 Step: 236400 Total Loss: 4.6450 Recon Loss: 4.6302 
[12/24 23:01:32 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000089 Step: 236450 Total Loss: 4.7103 Recon Loss: 4.6953 
[12/24 23:02:13 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000089 Step: 236500 Total Loss: 3.9096 Recon Loss: 3.8948 
[12/24 23:02:54 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000089 Step: 236550 Total Loss: 4.0490 Recon Loss: 4.0342 
[12/24 23:03:35 TiTok]: Data (t): 0.0011, 39.48/s/gpu Batch (t): 0.8105 LR: 0.000089 Step: 236600 Total Loss: 6.1223 Recon Loss: 6.1073 
[12/24 23:04:16 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000089 Step: 236650 Total Loss: 5.3933 Recon Loss: 5.3785 
[12/24 23:04:56 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8156 LR: 0.000089 Step: 236700 Total Loss: 5.4348 Recon Loss: 5.4200 
[12/24 23:05:37 TiTok]: Data (t): 0.0011, 39.35/s/gpu Batch (t): 0.8133 LR: 0.000089 Step: 236750 Total Loss: 4.6996 Recon Loss: 4.6848 
[12/24 23:06:18 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000089 Step: 236800 Total Loss: 4.6936 Recon Loss: 4.6788 
[12/24 23:06:59 TiTok]: Data (t): 0.0011, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000089 Step: 236850 Total Loss: 6.1267 Recon Loss: 6.1119 
[12/24 23:07:40 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8221 LR: 0.000089 Step: 236900 Total Loss: 4.7419 Recon Loss: 4.7271 
[12/24 23:08:21 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000089 Step: 236950 Total Loss: 3.9539 Recon Loss: 3.9390 
[12/24 23:09:02 TiTok]: Data (t): 0.0010, 35.07/s/gpu Batch (t): 0.9126 LR: 0.000089 Step: 237000 Total Loss: 5.3975 Recon Loss: 5.3827 
[12/24 23:09:43 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8158 LR: 0.000089 Step: 237050 Total Loss: 4.6805 Recon Loss: 4.6657 
[12/24 23:10:24 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000089 Step: 237100 Total Loss: 4.7051 Recon Loss: 4.6903 
[12/24 23:11:05 TiTok]: Data (t): 0.0010, 38.74/s/gpu Batch (t): 0.8259 LR: 0.000089 Step: 237150 Total Loss: 4.7496 Recon Loss: 4.7348 
[12/24 23:11:45 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000089 Step: 237200 Total Loss: 3.9886 Recon Loss: 3.9739 
[12/24 23:12:26 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000089 Step: 237250 Total Loss: 4.7203 Recon Loss: 4.7055 
[12/24 23:13:07 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8231 LR: 0.000089 Step: 237300 Total Loss: 3.9851 Recon Loss: 3.9703 
[12/24 23:13:48 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000089 Step: 237350 Total Loss: 4.6877 Recon Loss: 4.6729 
[12/24 23:14:29 TiTok]: Data (t): 0.0011, 38.81/s/gpu Batch (t): 0.8245 LR: 0.000089 Step: 237400 Total Loss: 4.7152 Recon Loss: 4.7004 
[12/24 23:15:10 TiTok]: Data (t): 0.0010, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000089 Step: 237450 Total Loss: 4.7205 Recon Loss: 4.7057 
[12/24 23:15:51 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000089 Step: 237500 Total Loss: 3.9616 Recon Loss: 3.9467 
[12/24 23:16:32 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000089 Step: 237550 Total Loss: 4.7865 Recon Loss: 4.7716 
[12/24 23:17:13 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000089 Step: 237600 Total Loss: 3.9572 Recon Loss: 3.9424 
[12/24 23:17:54 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8179 LR: 0.000089 Step: 237650 Total Loss: 4.7496 Recon Loss: 4.7348 
[12/24 23:18:34 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000089 Step: 237700 Total Loss: 3.9551 Recon Loss: 3.9402 
[12/24 23:19:15 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000089 Step: 237750 Total Loss: 5.4053 Recon Loss: 5.3904 
[12/24 23:19:56 TiTok]: Data (t): 0.0011, 39.35/s/gpu Batch (t): 0.8133 LR: 0.000089 Step: 237800 Total Loss: 4.7305 Recon Loss: 4.7156 
[12/24 23:20:37 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8173 LR: 0.000089 Step: 237850 Total Loss: 3.9841 Recon Loss: 3.9693 
[12/24 23:21:18 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000089 Step: 237900 Total Loss: 4.7099 Recon Loss: 4.6951 
[12/24 23:21:59 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000089 Step: 237950 Total Loss: 5.3952 Recon Loss: 5.3804 
[12/24 23:22:40 TiTok]: Data (t): 0.0011, 34.43/s/gpu Batch (t): 0.9294 LR: 0.000089 Step: 238000 Total Loss: 6.1238 Recon Loss: 6.1089 
[12/24 23:23:21 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000089 Step: 238050 Total Loss: 3.9581 Recon Loss: 3.9433 
[12/24 23:24:02 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000089 Step: 238100 Total Loss: 4.7274 Recon Loss: 4.7126 
[12/24 23:24:43 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000089 Step: 238150 Total Loss: 3.9831 Recon Loss: 3.9683 
[12/24 23:25:23 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000089 Step: 238200 Total Loss: 4.0448 Recon Loss: 4.0299 
[12/24 23:26:04 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000089 Step: 238250 Total Loss: 5.4439 Recon Loss: 5.4291 
[12/24 23:26:45 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8179 LR: 0.000089 Step: 238300 Total Loss: 4.6853 Recon Loss: 4.6706 
[12/24 23:27:26 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000089 Step: 238350 Total Loss: 4.7331 Recon Loss: 4.7183 
[12/24 23:28:07 TiTok]: Data (t): 0.0012, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000089 Step: 238400 Total Loss: 4.6880 Recon Loss: 4.6732 
[12/24 23:28:48 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000089 Step: 238450 Total Loss: 4.7600 Recon Loss: 4.7452 
[12/24 23:29:29 TiTok]: Data (t): 0.0011, 38.85/s/gpu Batch (t): 0.8237 LR: 0.000089 Step: 238500 Total Loss: 4.7072 Recon Loss: 4.6924 
[12/24 23:30:10 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000089 Step: 238550 Total Loss: 5.4182 Recon Loss: 5.4033 
[12/24 23:30:51 TiTok]: Data (t): 0.0011, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000089 Step: 238600 Total Loss: 4.6821 Recon Loss: 4.6673 
[12/24 23:31:32 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000089 Step: 238650 Total Loss: 3.9993 Recon Loss: 3.9844 
[12/24 23:32:12 TiTok]: Data (t): 0.0011, 38.91/s/gpu Batch (t): 0.8223 LR: 0.000089 Step: 238700 Total Loss: 4.6896 Recon Loss: 4.6747 
[12/24 23:32:53 TiTok]: Data (t): 0.0011, 38.82/s/gpu Batch (t): 0.8242 LR: 0.000089 Step: 238750 Total Loss: 5.4109 Recon Loss: 5.3961 
[12/24 23:33:34 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000089 Step: 238800 Total Loss: 4.0347 Recon Loss: 4.0199 
[12/24 23:34:15 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8181 LR: 0.000089 Step: 238850 Total Loss: 4.0653 Recon Loss: 4.0504 
[12/24 23:34:56 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000089 Step: 238900 Total Loss: 5.4073 Recon Loss: 5.3925 
[12/24 23:35:37 TiTok]: Data (t): 0.0011, 38.18/s/gpu Batch (t): 0.8381 LR: 0.000089 Step: 238950 Total Loss: 4.7505 Recon Loss: 4.7357 
[12/24 23:36:18 TiTok]: Data (t): 0.0011, 33.99/s/gpu Batch (t): 0.9415 LR: 0.000089 Step: 239000 Total Loss: 4.6826 Recon Loss: 4.6677 
[12/24 23:36:59 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000089 Step: 239050 Total Loss: 4.0219 Recon Loss: 4.0071 
[12/24 23:37:40 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000089 Step: 239100 Total Loss: 4.0265 Recon Loss: 4.0116 
[12/24 23:38:20 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000089 Step: 239150 Total Loss: 6.1385 Recon Loss: 6.1237 
[12/24 23:39:01 TiTok]: Data (t): 0.0011, 39.48/s/gpu Batch (t): 0.8105 LR: 0.000089 Step: 239200 Total Loss: 5.3997 Recon Loss: 5.3849 
[12/24 23:39:42 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000089 Step: 239250 Total Loss: 5.3965 Recon Loss: 5.3816 
[12/24 23:40:23 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000089 Step: 239300 Total Loss: 4.6855 Recon Loss: 4.6708 
[12/24 23:41:04 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000089 Step: 239350 Total Loss: 6.1427 Recon Loss: 6.1279 
[12/24 23:41:45 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000089 Step: 239400 Total Loss: 5.4439 Recon Loss: 5.4291 
[12/24 23:42:26 TiTok]: Data (t): 0.0010, 39.40/s/gpu Batch (t): 0.8121 LR: 0.000089 Step: 239450 Total Loss: 5.3762 Recon Loss: 5.3614 
[12/24 23:43:07 TiTok]: Data (t): 0.0011, 38.80/s/gpu Batch (t): 0.8247 LR: 0.000089 Step: 239500 Total Loss: 4.0708 Recon Loss: 4.0559 
[12/24 23:43:48 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000089 Step: 239550 Total Loss: 4.7369 Recon Loss: 4.7220 
[12/24 23:44:28 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000089 Step: 239600 Total Loss: 4.7168 Recon Loss: 4.7020 
[12/24 23:45:09 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000089 Step: 239650 Total Loss: 4.0632 Recon Loss: 4.0483 
[12/24 23:45:50 TiTok]: Data (t): 0.0010, 38.83/s/gpu Batch (t): 0.8242 LR: 0.000089 Step: 239700 Total Loss: 4.0099 Recon Loss: 3.9951 
[12/24 23:46:31 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8204 LR: 0.000089 Step: 239750 Total Loss: 3.9790 Recon Loss: 3.9642 
[12/24 23:47:12 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000089 Step: 239800 Total Loss: 5.3916 Recon Loss: 5.3768 
[12/24 23:47:53 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000089 Step: 239850 Total Loss: 4.7081 Recon Loss: 4.6934 
[12/24 23:48:34 TiTok]: Data (t): 0.0011, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000089 Step: 239900 Total Loss: 3.9695 Recon Loss: 3.9548 
[12/24 23:49:15 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8137 LR: 0.000089 Step: 239950 Total Loss: 5.4067 Recon Loss: 5.3919 
[12/24 23:49:56 TiTok]: Data (t): 0.0010, 35.40/s/gpu Batch (t): 0.9039 LR: 0.000089 Step: 240000 Total Loss: 4.6368 Recon Loss: 4.6219 
[12/24 23:49:57 TiTok]: Reconstructing images...
[12/24 23:50:38 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000089 Step: 240050 Total Loss: 4.6919 Recon Loss: 4.6771 
[12/24 23:51:19 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000089 Step: 240100 Total Loss: 4.0121 Recon Loss: 3.9973 
[12/24 23:52:00 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000089 Step: 240150 Total Loss: 5.3969 Recon Loss: 5.3821 
[12/24 23:52:41 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000089 Step: 240200 Total Loss: 5.3886 Recon Loss: 5.3738 
[12/24 23:53:22 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000089 Step: 240250 Total Loss: 4.6877 Recon Loss: 4.6728 
[12/24 23:54:02 TiTok]: Data (t): 0.0013, 39.15/s/gpu Batch (t): 0.8175 LR: 0.000089 Step: 240300 Total Loss: 4.7200 Recon Loss: 4.7052 
[12/24 23:54:43 TiTok]: Data (t): 0.0012, 39.26/s/gpu Batch (t): 0.8150 LR: 0.000089 Step: 240350 Total Loss: 3.9603 Recon Loss: 3.9456 
[12/24 23:55:24 TiTok]: Data (t): 0.0011, 39.40/s/gpu Batch (t): 0.8121 LR: 0.000089 Step: 240400 Total Loss: 4.0183 Recon Loss: 4.0034 
[12/24 23:56:05 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000089 Step: 240450 Total Loss: 4.7523 Recon Loss: 4.7374 
Epoch 24/99 started.
[12/24 23:56:47 TiTok]: Data (t): 0.0011, 39.43/s/gpu Batch (t): 0.8116 LR: 0.000088 Step: 240500 Total Loss: 5.3822 Recon Loss: 5.3674 
[12/24 23:57:28 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000088 Step: 240550 Total Loss: 4.7060 Recon Loss: 4.6912 
[12/24 23:58:09 TiTok]: Data (t): 0.0011, 39.38/s/gpu Batch (t): 0.8127 LR: 0.000088 Step: 240600 Total Loss: 5.3931 Recon Loss: 5.3783 
[12/24 23:58:50 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000088 Step: 240650 Total Loss: 3.9520 Recon Loss: 3.9372 
[12/24 23:59:31 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8221 LR: 0.000088 Step: 240700 Total Loss: 4.0105 Recon Loss: 3.9957 
[12/25 00:00:11 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8181 LR: 0.000088 Step: 240750 Total Loss: 4.0143 Recon Loss: 3.9995 
[12/25 00:00:52 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000088 Step: 240800 Total Loss: 4.6635 Recon Loss: 4.6487 
[12/25 00:01:33 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8187 LR: 0.000088 Step: 240850 Total Loss: 5.3601 Recon Loss: 5.3454 
[12/25 00:02:14 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8196 LR: 0.000088 Step: 240900 Total Loss: 4.6920 Recon Loss: 4.6772 
[12/25 00:02:55 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8209 LR: 0.000088 Step: 240950 Total Loss: 4.6465 Recon Loss: 4.6317 
[12/25 00:03:36 TiTok]: Data (t): 0.0011, 34.09/s/gpu Batch (t): 0.9387 LR: 0.000088 Step: 241000 Total Loss: 4.6994 Recon Loss: 4.6845 
[12/25 00:04:17 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8129 LR: 0.000088 Step: 241050 Total Loss: 6.1163 Recon Loss: 6.1014 
[12/25 00:04:58 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000088 Step: 241100 Total Loss: 4.7524 Recon Loss: 4.7377 
[12/25 00:05:38 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000088 Step: 241150 Total Loss: 4.6548 Recon Loss: 4.6400 
[12/25 00:06:19 TiTok]: Data (t): 0.0009, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000088 Step: 241200 Total Loss: 3.9750 Recon Loss: 3.9602 
[12/25 00:07:00 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000088 Step: 241250 Total Loss: 6.1360 Recon Loss: 6.1211 
[12/25 00:07:41 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000088 Step: 241300 Total Loss: 6.1081 Recon Loss: 6.0934 
[12/25 00:08:22 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000088 Step: 241350 Total Loss: 4.0616 Recon Loss: 4.0467 
[12/25 00:09:03 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000088 Step: 241400 Total Loss: 3.9628 Recon Loss: 3.9479 
[12/25 00:09:44 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8221 LR: 0.000088 Step: 241450 Total Loss: 4.0253 Recon Loss: 4.0105 
[12/25 00:10:25 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000088 Step: 241500 Total Loss: 3.9996 Recon Loss: 3.9848 
[12/25 00:11:06 TiTok]: Data (t): 0.0011, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000088 Step: 241550 Total Loss: 4.7688 Recon Loss: 4.7540 
[12/25 00:11:47 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000088 Step: 241600 Total Loss: 4.6520 Recon Loss: 4.6372 
[12/25 00:12:27 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000088 Step: 241650 Total Loss: 4.0137 Recon Loss: 3.9990 
[12/25 00:13:08 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000088 Step: 241700 Total Loss: 3.9059 Recon Loss: 3.8911 
[12/25 00:13:49 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000088 Step: 241750 Total Loss: 5.4038 Recon Loss: 5.3890 
[12/25 00:14:30 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000088 Step: 241800 Total Loss: 3.9984 Recon Loss: 3.9836 
[12/25 00:15:11 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000088 Step: 241850 Total Loss: 4.7524 Recon Loss: 4.7376 
[12/25 00:15:52 TiTok]: Data (t): 0.0011, 39.38/s/gpu Batch (t): 0.8125 LR: 0.000088 Step: 241900 Total Loss: 4.7102 Recon Loss: 4.6954 
[12/25 00:16:33 TiTok]: Data (t): 0.0013, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000088 Step: 241950 Total Loss: 4.0082 Recon Loss: 3.9935 
[12/25 00:17:14 TiTok]: Data (t): 0.0011, 35.02/s/gpu Batch (t): 0.9138 LR: 0.000088 Step: 242000 Total Loss: 3.9657 Recon Loss: 3.9509 
[12/25 00:17:54 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000088 Step: 242050 Total Loss: 4.7694 Recon Loss: 4.7545 
[12/25 00:18:35 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000088 Step: 242100 Total Loss: 4.0207 Recon Loss: 4.0059 
[12/25 00:19:16 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8177 LR: 0.000088 Step: 242150 Total Loss: 4.0646 Recon Loss: 4.0498 
[12/25 00:19:57 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000088 Step: 242200 Total Loss: 5.4501 Recon Loss: 5.4352 
[12/25 00:20:38 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000088 Step: 242250 Total Loss: 3.9891 Recon Loss: 3.9742 
[12/25 00:21:19 TiTok]: Data (t): 0.0009, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000088 Step: 242300 Total Loss: 4.7217 Recon Loss: 4.7069 
[12/25 00:22:00 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8150 LR: 0.000088 Step: 242350 Total Loss: 6.0781 Recon Loss: 6.0633 
[12/25 00:22:41 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000088 Step: 242400 Total Loss: 4.6933 Recon Loss: 4.6785 
[12/25 00:23:21 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8152 LR: 0.000088 Step: 242450 Total Loss: 4.6804 Recon Loss: 4.6655 
[12/25 00:24:02 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000088 Step: 242500 Total Loss: 4.0677 Recon Loss: 4.0530 
[12/25 00:24:43 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000088 Step: 242550 Total Loss: 5.3948 Recon Loss: 5.3800 
[12/25 00:25:24 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8137 LR: 0.000088 Step: 242600 Total Loss: 4.7297 Recon Loss: 4.7148 
[12/25 00:26:05 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000088 Step: 242650 Total Loss: 4.0006 Recon Loss: 3.9858 
[12/25 00:26:46 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8156 LR: 0.000088 Step: 242700 Total Loss: 4.0375 Recon Loss: 4.0227 
[12/25 00:27:27 TiTok]: Data (t): 0.0010, 38.83/s/gpu Batch (t): 0.8241 LR: 0.000088 Step: 242750 Total Loss: 4.0176 Recon Loss: 4.0026 
[12/25 00:28:08 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000088 Step: 242800 Total Loss: 5.4120 Recon Loss: 5.3971 
[12/25 00:28:48 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000088 Step: 242850 Total Loss: 4.0415 Recon Loss: 4.0267 
[12/25 00:29:29 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8208 LR: 0.000088 Step: 242900 Total Loss: 4.6774 Recon Loss: 4.6625 
[12/25 00:30:10 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8223 LR: 0.000088 Step: 242950 Total Loss: 4.0613 Recon Loss: 4.0465 
[12/25 00:30:51 TiTok]: Data (t): 0.0010, 35.16/s/gpu Batch (t): 0.9101 LR: 0.000088 Step: 243000 Total Loss: 5.3761 Recon Loss: 5.3612 
[12/25 00:31:32 TiTok]: Data (t): 0.0011, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000088 Step: 243050 Total Loss: 5.3797 Recon Loss: 5.3648 
[12/25 00:32:13 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8202 LR: 0.000088 Step: 243100 Total Loss: 3.9844 Recon Loss: 3.9696 
[12/25 00:32:54 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000088 Step: 243150 Total Loss: 5.4310 Recon Loss: 5.4161 
[12/25 00:33:35 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000088 Step: 243200 Total Loss: 4.0567 Recon Loss: 4.0420 
[12/25 00:34:16 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000088 Step: 243250 Total Loss: 5.4001 Recon Loss: 5.3852 
[12/25 00:34:56 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000088 Step: 243300 Total Loss: 4.7259 Recon Loss: 4.7111 
[12/25 00:35:37 TiTok]: Data (t): 0.0011, 38.83/s/gpu Batch (t): 0.8241 LR: 0.000088 Step: 243350 Total Loss: 4.7426 Recon Loss: 4.7276 
[12/25 00:36:18 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000088 Step: 243400 Total Loss: 5.3896 Recon Loss: 5.3748 
[12/25 00:36:59 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8196 LR: 0.000088 Step: 243450 Total Loss: 5.4237 Recon Loss: 5.4089 
[12/25 00:37:40 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000088 Step: 243500 Total Loss: 3.9610 Recon Loss: 3.9462 
[12/25 00:38:21 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000088 Step: 243550 Total Loss: 5.4098 Recon Loss: 5.3949 
[12/25 00:39:02 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000088 Step: 243600 Total Loss: 5.4011 Recon Loss: 5.3862 
[12/25 00:39:42 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000088 Step: 243650 Total Loss: 4.7200 Recon Loss: 4.7052 
[12/25 00:40:23 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8181 LR: 0.000088 Step: 243700 Total Loss: 4.7041 Recon Loss: 4.6893 
[12/25 00:41:04 TiTok]: Data (t): 0.0010, 39.39/s/gpu Batch (t): 0.8123 LR: 0.000088 Step: 243750 Total Loss: 4.6440 Recon Loss: 4.6292 
[12/25 00:41:45 TiTok]: Data (t): 0.0010, 38.87/s/gpu Batch (t): 0.8232 LR: 0.000088 Step: 243800 Total Loss: 5.3824 Recon Loss: 5.3676 
[12/25 00:42:26 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000088 Step: 243850 Total Loss: 4.7015 Recon Loss: 4.6868 
[12/25 00:43:07 TiTok]: Data (t): 0.0010, 38.83/s/gpu Batch (t): 0.8242 LR: 0.000088 Step: 243900 Total Loss: 4.0175 Recon Loss: 4.0027 
[12/25 00:43:48 TiTok]: Data (t): 0.0010, 38.72/s/gpu Batch (t): 0.8264 LR: 0.000088 Step: 243950 Total Loss: 5.4212 Recon Loss: 5.4064 
[12/25 00:44:29 TiTok]: Data (t): 0.0010, 35.49/s/gpu Batch (t): 0.9016 LR: 0.000088 Step: 244000 Total Loss: 4.6185 Recon Loss: 4.6037 
[12/25 00:45:10 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000088 Step: 244050 Total Loss: 4.6934 Recon Loss: 4.6786 
[12/25 00:45:50 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000088 Step: 244100 Total Loss: 3.9317 Recon Loss: 3.9169 
[12/25 00:46:31 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000088 Step: 244150 Total Loss: 6.1163 Recon Loss: 6.1016 
[12/25 00:47:12 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000088 Step: 244200 Total Loss: 5.3963 Recon Loss: 5.3815 
[12/25 00:47:53 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000088 Step: 244250 Total Loss: 4.6213 Recon Loss: 4.6065 
[12/25 00:48:34 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8209 LR: 0.000088 Step: 244300 Total Loss: 4.7133 Recon Loss: 4.6985 
[12/25 00:49:15 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000088 Step: 244350 Total Loss: 4.0150 Recon Loss: 4.0001 
[12/25 00:49:56 TiTok]: Data (t): 0.0011, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000088 Step: 244400 Total Loss: 5.4231 Recon Loss: 5.4081 
[12/25 00:50:37 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000088 Step: 244450 Total Loss: 4.0074 Recon Loss: 3.9925 
[12/25 00:51:18 TiTok]: Data (t): 0.0012, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000088 Step: 244500 Total Loss: 4.0590 Recon Loss: 4.0442 
[12/25 00:51:58 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000088 Step: 244550 Total Loss: 4.7199 Recon Loss: 4.7050 
[12/25 00:52:39 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000088 Step: 244600 Total Loss: 4.6966 Recon Loss: 4.6819 
[12/25 00:53:20 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000088 Step: 244650 Total Loss: 4.7130 Recon Loss: 4.6982 
[12/25 00:54:01 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000088 Step: 244700 Total Loss: 4.0780 Recon Loss: 4.0632 
[12/25 00:54:42 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000088 Step: 244750 Total Loss: 4.0359 Recon Loss: 4.0211 
[12/25 00:55:23 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000088 Step: 244800 Total Loss: 5.3959 Recon Loss: 5.3811 
[12/25 00:56:04 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000088 Step: 244850 Total Loss: 4.7426 Recon Loss: 4.7278 
[12/25 00:56:45 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000088 Step: 244900 Total Loss: 3.9836 Recon Loss: 3.9689 
[12/25 00:57:26 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000088 Step: 244950 Total Loss: 3.9820 Recon Loss: 3.9671 
[12/25 00:58:07 TiTok]: Data (t): 0.0011, 35.26/s/gpu Batch (t): 0.9076 LR: 0.000088 Step: 245000 Total Loss: 4.7335 Recon Loss: 4.7187 
[12/25 00:58:08 TiTok]: Reconstructing images...
[12/25 00:58:49 TiTok]: Data (t): 0.0011, 39.42/s/gpu Batch (t): 0.8118 LR: 0.000088 Step: 245050 Total Loss: 4.0284 Recon Loss: 4.0136 
[12/25 00:59:30 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000088 Step: 245100 Total Loss: 4.6809 Recon Loss: 4.6660 
[12/25 01:00:11 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8127 LR: 0.000088 Step: 245150 Total Loss: 4.6703 Recon Loss: 4.6555 
[12/25 01:00:52 TiTok]: Data (t): 0.0011, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000088 Step: 245200 Total Loss: 3.9982 Recon Loss: 3.9834 
[12/25 01:01:33 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000088 Step: 245250 Total Loss: 4.6889 Recon Loss: 4.6741 
[12/25 01:02:14 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000088 Step: 245300 Total Loss: 4.0048 Recon Loss: 3.9901 
[12/25 01:02:54 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000088 Step: 245350 Total Loss: 4.0389 Recon Loss: 4.0241 
[12/25 01:03:35 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8231 LR: 0.000088 Step: 245400 Total Loss: 4.7353 Recon Loss: 4.7205 
[12/25 01:04:16 TiTok]: Data (t): 0.0011, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000088 Step: 245450 Total Loss: 3.9559 Recon Loss: 3.9411 
[12/25 01:04:57 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000088 Step: 245500 Total Loss: 4.0704 Recon Loss: 4.0556 
[12/25 01:05:38 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8181 LR: 0.000088 Step: 245550 Total Loss: 4.7288 Recon Loss: 4.7140 
[12/25 01:06:19 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000088 Step: 245600 Total Loss: 5.3561 Recon Loss: 5.3414 
[12/25 01:07:00 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000088 Step: 245650 Total Loss: 5.4077 Recon Loss: 5.3928 
[12/25 01:07:41 TiTok]: Data (t): 0.0011, 39.40/s/gpu Batch (t): 0.8123 LR: 0.000088 Step: 245700 Total Loss: 6.8431 Recon Loss: 6.8283 
[12/25 01:08:22 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000088 Step: 245750 Total Loss: 4.6705 Recon Loss: 4.6557 
[12/25 01:09:03 TiTok]: Data (t): 0.0011, 39.38/s/gpu Batch (t): 0.8125 LR: 0.000088 Step: 245800 Total Loss: 5.4011 Recon Loss: 5.3863 
[12/25 01:09:43 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000088 Step: 245850 Total Loss: 5.3897 Recon Loss: 5.3749 
[12/25 01:10:24 TiTok]: Data (t): 0.0010, 38.82/s/gpu Batch (t): 0.8244 LR: 0.000088 Step: 245900 Total Loss: 3.9435 Recon Loss: 3.9287 
[12/25 01:11:05 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000088 Step: 245950 Total Loss: 3.9589 Recon Loss: 3.9441 
[12/25 01:11:46 TiTok]: Data (t): 0.0010, 34.29/s/gpu Batch (t): 0.9332 LR: 0.000088 Step: 246000 Total Loss: 4.6284 Recon Loss: 4.6136 
[12/25 01:12:27 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000088 Step: 246050 Total Loss: 5.3925 Recon Loss: 5.3777 
[12/25 01:13:08 TiTok]: Data (t): 0.0011, 38.95/s/gpu Batch (t): 0.8217 LR: 0.000088 Step: 246100 Total Loss: 3.9719 Recon Loss: 3.9572 
[12/25 01:13:49 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000088 Step: 246150 Total Loss: 5.4698 Recon Loss: 5.4550 
[12/25 01:14:30 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000088 Step: 246200 Total Loss: 3.9439 Recon Loss: 3.9290 
[12/25 01:15:11 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8209 LR: 0.000088 Step: 246250 Total Loss: 4.6827 Recon Loss: 4.6679 
[12/25 01:15:52 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000088 Step: 246300 Total Loss: 4.6830 Recon Loss: 4.6682 
[12/25 01:16:32 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000088 Step: 246350 Total Loss: 4.7349 Recon Loss: 4.7201 
[12/25 01:17:13 TiTok]: Data (t): 0.0011, 38.81/s/gpu Batch (t): 0.8246 LR: 0.000088 Step: 246400 Total Loss: 3.9999 Recon Loss: 3.9850 
[12/25 01:17:54 TiTok]: Data (t): 0.0011, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000088 Step: 246450 Total Loss: 4.7689 Recon Loss: 4.7541 
[12/25 01:18:35 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000088 Step: 246500 Total Loss: 4.7304 Recon Loss: 4.7156 
[12/25 01:19:16 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000088 Step: 246550 Total Loss: 4.6773 Recon Loss: 4.6625 
[12/25 01:19:57 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000088 Step: 246600 Total Loss: 3.9500 Recon Loss: 3.9352 
[12/25 01:20:38 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000088 Step: 246650 Total Loss: 5.4548 Recon Loss: 5.4400 
[12/25 01:21:19 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000088 Step: 246700 Total Loss: 4.0102 Recon Loss: 3.9953 
[12/25 01:22:00 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8154 LR: 0.000088 Step: 246750 Total Loss: 5.4063 Recon Loss: 5.3914 
[12/25 01:22:41 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000088 Step: 246800 Total Loss: 4.0055 Recon Loss: 3.9908 
[12/25 01:23:22 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000088 Step: 246850 Total Loss: 5.4400 Recon Loss: 5.4252 
[12/25 01:24:02 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000088 Step: 246900 Total Loss: 4.0348 Recon Loss: 4.0199 
[12/25 01:24:43 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000088 Step: 246950 Total Loss: 5.4131 Recon Loss: 5.3982 
[12/25 01:25:24 TiTok]: Data (t): 0.0011, 35.55/s/gpu Batch (t): 0.9002 LR: 0.000088 Step: 247000 Total Loss: 4.6394 Recon Loss: 4.6245 
[12/25 01:26:05 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8229 LR: 0.000088 Step: 247050 Total Loss: 4.6951 Recon Loss: 4.6802 
[12/25 01:26:46 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000088 Step: 247100 Total Loss: 3.9057 Recon Loss: 3.8910 
[12/25 01:27:27 TiTok]: Data (t): 0.0022, 39.52/s/gpu Batch (t): 0.8097 LR: 0.000088 Step: 247150 Total Loss: 4.0757 Recon Loss: 4.0609 
[12/25 01:28:08 TiTok]: Data (t): 0.0021, 39.62/s/gpu Batch (t): 0.8078 LR: 0.000088 Step: 247200 Total Loss: 4.7487 Recon Loss: 4.7339 
[12/25 01:28:49 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000088 Step: 247250 Total Loss: 4.7178 Recon Loss: 4.7030 
[12/25 01:29:30 TiTok]: Data (t): 0.0011, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000088 Step: 247300 Total Loss: 4.0552 Recon Loss: 4.0403 
[12/25 01:30:10 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000088 Step: 247350 Total Loss: 3.9784 Recon Loss: 3.9635 
[12/25 01:30:51 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000088 Step: 247400 Total Loss: 4.7253 Recon Loss: 4.7105 
[12/25 01:31:32 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8154 LR: 0.000088 Step: 247450 Total Loss: 4.0174 Recon Loss: 4.0025 
[12/25 01:32:13 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8202 LR: 0.000088 Step: 247500 Total Loss: 4.0072 Recon Loss: 3.9924 
[12/25 01:32:54 TiTok]: Data (t): 0.0011, 38.58/s/gpu Batch (t): 0.8295 LR: 0.000088 Step: 247550 Total Loss: 5.4195 Recon Loss: 5.4047 
[12/25 01:33:35 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000088 Step: 247600 Total Loss: 3.9557 Recon Loss: 3.9408 
[12/25 01:34:16 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000088 Step: 247650 Total Loss: 3.8904 Recon Loss: 3.8756 
[12/25 01:34:57 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000088 Step: 247700 Total Loss: 5.4296 Recon Loss: 5.4147 
[12/25 01:35:38 TiTok]: Data (t): 0.0011, 38.85/s/gpu Batch (t): 0.8237 LR: 0.000088 Step: 247750 Total Loss: 6.1401 Recon Loss: 6.1252 
[12/25 01:36:18 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000088 Step: 247800 Total Loss: 4.7879 Recon Loss: 4.7731 
[12/25 01:36:59 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000088 Step: 247850 Total Loss: 4.6662 Recon Loss: 4.6513 
[12/25 01:37:40 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000088 Step: 247900 Total Loss: 4.6874 Recon Loss: 4.6726 
[12/25 01:38:21 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000088 Step: 247950 Total Loss: 5.3865 Recon Loss: 5.3716 
[12/25 01:39:02 TiTok]: Data (t): 0.0010, 34.53/s/gpu Batch (t): 0.9267 LR: 0.000088 Step: 248000 Total Loss: 4.0408 Recon Loss: 4.0259 
[12/25 01:39:43 TiTok]: Data (t): 0.0011, 39.40/s/gpu Batch (t): 0.8121 LR: 0.000088 Step: 248050 Total Loss: 6.1191 Recon Loss: 6.1043 
[12/25 01:40:24 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8202 LR: 0.000088 Step: 248100 Total Loss: 4.7606 Recon Loss: 4.7457 
[12/25 01:41:05 TiTok]: Data (t): 0.0010, 38.81/s/gpu Batch (t): 0.8245 LR: 0.000088 Step: 248150 Total Loss: 4.0070 Recon Loss: 3.9921 
[12/25 01:41:46 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000088 Step: 248200 Total Loss: 6.1338 Recon Loss: 6.1189 
[12/25 01:42:27 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000088 Step: 248250 Total Loss: 4.7127 Recon Loss: 4.6979 
[12/25 01:43:07 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000088 Step: 248300 Total Loss: 4.6709 Recon Loss: 4.6561 
[12/25 01:43:48 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000088 Step: 248350 Total Loss: 3.9249 Recon Loss: 3.9101 
[12/25 01:44:29 TiTok]: Data (t): 0.0012, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000088 Step: 248400 Total Loss: 3.9635 Recon Loss: 3.9486 
[12/25 01:45:10 TiTok]: Data (t): 0.0012, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000088 Step: 248450 Total Loss: 5.4362 Recon Loss: 5.4214 
[12/25 01:45:51 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000088 Step: 248500 Total Loss: 5.4155 Recon Loss: 5.4007 
[12/25 01:46:32 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000088 Step: 248550 Total Loss: 3.9088 Recon Loss: 3.8940 
[12/25 01:47:13 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000088 Step: 248600 Total Loss: 4.7309 Recon Loss: 4.7161 
[12/25 01:47:54 TiTok]: Data (t): 0.0011, 39.53/s/gpu Batch (t): 0.8096 LR: 0.000088 Step: 248650 Total Loss: 4.0156 Recon Loss: 4.0009 
[12/25 01:48:35 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000088 Step: 248700 Total Loss: 5.4275 Recon Loss: 5.4127 
[12/25 01:49:16 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000088 Step: 248750 Total Loss: 4.0232 Recon Loss: 4.0084 
[12/25 01:49:57 TiTok]: Data (t): 0.0011, 38.63/s/gpu Batch (t): 0.8284 LR: 0.000088 Step: 248800 Total Loss: 4.0029 Recon Loss: 3.9881 
[12/25 01:50:38 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000088 Step: 248850 Total Loss: 4.6770 Recon Loss: 4.6623 
[12/25 01:51:18 TiTok]: Data (t): 0.0011, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000088 Step: 248900 Total Loss: 4.6943 Recon Loss: 4.6794 
[12/25 01:51:59 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000088 Step: 248950 Total Loss: 4.7314 Recon Loss: 4.7166 
[12/25 01:52:40 TiTok]: Data (t): 0.0011, 34.97/s/gpu Batch (t): 0.9152 LR: 0.000088 Step: 249000 Total Loss: 4.7304 Recon Loss: 4.7157 
[12/25 01:53:21 TiTok]: Data (t): 0.0011, 39.39/s/gpu Batch (t): 0.8124 LR: 0.000088 Step: 249050 Total Loss: 5.3271 Recon Loss: 5.3124 
[12/25 01:54:02 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000088 Step: 249100 Total Loss: 4.0424 Recon Loss: 4.0276 
[12/25 01:54:43 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000088 Step: 249150 Total Loss: 3.9991 Recon Loss: 3.9843 
[12/25 01:55:24 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000088 Step: 249200 Total Loss: 5.3911 Recon Loss: 5.3762 
[12/25 01:56:05 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8154 LR: 0.000088 Step: 249250 Total Loss: 5.4405 Recon Loss: 5.4256 
[12/25 01:56:46 TiTok]: Data (t): 0.0011, 38.76/s/gpu Batch (t): 0.8256 LR: 0.000088 Step: 249300 Total Loss: 4.6436 Recon Loss: 4.6288 
[12/25 01:57:27 TiTok]: Data (t): 0.0011, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000088 Step: 249350 Total Loss: 4.6559 Recon Loss: 4.6411 
[12/25 01:58:08 TiTok]: Data (t): 0.0016, 38.92/s/gpu Batch (t): 0.8223 LR: 0.000088 Step: 249400 Total Loss: 3.9514 Recon Loss: 3.9366 
[12/25 01:58:48 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000088 Step: 249450 Total Loss: 3.9620 Recon Loss: 3.9472 
[12/25 01:59:29 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000088 Step: 249500 Total Loss: 4.0231 Recon Loss: 4.0083 
[12/25 02:00:10 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000088 Step: 249550 Total Loss: 4.7210 Recon Loss: 4.7062 
[12/25 02:00:51 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000088 Step: 249600 Total Loss: 4.0472 Recon Loss: 4.0324 
[12/25 02:01:32 TiTok]: Data (t): 0.0012, 38.98/s/gpu Batch (t): 0.8209 LR: 0.000088 Step: 249650 Total Loss: 4.7331 Recon Loss: 4.7183 
[12/25 02:02:13 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000088 Step: 249700 Total Loss: 3.9603 Recon Loss: 3.9454 
[12/25 02:02:54 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8198 LR: 0.000088 Step: 249750 Total Loss: 4.6291 Recon Loss: 4.6144 
[12/25 02:03:35 TiTok]: Data (t): 0.0012, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000088 Step: 249800 Total Loss: 5.3923 Recon Loss: 5.3775 
[12/25 02:04:16 TiTok]: Data (t): 0.0010, 39.48/s/gpu Batch (t): 0.8106 LR: 0.000088 Step: 249850 Total Loss: 5.4294 Recon Loss: 5.4146 
[12/25 02:04:57 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000088 Step: 249900 Total Loss: 6.1537 Recon Loss: 6.1390 
[12/25 02:05:37 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8221 LR: 0.000088 Step: 249950 Total Loss: 4.7340 Recon Loss: 4.7193 
[12/25 02:06:18 TiTok]: Data (t): 0.0011, 35.93/s/gpu Batch (t): 0.8907 LR: 0.000088 Step: 250000 Total Loss: 6.1312 Recon Loss: 6.1165 
Model weights saved in titok_b64_stage1_run1/checkpoint-250000/unwrapped_model/pytorch_model.bin
[12/25 02:06:20 TiTok]: Saved state to titok_b64_stage1_run1/checkpoint-250000
Model weights saved in titok_b64_stage1_run1/checkpoint-250000/ema_model/pytorch_model.bin
[12/25 02:06:37 TiTok]: Reconstructing images...
[12/25 02:06:38 TiTok]: Computing metrics on the validation set.
[12/25 02:21:22 TiTok]: EMA EVALUATION Step: 250000 
[12/25 02:21:22 TiTok]: {'CodebookEntropy': tensor(11.6439, device='cuda:0', dtype=torch.float64),
 'CodebookUsage': 0.784423828125,
 'InceptionScore': 36.0447791008942,
 'rFID': 89.34955594773976}
[12/25 02:22:28 TiTok]: Data (t): 0.0010, 39.64/s/gpu Batch (t): 0.8073 LR: 0.000088 Step: 250050 Total Loss: 4.6820 Recon Loss: 4.6672 
[12/25 02:23:08 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000088 Step: 250100 Total Loss: 4.0169 Recon Loss: 4.0021 
[12/25 02:23:49 TiTok]: Data (t): 0.0010, 39.51/s/gpu Batch (t): 0.8100 LR: 0.000088 Step: 250150 Total Loss: 5.4019 Recon Loss: 5.3872 
[12/25 02:24:30 TiTok]: Data (t): 0.0011, 39.41/s/gpu Batch (t): 0.8119 LR: 0.000088 Step: 250200 Total Loss: 5.4169 Recon Loss: 5.4021 
[12/25 02:25:11 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000088 Step: 250250 Total Loss: 4.7658 Recon Loss: 4.7509 
[12/25 02:25:52 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8217 LR: 0.000088 Step: 250300 Total Loss: 3.9518 Recon Loss: 3.9369 
[12/25 02:26:33 TiTok]: Data (t): 0.0011, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000088 Step: 250350 Total Loss: 4.6703 Recon Loss: 4.6556 
[12/25 02:27:13 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8175 LR: 0.000088 Step: 250400 Total Loss: 4.0372 Recon Loss: 4.0223 
[12/25 02:27:54 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000088 Step: 250450 Total Loss: 3.9731 Recon Loss: 3.9583 
[12/25 02:28:35 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8225 LR: 0.000088 Step: 250500 Total Loss: 6.0956 Recon Loss: 6.0808 
Epoch 25/99 started.
[12/25 02:29:17 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000088 Step: 250550 Total Loss: 4.0692 Recon Loss: 4.0544 
[12/25 02:29:58 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8162 LR: 0.000088 Step: 250600 Total Loss: 4.6520 Recon Loss: 4.6372 
[12/25 02:30:39 TiTok]: Data (t): 0.0010, 39.42/s/gpu Batch (t): 0.8118 LR: 0.000088 Step: 250650 Total Loss: 6.8271 Recon Loss: 6.8124 
[12/25 02:31:20 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000088 Step: 250700 Total Loss: 4.7009 Recon Loss: 4.6862 
[12/25 02:32:00 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000088 Step: 250750 Total Loss: 4.6899 Recon Loss: 4.6750 
[12/25 02:32:41 TiTok]: Data (t): 0.0010, 39.48/s/gpu Batch (t): 0.8105 LR: 0.000087 Step: 250800 Total Loss: 4.7030 Recon Loss: 4.6882 
[12/25 02:33:22 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000087 Step: 250850 Total Loss: 4.7086 Recon Loss: 4.6937 
[12/25 02:34:03 TiTok]: Data (t): 0.0013, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000087 Step: 250900 Total Loss: 4.7304 Recon Loss: 4.7155 
[12/25 02:34:44 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000087 Step: 250950 Total Loss: 3.9035 Recon Loss: 3.8886 
[12/25 02:35:25 TiTok]: Data (t): 0.0011, 34.48/s/gpu Batch (t): 0.9280 LR: 0.000087 Step: 251000 Total Loss: 4.0103 Recon Loss: 3.9954 
[12/25 02:36:06 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000087 Step: 251050 Total Loss: 4.7009 Recon Loss: 4.6861 
[12/25 02:36:47 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000087 Step: 251100 Total Loss: 5.3843 Recon Loss: 5.3694 
[12/25 02:37:28 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000087 Step: 251150 Total Loss: 3.8763 Recon Loss: 3.8615 
[12/25 02:38:08 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000087 Step: 251200 Total Loss: 5.3787 Recon Loss: 5.3639 
[12/25 02:38:49 TiTok]: Data (t): 0.0009, 38.56/s/gpu Batch (t): 0.8298 LR: 0.000087 Step: 251250 Total Loss: 3.9889 Recon Loss: 3.9742 
[12/25 02:39:30 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000087 Step: 251300 Total Loss: 4.0282 Recon Loss: 4.0134 
[12/25 02:40:11 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000087 Step: 251350 Total Loss: 4.6789 Recon Loss: 4.6641 
[12/25 02:40:52 TiTok]: Data (t): 0.0022, 38.87/s/gpu Batch (t): 0.8233 LR: 0.000087 Step: 251400 Total Loss: 4.7101 Recon Loss: 4.6952 
[12/25 02:41:33 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000087 Step: 251450 Total Loss: 4.7547 Recon Loss: 4.7398 
[12/25 02:42:14 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000087 Step: 251500 Total Loss: 5.4421 Recon Loss: 5.4272 
[12/25 02:42:55 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000087 Step: 251550 Total Loss: 5.4351 Recon Loss: 5.4203 
[12/25 02:43:36 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000087 Step: 251600 Total Loss: 4.7328 Recon Loss: 4.7180 
[12/25 02:44:17 TiTok]: Data (t): 0.0011, 38.85/s/gpu Batch (t): 0.8236 LR: 0.000087 Step: 251650 Total Loss: 5.4088 Recon Loss: 5.3940 
[12/25 02:44:57 TiTok]: Data (t): 0.0010, 38.74/s/gpu Batch (t): 0.8260 LR: 0.000087 Step: 251700 Total Loss: 4.1038 Recon Loss: 4.0890 
[12/25 02:45:38 TiTok]: Data (t): 0.0011, 39.40/s/gpu Batch (t): 0.8121 LR: 0.000087 Step: 251750 Total Loss: 4.0160 Recon Loss: 4.0011 
[12/25 02:46:19 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000087 Step: 251800 Total Loss: 4.6616 Recon Loss: 4.6467 
[12/25 02:47:00 TiTok]: Data (t): 0.0011, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000087 Step: 251850 Total Loss: 3.9446 Recon Loss: 3.9297 
[12/25 02:47:41 TiTok]: Data (t): 0.0010, 38.80/s/gpu Batch (t): 0.8248 LR: 0.000087 Step: 251900 Total Loss: 3.9588 Recon Loss: 3.9440 
[12/25 02:48:22 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000087 Step: 251950 Total Loss: 6.0857 Recon Loss: 6.0709 
[12/25 02:49:03 TiTok]: Data (t): 0.0010, 35.20/s/gpu Batch (t): 0.9091 LR: 0.000087 Step: 252000 Total Loss: 4.7416 Recon Loss: 4.7267 
[12/25 02:49:44 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8196 LR: 0.000087 Step: 252050 Total Loss: 5.4366 Recon Loss: 5.4217 
[12/25 02:50:25 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8204 LR: 0.000087 Step: 252100 Total Loss: 4.6494 Recon Loss: 4.6346 
[12/25 02:51:05 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000087 Step: 252150 Total Loss: 5.3550 Recon Loss: 5.3402 
[12/25 02:51:46 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000087 Step: 252200 Total Loss: 4.6387 Recon Loss: 4.6239 
[12/25 02:52:27 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000087 Step: 252250 Total Loss: 5.4194 Recon Loss: 5.4045 
[12/25 02:53:08 TiTok]: Data (t): 0.0011, 39.40/s/gpu Batch (t): 0.8122 LR: 0.000087 Step: 252300 Total Loss: 6.1033 Recon Loss: 6.0885 
[12/25 02:53:49 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000087 Step: 252350 Total Loss: 4.7033 Recon Loss: 4.6885 
[12/25 02:54:30 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000087 Step: 252400 Total Loss: 5.4250 Recon Loss: 5.4102 
[12/25 02:55:11 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000087 Step: 252450 Total Loss: 5.3924 Recon Loss: 5.3775 
[12/25 02:55:52 TiTok]: Data (t): 0.0010, 39.39/s/gpu Batch (t): 0.8123 LR: 0.000087 Step: 252500 Total Loss: 4.6878 Recon Loss: 4.6730 
[12/25 02:56:33 TiTok]: Data (t): 0.0017, 39.49/s/gpu Batch (t): 0.8104 LR: 0.000087 Step: 252550 Total Loss: 4.7134 Recon Loss: 4.6985 
[12/25 02:57:14 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000087 Step: 252600 Total Loss: 4.0110 Recon Loss: 3.9962 
[12/25 02:57:55 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000087 Step: 252650 Total Loss: 3.9703 Recon Loss: 3.9556 
[12/25 02:58:35 TiTok]: Data (t): 0.0011, 38.85/s/gpu Batch (t): 0.8237 LR: 0.000087 Step: 252700 Total Loss: 4.7210 Recon Loss: 4.7061 
[12/25 02:59:16 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000087 Step: 252750 Total Loss: 4.6790 Recon Loss: 4.6642 
[12/25 02:59:57 TiTok]: Data (t): 0.0011, 38.80/s/gpu Batch (t): 0.8248 LR: 0.000087 Step: 252800 Total Loss: 5.3980 Recon Loss: 5.3832 
[12/25 03:00:38 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000087 Step: 252850 Total Loss: 4.6859 Recon Loss: 4.6711 
[12/25 03:01:19 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8206 LR: 0.000087 Step: 252900 Total Loss: 5.3775 Recon Loss: 5.3627 
[12/25 03:02:00 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000087 Step: 252950 Total Loss: 4.0262 Recon Loss: 4.0114 
[12/25 03:02:41 TiTok]: Data (t): 0.0010, 34.95/s/gpu Batch (t): 0.9156 LR: 0.000087 Step: 253000 Total Loss: 5.4301 Recon Loss: 5.4153 
[12/25 03:03:22 TiTok]: Data (t): 0.0010, 38.85/s/gpu Batch (t): 0.8238 LR: 0.000087 Step: 253050 Total Loss: 5.4080 Recon Loss: 5.3932 
[12/25 03:04:03 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000087 Step: 253100 Total Loss: 4.0563 Recon Loss: 4.0415 
[12/25 03:04:44 TiTok]: Data (t): 0.0010, 38.81/s/gpu Batch (t): 0.8245 LR: 0.000087 Step: 253150 Total Loss: 4.0003 Recon Loss: 3.9855 
[12/25 03:05:25 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000087 Step: 253200 Total Loss: 3.9157 Recon Loss: 3.9009 
[12/25 03:06:06 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000087 Step: 253250 Total Loss: 4.7579 Recon Loss: 4.7430 
[12/25 03:06:46 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000087 Step: 253300 Total Loss: 4.0352 Recon Loss: 4.0204 
[12/25 03:07:27 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000087 Step: 253350 Total Loss: 4.0906 Recon Loss: 4.0758 
[12/25 03:08:08 TiTok]: Data (t): 0.0010, 38.42/s/gpu Batch (t): 0.8329 LR: 0.000087 Step: 253400 Total Loss: 3.9740 Recon Loss: 3.9591 
[12/25 03:08:49 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000087 Step: 253450 Total Loss: 4.0385 Recon Loss: 4.0237 
[12/25 03:09:30 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000087 Step: 253500 Total Loss: 4.6616 Recon Loss: 4.6468 
[12/25 03:10:11 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000087 Step: 253550 Total Loss: 4.7067 Recon Loss: 4.6918 
[12/25 03:10:52 TiTok]: Data (t): 0.0011, 39.51/s/gpu Batch (t): 0.8099 LR: 0.000087 Step: 253600 Total Loss: 3.9830 Recon Loss: 3.9682 
[12/25 03:11:33 TiTok]: Data (t): 0.0011, 39.48/s/gpu Batch (t): 0.8106 LR: 0.000087 Step: 253650 Total Loss: 5.4113 Recon Loss: 5.3965 
[12/25 03:12:14 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000087 Step: 253700 Total Loss: 5.3968 Recon Loss: 5.3819 
[12/25 03:12:55 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000087 Step: 253750 Total Loss: 4.0580 Recon Loss: 4.0433 
[12/25 03:13:36 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000087 Step: 253800 Total Loss: 5.4940 Recon Loss: 5.4792 
[12/25 03:14:16 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8209 LR: 0.000087 Step: 253850 Total Loss: 4.6973 Recon Loss: 4.6825 
[12/25 03:14:57 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000087 Step: 253900 Total Loss: 4.0090 Recon Loss: 3.9942 
[12/25 03:15:38 TiTok]: Data (t): 0.0011, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000087 Step: 253950 Total Loss: 3.9977 Recon Loss: 3.9828 
[12/25 03:16:19 TiTok]: Data (t): 0.0010, 34.46/s/gpu Batch (t): 0.9285 LR: 0.000087 Step: 254000 Total Loss: 5.4230 Recon Loss: 5.4082 
[12/25 03:17:00 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8154 LR: 0.000087 Step: 254050 Total Loss: 4.0208 Recon Loss: 4.0059 
[12/25 03:17:41 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000087 Step: 254100 Total Loss: 4.6957 Recon Loss: 4.6808 
[12/25 03:18:22 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8204 LR: 0.000087 Step: 254150 Total Loss: 4.6805 Recon Loss: 4.6656 
[12/25 03:19:03 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8200 LR: 0.000087 Step: 254200 Total Loss: 6.1229 Recon Loss: 6.1081 
[12/25 03:19:44 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000087 Step: 254250 Total Loss: 4.6782 Recon Loss: 4.6634 
[12/25 03:20:25 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000087 Step: 254300 Total Loss: 5.3919 Recon Loss: 5.3771 
[12/25 03:21:05 TiTok]: Data (t): 0.0011, 39.48/s/gpu Batch (t): 0.8105 LR: 0.000087 Step: 254350 Total Loss: 4.6920 Recon Loss: 4.6770 
[12/25 03:21:46 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000087 Step: 254400 Total Loss: 3.9966 Recon Loss: 3.9818 
[12/25 03:22:27 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000087 Step: 254450 Total Loss: 3.9870 Recon Loss: 3.9722 
[12/25 03:23:08 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000087 Step: 254500 Total Loss: 5.3757 Recon Loss: 5.3608 
[12/25 03:23:49 TiTok]: Data (t): 0.0019, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000087 Step: 254550 Total Loss: 4.7083 Recon Loss: 4.6935 
[12/25 03:24:30 TiTok]: Data (t): 0.0011, 39.43/s/gpu Batch (t): 0.8116 LR: 0.000087 Step: 254600 Total Loss: 4.6994 Recon Loss: 4.6846 
[12/25 03:25:11 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000087 Step: 254650 Total Loss: 5.4334 Recon Loss: 5.4185 
[12/25 03:25:52 TiTok]: Data (t): 0.0011, 39.33/s/gpu Batch (t): 0.8137 LR: 0.000087 Step: 254700 Total Loss: 4.6835 Recon Loss: 4.6687 
[12/25 03:26:33 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000087 Step: 254750 Total Loss: 4.6193 Recon Loss: 4.6045 
[12/25 03:27:14 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000087 Step: 254800 Total Loss: 4.0430 Recon Loss: 4.0282 
[12/25 03:27:54 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000087 Step: 254850 Total Loss: 3.9049 Recon Loss: 3.8901 
[12/25 03:28:35 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000087 Step: 254900 Total Loss: 4.7236 Recon Loss: 4.7088 
[12/25 03:29:16 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000087 Step: 254950 Total Loss: 4.7773 Recon Loss: 4.7625 
[12/25 03:29:57 TiTok]: Data (t): 0.0011, 34.92/s/gpu Batch (t): 0.9163 LR: 0.000087 Step: 255000 Total Loss: 6.1773 Recon Loss: 6.1624 
[12/25 03:29:58 TiTok]: Reconstructing images...
[12/25 03:30:40 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000087 Step: 255050 Total Loss: 5.3598 Recon Loss: 5.3450 
[12/25 03:31:21 TiTok]: Data (t): 0.0011, 38.82/s/gpu Batch (t): 0.8243 LR: 0.000087 Step: 255100 Total Loss: 3.9320 Recon Loss: 3.9171 
[12/25 03:32:01 TiTok]: Data (t): 0.0011, 39.42/s/gpu Batch (t): 0.8118 LR: 0.000087 Step: 255150 Total Loss: 5.4802 Recon Loss: 5.4653 
[12/25 03:32:42 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8229 LR: 0.000087 Step: 255200 Total Loss: 6.1185 Recon Loss: 6.1037 
[12/25 03:33:23 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000087 Step: 255250 Total Loss: 4.6880 Recon Loss: 4.6733 
[12/25 03:34:04 TiTok]: Data (t): 0.0010, 38.74/s/gpu Batch (t): 0.8260 LR: 0.000087 Step: 255300 Total Loss: 4.6070 Recon Loss: 4.5921 
[12/25 03:34:45 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000087 Step: 255350 Total Loss: 5.4198 Recon Loss: 5.4050 
[12/25 03:35:26 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000087 Step: 255400 Total Loss: 4.7474 Recon Loss: 4.7325 
[12/25 03:36:07 TiTok]: Data (t): 0.0010, 38.74/s/gpu Batch (t): 0.8261 LR: 0.000087 Step: 255450 Total Loss: 4.7537 Recon Loss: 4.7388 
[12/25 03:36:48 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000087 Step: 255500 Total Loss: 3.9680 Recon Loss: 3.9531 
[12/25 03:37:29 TiTok]: Data (t): 0.0011, 39.46/s/gpu Batch (t): 0.8110 LR: 0.000087 Step: 255550 Total Loss: 4.7396 Recon Loss: 4.7248 
[12/25 03:38:10 TiTok]: Data (t): 0.0010, 39.46/s/gpu Batch (t): 0.8109 LR: 0.000087 Step: 255600 Total Loss: 3.9790 Recon Loss: 3.9642 
[12/25 03:38:51 TiTok]: Data (t): 0.0011, 39.44/s/gpu Batch (t): 0.8113 LR: 0.000087 Step: 255650 Total Loss: 5.4065 Recon Loss: 5.3917 
[12/25 03:39:31 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000087 Step: 255700 Total Loss: 5.3964 Recon Loss: 5.3815 
[12/25 03:40:12 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000087 Step: 255750 Total Loss: 4.0063 Recon Loss: 3.9915 
[12/25 03:40:53 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000087 Step: 255800 Total Loss: 5.3851 Recon Loss: 5.3702 
[12/25 03:41:34 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000087 Step: 255850 Total Loss: 4.7881 Recon Loss: 4.7732 
[12/25 03:42:15 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000087 Step: 255900 Total Loss: 5.3966 Recon Loss: 5.3819 
[12/25 03:42:56 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000087 Step: 255950 Total Loss: 3.9454 Recon Loss: 3.9306 
[12/25 03:43:37 TiTok]: Data (t): 0.0011, 33.70/s/gpu Batch (t): 0.9496 LR: 0.000087 Step: 256000 Total Loss: 4.6870 Recon Loss: 4.6721 
[12/25 03:44:18 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8152 LR: 0.000087 Step: 256050 Total Loss: 3.9577 Recon Loss: 3.9429 
[12/25 03:44:59 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8177 LR: 0.000087 Step: 256100 Total Loss: 4.7438 Recon Loss: 4.7290 
[12/25 03:45:40 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000087 Step: 256150 Total Loss: 3.9919 Recon Loss: 3.9771 
[12/25 03:46:20 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000087 Step: 256200 Total Loss: 4.0548 Recon Loss: 4.0399 
[12/25 03:47:01 TiTok]: Data (t): 0.0016, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000087 Step: 256250 Total Loss: 5.3915 Recon Loss: 5.3766 
[12/25 03:47:42 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000087 Step: 256300 Total Loss: 3.9270 Recon Loss: 3.9123 
[12/25 03:48:23 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000087 Step: 256350 Total Loss: 5.3767 Recon Loss: 5.3620 
[12/25 03:49:04 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000087 Step: 256400 Total Loss: 5.4428 Recon Loss: 5.4281 
[12/25 03:49:45 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000087 Step: 256450 Total Loss: 4.7019 Recon Loss: 4.6870 
[12/25 03:50:26 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000087 Step: 256500 Total Loss: 3.9802 Recon Loss: 3.9653 
[12/25 03:51:07 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000087 Step: 256550 Total Loss: 4.6758 Recon Loss: 4.6610 
[12/25 03:51:48 TiTok]: Data (t): 0.0011, 39.42/s/gpu Batch (t): 0.8117 LR: 0.000087 Step: 256600 Total Loss: 5.3494 Recon Loss: 5.3345 
[12/25 03:52:29 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000087 Step: 256650 Total Loss: 6.0862 Recon Loss: 6.0713 
[12/25 03:53:09 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000087 Step: 256700 Total Loss: 3.9877 Recon Loss: 3.9729 
[12/25 03:53:50 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000087 Step: 256750 Total Loss: 6.1318 Recon Loss: 6.1169 
[12/25 03:54:31 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8181 LR: 0.000087 Step: 256800 Total Loss: 5.4263 Recon Loss: 5.4114 
[12/25 03:55:12 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000087 Step: 256850 Total Loss: 4.6767 Recon Loss: 4.6619 
[12/25 03:55:53 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000087 Step: 256900 Total Loss: 3.9441 Recon Loss: 3.9293 
[12/25 03:56:34 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000087 Step: 256950 Total Loss: 3.9653 Recon Loss: 3.9504 
[12/25 03:57:15 TiTok]: Data (t): 0.0011, 34.91/s/gpu Batch (t): 0.9167 LR: 0.000087 Step: 257000 Total Loss: 5.3984 Recon Loss: 5.3836 
[12/25 03:57:56 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000087 Step: 257050 Total Loss: 6.1036 Recon Loss: 6.0887 
[12/25 03:58:36 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000087 Step: 257100 Total Loss: 3.9638 Recon Loss: 3.9489 
[12/25 03:59:17 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000087 Step: 257150 Total Loss: 4.6914 Recon Loss: 4.6766 
[12/25 03:59:58 TiTok]: Data (t): 0.0011, 38.86/s/gpu Batch (t): 0.8235 LR: 0.000087 Step: 257200 Total Loss: 4.6720 Recon Loss: 4.6571 
[12/25 04:00:39 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000087 Step: 257250 Total Loss: 3.9690 Recon Loss: 3.9542 
[12/25 04:01:20 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000087 Step: 257300 Total Loss: 4.0031 Recon Loss: 3.9882 
[12/25 04:02:01 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8200 LR: 0.000087 Step: 257350 Total Loss: 4.6319 Recon Loss: 4.6170 
[12/25 04:02:42 TiTok]: Data (t): 0.0011, 38.78/s/gpu Batch (t): 0.8251 LR: 0.000087 Step: 257400 Total Loss: 4.6737 Recon Loss: 4.6589 
[12/25 04:03:23 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000087 Step: 257450 Total Loss: 5.4016 Recon Loss: 5.3868 
[12/25 04:04:03 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8231 LR: 0.000087 Step: 257500 Total Loss: 3.9613 Recon Loss: 3.9465 
[12/25 04:04:44 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000087 Step: 257550 Total Loss: 3.9415 Recon Loss: 3.9267 
[12/25 04:05:25 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000087 Step: 257600 Total Loss: 4.6512 Recon Loss: 4.6364 
[12/25 04:06:06 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8229 LR: 0.000087 Step: 257650 Total Loss: 4.6936 Recon Loss: 4.6788 
[12/25 04:06:47 TiTok]: Data (t): 0.0010, 38.73/s/gpu Batch (t): 0.8263 LR: 0.000087 Step: 257700 Total Loss: 4.7200 Recon Loss: 4.7052 
[12/25 04:07:28 TiTok]: Data (t): 0.0010, 39.52/s/gpu Batch (t): 0.8097 LR: 0.000087 Step: 257750 Total Loss: 4.6933 Recon Loss: 4.6785 
[12/25 04:08:09 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8217 LR: 0.000087 Step: 257800 Total Loss: 4.6969 Recon Loss: 4.6821 
[12/25 04:08:50 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000087 Step: 257850 Total Loss: 4.5766 Recon Loss: 4.5618 
[12/25 04:09:31 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000087 Step: 257900 Total Loss: 4.7478 Recon Loss: 4.7330 
[12/25 04:10:12 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000087 Step: 257950 Total Loss: 3.9694 Recon Loss: 3.9546 
[12/25 04:10:53 TiTok]: Data (t): 0.0011, 34.98/s/gpu Batch (t): 0.9149 LR: 0.000087 Step: 258000 Total Loss: 5.4084 Recon Loss: 5.3937 
[12/25 04:11:33 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000087 Step: 258050 Total Loss: 4.0106 Recon Loss: 3.9957 
[12/25 04:12:14 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000087 Step: 258100 Total Loss: 4.6278 Recon Loss: 4.6129 
[12/25 04:12:55 TiTok]: Data (t): 0.0012, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000087 Step: 258150 Total Loss: 5.3473 Recon Loss: 5.3326 
[12/25 04:13:36 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000087 Step: 258200 Total Loss: 4.6616 Recon Loss: 4.6468 
[12/25 04:14:17 TiTok]: Data (t): 0.0011, 38.79/s/gpu Batch (t): 0.8249 LR: 0.000087 Step: 258250 Total Loss: 4.0460 Recon Loss: 4.0312 
[12/25 04:14:58 TiTok]: Data (t): 0.0010, 39.42/s/gpu Batch (t): 0.8118 LR: 0.000087 Step: 258300 Total Loss: 5.4121 Recon Loss: 5.3972 
[12/25 04:15:39 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000087 Step: 258350 Total Loss: 5.3875 Recon Loss: 5.3726 
[12/25 04:16:19 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000087 Step: 258400 Total Loss: 5.4299 Recon Loss: 5.4151 
[12/25 04:17:00 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000087 Step: 258450 Total Loss: 4.0000 Recon Loss: 3.9852 
[12/25 04:17:41 TiTok]: Data (t): 0.0011, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000087 Step: 258500 Total Loss: 4.0147 Recon Loss: 3.9998 
[12/25 04:18:22 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000087 Step: 258550 Total Loss: 5.4292 Recon Loss: 5.4143 
[12/25 04:19:03 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000087 Step: 258600 Total Loss: 5.3311 Recon Loss: 5.3163 
[12/25 04:19:44 TiTok]: Data (t): 0.0010, 38.87/s/gpu Batch (t): 0.8232 LR: 0.000087 Step: 258650 Total Loss: 3.9677 Recon Loss: 3.9529 
[12/25 04:20:25 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000087 Step: 258700 Total Loss: 5.3694 Recon Loss: 5.3546 
[12/25 04:21:06 TiTok]: Data (t): 0.0011, 38.89/s/gpu Batch (t): 0.8227 LR: 0.000087 Step: 258750 Total Loss: 3.8852 Recon Loss: 3.8703 
[12/25 04:21:47 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000087 Step: 258800 Total Loss: 5.3873 Recon Loss: 5.3724 
[12/25 04:22:27 TiTok]: Data (t): 0.0011, 38.87/s/gpu Batch (t): 0.8232 LR: 0.000087 Step: 258850 Total Loss: 4.6694 Recon Loss: 4.6545 
[12/25 04:23:08 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000087 Step: 258900 Total Loss: 4.0297 Recon Loss: 4.0149 
[12/25 04:23:49 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000087 Step: 258950 Total Loss: 4.7057 Recon Loss: 4.6909 
[12/25 04:24:30 TiTok]: Data (t): 0.0011, 35.27/s/gpu Batch (t): 0.9073 LR: 0.000087 Step: 259000 Total Loss: 4.6432 Recon Loss: 4.6284 
[12/25 04:25:11 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000087 Step: 259050 Total Loss: 4.7068 Recon Loss: 4.6919 
[12/25 04:25:52 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8164 LR: 0.000087 Step: 259100 Total Loss: 4.6551 Recon Loss: 4.6403 
[12/25 04:26:33 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000087 Step: 259150 Total Loss: 4.6605 Recon Loss: 4.6457 
[12/25 04:27:14 TiTok]: Data (t): 0.0010, 38.09/s/gpu Batch (t): 0.8401 LR: 0.000087 Step: 259200 Total Loss: 4.6642 Recon Loss: 4.6494 
[12/25 04:27:55 TiTok]: Data (t): 0.0010, 38.17/s/gpu Batch (t): 0.8384 LR: 0.000087 Step: 259250 Total Loss: 3.9371 Recon Loss: 3.9223 
[12/25 04:28:35 TiTok]: Data (t): 0.0011, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000087 Step: 259300 Total Loss: 5.4011 Recon Loss: 5.3863 
[12/25 04:29:16 TiTok]: Data (t): 0.0010, 39.48/s/gpu Batch (t): 0.8106 LR: 0.000087 Step: 259350 Total Loss: 4.7445 Recon Loss: 4.7296 
[12/25 04:29:57 TiTok]: Data (t): 0.0011, 37.60/s/gpu Batch (t): 0.8510 LR: 0.000087 Step: 259400 Total Loss: 4.0373 Recon Loss: 4.0225 
[12/25 04:30:38 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8228 LR: 0.000087 Step: 259450 Total Loss: 5.4257 Recon Loss: 5.4108 
[12/25 04:31:19 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000087 Step: 259500 Total Loss: 4.7024 Recon Loss: 4.6875 
[12/25 04:32:00 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000087 Step: 259550 Total Loss: 4.7379 Recon Loss: 4.7231 
[12/25 04:32:41 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000087 Step: 259600 Total Loss: 3.8959 Recon Loss: 3.8811 
[12/25 04:33:22 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000087 Step: 259650 Total Loss: 6.1387 Recon Loss: 6.1238 
[12/25 04:34:02 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8231 LR: 0.000087 Step: 259700 Total Loss: 3.9886 Recon Loss: 3.9738 
[12/25 04:34:43 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000087 Step: 259750 Total Loss: 3.9771 Recon Loss: 3.9623 
[12/25 04:35:24 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000087 Step: 259800 Total Loss: 4.7144 Recon Loss: 4.6996 
[12/25 04:36:05 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000087 Step: 259850 Total Loss: 3.9264 Recon Loss: 3.9116 
[12/25 04:36:46 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000087 Step: 259900 Total Loss: 4.0259 Recon Loss: 4.0111 
[12/25 04:37:27 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8223 LR: 0.000087 Step: 259950 Total Loss: 4.7077 Recon Loss: 4.6928 
[12/25 04:38:08 TiTok]: Data (t): 0.0010, 35.31/s/gpu Batch (t): 0.9064 LR: 0.000087 Step: 260000 Total Loss: 4.6578 Recon Loss: 4.6429 
[12/25 04:38:09 TiTok]: Reconstructing images...
[12/25 04:38:50 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000087 Step: 260050 Total Loss: 4.6690 Recon Loss: 4.6543 
[12/25 04:39:31 TiTok]: Data (t): 0.0011, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000087 Step: 260100 Total Loss: 6.1326 Recon Loss: 6.1178 
[12/25 04:40:12 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8177 LR: 0.000087 Step: 260150 Total Loss: 5.4036 Recon Loss: 5.3887 
[12/25 04:40:53 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000087 Step: 260200 Total Loss: 4.6362 Recon Loss: 4.6214 
[12/25 04:41:34 TiTok]: Data (t): 0.0016, 39.35/s/gpu Batch (t): 0.8133 LR: 0.000087 Step: 260250 Total Loss: 3.9817 Recon Loss: 3.9668 
[12/25 04:42:15 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000087 Step: 260300 Total Loss: 4.7053 Recon Loss: 4.6904 
[12/25 04:42:56 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000087 Step: 260350 Total Loss: 4.6893 Recon Loss: 4.6745 
[12/25 04:43:36 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000087 Step: 260400 Total Loss: 4.6406 Recon Loss: 4.6257 
[12/25 04:44:17 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000087 Step: 260450 Total Loss: 3.9724 Recon Loss: 3.9576 
[12/25 04:44:58 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000087 Step: 260500 Total Loss: 4.6832 Recon Loss: 4.6683 
Epoch 26/99 started.
[12/25 04:45:40 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000087 Step: 260550 Total Loss: 4.7107 Recon Loss: 4.6958 
[12/25 04:46:21 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000087 Step: 260600 Total Loss: 3.9568 Recon Loss: 3.9420 
[12/25 04:47:02 TiTok]: Data (t): 0.0011, 39.52/s/gpu Batch (t): 0.8098 LR: 0.000087 Step: 260650 Total Loss: 5.3324 Recon Loss: 5.3175 
[12/25 04:47:43 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000087 Step: 260700 Total Loss: 3.9957 Recon Loss: 3.9809 
[12/25 04:48:23 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000087 Step: 260750 Total Loss: 4.0040 Recon Loss: 3.9891 
[12/25 04:49:04 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000086 Step: 260800 Total Loss: 4.6816 Recon Loss: 4.6667 
[12/25 04:49:45 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000086 Step: 260850 Total Loss: 3.9654 Recon Loss: 3.9507 
[12/25 04:50:26 TiTok]: Data (t): 0.0011, 39.38/s/gpu Batch (t): 0.8127 LR: 0.000086 Step: 260900 Total Loss: 5.4007 Recon Loss: 5.3859 
[12/25 04:51:07 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000086 Step: 260950 Total Loss: 6.1281 Recon Loss: 6.1134 
[12/25 04:51:48 TiTok]: Data (t): 0.0011, 35.13/s/gpu Batch (t): 0.9109 LR: 0.000086 Step: 261000 Total Loss: 3.9766 Recon Loss: 3.9618 
[12/25 04:52:29 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000086 Step: 261050 Total Loss: 4.6731 Recon Loss: 4.6582 
[12/25 04:53:10 TiTok]: Data (t): 0.0011, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000086 Step: 261100 Total Loss: 4.6947 Recon Loss: 4.6800 
[12/25 04:53:51 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000086 Step: 261150 Total Loss: 3.9754 Recon Loss: 3.9606 
[12/25 04:54:31 TiTok]: Data (t): 0.0015, 40.12/s/gpu Batch (t): 0.7977 LR: 0.000086 Step: 261200 Total Loss: 4.7064 Recon Loss: 4.6915 
[12/25 04:55:12 TiTok]: Data (t): 0.0010, 39.40/s/gpu Batch (t): 0.8122 LR: 0.000086 Step: 261250 Total Loss: 5.3700 Recon Loss: 5.3553 
[12/25 04:55:53 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8219 LR: 0.000086 Step: 261300 Total Loss: 4.0056 Recon Loss: 3.9908 
[12/25 04:56:34 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000086 Step: 261350 Total Loss: 4.6503 Recon Loss: 4.6355 
[12/25 04:57:15 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000086 Step: 261400 Total Loss: 4.7241 Recon Loss: 4.7093 
[12/25 04:57:56 TiTok]: Data (t): 0.0011, 38.91/s/gpu Batch (t): 0.8223 LR: 0.000086 Step: 261450 Total Loss: 3.9806 Recon Loss: 3.9658 
[12/25 04:58:37 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8162 LR: 0.000086 Step: 261500 Total Loss: 4.0528 Recon Loss: 4.0380 
[12/25 04:59:17 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000086 Step: 261550 Total Loss: 4.6945 Recon Loss: 4.6797 
[12/25 04:59:58 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8204 LR: 0.000086 Step: 261600 Total Loss: 6.1269 Recon Loss: 6.1121 
[12/25 05:00:39 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000086 Step: 261650 Total Loss: 3.9451 Recon Loss: 3.9303 
[12/25 05:01:20 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8183 LR: 0.000086 Step: 261700 Total Loss: 4.7122 Recon Loss: 4.6975 
[12/25 05:02:01 TiTok]: Data (t): 0.0011, 38.88/s/gpu Batch (t): 0.8231 LR: 0.000086 Step: 261750 Total Loss: 5.3566 Recon Loss: 5.3419 
[12/25 05:02:42 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000086 Step: 261800 Total Loss: 3.9752 Recon Loss: 3.9604 
[12/25 05:03:23 TiTok]: Data (t): 0.0010, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000086 Step: 261850 Total Loss: 3.9657 Recon Loss: 3.9508 
[12/25 05:04:04 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000086 Step: 261900 Total Loss: 3.9444 Recon Loss: 3.9296 
[12/25 05:04:45 TiTok]: Data (t): 0.0011, 39.42/s/gpu Batch (t): 0.8117 LR: 0.000086 Step: 261950 Total Loss: 6.1186 Recon Loss: 6.1039 
[12/25 05:05:25 TiTok]: Data (t): 0.0010, 35.20/s/gpu Batch (t): 0.9090 LR: 0.000086 Step: 262000 Total Loss: 4.0329 Recon Loss: 4.0181 
[12/25 05:06:06 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8156 LR: 0.000086 Step: 262050 Total Loss: 4.6645 Recon Loss: 4.6497 
[12/25 05:06:47 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000086 Step: 262100 Total Loss: 4.6731 Recon Loss: 4.6583 
[12/25 05:07:28 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000086 Step: 262150 Total Loss: 4.6926 Recon Loss: 4.6778 
[12/25 05:08:09 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000086 Step: 262200 Total Loss: 4.7302 Recon Loss: 4.7154 
[12/25 05:08:50 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000086 Step: 262250 Total Loss: 4.6395 Recon Loss: 4.6247 
[12/25 05:09:31 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000086 Step: 262300 Total Loss: 5.3732 Recon Loss: 5.3584 
[12/25 05:10:11 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000086 Step: 262350 Total Loss: 4.6701 Recon Loss: 4.6553 
[12/25 05:10:52 TiTok]: Data (t): 0.0011, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000086 Step: 262400 Total Loss: 3.9173 Recon Loss: 3.9025 
[12/25 05:11:33 TiTok]: Data (t): 0.0010, 38.34/s/gpu Batch (t): 0.8346 LR: 0.000086 Step: 262450 Total Loss: 5.3781 Recon Loss: 5.3633 
[12/25 05:12:14 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000086 Step: 262500 Total Loss: 4.6844 Recon Loss: 4.6697 
[12/25 05:12:55 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000086 Step: 262550 Total Loss: 3.8564 Recon Loss: 3.8416 
[12/25 05:13:36 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000086 Step: 262600 Total Loss: 4.6378 Recon Loss: 4.6230 
[12/25 05:14:17 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000086 Step: 262650 Total Loss: 4.7085 Recon Loss: 4.6936 
[12/25 05:14:58 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000086 Step: 262700 Total Loss: 3.9067 Recon Loss: 3.8918 
[12/25 05:15:39 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000086 Step: 262750 Total Loss: 5.4158 Recon Loss: 5.4011 
[12/25 05:16:19 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000086 Step: 262800 Total Loss: 3.9283 Recon Loss: 3.9134 
[12/25 05:17:00 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000086 Step: 262850 Total Loss: 4.7284 Recon Loss: 4.7136 
[12/25 05:17:41 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000086 Step: 262900 Total Loss: 5.4084 Recon Loss: 5.3935 
[12/25 05:18:22 TiTok]: Data (t): 0.0011, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000086 Step: 262950 Total Loss: 6.1205 Recon Loss: 6.1057 
[12/25 05:19:03 TiTok]: Data (t): 0.0011, 35.39/s/gpu Batch (t): 0.9042 LR: 0.000086 Step: 263000 Total Loss: 5.4331 Recon Loss: 5.4183 
[12/25 05:19:44 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000086 Step: 263050 Total Loss: 4.0406 Recon Loss: 4.0259 
[12/25 05:20:25 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000086 Step: 263100 Total Loss: 4.7268 Recon Loss: 4.7121 
[12/25 05:21:06 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000086 Step: 263150 Total Loss: 4.6961 Recon Loss: 4.6812 
[12/25 05:21:47 TiTok]: Data (t): 0.0010, 39.48/s/gpu Batch (t): 0.8104 LR: 0.000086 Step: 263200 Total Loss: 5.4043 Recon Loss: 5.3895 
[12/25 05:22:27 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8150 LR: 0.000086 Step: 263250 Total Loss: 6.1561 Recon Loss: 6.1412 
[12/25 05:23:08 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000086 Step: 263300 Total Loss: 4.6592 Recon Loss: 4.6443 
[12/25 05:23:49 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000086 Step: 263350 Total Loss: 5.3923 Recon Loss: 5.3775 
[12/25 05:24:30 TiTok]: Data (t): 0.0010, 38.60/s/gpu Batch (t): 0.8290 LR: 0.000086 Step: 263400 Total Loss: 4.6885 Recon Loss: 4.6738 
[12/25 05:25:11 TiTok]: Data (t): 0.0011, 39.44/s/gpu Batch (t): 0.8113 LR: 0.000086 Step: 263450 Total Loss: 4.6523 Recon Loss: 4.6376 
[12/25 05:25:52 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8209 LR: 0.000086 Step: 263500 Total Loss: 4.7105 Recon Loss: 4.6957 
[12/25 05:26:33 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000086 Step: 263550 Total Loss: 4.6644 Recon Loss: 4.6496 
[12/25 05:27:14 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000086 Step: 263600 Total Loss: 5.4002 Recon Loss: 5.3853 
[12/25 05:27:55 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000086 Step: 263650 Total Loss: 4.7211 Recon Loss: 4.7063 
[12/25 05:28:35 TiTok]: Data (t): 0.0011, 38.89/s/gpu Batch (t): 0.8229 LR: 0.000086 Step: 263700 Total Loss: 3.9647 Recon Loss: 3.9498 
[12/25 05:29:16 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000086 Step: 263750 Total Loss: 3.9513 Recon Loss: 3.9365 
[12/25 05:29:57 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000086 Step: 263800 Total Loss: 5.4259 Recon Loss: 5.4111 
[12/25 05:30:38 TiTok]: Data (t): 0.0011, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000086 Step: 263850 Total Loss: 5.3957 Recon Loss: 5.3809 
[12/25 05:31:19 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000086 Step: 263900 Total Loss: 4.7279 Recon Loss: 4.7131 
[12/25 05:32:00 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000086 Step: 263950 Total Loss: 3.9512 Recon Loss: 3.9363 
[12/25 05:32:41 TiTok]: Data (t): 0.0011, 35.13/s/gpu Batch (t): 0.9108 LR: 0.000086 Step: 264000 Total Loss: 3.9609 Recon Loss: 3.9462 
[12/25 05:33:22 TiTok]: Data (t): 0.0011, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000086 Step: 264050 Total Loss: 4.7039 Recon Loss: 4.6891 
[12/25 05:34:02 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000086 Step: 264100 Total Loss: 4.6683 Recon Loss: 4.6536 
[12/25 05:34:43 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000086 Step: 264150 Total Loss: 4.0068 Recon Loss: 3.9920 
[12/25 05:35:24 TiTok]: Data (t): 0.0011, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000086 Step: 264200 Total Loss: 5.3647 Recon Loss: 5.3498 
[12/25 05:36:05 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000086 Step: 264250 Total Loss: 4.6199 Recon Loss: 4.6052 
[12/25 05:36:46 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000086 Step: 264300 Total Loss: 4.0184 Recon Loss: 4.0036 
[12/25 05:37:27 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000086 Step: 264350 Total Loss: 4.6917 Recon Loss: 4.6768 
[12/25 05:38:08 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000086 Step: 264400 Total Loss: 4.7305 Recon Loss: 4.7157 
[12/25 05:38:49 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8150 LR: 0.000086 Step: 264450 Total Loss: 4.6982 Recon Loss: 4.6834 
[12/25 05:39:30 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000086 Step: 264500 Total Loss: 4.7714 Recon Loss: 4.7565 
[12/25 05:40:10 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000086 Step: 264550 Total Loss: 5.3300 Recon Loss: 5.3151 
[12/25 05:40:51 TiTok]: Data (t): 0.0011, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000086 Step: 264600 Total Loss: 4.6478 Recon Loss: 4.6330 
[12/25 05:41:32 TiTok]: Data (t): 0.0010, 39.52/s/gpu Batch (t): 0.8098 LR: 0.000086 Step: 264650 Total Loss: 5.4334 Recon Loss: 5.4185 
[12/25 05:42:13 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000086 Step: 264700 Total Loss: 4.6486 Recon Loss: 4.6338 
[12/25 05:42:54 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000086 Step: 264750 Total Loss: 4.6636 Recon Loss: 4.6488 
[12/25 05:43:35 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000086 Step: 264800 Total Loss: 5.3639 Recon Loss: 5.3491 
[12/25 05:44:16 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000086 Step: 264850 Total Loss: 4.7571 Recon Loss: 4.7422 
[12/25 05:44:57 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000086 Step: 264900 Total Loss: 5.3735 Recon Loss: 5.3586 
[12/25 05:45:38 TiTok]: Data (t): 0.0011, 39.44/s/gpu Batch (t): 0.8114 LR: 0.000086 Step: 264950 Total Loss: 4.6844 Recon Loss: 4.6696 
[12/25 05:46:19 TiTok]: Data (t): 0.0010, 35.16/s/gpu Batch (t): 0.9102 LR: 0.000086 Step: 265000 Total Loss: 4.6643 Recon Loss: 4.6495 
[12/25 05:46:20 TiTok]: Reconstructing images...
[12/25 05:47:01 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000086 Step: 265050 Total Loss: 3.9811 Recon Loss: 3.9663 
[12/25 05:47:42 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000086 Step: 265100 Total Loss: 4.6117 Recon Loss: 4.5970 
[12/25 05:48:23 TiTok]: Data (t): 0.0011, 39.41/s/gpu Batch (t): 0.8119 LR: 0.000086 Step: 265150 Total Loss: 4.7284 Recon Loss: 4.7136 
[12/25 05:49:04 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8152 LR: 0.000086 Step: 265200 Total Loss: 5.4250 Recon Loss: 5.4102 
[12/25 05:49:45 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000086 Step: 265250 Total Loss: 4.0138 Recon Loss: 3.9989 
[12/25 05:50:25 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000086 Step: 265300 Total Loss: 4.7311 Recon Loss: 4.7163 
[12/25 05:51:06 TiTok]: Data (t): 0.0011, 38.93/s/gpu Batch (t): 0.8219 LR: 0.000086 Step: 265350 Total Loss: 5.3595 Recon Loss: 5.3447 
[12/25 05:51:47 TiTok]: Data (t): 0.0020, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000086 Step: 265400 Total Loss: 4.7021 Recon Loss: 4.6872 
[12/25 05:52:28 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000086 Step: 265450 Total Loss: 4.7329 Recon Loss: 4.7181 
[12/25 05:53:09 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000086 Step: 265500 Total Loss: 4.6559 Recon Loss: 4.6411 
[12/25 05:53:50 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000086 Step: 265550 Total Loss: 4.6690 Recon Loss: 4.6542 
[12/25 05:54:31 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000086 Step: 265600 Total Loss: 4.6957 Recon Loss: 4.6808 
[12/25 05:55:12 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000086 Step: 265650 Total Loss: 3.9418 Recon Loss: 3.9271 
[12/25 05:55:52 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000086 Step: 265700 Total Loss: 5.4296 Recon Loss: 5.4147 
[12/25 05:56:33 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000086 Step: 265750 Total Loss: 4.6791 Recon Loss: 4.6643 
[12/25 05:57:14 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000086 Step: 265800 Total Loss: 5.4122 Recon Loss: 5.3975 
[12/25 05:57:55 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000086 Step: 265850 Total Loss: 4.6140 Recon Loss: 4.5992 
[12/25 05:58:36 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8217 LR: 0.000086 Step: 265900 Total Loss: 4.7073 Recon Loss: 4.6924 
[12/25 05:59:17 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000086 Step: 265950 Total Loss: 6.1238 Recon Loss: 6.1089 
[12/25 05:59:58 TiTok]: Data (t): 0.0013, 34.42/s/gpu Batch (t): 0.9297 LR: 0.000086 Step: 266000 Total Loss: 3.9678 Recon Loss: 3.9530 
[12/25 06:00:39 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000086 Step: 266050 Total Loss: 4.7418 Recon Loss: 4.7269 
[12/25 06:01:20 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000086 Step: 266100 Total Loss: 5.3922 Recon Loss: 5.3774 
[12/25 06:02:01 TiTok]: Data (t): 0.0011, 39.37/s/gpu Batch (t): 0.8127 LR: 0.000086 Step: 266150 Total Loss: 5.4300 Recon Loss: 5.4151 
[12/25 06:02:41 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000086 Step: 266200 Total Loss: 4.0063 Recon Loss: 3.9915 
[12/25 06:03:22 TiTok]: Data (t): 0.0011, 38.77/s/gpu Batch (t): 0.8255 LR: 0.000086 Step: 266250 Total Loss: 3.9149 Recon Loss: 3.9001 
[12/25 06:04:03 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000086 Step: 266300 Total Loss: 4.7199 Recon Loss: 4.7051 
[12/25 06:04:44 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8160 LR: 0.000086 Step: 266350 Total Loss: 4.6897 Recon Loss: 4.6749 
[12/25 06:05:25 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000086 Step: 266400 Total Loss: 4.6632 Recon Loss: 4.6484 
[12/25 06:06:06 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000086 Step: 266450 Total Loss: 4.7301 Recon Loss: 4.7153 
[12/25 06:06:47 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000086 Step: 266500 Total Loss: 4.0204 Recon Loss: 4.0056 
[12/25 06:07:28 TiTok]: Data (t): 0.0011, 38.87/s/gpu Batch (t): 0.8233 LR: 0.000086 Step: 266550 Total Loss: 6.1230 Recon Loss: 6.1082 
[12/25 06:08:08 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000086 Step: 266600 Total Loss: 5.3832 Recon Loss: 5.3684 
[12/25 06:08:49 TiTok]: Data (t): 0.0011, 39.44/s/gpu Batch (t): 0.8113 LR: 0.000086 Step: 266650 Total Loss: 4.6886 Recon Loss: 4.6738 
[12/25 06:09:30 TiTok]: Data (t): 0.0010, 39.53/s/gpu Batch (t): 0.8095 LR: 0.000086 Step: 266700 Total Loss: 4.0363 Recon Loss: 4.0215 
[12/25 06:10:11 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000086 Step: 266750 Total Loss: 3.9848 Recon Loss: 3.9700 
[12/25 06:10:52 TiTok]: Data (t): 0.0011, 39.52/s/gpu Batch (t): 0.8096 LR: 0.000086 Step: 266800 Total Loss: 4.6208 Recon Loss: 4.6059 
[12/25 06:11:33 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000086 Step: 266850 Total Loss: 4.6843 Recon Loss: 4.6694 
[12/25 06:12:14 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000086 Step: 266900 Total Loss: 4.0127 Recon Loss: 3.9979 
[12/25 06:12:55 TiTok]: Data (t): 0.0010, 39.46/s/gpu Batch (t): 0.8110 LR: 0.000086 Step: 266950 Total Loss: 5.4069 Recon Loss: 5.3921 
[12/25 06:13:36 TiTok]: Data (t): 0.0011, 35.08/s/gpu Batch (t): 0.9121 LR: 0.000086 Step: 267000 Total Loss: 3.9744 Recon Loss: 3.9596 
[12/25 06:14:16 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000086 Step: 267050 Total Loss: 4.7447 Recon Loss: 4.7298 
[12/25 06:14:57 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000086 Step: 267100 Total Loss: 3.9268 Recon Loss: 3.9120 
[12/25 06:15:38 TiTok]: Data (t): 0.0011, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000086 Step: 267150 Total Loss: 4.6458 Recon Loss: 4.6310 
[12/25 06:16:19 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8225 LR: 0.000086 Step: 267200 Total Loss: 4.0374 Recon Loss: 4.0226 
[12/25 06:17:00 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000086 Step: 267250 Total Loss: 3.9948 Recon Loss: 3.9799 
[12/25 06:17:41 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000086 Step: 267300 Total Loss: 3.9146 Recon Loss: 3.8997 
[12/25 06:18:22 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000086 Step: 267350 Total Loss: 3.9246 Recon Loss: 3.9098 
[12/25 06:19:02 TiTok]: Data (t): 0.0011, 38.60/s/gpu Batch (t): 0.8290 LR: 0.000086 Step: 267400 Total Loss: 4.6600 Recon Loss: 4.6453 
[12/25 06:19:43 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000086 Step: 267450 Total Loss: 4.7589 Recon Loss: 4.7441 
[12/25 06:20:24 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000086 Step: 267500 Total Loss: 5.3761 Recon Loss: 5.3613 
[12/25 06:21:05 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000086 Step: 267550 Total Loss: 4.7021 Recon Loss: 4.6873 
[12/25 06:21:46 TiTok]: Data (t): 0.0011, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000086 Step: 267600 Total Loss: 3.9603 Recon Loss: 3.9454 
[12/25 06:22:27 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000086 Step: 267650 Total Loss: 6.1008 Recon Loss: 6.0860 
[12/25 06:23:08 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8162 LR: 0.000086 Step: 267700 Total Loss: 4.7035 Recon Loss: 4.6887 
[12/25 06:23:49 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000086 Step: 267750 Total Loss: 4.6761 Recon Loss: 4.6614 
[12/25 06:24:30 TiTok]: Data (t): 0.0010, 39.46/s/gpu Batch (t): 0.8110 LR: 0.000086 Step: 267800 Total Loss: 4.6903 Recon Loss: 4.6756 
[12/25 06:25:10 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000086 Step: 267850 Total Loss: 4.6981 Recon Loss: 4.6833 
[12/25 06:25:51 TiTok]: Data (t): 0.0011, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000086 Step: 267900 Total Loss: 4.6249 Recon Loss: 4.6100 
[12/25 06:26:32 TiTok]: Data (t): 0.0034, 39.48/s/gpu Batch (t): 0.8105 LR: 0.000086 Step: 267950 Total Loss: 4.6724 Recon Loss: 4.6575 
[12/25 06:27:13 TiTok]: Data (t): 0.0010, 34.97/s/gpu Batch (t): 0.9150 LR: 0.000086 Step: 268000 Total Loss: 4.7454 Recon Loss: 4.7306 
[12/25 06:27:54 TiTok]: Data (t): 0.0011, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000086 Step: 268050 Total Loss: 3.9581 Recon Loss: 3.9433 
[12/25 06:28:35 TiTok]: Data (t): 0.0011, 39.39/s/gpu Batch (t): 0.8124 LR: 0.000086 Step: 268100 Total Loss: 4.7014 Recon Loss: 4.6866 
[12/25 06:29:16 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8200 LR: 0.000086 Step: 268150 Total Loss: 4.6572 Recon Loss: 4.6424 
[12/25 06:29:57 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000086 Step: 268200 Total Loss: 3.9534 Recon Loss: 3.9386 
[12/25 06:30:37 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000086 Step: 268250 Total Loss: 4.0030 Recon Loss: 3.9882 
[12/25 06:31:18 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000086 Step: 268300 Total Loss: 5.3974 Recon Loss: 5.3827 
[12/25 06:31:59 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000086 Step: 268350 Total Loss: 4.6937 Recon Loss: 4.6788 
[12/25 06:32:40 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000086 Step: 268400 Total Loss: 4.0020 Recon Loss: 3.9872 
[12/25 06:33:21 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000086 Step: 268450 Total Loss: 5.3962 Recon Loss: 5.3814 
[12/25 06:34:02 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000086 Step: 268500 Total Loss: 5.4247 Recon Loss: 5.4099 
[12/25 06:34:43 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000086 Step: 268550 Total Loss: 4.0125 Recon Loss: 3.9976 
[12/25 06:35:24 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000086 Step: 268600 Total Loss: 3.9708 Recon Loss: 3.9559 
[12/25 06:36:04 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8175 LR: 0.000086 Step: 268650 Total Loss: 3.9742 Recon Loss: 3.9594 
[12/25 06:36:45 TiTok]: Data (t): 0.0011, 38.83/s/gpu Batch (t): 0.8241 LR: 0.000086 Step: 268700 Total Loss: 4.0049 Recon Loss: 3.9900 
[12/25 06:37:26 TiTok]: Data (t): 0.0011, 38.80/s/gpu Batch (t): 0.8248 LR: 0.000086 Step: 268750 Total Loss: 4.0233 Recon Loss: 4.0084 
[12/25 06:38:07 TiTok]: Data (t): 0.0010, 39.39/s/gpu Batch (t): 0.8124 LR: 0.000086 Step: 268800 Total Loss: 4.0114 Recon Loss: 3.9967 
[12/25 06:38:48 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000086 Step: 268850 Total Loss: 4.0096 Recon Loss: 3.9948 
[12/25 06:39:29 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000086 Step: 268900 Total Loss: 3.9439 Recon Loss: 3.9290 
[12/25 06:40:10 TiTok]: Data (t): 0.0034, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000086 Step: 268950 Total Loss: 3.9430 Recon Loss: 3.9282 
[12/25 06:40:51 TiTok]: Data (t): 0.0011, 35.37/s/gpu Batch (t): 0.9048 LR: 0.000086 Step: 269000 Total Loss: 4.6749 Recon Loss: 4.6601 
[12/25 06:41:31 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8158 LR: 0.000086 Step: 269050 Total Loss: 5.4043 Recon Loss: 5.3895 
[12/25 06:42:12 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000086 Step: 269100 Total Loss: 6.1435 Recon Loss: 6.1286 
[12/25 06:42:53 TiTok]: Data (t): 0.0010, 39.39/s/gpu Batch (t): 0.8124 LR: 0.000086 Step: 269150 Total Loss: 4.6532 Recon Loss: 4.6384 
[12/25 06:43:34 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000086 Step: 269200 Total Loss: 4.6900 Recon Loss: 4.6751 
[12/25 06:44:15 TiTok]: Data (t): 0.0010, 39.49/s/gpu Batch (t): 0.8103 LR: 0.000086 Step: 269250 Total Loss: 4.6817 Recon Loss: 4.6668 
[12/25 06:44:56 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000086 Step: 269300 Total Loss: 6.1131 Recon Loss: 6.0982 
[12/25 06:45:37 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000086 Step: 269350 Total Loss: 5.3233 Recon Loss: 5.3086 
[12/25 06:46:18 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8177 LR: 0.000086 Step: 269400 Total Loss: 3.9738 Recon Loss: 3.9591 
[12/25 06:46:59 TiTok]: Data (t): 0.0010, 38.85/s/gpu Batch (t): 0.8237 LR: 0.000086 Step: 269450 Total Loss: 4.6511 Recon Loss: 4.6364 
[12/25 06:47:39 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8223 LR: 0.000086 Step: 269500 Total Loss: 4.0115 Recon Loss: 3.9966 
[12/25 06:48:20 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000086 Step: 269550 Total Loss: 4.0068 Recon Loss: 3.9920 
[12/25 06:49:01 TiTok]: Data (t): 0.0011, 39.51/s/gpu Batch (t): 0.8099 LR: 0.000086 Step: 269600 Total Loss: 4.6385 Recon Loss: 4.6237 
[12/25 06:49:42 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8160 LR: 0.000086 Step: 269650 Total Loss: 3.9031 Recon Loss: 3.8882 
[12/25 06:50:23 TiTok]: Data (t): 0.0010, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000086 Step: 269700 Total Loss: 4.7060 Recon Loss: 4.6911 
[12/25 06:51:04 TiTok]: Data (t): 0.0053, 38.98/s/gpu Batch (t): 0.8209 LR: 0.000086 Step: 269750 Total Loss: 3.9327 Recon Loss: 3.9179 
[12/25 06:51:45 TiTok]: Data (t): 0.0011, 38.89/s/gpu Batch (t): 0.8229 LR: 0.000086 Step: 269800 Total Loss: 3.9156 Recon Loss: 3.9009 
[12/25 06:52:25 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000086 Step: 269850 Total Loss: 5.4421 Recon Loss: 5.4273 
[12/25 06:53:06 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8158 LR: 0.000086 Step: 269900 Total Loss: 6.1064 Recon Loss: 6.0916 
[12/25 06:53:47 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000086 Step: 269950 Total Loss: 6.1157 Recon Loss: 6.1009 
[12/25 06:54:28 TiTok]: Data (t): 0.0011, 34.76/s/gpu Batch (t): 0.9205 LR: 0.000086 Step: 270000 Total Loss: 5.3728 Recon Loss: 5.3579 
[12/25 06:54:29 TiTok]: Reconstructing images...
[12/25 06:55:11 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8202 LR: 0.000086 Step: 270050 Total Loss: 4.0287 Recon Loss: 4.0138 
[12/25 06:55:51 TiTok]: Data (t): 0.0011, 38.98/s/gpu Batch (t): 0.8209 LR: 0.000086 Step: 270100 Total Loss: 4.6843 Recon Loss: 4.6694 
[12/25 06:56:32 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000086 Step: 270150 Total Loss: 3.9376 Recon Loss: 3.9228 
[12/25 06:57:13 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000086 Step: 270200 Total Loss: 4.6593 Recon Loss: 4.6445 
[12/25 06:57:54 TiTok]: Data (t): 0.0010, 39.50/s/gpu Batch (t): 0.8102 LR: 0.000086 Step: 270250 Total Loss: 5.4423 Recon Loss: 5.4275 
[12/25 06:58:35 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000086 Step: 270300 Total Loss: 3.9918 Recon Loss: 3.9771 
[12/25 06:59:16 TiTok]: Data (t): 0.0010, 38.83/s/gpu Batch (t): 0.8242 LR: 0.000086 Step: 270350 Total Loss: 4.7207 Recon Loss: 4.7059 
[12/25 06:59:57 TiTok]: Data (t): 0.0011, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000086 Step: 270400 Total Loss: 4.6644 Recon Loss: 4.6496 
[12/25 07:00:38 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000085 Step: 270450 Total Loss: 3.9615 Recon Loss: 3.9466 
[12/25 07:01:18 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000085 Step: 270500 Total Loss: 4.0035 Recon Loss: 3.9887 
Epoch 27/99 started.
[12/25 07:02:00 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8209 LR: 0.000085 Step: 270550 Total Loss: 5.3773 Recon Loss: 5.3625 
[12/25 07:02:41 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000085 Step: 270600 Total Loss: 5.4214 Recon Loss: 5.4065 
[12/25 07:03:22 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8202 LR: 0.000085 Step: 270650 Total Loss: 4.6981 Recon Loss: 4.6833 
[12/25 07:04:03 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000085 Step: 270700 Total Loss: 4.7077 Recon Loss: 4.6928 
[12/25 07:04:44 TiTok]: Data (t): 0.0010, 39.45/s/gpu Batch (t): 0.8111 LR: 0.000085 Step: 270750 Total Loss: 3.9931 Recon Loss: 3.9784 
[12/25 07:05:25 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000085 Step: 270800 Total Loss: 5.3989 Recon Loss: 5.3841 
[12/25 07:06:06 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000085 Step: 270850 Total Loss: 3.9396 Recon Loss: 3.9249 
[12/25 07:06:46 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000085 Step: 270900 Total Loss: 4.6518 Recon Loss: 4.6370 
[12/25 07:07:27 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000085 Step: 270950 Total Loss: 4.6919 Recon Loss: 4.6771 
[12/25 07:08:08 TiTok]: Data (t): 0.0010, 35.26/s/gpu Batch (t): 0.9076 LR: 0.000085 Step: 271000 Total Loss: 3.9817 Recon Loss: 3.9669 
[12/25 07:08:49 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000085 Step: 271050 Total Loss: 4.6651 Recon Loss: 4.6503 
[12/25 07:09:30 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000085 Step: 271100 Total Loss: 5.4449 Recon Loss: 5.4300 
[12/25 07:10:11 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8129 LR: 0.000085 Step: 271150 Total Loss: 3.9407 Recon Loss: 3.9259 
[12/25 07:10:52 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000085 Step: 271200 Total Loss: 4.6559 Recon Loss: 4.6411 
[12/25 07:11:32 TiTok]: Data (t): 0.0011, 38.78/s/gpu Batch (t): 0.8252 LR: 0.000085 Step: 271250 Total Loss: 3.9709 Recon Loss: 3.9560 
[12/25 07:12:13 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000085 Step: 271300 Total Loss: 3.9669 Recon Loss: 3.9521 
[12/25 07:12:54 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000085 Step: 271350 Total Loss: 4.7167 Recon Loss: 4.7019 
[12/25 07:13:35 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000085 Step: 271400 Total Loss: 5.3954 Recon Loss: 5.3806 
[12/25 07:14:16 TiTok]: Data (t): 0.0010, 39.40/s/gpu Batch (t): 0.8121 LR: 0.000085 Step: 271450 Total Loss: 4.6865 Recon Loss: 4.6717 
[12/25 07:14:57 TiTok]: Data (t): 0.0010, 39.43/s/gpu Batch (t): 0.8116 LR: 0.000085 Step: 271500 Total Loss: 4.7153 Recon Loss: 4.7005 
[12/25 07:15:38 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000085 Step: 271550 Total Loss: 4.7172 Recon Loss: 4.7023 
[12/25 07:16:18 TiTok]: Data (t): 0.0010, 38.83/s/gpu Batch (t): 0.8241 LR: 0.000085 Step: 271600 Total Loss: 4.6957 Recon Loss: 4.6808 
[12/25 07:16:59 TiTok]: Data (t): 0.0010, 39.41/s/gpu Batch (t): 0.8119 LR: 0.000085 Step: 271650 Total Loss: 3.9348 Recon Loss: 3.9200 
[12/25 07:17:40 TiTok]: Data (t): 0.0012, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000085 Step: 271700 Total Loss: 3.9637 Recon Loss: 3.9489 
[12/25 07:18:21 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000085 Step: 271750 Total Loss: 3.9725 Recon Loss: 3.9577 
[12/25 07:19:02 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000085 Step: 271800 Total Loss: 3.9366 Recon Loss: 3.9218 
[12/25 07:19:43 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8158 LR: 0.000085 Step: 271850 Total Loss: 4.0951 Recon Loss: 4.0802 
[12/25 07:20:24 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000085 Step: 271900 Total Loss: 4.7045 Recon Loss: 4.6897 
[12/25 07:21:05 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000085 Step: 271950 Total Loss: 5.4059 Recon Loss: 5.3911 
[12/25 07:21:45 TiTok]: Data (t): 0.0010, 35.20/s/gpu Batch (t): 0.9090 LR: 0.000085 Step: 272000 Total Loss: 4.6645 Recon Loss: 4.6498 
[12/25 07:22:26 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000085 Step: 272050 Total Loss: 4.7455 Recon Loss: 4.7306 
[12/25 07:23:07 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8209 LR: 0.000085 Step: 272100 Total Loss: 3.9472 Recon Loss: 3.9325 
[12/25 07:23:48 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000085 Step: 272150 Total Loss: 4.7274 Recon Loss: 4.7126 
[12/25 07:24:29 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000085 Step: 272200 Total Loss: 4.6945 Recon Loss: 4.6796 
[12/25 07:25:10 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000085 Step: 272250 Total Loss: 4.0198 Recon Loss: 4.0050 
[12/25 07:25:51 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000085 Step: 272300 Total Loss: 3.9961 Recon Loss: 3.9813 
[12/25 07:26:32 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8133 LR: 0.000085 Step: 272350 Total Loss: 4.6541 Recon Loss: 4.6393 
[12/25 07:27:12 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000085 Step: 272400 Total Loss: 4.6626 Recon Loss: 4.6477 
[12/25 07:27:53 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8187 LR: 0.000085 Step: 272450 Total Loss: 5.3620 Recon Loss: 5.3472 
[12/25 07:28:34 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000085 Step: 272500 Total Loss: 5.4106 Recon Loss: 5.3958 
[12/25 07:29:15 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8187 LR: 0.000085 Step: 272550 Total Loss: 4.6503 Recon Loss: 4.6355 
[12/25 07:29:56 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000085 Step: 272600 Total Loss: 6.1469 Recon Loss: 6.1321 
[12/25 07:30:37 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000085 Step: 272650 Total Loss: 4.6499 Recon Loss: 4.6350 
[12/25 07:31:18 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000085 Step: 272700 Total Loss: 3.9853 Recon Loss: 3.9705 
[12/25 07:31:59 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000085 Step: 272750 Total Loss: 3.9664 Recon Loss: 3.9516 
[12/25 07:32:39 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000085 Step: 272800 Total Loss: 5.3560 Recon Loss: 5.3411 
[12/25 07:33:20 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8189 LR: 0.000085 Step: 272850 Total Loss: 4.0208 Recon Loss: 4.0060 
[12/25 07:34:01 TiTok]: Data (t): 0.0011, 39.44/s/gpu Batch (t): 0.8115 LR: 0.000085 Step: 272900 Total Loss: 5.4594 Recon Loss: 5.4446 
[12/25 07:34:42 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000085 Step: 272950 Total Loss: 5.3616 Recon Loss: 5.3468 
[12/25 07:35:23 TiTok]: Data (t): 0.0011, 34.99/s/gpu Batch (t): 0.9145 LR: 0.000085 Step: 273000 Total Loss: 3.9821 Recon Loss: 3.9672 
[12/25 07:36:04 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8183 LR: 0.000085 Step: 273050 Total Loss: 4.0041 Recon Loss: 3.9893 
[12/25 07:36:45 TiTok]: Data (t): 0.0011, 39.48/s/gpu Batch (t): 0.8105 LR: 0.000085 Step: 273100 Total Loss: 4.6212 Recon Loss: 4.6064 
[12/25 07:37:26 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000085 Step: 273150 Total Loss: 3.9423 Recon Loss: 3.9274 
[12/25 07:38:06 TiTok]: Data (t): 0.0010, 39.47/s/gpu Batch (t): 0.8108 LR: 0.000085 Step: 273200 Total Loss: 5.4140 Recon Loss: 5.3992 
[12/25 07:38:47 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000085 Step: 273250 Total Loss: 4.7517 Recon Loss: 4.7369 
[12/25 07:39:28 TiTok]: Data (t): 0.0011, 39.43/s/gpu Batch (t): 0.8117 LR: 0.000085 Step: 273300 Total Loss: 4.6721 Recon Loss: 4.6572 
[12/25 07:40:09 TiTok]: Data (t): 0.0010, 39.40/s/gpu Batch (t): 0.8122 LR: 0.000085 Step: 273350 Total Loss: 5.4466 Recon Loss: 5.4318 
[12/25 07:40:50 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000085 Step: 273400 Total Loss: 5.3950 Recon Loss: 5.3802 
[12/25 07:41:31 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000085 Step: 273450 Total Loss: 4.6954 Recon Loss: 4.6807 
[12/25 07:42:12 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000085 Step: 273500 Total Loss: 3.9186 Recon Loss: 3.9038 
[12/25 07:42:52 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000085 Step: 273550 Total Loss: 3.9939 Recon Loss: 3.9791 
[12/25 07:43:33 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000085 Step: 273600 Total Loss: 3.9108 Recon Loss: 3.8960 
[12/25 07:44:14 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8175 LR: 0.000085 Step: 273650 Total Loss: 5.4056 Recon Loss: 5.3909 
[12/25 07:44:55 TiTok]: Data (t): 0.0010, 38.27/s/gpu Batch (t): 0.8361 LR: 0.000085 Step: 273700 Total Loss: 5.4205 Recon Loss: 5.4056 
[12/25 07:45:36 TiTok]: Data (t): 0.0011, 38.63/s/gpu Batch (t): 0.8284 LR: 0.000085 Step: 273750 Total Loss: 3.9775 Recon Loss: 3.9627 
[12/25 07:46:17 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8200 LR: 0.000085 Step: 273800 Total Loss: 4.0382 Recon Loss: 4.0234 
[12/25 07:46:58 TiTok]: Data (t): 0.0010, 39.49/s/gpu Batch (t): 0.8103 LR: 0.000085 Step: 273850 Total Loss: 4.7000 Recon Loss: 4.6852 
[12/25 07:47:39 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000085 Step: 273900 Total Loss: 4.6894 Recon Loss: 4.6746 
[12/25 07:48:19 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000085 Step: 273950 Total Loss: 3.9889 Recon Loss: 3.9741 
[12/25 07:49:00 TiTok]: Data (t): 0.0010, 35.63/s/gpu Batch (t): 0.8982 LR: 0.000085 Step: 274000 Total Loss: 5.4038 Recon Loss: 5.3891 
[12/25 07:49:41 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000085 Step: 274050 Total Loss: 4.6525 Recon Loss: 4.6377 
[12/25 07:50:22 TiTok]: Data (t): 0.0011, 39.45/s/gpu Batch (t): 0.8112 LR: 0.000085 Step: 274100 Total Loss: 4.6925 Recon Loss: 4.6777 
[12/25 07:51:03 TiTok]: Data (t): 0.0011, 39.48/s/gpu Batch (t): 0.8105 LR: 0.000085 Step: 274150 Total Loss: 6.1765 Recon Loss: 6.1617 
[12/25 07:51:44 TiTok]: Data (t): 0.0010, 38.58/s/gpu Batch (t): 0.8295 LR: 0.000085 Step: 274200 Total Loss: 3.8866 Recon Loss: 3.8718 
[12/25 07:52:25 TiTok]: Data (t): 0.0011, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000085 Step: 274250 Total Loss: 4.6567 Recon Loss: 4.6418 
[12/25 07:53:06 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000085 Step: 274300 Total Loss: 4.6582 Recon Loss: 4.6433 
[12/25 07:53:46 TiTok]: Data (t): 0.0010, 38.82/s/gpu Batch (t): 0.8244 LR: 0.000085 Step: 274350 Total Loss: 4.6980 Recon Loss: 4.6832 
[12/25 07:54:27 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000085 Step: 274400 Total Loss: 4.6948 Recon Loss: 4.6798 
[12/25 07:55:08 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000085 Step: 274450 Total Loss: 3.9574 Recon Loss: 3.9425 
[12/25 07:55:49 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000085 Step: 274500 Total Loss: 4.6969 Recon Loss: 4.6822 
[12/25 07:56:30 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8166 LR: 0.000085 Step: 274550 Total Loss: 4.6677 Recon Loss: 4.6529 
[12/25 07:57:11 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000085 Step: 274600 Total Loss: 4.0227 Recon Loss: 4.0078 
[12/25 07:57:52 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8223 LR: 0.000085 Step: 274650 Total Loss: 4.6926 Recon Loss: 4.6778 
[12/25 07:58:33 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000085 Step: 274700 Total Loss: 3.9880 Recon Loss: 3.9732 
[12/25 07:59:13 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000085 Step: 274750 Total Loss: 3.8600 Recon Loss: 3.8452 
[12/25 07:59:54 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000085 Step: 274800 Total Loss: 4.7856 Recon Loss: 4.7707 
[12/25 08:00:35 TiTok]: Data (t): 0.0011, 38.82/s/gpu Batch (t): 0.8243 LR: 0.000085 Step: 274850 Total Loss: 4.7141 Recon Loss: 4.6993 
[12/25 08:01:16 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000085 Step: 274900 Total Loss: 3.9832 Recon Loss: 3.9684 
[12/25 08:01:57 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8162 LR: 0.000085 Step: 274950 Total Loss: 4.6727 Recon Loss: 4.6579 
[12/25 08:02:38 TiTok]: Data (t): 0.0010, 35.11/s/gpu Batch (t): 0.9115 LR: 0.000085 Step: 275000 Total Loss: 5.4165 Recon Loss: 5.4017 
[12/25 08:02:39 TiTok]: Reconstructing images...
[12/25 08:03:20 TiTok]: Data (t): 0.0011, 39.46/s/gpu Batch (t): 0.8110 LR: 0.000085 Step: 275050 Total Loss: 4.7067 Recon Loss: 4.6919 
[12/25 08:04:01 TiTok]: Data (t): 0.0012, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000085 Step: 275100 Total Loss: 4.7429 Recon Loss: 4.7280 
[12/25 08:04:42 TiTok]: Data (t): 0.0015, 39.47/s/gpu Batch (t): 0.8108 LR: 0.000085 Step: 275150 Total Loss: 4.6551 Recon Loss: 4.6403 
[12/25 08:05:23 TiTok]: Data (t): 0.0011, 38.23/s/gpu Batch (t): 0.8370 LR: 0.000085 Step: 275200 Total Loss: 4.7465 Recon Loss: 4.7317 
[12/25 08:06:04 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000085 Step: 275250 Total Loss: 3.8950 Recon Loss: 3.8803 
[12/25 08:06:45 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8158 LR: 0.000085 Step: 275300 Total Loss: 5.4216 Recon Loss: 5.4068 
[12/25 08:07:25 TiTok]: Data (t): 0.0011, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000085 Step: 275350 Total Loss: 4.6887 Recon Loss: 4.6739 
[12/25 08:08:06 TiTok]: Data (t): 0.0011, 38.89/s/gpu Batch (t): 0.8228 LR: 0.000085 Step: 275400 Total Loss: 4.6880 Recon Loss: 4.6731 
[12/25 08:08:47 TiTok]: Data (t): 0.0011, 38.85/s/gpu Batch (t): 0.8236 LR: 0.000085 Step: 275450 Total Loss: 5.4642 Recon Loss: 5.4494 
[12/25 08:09:28 TiTok]: Data (t): 0.0011, 39.51/s/gpu Batch (t): 0.8098 LR: 0.000085 Step: 275500 Total Loss: 4.6717 Recon Loss: 4.6568 
[12/25 08:10:09 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000085 Step: 275550 Total Loss: 3.8876 Recon Loss: 3.8728 
[12/25 08:10:50 TiTok]: Data (t): 0.0011, 39.40/s/gpu Batch (t): 0.8121 LR: 0.000085 Step: 275600 Total Loss: 4.7066 Recon Loss: 4.6918 
[12/25 08:11:31 TiTok]: Data (t): 0.0034, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000085 Step: 275650 Total Loss: 3.9565 Recon Loss: 3.9416 
[12/25 08:12:12 TiTok]: Data (t): 0.0011, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000085 Step: 275700 Total Loss: 5.4368 Recon Loss: 5.4220 
[12/25 08:12:52 TiTok]: Data (t): 0.0011, 39.47/s/gpu Batch (t): 0.8108 LR: 0.000085 Step: 275750 Total Loss: 4.7031 Recon Loss: 4.6881 
[12/25 08:13:33 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8223 LR: 0.000085 Step: 275800 Total Loss: 4.0122 Recon Loss: 3.9974 
[12/25 08:14:14 TiTok]: Data (t): 0.0011, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000085 Step: 275850 Total Loss: 4.6594 Recon Loss: 4.6446 
[12/25 08:14:55 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000085 Step: 275900 Total Loss: 4.7056 Recon Loss: 4.6907 
[12/25 08:15:36 TiTok]: Data (t): 0.0011, 39.35/s/gpu Batch (t): 0.8133 LR: 0.000085 Step: 275950 Total Loss: 6.1253 Recon Loss: 6.1104 
[12/25 08:16:17 TiTok]: Data (t): 0.0010, 34.36/s/gpu Batch (t): 0.9314 LR: 0.000085 Step: 276000 Total Loss: 4.7178 Recon Loss: 4.7029 
[12/25 08:16:58 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000085 Step: 276050 Total Loss: 3.9692 Recon Loss: 3.9543 
[12/25 08:17:38 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000085 Step: 276100 Total Loss: 5.3591 Recon Loss: 5.3441 
[12/25 08:18:19 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000085 Step: 276150 Total Loss: 4.6900 Recon Loss: 4.6752 
[12/25 08:19:00 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000085 Step: 276200 Total Loss: 4.6280 Recon Loss: 4.6131 
[12/25 08:19:41 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000085 Step: 276250 Total Loss: 3.9762 Recon Loss: 3.9613 
[12/25 08:20:22 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8223 LR: 0.000085 Step: 276300 Total Loss: 5.4219 Recon Loss: 5.4071 
[12/25 08:21:03 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8158 LR: 0.000085 Step: 276350 Total Loss: 4.7030 Recon Loss: 4.6881 
[12/25 08:21:44 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000085 Step: 276400 Total Loss: 4.6186 Recon Loss: 4.6039 
[12/25 08:22:25 TiTok]: Data (t): 0.0010, 38.87/s/gpu Batch (t): 0.8232 LR: 0.000085 Step: 276450 Total Loss: 4.0271 Recon Loss: 4.0122 
[12/25 08:23:05 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000085 Step: 276500 Total Loss: 4.6843 Recon Loss: 4.6695 
[12/25 08:23:46 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000085 Step: 276550 Total Loss: 4.6557 Recon Loss: 4.6409 
[12/25 08:24:27 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000085 Step: 276600 Total Loss: 3.9994 Recon Loss: 3.9846 
[12/25 08:25:08 TiTok]: Data (t): 0.0011, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000085 Step: 276650 Total Loss: 4.7945 Recon Loss: 4.7797 
[12/25 08:25:49 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000085 Step: 276700 Total Loss: 3.9851 Recon Loss: 3.9703 
[12/25 08:26:30 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000085 Step: 276750 Total Loss: 3.9494 Recon Loss: 3.9345 
[12/25 08:27:10 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000085 Step: 276800 Total Loss: 4.6927 Recon Loss: 4.6780 
[12/25 08:27:51 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000085 Step: 276850 Total Loss: 4.7147 Recon Loss: 4.6998 
[12/25 08:28:32 TiTok]: Data (t): 0.0010, 38.08/s/gpu Batch (t): 0.8402 LR: 0.000085 Step: 276900 Total Loss: 3.9398 Recon Loss: 3.9250 
[12/25 08:29:13 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000085 Step: 276950 Total Loss: 4.6859 Recon Loss: 4.6711 
[12/25 08:29:54 TiTok]: Data (t): 0.0010, 34.97/s/gpu Batch (t): 0.9151 LR: 0.000085 Step: 277000 Total Loss: 3.9165 Recon Loss: 3.9017 
[12/25 08:30:35 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000085 Step: 277050 Total Loss: 4.7292 Recon Loss: 4.7143 
[12/25 08:31:16 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8217 LR: 0.000085 Step: 277100 Total Loss: 5.4162 Recon Loss: 5.4013 
[12/25 08:31:56 TiTok]: Data (t): 0.0017, 39.58/s/gpu Batch (t): 0.8085 LR: 0.000085 Step: 277150 Total Loss: 6.1288 Recon Loss: 6.1139 
[12/25 08:32:37 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000085 Step: 277200 Total Loss: 4.6297 Recon Loss: 4.6149 
[12/25 08:33:18 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000085 Step: 277250 Total Loss: 5.4237 Recon Loss: 5.4089 
[12/25 08:33:59 TiTok]: Data (t): 0.0011, 39.38/s/gpu Batch (t): 0.8125 LR: 0.000085 Step: 277300 Total Loss: 3.9561 Recon Loss: 3.9413 
[12/25 08:34:40 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000085 Step: 277350 Total Loss: 5.3582 Recon Loss: 5.3433 
[12/25 08:35:21 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8234 LR: 0.000085 Step: 277400 Total Loss: 3.9554 Recon Loss: 3.9406 
[12/25 08:36:02 TiTok]: Data (t): 0.0010, 39.44/s/gpu Batch (t): 0.8113 LR: 0.000085 Step: 277450 Total Loss: 4.7006 Recon Loss: 4.6858 
[12/25 08:36:42 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000085 Step: 277500 Total Loss: 4.7393 Recon Loss: 4.7245 
[12/25 08:37:23 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000085 Step: 277550 Total Loss: 5.3784 Recon Loss: 5.3636 
[12/25 08:38:04 TiTok]: Data (t): 0.0011, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000085 Step: 277600 Total Loss: 4.7292 Recon Loss: 4.7144 
[12/25 08:38:45 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000085 Step: 277650 Total Loss: 5.3664 Recon Loss: 5.3517 
[12/25 08:39:26 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8175 LR: 0.000085 Step: 277700 Total Loss: 5.4757 Recon Loss: 5.4608 
[12/25 08:40:07 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000085 Step: 277750 Total Loss: 4.0310 Recon Loss: 4.0162 
[12/25 08:40:48 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000085 Step: 277800 Total Loss: 4.7110 Recon Loss: 4.6962 
[12/25 08:41:28 TiTok]: Data (t): 0.0010, 38.82/s/gpu Batch (t): 0.8244 LR: 0.000085 Step: 277850 Total Loss: 4.7919 Recon Loss: 4.7770 
[12/25 08:42:09 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000085 Step: 277900 Total Loss: 4.6793 Recon Loss: 4.6645 
[12/25 08:42:50 TiTok]: Data (t): 0.0010, 39.56/s/gpu Batch (t): 0.8090 LR: 0.000085 Step: 277950 Total Loss: 5.4084 Recon Loss: 5.3936 
[12/25 08:43:31 TiTok]: Data (t): 0.0010, 35.32/s/gpu Batch (t): 0.9060 LR: 0.000085 Step: 278000 Total Loss: 4.6585 Recon Loss: 4.6436 
[12/25 08:44:12 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000085 Step: 278050 Total Loss: 3.9594 Recon Loss: 3.9446 
[12/25 08:44:53 TiTok]: Data (t): 0.0010, 39.53/s/gpu Batch (t): 0.8095 LR: 0.000085 Step: 278100 Total Loss: 6.1122 Recon Loss: 6.0974 
[12/25 08:45:34 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000085 Step: 278150 Total Loss: 4.6895 Recon Loss: 4.6746 
[12/25 08:46:14 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000085 Step: 278200 Total Loss: 3.9372 Recon Loss: 3.9224 
[12/25 08:46:55 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000085 Step: 278250 Total Loss: 3.9345 Recon Loss: 3.9196 
[12/25 08:47:36 TiTok]: Data (t): 0.0014, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000085 Step: 278300 Total Loss: 3.9861 Recon Loss: 3.9713 
[12/25 08:48:17 TiTok]: Data (t): 0.0035, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000085 Step: 278350 Total Loss: 4.6495 Recon Loss: 4.6348 
[12/25 08:48:58 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000085 Step: 278400 Total Loss: 5.4118 Recon Loss: 5.3970 
[12/25 08:49:39 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8204 LR: 0.000085 Step: 278450 Total Loss: 4.6732 Recon Loss: 4.6585 
[12/25 08:50:20 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000085 Step: 278500 Total Loss: 4.6591 Recon Loss: 4.6442 
[12/25 08:51:01 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8129 LR: 0.000085 Step: 278550 Total Loss: 4.6899 Recon Loss: 4.6751 
[12/25 08:51:41 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000085 Step: 278600 Total Loss: 3.9914 Recon Loss: 3.9766 
[12/25 08:52:22 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000085 Step: 278650 Total Loss: 4.6962 Recon Loss: 4.6813 
[12/25 08:53:03 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000085 Step: 278700 Total Loss: 4.7014 Recon Loss: 4.6866 
[12/25 08:53:44 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000085 Step: 278750 Total Loss: 4.7045 Recon Loss: 4.6897 
[12/25 08:54:25 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000085 Step: 278800 Total Loss: 6.0964 Recon Loss: 6.0816 
[12/25 08:55:06 TiTok]: Data (t): 0.0011, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000085 Step: 278850 Total Loss: 3.9460 Recon Loss: 3.9312 
[12/25 08:55:47 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000085 Step: 278900 Total Loss: 4.6446 Recon Loss: 4.6297 
[12/25 08:56:27 TiTok]: Data (t): 0.0013, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000085 Step: 278950 Total Loss: 4.0381 Recon Loss: 4.0232 
[12/25 08:57:08 TiTok]: Data (t): 0.0010, 35.06/s/gpu Batch (t): 0.9127 LR: 0.000085 Step: 279000 Total Loss: 4.0007 Recon Loss: 3.9859 
[12/25 08:57:49 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8140 LR: 0.000085 Step: 279050 Total Loss: 5.3986 Recon Loss: 5.3838 
[12/25 08:58:30 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000085 Step: 279100 Total Loss: 3.9772 Recon Loss: 3.9623 
[12/25 08:59:11 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000085 Step: 279150 Total Loss: 3.9812 Recon Loss: 3.9664 
[12/25 08:59:52 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8135 LR: 0.000085 Step: 279200 Total Loss: 4.6371 Recon Loss: 4.6223 
[12/25 09:00:33 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000085 Step: 279250 Total Loss: 4.0368 Recon Loss: 4.0220 
[12/25 09:01:13 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000085 Step: 279300 Total Loss: 3.9886 Recon Loss: 3.9738 
[12/25 09:01:54 TiTok]: Data (t): 0.0011, 39.51/s/gpu Batch (t): 0.8100 LR: 0.000085 Step: 279350 Total Loss: 4.7389 Recon Loss: 4.7241 
[12/25 09:02:35 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000085 Step: 279400 Total Loss: 4.7107 Recon Loss: 4.6958 
[12/25 09:03:16 TiTok]: Data (t): 0.0012, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000085 Step: 279450 Total Loss: 4.7397 Recon Loss: 4.7248 
[12/25 09:03:57 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8209 LR: 0.000085 Step: 279500 Total Loss: 4.6949 Recon Loss: 4.6801 
[12/25 09:04:38 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000085 Step: 279550 Total Loss: 4.6807 Recon Loss: 4.6658 
[12/25 09:05:19 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000085 Step: 279600 Total Loss: 6.1469 Recon Loss: 6.1320 
[12/25 09:06:00 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8215 LR: 0.000085 Step: 279650 Total Loss: 4.7211 Recon Loss: 4.7062 
[12/25 09:06:40 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000085 Step: 279700 Total Loss: 4.6282 Recon Loss: 4.6134 
[12/25 09:07:21 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000085 Step: 279750 Total Loss: 4.7314 Recon Loss: 4.7166 
[12/25 09:08:02 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000085 Step: 279800 Total Loss: 3.9508 Recon Loss: 3.9360 
[12/25 09:08:43 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000084 Step: 279850 Total Loss: 3.9664 Recon Loss: 3.9516 
[12/25 09:09:24 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000084 Step: 279900 Total Loss: 3.9311 Recon Loss: 3.9163 
[12/25 09:10:05 TiTok]: Data (t): 0.0010, 38.82/s/gpu Batch (t): 0.8243 LR: 0.000084 Step: 279950 Total Loss: 3.9983 Recon Loss: 3.9835 
[12/25 09:10:46 TiTok]: Data (t): 0.0011, 34.98/s/gpu Batch (t): 0.9148 LR: 0.000084 Step: 280000 Total Loss: 4.6438 Recon Loss: 4.6290 
[12/25 09:10:47 TiTok]: Reconstructing images...
[12/25 09:11:28 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000084 Step: 280050 Total Loss: 3.9800 Recon Loss: 3.9652 
[12/25 09:12:09 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000084 Step: 280100 Total Loss: 5.4015 Recon Loss: 5.3867 
[12/25 09:12:50 TiTok]: Data (t): 0.0010, 39.41/s/gpu Batch (t): 0.8120 LR: 0.000084 Step: 280150 Total Loss: 4.6764 Recon Loss: 4.6615 
[12/25 09:13:31 TiTok]: Data (t): 0.0010, 38.78/s/gpu Batch (t): 0.8251 LR: 0.000084 Step: 280200 Total Loss: 5.3925 Recon Loss: 5.3777 
[12/25 09:14:12 TiTok]: Data (t): 0.0010, 38.05/s/gpu Batch (t): 0.8409 LR: 0.000084 Step: 280250 Total Loss: 4.6822 Recon Loss: 4.6673 
[12/25 09:14:53 TiTok]: Data (t): 0.0018, 39.55/s/gpu Batch (t): 0.8091 LR: 0.000084 Step: 280300 Total Loss: 4.6286 Recon Loss: 4.6139 
[12/25 09:15:34 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000084 Step: 280350 Total Loss: 4.6829 Recon Loss: 4.6681 
[12/25 09:16:14 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000084 Step: 280400 Total Loss: 5.3998 Recon Loss: 5.3850 
[12/25 09:16:55 TiTok]: Data (t): 0.0010, 39.42/s/gpu Batch (t): 0.8117 LR: 0.000084 Step: 280450 Total Loss: 4.7244 Recon Loss: 4.7096 
[12/25 09:17:36 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000084 Step: 280500 Total Loss: 3.9854 Recon Loss: 3.9706 
[12/25 09:18:17 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000084 Step: 280550 Total Loss: 3.9859 Recon Loss: 3.9712 
Epoch 28/99 started.
[12/25 09:18:59 TiTok]: Data (t): 0.0011, 38.82/s/gpu Batch (t): 0.8243 LR: 0.000084 Step: 280600 Total Loss: 4.7129 Recon Loss: 4.6980 
[12/25 09:19:40 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000084 Step: 280650 Total Loss: 3.9739 Recon Loss: 3.9591 
[12/25 09:20:20 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000084 Step: 280700 Total Loss: 4.7023 Recon Loss: 4.6874 
[12/25 09:21:01 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000084 Step: 280750 Total Loss: 4.6657 Recon Loss: 4.6509 
[12/25 09:21:42 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8221 LR: 0.000084 Step: 280800 Total Loss: 5.3840 Recon Loss: 5.3692 
[12/25 09:22:23 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8150 LR: 0.000084 Step: 280850 Total Loss: 3.9283 Recon Loss: 3.9135 
[12/25 09:23:04 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000084 Step: 280900 Total Loss: 4.6695 Recon Loss: 4.6547 
[12/25 09:23:45 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000084 Step: 280950 Total Loss: 3.9239 Recon Loss: 3.9091 
[12/25 09:24:26 TiTok]: Data (t): 0.0010, 35.04/s/gpu Batch (t): 0.9132 LR: 0.000084 Step: 281000 Total Loss: 3.9299 Recon Loss: 3.9151 
[12/25 09:25:07 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000084 Step: 281050 Total Loss: 4.7724 Recon Loss: 4.7576 
[12/25 09:25:47 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000084 Step: 281100 Total Loss: 4.0279 Recon Loss: 4.0132 
[12/25 09:26:28 TiTok]: Data (t): 0.0010, 38.61/s/gpu Batch (t): 0.8288 LR: 0.000084 Step: 281150 Total Loss: 3.9816 Recon Loss: 3.9668 
[12/25 09:27:09 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8221 LR: 0.000084 Step: 281200 Total Loss: 5.3804 Recon Loss: 5.3656 
[12/25 09:27:50 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000084 Step: 281250 Total Loss: 3.9992 Recon Loss: 3.9844 
[12/25 09:28:31 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000084 Step: 281300 Total Loss: 5.4504 Recon Loss: 5.4355 
[12/25 09:29:12 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000084 Step: 281350 Total Loss: 5.4083 Recon Loss: 5.3935 
[12/25 09:29:53 TiTok]: Data (t): 0.0011, 39.06/s/gpu Batch (t): 0.8194 LR: 0.000084 Step: 281400 Total Loss: 6.1163 Recon Loss: 6.1015 
[12/25 09:30:33 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000084 Step: 281450 Total Loss: 5.3477 Recon Loss: 5.3328 
[12/25 09:31:14 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8231 LR: 0.000084 Step: 281500 Total Loss: 4.6748 Recon Loss: 4.6599 
[12/25 09:31:55 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000084 Step: 281550 Total Loss: 3.8853 Recon Loss: 3.8705 
[12/25 09:32:36 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000084 Step: 281600 Total Loss: 4.7242 Recon Loss: 4.7093 
[12/25 09:33:17 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8204 LR: 0.000084 Step: 281650 Total Loss: 4.6272 Recon Loss: 4.6124 
[12/25 09:33:58 TiTok]: Data (t): 0.0011, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000084 Step: 281700 Total Loss: 3.8941 Recon Loss: 3.8793 
[12/25 09:34:39 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000084 Step: 281750 Total Loss: 3.9915 Recon Loss: 3.9766 
[12/25 09:35:19 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000084 Step: 281800 Total Loss: 4.0082 Recon Loss: 3.9933 
[12/25 09:36:00 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000084 Step: 281850 Total Loss: 5.3395 Recon Loss: 5.3247 
[12/25 09:36:41 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000084 Step: 281900 Total Loss: 5.3837 Recon Loss: 5.3689 
[12/25 09:37:22 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000084 Step: 281950 Total Loss: 4.0243 Recon Loss: 4.0094 
[12/25 09:38:03 TiTok]: Data (t): 0.0011, 35.07/s/gpu Batch (t): 0.9126 LR: 0.000084 Step: 282000 Total Loss: 3.8935 Recon Loss: 3.8788 
[12/25 09:38:44 TiTok]: Data (t): 0.0010, 39.42/s/gpu Batch (t): 0.8118 LR: 0.000084 Step: 282050 Total Loss: 6.8395 Recon Loss: 6.8247 
[12/25 09:39:25 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8210 LR: 0.000084 Step: 282100 Total Loss: 4.6708 Recon Loss: 4.6561 
[12/25 09:40:06 TiTok]: Data (t): 0.0011, 39.53/s/gpu Batch (t): 0.8096 LR: 0.000084 Step: 282150 Total Loss: 6.0927 Recon Loss: 6.0779 
[12/25 09:40:46 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000084 Step: 282200 Total Loss: 3.9484 Recon Loss: 3.9336 
[12/25 09:41:27 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000084 Step: 282250 Total Loss: 3.9410 Recon Loss: 3.9261 
[12/25 09:42:08 TiTok]: Data (t): 0.0010, 38.42/s/gpu Batch (t): 0.8329 LR: 0.000084 Step: 282300 Total Loss: 4.6571 Recon Loss: 4.6423 
[12/25 09:42:49 TiTok]: Data (t): 0.0019, 39.91/s/gpu Batch (t): 0.8017 LR: 0.000084 Step: 282350 Total Loss: 3.9559 Recon Loss: 3.9411 
[12/25 09:43:30 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000084 Step: 282400 Total Loss: 3.9831 Recon Loss: 3.9682 
[12/25 09:44:11 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000084 Step: 282450 Total Loss: 3.9703 Recon Loss: 3.9555 
[12/25 09:44:52 TiTok]: Data (t): 0.0011, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000084 Step: 282500 Total Loss: 5.4187 Recon Loss: 5.4039 
[12/25 09:45:32 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000084 Step: 282550 Total Loss: 4.6889 Recon Loss: 4.6741 
[12/25 09:46:13 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8236 LR: 0.000084 Step: 282600 Total Loss: 4.6692 Recon Loss: 4.6545 
[12/25 09:46:54 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8158 LR: 0.000084 Step: 282650 Total Loss: 4.6663 Recon Loss: 4.6513 
[12/25 09:47:35 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8156 LR: 0.000084 Step: 282700 Total Loss: 5.4246 Recon Loss: 5.4097 
[12/25 09:48:16 TiTok]: Data (t): 0.0010, 39.47/s/gpu Batch (t): 0.8108 LR: 0.000084 Step: 282750 Total Loss: 5.4176 Recon Loss: 5.4027 
[12/25 09:48:57 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000084 Step: 282800 Total Loss: 4.7156 Recon Loss: 4.7007 
[12/25 09:49:38 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000084 Step: 282850 Total Loss: 4.6624 Recon Loss: 4.6476 
[12/25 09:50:19 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000084 Step: 282900 Total Loss: 4.7446 Recon Loss: 4.7298 
[12/25 09:50:59 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000084 Step: 282950 Total Loss: 5.3529 Recon Loss: 5.3381 
[12/25 09:51:40 TiTok]: Data (t): 0.0010, 35.29/s/gpu Batch (t): 0.9068 LR: 0.000084 Step: 283000 Total Loss: 4.6354 Recon Loss: 4.6206 
[12/25 09:52:21 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000084 Step: 283050 Total Loss: 3.9759 Recon Loss: 3.9611 
[12/25 09:53:02 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000084 Step: 283100 Total Loss: 4.6561 Recon Loss: 4.6413 
[12/25 09:53:43 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000084 Step: 283150 Total Loss: 4.6662 Recon Loss: 4.6513 
[12/25 09:54:24 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000084 Step: 283200 Total Loss: 4.6341 Recon Loss: 4.6193 
[12/25 09:55:05 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000084 Step: 283250 Total Loss: 4.6814 Recon Loss: 4.6666 
[12/25 09:55:45 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000084 Step: 283300 Total Loss: 4.7349 Recon Loss: 4.7202 
[12/25 09:56:26 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8185 LR: 0.000084 Step: 283350 Total Loss: 4.6811 Recon Loss: 4.6663 
[12/25 09:57:07 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8221 LR: 0.000084 Step: 283400 Total Loss: 4.7179 Recon Loss: 4.7031 
[12/25 09:57:48 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000084 Step: 283450 Total Loss: 3.9740 Recon Loss: 3.9591 
[12/25 09:58:29 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000084 Step: 283500 Total Loss: 3.9932 Recon Loss: 3.9783 
[12/25 09:59:10 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000084 Step: 283550 Total Loss: 3.9137 Recon Loss: 3.8988 
[12/25 09:59:50 TiTok]: Data (t): 0.0011, 39.44/s/gpu Batch (t): 0.8114 LR: 0.000084 Step: 283600 Total Loss: 4.0193 Recon Loss: 4.0044 
[12/25 10:00:31 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000084 Step: 283650 Total Loss: 4.7004 Recon Loss: 4.6856 
[12/25 10:01:12 TiTok]: Data (t): 0.0010, 39.45/s/gpu Batch (t): 0.8111 LR: 0.000084 Step: 283700 Total Loss: 6.0673 Recon Loss: 6.0524 
[12/25 10:01:53 TiTok]: Data (t): 0.0018, 39.52/s/gpu Batch (t): 0.8097 LR: 0.000084 Step: 283750 Total Loss: 4.6360 Recon Loss: 4.6212 
[12/25 10:02:34 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000084 Step: 283800 Total Loss: 3.9849 Recon Loss: 3.9701 
[12/25 10:03:15 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000084 Step: 283850 Total Loss: 5.3745 Recon Loss: 5.3596 
[12/25 10:03:56 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000084 Step: 283900 Total Loss: 6.0638 Recon Loss: 6.0490 
[12/25 10:04:37 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8154 LR: 0.000084 Step: 283950 Total Loss: 5.4090 Recon Loss: 5.3942 
[12/25 10:05:18 TiTok]: Data (t): 0.0010, 34.70/s/gpu Batch (t): 0.9221 LR: 0.000084 Step: 284000 Total Loss: 4.7020 Recon Loss: 4.6872 
[12/25 10:05:58 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000084 Step: 284050 Total Loss: 4.7105 Recon Loss: 4.6957 
[12/25 10:06:39 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000084 Step: 284100 Total Loss: 3.9472 Recon Loss: 3.9324 
[12/25 10:07:20 TiTok]: Data (t): 0.0010, 38.76/s/gpu Batch (t): 0.8257 LR: 0.000084 Step: 284150 Total Loss: 4.0007 Recon Loss: 3.9858 
[12/25 10:08:01 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8154 LR: 0.000084 Step: 284200 Total Loss: 5.4194 Recon Loss: 5.4044 
[12/25 10:08:42 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8152 LR: 0.000084 Step: 284250 Total Loss: 3.9370 Recon Loss: 3.9223 
[12/25 10:09:23 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000084 Step: 284300 Total Loss: 4.7098 Recon Loss: 4.6949 
[12/25 10:10:04 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8176 LR: 0.000084 Step: 284350 Total Loss: 3.9325 Recon Loss: 3.9177 
[12/25 10:10:44 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000084 Step: 284400 Total Loss: 4.6695 Recon Loss: 4.6547 
[12/25 10:11:25 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8131 LR: 0.000084 Step: 284450 Total Loss: 6.1116 Recon Loss: 6.0968 
[12/25 10:12:06 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000084 Step: 284500 Total Loss: 4.6933 Recon Loss: 4.6785 
[12/25 10:12:47 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8209 LR: 0.000084 Step: 284550 Total Loss: 6.1524 Recon Loss: 6.1376 
[12/25 10:13:28 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000084 Step: 284600 Total Loss: 3.9441 Recon Loss: 3.9293 
[12/25 10:14:09 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000084 Step: 284650 Total Loss: 6.1085 Recon Loss: 6.0937 
[12/25 10:14:50 TiTok]: Data (t): 0.0010, 39.60/s/gpu Batch (t): 0.8082 LR: 0.000084 Step: 284700 Total Loss: 4.0097 Recon Loss: 3.9948 
[12/25 10:15:31 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000084 Step: 284750 Total Loss: 5.3930 Recon Loss: 5.3782 
[12/25 10:16:11 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000084 Step: 284800 Total Loss: 4.6869 Recon Loss: 4.6720 
[12/25 10:16:52 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000084 Step: 284850 Total Loss: 4.0018 Recon Loss: 3.9870 
[12/25 10:17:33 TiTok]: Data (t): 0.0015, 40.84/s/gpu Batch (t): 0.7836 LR: 0.000084 Step: 284900 Total Loss: 4.6482 Recon Loss: 4.6334 
[12/25 10:18:14 TiTok]: Data (t): 0.0010, 39.41/s/gpu Batch (t): 0.8120 LR: 0.000084 Step: 284950 Total Loss: 4.6987 Recon Loss: 4.6838 
[12/25 10:18:55 TiTok]: Data (t): 0.0011, 35.22/s/gpu Batch (t): 0.9086 LR: 0.000084 Step: 285000 Total Loss: 4.7400 Recon Loss: 4.7252 
[12/25 10:18:56 TiTok]: Reconstructing images...
[12/25 10:19:38 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000084 Step: 285050 Total Loss: 5.4248 Recon Loss: 5.4101 
[12/25 10:20:18 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000084 Step: 285100 Total Loss: 4.0051 Recon Loss: 3.9903 
[12/25 10:20:59 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000084 Step: 285150 Total Loss: 4.7091 Recon Loss: 4.6943 
[12/25 10:21:40 TiTok]: Data (t): 0.0011, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000084 Step: 285200 Total Loss: 4.6808 Recon Loss: 4.6659 
[12/25 10:22:21 TiTok]: Data (t): 0.0010, 38.85/s/gpu Batch (t): 0.8237 LR: 0.000084 Step: 285250 Total Loss: 3.9528 Recon Loss: 3.9379 
[12/25 10:23:02 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8177 LR: 0.000084 Step: 285300 Total Loss: 3.9117 Recon Loss: 3.8968 
[12/25 10:23:43 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000084 Step: 285350 Total Loss: 4.6965 Recon Loss: 4.6817 
[12/25 10:24:24 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000084 Step: 285400 Total Loss: 3.9536 Recon Loss: 3.9388 
[12/25 10:25:05 TiTok]: Data (t): 0.0010, 38.77/s/gpu Batch (t): 0.8254 LR: 0.000084 Step: 285450 Total Loss: 4.0306 Recon Loss: 4.0157 
[12/25 10:25:45 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000084 Step: 285500 Total Loss: 6.1112 Recon Loss: 6.0963 
[12/25 10:26:26 TiTok]: Data (t): 0.0015, 40.32/s/gpu Batch (t): 0.7936 LR: 0.000084 Step: 285550 Total Loss: 6.1059 Recon Loss: 6.0910 
[12/25 10:27:07 TiTok]: Data (t): 0.0011, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000084 Step: 285600 Total Loss: 5.3947 Recon Loss: 5.3798 
[12/25 10:27:48 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8183 LR: 0.000084 Step: 285650 Total Loss: 3.9465 Recon Loss: 3.9317 
[12/25 10:28:29 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8127 LR: 0.000084 Step: 285700 Total Loss: 4.6707 Recon Loss: 4.6559 
[12/25 10:29:10 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000084 Step: 285750 Total Loss: 3.9717 Recon Loss: 3.9570 
[12/25 10:29:50 TiTok]: Data (t): 0.0011, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000084 Step: 285800 Total Loss: 5.3951 Recon Loss: 5.3802 
[12/25 10:30:31 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8204 LR: 0.000084 Step: 285850 Total Loss: 3.9774 Recon Loss: 3.9625 
[12/25 10:31:12 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8158 LR: 0.000084 Step: 285900 Total Loss: 5.4212 Recon Loss: 5.4063 
[12/25 10:31:53 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000084 Step: 285950 Total Loss: 4.6424 Recon Loss: 4.6276 
[12/25 10:32:34 TiTok]: Data (t): 0.0010, 34.40/s/gpu Batch (t): 0.9303 LR: 0.000084 Step: 286000 Total Loss: 4.6423 Recon Loss: 4.6275 
[12/25 10:33:15 TiTok]: Data (t): 0.0011, 38.98/s/gpu Batch (t): 0.8208 LR: 0.000084 Step: 286050 Total Loss: 4.7627 Recon Loss: 4.7479 
[12/25 10:33:56 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000084 Step: 286100 Total Loss: 5.4106 Recon Loss: 5.3958 
[12/25 10:34:36 TiTok]: Data (t): 0.0010, 39.63/s/gpu Batch (t): 0.8075 LR: 0.000084 Step: 286150 Total Loss: 4.6646 Recon Loss: 4.6497 
[12/25 10:35:17 TiTok]: Data (t): 0.0010, 39.47/s/gpu Batch (t): 0.8108 LR: 0.000084 Step: 286200 Total Loss: 5.4157 Recon Loss: 5.4008 
[12/25 10:35:58 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000084 Step: 286250 Total Loss: 5.3979 Recon Loss: 5.3831 
[12/25 10:36:39 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000084 Step: 286300 Total Loss: 3.9575 Recon Loss: 3.9426 
[12/25 10:37:20 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000084 Step: 286350 Total Loss: 3.9585 Recon Loss: 3.9437 
[12/25 10:38:01 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000084 Step: 286400 Total Loss: 3.9813 Recon Loss: 3.9666 
[12/25 10:38:42 TiTok]: Data (t): 0.0036, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000084 Step: 286450 Total Loss: 4.6271 Recon Loss: 4.6123 
[12/25 10:39:22 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000084 Step: 286500 Total Loss: 3.9711 Recon Loss: 3.9563 
[12/25 10:40:03 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000084 Step: 286550 Total Loss: 4.6269 Recon Loss: 4.6121 
[12/25 10:40:44 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8206 LR: 0.000084 Step: 286600 Total Loss: 5.3778 Recon Loss: 5.3629 
[12/25 10:41:25 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000084 Step: 286650 Total Loss: 4.6635 Recon Loss: 4.6488 
[12/25 10:42:06 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000084 Step: 286700 Total Loss: 3.9862 Recon Loss: 3.9713 
[12/25 10:42:47 TiTok]: Data (t): 0.0011, 38.85/s/gpu Batch (t): 0.8236 LR: 0.000084 Step: 286750 Total Loss: 5.4070 Recon Loss: 5.3922 
[12/25 10:43:28 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000084 Step: 286800 Total Loss: 3.9185 Recon Loss: 3.9038 
[12/25 10:44:09 TiTok]: Data (t): 0.0011, 39.43/s/gpu Batch (t): 0.8116 LR: 0.000084 Step: 286850 Total Loss: 5.4575 Recon Loss: 5.4427 
[12/25 10:44:49 TiTok]: Data (t): 0.0018, 39.67/s/gpu Batch (t): 0.8067 LR: 0.000084 Step: 286900 Total Loss: 3.9198 Recon Loss: 3.9049 
[12/25 10:45:30 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000084 Step: 286950 Total Loss: 5.3878 Recon Loss: 5.3730 
[12/25 10:46:11 TiTok]: Data (t): 0.0010, 34.94/s/gpu Batch (t): 0.9158 LR: 0.000084 Step: 287000 Total Loss: 5.3823 Recon Loss: 5.3674 
[12/25 10:46:52 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8223 LR: 0.000084 Step: 287050 Total Loss: 3.9770 Recon Loss: 3.9622 
[12/25 10:47:33 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000084 Step: 287100 Total Loss: 6.0825 Recon Loss: 6.0676 
[12/25 10:48:14 TiTok]: Data (t): 0.0010, 39.47/s/gpu Batch (t): 0.8108 LR: 0.000084 Step: 287150 Total Loss: 6.8429 Recon Loss: 6.8281 
[12/25 10:48:55 TiTok]: Data (t): 0.0011, 38.87/s/gpu Batch (t): 0.8233 LR: 0.000084 Step: 287200 Total Loss: 4.0465 Recon Loss: 4.0318 
[12/25 10:49:35 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000084 Step: 287250 Total Loss: 4.0241 Recon Loss: 4.0092 
[12/25 10:50:16 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000084 Step: 287300 Total Loss: 5.4310 Recon Loss: 5.4161 
[12/25 10:50:57 TiTok]: Data (t): 0.0032, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000084 Step: 287350 Total Loss: 4.6583 Recon Loss: 4.6434 
[12/25 10:51:38 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000084 Step: 287400 Total Loss: 4.7095 Recon Loss: 4.6947 
[12/25 10:52:19 TiTok]: Data (t): 0.0011, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000084 Step: 287450 Total Loss: 4.6890 Recon Loss: 4.6742 
[12/25 10:53:00 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000084 Step: 287500 Total Loss: 4.6786 Recon Loss: 4.6638 
[12/25 10:53:41 TiTok]: Data (t): 0.0010, 39.48/s/gpu Batch (t): 0.8105 LR: 0.000084 Step: 287550 Total Loss: 5.4250 Recon Loss: 5.4102 
[12/25 10:54:22 TiTok]: Data (t): 0.0013, 40.36/s/gpu Batch (t): 0.7929 LR: 0.000084 Step: 287600 Total Loss: 4.6725 Recon Loss: 4.6576 
[12/25 10:55:02 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000084 Step: 287650 Total Loss: 3.9045 Recon Loss: 3.8896 
[12/25 10:55:43 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000084 Step: 287700 Total Loss: 4.7776 Recon Loss: 4.7627 
[12/25 10:56:24 TiTok]: Data (t): 0.0010, 38.66/s/gpu Batch (t): 0.8278 LR: 0.000084 Step: 287750 Total Loss: 4.6980 Recon Loss: 4.6833 
[12/25 10:57:05 TiTok]: Data (t): 0.0011, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000084 Step: 287800 Total Loss: 6.0642 Recon Loss: 6.0493 
[12/25 10:57:46 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8200 LR: 0.000084 Step: 287850 Total Loss: 4.6479 Recon Loss: 4.6331 
[12/25 10:58:27 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8206 LR: 0.000084 Step: 287900 Total Loss: 4.7219 Recon Loss: 4.7070 
[12/25 10:59:08 TiTok]: Data (t): 0.0010, 38.98/s/gpu Batch (t): 0.8210 LR: 0.000084 Step: 287950 Total Loss: 6.0780 Recon Loss: 6.0632 
[12/25 10:59:49 TiTok]: Data (t): 0.0011, 34.45/s/gpu Batch (t): 0.9289 LR: 0.000084 Step: 288000 Total Loss: 3.9243 Recon Loss: 3.9094 
[12/25 11:00:29 TiTok]: Data (t): 0.0018, 39.38/s/gpu Batch (t): 0.8125 LR: 0.000084 Step: 288050 Total Loss: 6.1035 Recon Loss: 6.0887 
[12/25 11:01:10 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000084 Step: 288100 Total Loss: 4.6453 Recon Loss: 4.6305 
[12/25 11:01:51 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000084 Step: 288150 Total Loss: 3.9374 Recon Loss: 3.9225 
[12/25 11:02:32 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000084 Step: 288200 Total Loss: 3.9608 Recon Loss: 3.9460 
[12/25 11:03:13 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000084 Step: 288250 Total Loss: 4.6305 Recon Loss: 4.6156 
[12/25 11:03:54 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000084 Step: 288300 Total Loss: 5.3952 Recon Loss: 5.3803 
[12/25 11:04:35 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000084 Step: 288350 Total Loss: 5.3687 Recon Loss: 5.3538 
[12/25 11:05:16 TiTok]: Data (t): 0.0026, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000084 Step: 288400 Total Loss: 4.7145 Recon Loss: 4.6997 
[12/25 11:05:56 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000084 Step: 288450 Total Loss: 6.1181 Recon Loss: 6.1033 
[12/25 11:06:37 TiTok]: Data (t): 0.0011, 39.36/s/gpu Batch (t): 0.8131 LR: 0.000084 Step: 288500 Total Loss: 3.9740 Recon Loss: 3.9591 
[12/25 11:07:18 TiTok]: Data (t): 0.0011, 38.45/s/gpu Batch (t): 0.8323 LR: 0.000084 Step: 288550 Total Loss: 5.3584 Recon Loss: 5.3435 
[12/25 11:07:59 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8177 LR: 0.000084 Step: 288600 Total Loss: 5.3817 Recon Loss: 5.3669 
[12/25 11:08:40 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000084 Step: 288650 Total Loss: 3.9665 Recon Loss: 3.9517 
[12/25 11:09:21 TiTok]: Data (t): 0.0010, 39.51/s/gpu Batch (t): 0.8100 LR: 0.000084 Step: 288700 Total Loss: 5.3487 Recon Loss: 5.3338 
[12/25 11:10:02 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000084 Step: 288750 Total Loss: 3.9765 Recon Loss: 3.9617 
[12/25 11:10:42 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000084 Step: 288800 Total Loss: 3.9091 Recon Loss: 3.8943 
[12/25 11:11:23 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000084 Step: 288850 Total Loss: 5.4011 Recon Loss: 5.3864 
[12/25 11:12:04 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000084 Step: 288900 Total Loss: 5.3693 Recon Loss: 5.3545 
[12/25 11:12:45 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000084 Step: 288950 Total Loss: 4.6104 Recon Loss: 4.5955 
[12/25 11:13:26 TiTok]: Data (t): 0.0011, 34.92/s/gpu Batch (t): 0.9163 LR: 0.000083 Step: 289000 Total Loss: 4.6918 Recon Loss: 4.6770 
[12/25 11:14:07 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000083 Step: 289050 Total Loss: 4.7104 Recon Loss: 4.6956 
[12/25 11:14:48 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000083 Step: 289100 Total Loss: 4.6747 Recon Loss: 4.6598 
[12/25 11:15:29 TiTok]: Data (t): 0.0010, 39.49/s/gpu Batch (t): 0.8104 LR: 0.000083 Step: 289150 Total Loss: 4.7069 Recon Loss: 4.6922 
[12/25 11:16:09 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000083 Step: 289200 Total Loss: 3.9894 Recon Loss: 3.9745 
[12/25 11:16:50 TiTok]: Data (t): 0.0013, 39.45/s/gpu Batch (t): 0.8112 LR: 0.000083 Step: 289250 Total Loss: 4.6423 Recon Loss: 4.6275 
[12/25 11:17:31 TiTok]: Data (t): 0.0010, 38.43/s/gpu Batch (t): 0.8326 LR: 0.000083 Step: 289300 Total Loss: 5.3915 Recon Loss: 5.3767 
[12/25 11:18:12 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000083 Step: 289350 Total Loss: 4.7699 Recon Loss: 4.7550 
[12/25 11:18:53 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000083 Step: 289400 Total Loss: 4.6320 Recon Loss: 4.6171 
[12/25 11:19:34 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000083 Step: 289450 Total Loss: 3.9634 Recon Loss: 3.9486 
[12/25 11:20:15 TiTok]: Data (t): 0.0011, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000083 Step: 289500 Total Loss: 3.8737 Recon Loss: 3.8589 
[12/25 11:20:56 TiTok]: Data (t): 0.0010, 39.61/s/gpu Batch (t): 0.8079 LR: 0.000083 Step: 289550 Total Loss: 6.8155 Recon Loss: 6.8006 
[12/25 11:21:37 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000083 Step: 289600 Total Loss: 4.0298 Recon Loss: 4.0150 
[12/25 11:22:17 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000083 Step: 289650 Total Loss: 3.9696 Recon Loss: 3.9548 
[12/25 11:22:58 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000083 Step: 289700 Total Loss: 3.9788 Recon Loss: 3.9639 
[12/25 11:23:39 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000083 Step: 289750 Total Loss: 4.6633 Recon Loss: 4.6484 
[12/25 11:24:20 TiTok]: Data (t): 0.0011, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000083 Step: 289800 Total Loss: 5.2788 Recon Loss: 5.2640 
[12/25 11:25:01 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8228 LR: 0.000083 Step: 289850 Total Loss: 4.7562 Recon Loss: 4.7414 
[12/25 11:25:42 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000083 Step: 289900 Total Loss: 4.6874 Recon Loss: 4.6726 
[12/25 11:26:23 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8202 LR: 0.000083 Step: 289950 Total Loss: 4.6667 Recon Loss: 4.6518 
[12/25 11:27:03 TiTok]: Data (t): 0.0010, 35.37/s/gpu Batch (t): 0.9047 LR: 0.000083 Step: 290000 Total Loss: 3.9464 Recon Loss: 3.9315 
[12/25 11:27:05 TiTok]: Reconstructing images...
[12/25 11:27:46 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000083 Step: 290050 Total Loss: 4.6314 Recon Loss: 4.6166 
[12/25 11:28:27 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000083 Step: 290100 Total Loss: 4.6038 Recon Loss: 4.5890 
[12/25 11:29:08 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8150 LR: 0.000083 Step: 290150 Total Loss: 4.6759 Recon Loss: 4.6611 
[12/25 11:29:48 TiTok]: Data (t): 0.0011, 38.67/s/gpu Batch (t): 0.8274 LR: 0.000083 Step: 290200 Total Loss: 5.4409 Recon Loss: 5.4260 
[12/25 11:30:29 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8144 LR: 0.000083 Step: 290250 Total Loss: 4.6100 Recon Loss: 4.5952 
[12/25 11:31:10 TiTok]: Data (t): 0.0010, 38.74/s/gpu Batch (t): 0.8259 LR: 0.000083 Step: 290300 Total Loss: 3.9971 Recon Loss: 3.9823 
[12/25 11:31:51 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000083 Step: 290350 Total Loss: 4.0007 Recon Loss: 3.9859 
[12/25 11:32:32 TiTok]: Data (t): 0.0010, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000083 Step: 290400 Total Loss: 4.7146 Recon Loss: 4.6997 
[12/25 11:33:13 TiTok]: Data (t): 0.0011, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000083 Step: 290450 Total Loss: 5.4441 Recon Loss: 5.4292 
[12/25 11:33:54 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000083 Step: 290500 Total Loss: 4.6489 Recon Loss: 4.6340 
[12/25 11:34:35 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8204 LR: 0.000083 Step: 290550 Total Loss: 3.9160 Recon Loss: 3.9011 
Epoch 29/99 started.
[12/25 11:35:16 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000083 Step: 290600 Total Loss: 4.6353 Recon Loss: 4.6205 
[12/25 11:35:57 TiTok]: Data (t): 0.0010, 38.84/s/gpu Batch (t): 0.8240 LR: 0.000083 Step: 290650 Total Loss: 3.9115 Recon Loss: 3.8968 
[12/25 11:36:38 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000083 Step: 290700 Total Loss: 4.7051 Recon Loss: 4.6903 
[12/25 11:37:19 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8228 LR: 0.000083 Step: 290750 Total Loss: 3.9472 Recon Loss: 3.9325 
[12/25 11:38:00 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8158 LR: 0.000083 Step: 290800 Total Loss: 3.8836 Recon Loss: 3.8688 
[12/25 11:38:41 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000083 Step: 290850 Total Loss: 3.9507 Recon Loss: 3.9360 
[12/25 11:39:22 TiTok]: Data (t): 0.0011, 39.44/s/gpu Batch (t): 0.8114 LR: 0.000083 Step: 290900 Total Loss: 5.4106 Recon Loss: 5.3958 
[12/25 11:40:02 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000083 Step: 290950 Total Loss: 5.3615 Recon Loss: 5.3467 
[12/25 11:40:43 TiTok]: Data (t): 0.0010, 35.32/s/gpu Batch (t): 0.9060 LR: 0.000083 Step: 291000 Total Loss: 4.6960 Recon Loss: 4.6812 
[12/25 11:41:24 TiTok]: Data (t): 0.0011, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000083 Step: 291050 Total Loss: 4.6821 Recon Loss: 4.6672 
[12/25 11:42:05 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000083 Step: 291100 Total Loss: 4.6829 Recon Loss: 4.6680 
[12/25 11:42:46 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000083 Step: 291150 Total Loss: 5.3778 Recon Loss: 5.3631 
[12/25 11:43:27 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8131 LR: 0.000083 Step: 291200 Total Loss: 5.3878 Recon Loss: 5.3730 
[12/25 11:44:08 TiTok]: Data (t): 0.0011, 39.37/s/gpu Batch (t): 0.8128 LR: 0.000083 Step: 291250 Total Loss: 3.9400 Recon Loss: 3.9252 
[12/25 11:44:49 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8158 LR: 0.000083 Step: 291300 Total Loss: 6.1287 Recon Loss: 6.1139 
[12/25 11:45:29 TiTok]: Data (t): 0.0011, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000083 Step: 291350 Total Loss: 5.3835 Recon Loss: 5.3686 
[12/25 11:46:10 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8206 LR: 0.000083 Step: 291400 Total Loss: 3.9097 Recon Loss: 3.8949 
[12/25 11:46:51 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000083 Step: 291450 Total Loss: 4.6080 Recon Loss: 4.5933 
[12/25 11:47:32 TiTok]: Data (t): 0.0010, 38.81/s/gpu Batch (t): 0.8245 LR: 0.000083 Step: 291500 Total Loss: 5.3795 Recon Loss: 5.3646 
[12/25 11:48:13 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8217 LR: 0.000083 Step: 291550 Total Loss: 4.6382 Recon Loss: 4.6235 
[12/25 11:48:54 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000083 Step: 291600 Total Loss: 4.6654 Recon Loss: 4.6505 
[12/25 11:49:35 TiTok]: Data (t): 0.0010, 39.13/s/gpu Batch (t): 0.8179 LR: 0.000083 Step: 291650 Total Loss: 4.6712 Recon Loss: 4.6563 
[12/25 11:50:15 TiTok]: Data (t): 0.0010, 38.87/s/gpu Batch (t): 0.8233 LR: 0.000083 Step: 291700 Total Loss: 3.9809 Recon Loss: 3.9660 
[12/25 11:50:56 TiTok]: Data (t): 0.0016, 39.78/s/gpu Batch (t): 0.8043 LR: 0.000083 Step: 291750 Total Loss: 5.3951 Recon Loss: 5.3803 
[12/25 11:51:37 TiTok]: Data (t): 0.0010, 39.47/s/gpu Batch (t): 0.8107 LR: 0.000083 Step: 291800 Total Loss: 5.4199 Recon Loss: 5.4051 
[12/25 11:52:18 TiTok]: Data (t): 0.0010, 38.85/s/gpu Batch (t): 0.8238 LR: 0.000083 Step: 291850 Total Loss: 4.6550 Recon Loss: 4.6402 
[12/25 11:52:59 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000083 Step: 291900 Total Loss: 5.3937 Recon Loss: 5.3789 
[12/25 11:53:40 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000083 Step: 291950 Total Loss: 6.1288 Recon Loss: 6.1140 
[12/25 11:54:21 TiTok]: Data (t): 0.0010, 35.29/s/gpu Batch (t): 0.9068 LR: 0.000083 Step: 292000 Total Loss: 3.8968 Recon Loss: 3.8820 
[12/25 11:55:02 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000083 Step: 292050 Total Loss: 3.9130 Recon Loss: 3.8982 
[12/25 11:55:42 TiTok]: Data (t): 0.0011, 39.42/s/gpu Batch (t): 0.8118 LR: 0.000083 Step: 292100 Total Loss: 4.5620 Recon Loss: 4.5472 
[12/25 11:56:23 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000083 Step: 292150 Total Loss: 4.7245 Recon Loss: 4.7097 
[12/25 11:57:04 TiTok]: Data (t): 0.0010, 38.83/s/gpu Batch (t): 0.8241 LR: 0.000083 Step: 292200 Total Loss: 4.0017 Recon Loss: 3.9868 
[12/25 11:57:45 TiTok]: Data (t): 0.0011, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000083 Step: 292250 Total Loss: 5.4002 Recon Loss: 5.3854 
[12/25 11:58:26 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000083 Step: 292300 Total Loss: 4.7032 Recon Loss: 4.6884 
[12/25 11:59:07 TiTok]: Data (t): 0.0011, 39.33/s/gpu Batch (t): 0.8137 LR: 0.000083 Step: 292350 Total Loss: 5.3263 Recon Loss: 5.3115 
[12/25 11:59:48 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000083 Step: 292400 Total Loss: 4.6731 Recon Loss: 4.6582 
[12/25 12:00:29 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8160 LR: 0.000083 Step: 292450 Total Loss: 4.6226 Recon Loss: 4.6078 
[12/25 12:01:09 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8155 LR: 0.000083 Step: 292500 Total Loss: 4.6632 Recon Loss: 4.6483 
[12/25 12:01:50 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000083 Step: 292550 Total Loss: 4.6061 Recon Loss: 4.5913 
[12/25 12:02:31 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000083 Step: 292600 Total Loss: 4.0384 Recon Loss: 4.0235 
[12/25 12:03:12 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000083 Step: 292650 Total Loss: 5.3886 Recon Loss: 5.3737 
[12/25 12:03:53 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000083 Step: 292700 Total Loss: 5.3763 Recon Loss: 5.3615 
[12/25 12:04:34 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8129 LR: 0.000083 Step: 292750 Total Loss: 4.6179 Recon Loss: 4.6032 
[12/25 12:05:15 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000083 Step: 292800 Total Loss: 6.1368 Recon Loss: 6.1220 
[12/25 12:05:55 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000083 Step: 292850 Total Loss: 4.6480 Recon Loss: 4.6331 
[12/25 12:06:36 TiTok]: Data (t): 0.0010, 38.76/s/gpu Batch (t): 0.8257 LR: 0.000083 Step: 292900 Total Loss: 4.6770 Recon Loss: 4.6622 
[12/25 12:07:17 TiTok]: Data (t): 0.0010, 39.45/s/gpu Batch (t): 0.8112 LR: 0.000083 Step: 292950 Total Loss: 4.6777 Recon Loss: 4.6629 
[12/25 12:07:58 TiTok]: Data (t): 0.0010, 35.32/s/gpu Batch (t): 0.9059 LR: 0.000083 Step: 293000 Total Loss: 4.0200 Recon Loss: 4.0052 
[12/25 12:08:39 TiTok]: Data (t): 0.0010, 39.44/s/gpu Batch (t): 0.8113 LR: 0.000083 Step: 293050 Total Loss: 4.7136 Recon Loss: 4.6987 
[12/25 12:09:20 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8181 LR: 0.000083 Step: 293100 Total Loss: 5.3808 Recon Loss: 5.3658 
[12/25 12:10:01 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000083 Step: 293150 Total Loss: 5.3753 Recon Loss: 5.3605 
[12/25 12:10:42 TiTok]: Data (t): 0.0068, 39.16/s/gpu Batch (t): 0.8171 LR: 0.000083 Step: 293200 Total Loss: 5.3887 Recon Loss: 5.3739 
[12/25 12:11:22 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8158 LR: 0.000083 Step: 293250 Total Loss: 4.7232 Recon Loss: 4.7083 
[12/25 12:12:03 TiTok]: Data (t): 0.0010, 38.81/s/gpu Batch (t): 0.8246 LR: 0.000083 Step: 293300 Total Loss: 4.6729 Recon Loss: 4.6581 
[12/25 12:12:44 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000083 Step: 293350 Total Loss: 6.0646 Recon Loss: 6.0497 
[12/25 12:13:25 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8156 LR: 0.000083 Step: 293400 Total Loss: 3.9120 Recon Loss: 3.8972 
[12/25 12:14:06 TiTok]: Data (t): 0.0010, 39.43/s/gpu Batch (t): 0.8116 LR: 0.000083 Step: 293450 Total Loss: 3.9710 Recon Loss: 3.9562 
[12/25 12:14:47 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8137 LR: 0.000083 Step: 293500 Total Loss: 5.2930 Recon Loss: 5.2782 
[12/25 12:15:28 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000083 Step: 293550 Total Loss: 4.6771 Recon Loss: 4.6623 
[12/25 12:16:08 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000083 Step: 293600 Total Loss: 5.3257 Recon Loss: 5.3109 
[12/25 12:16:49 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000083 Step: 293650 Total Loss: 4.6967 Recon Loss: 4.6818 
[12/25 12:17:30 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000083 Step: 293700 Total Loss: 5.4069 Recon Loss: 5.3921 
[12/25 12:18:11 TiTok]: Data (t): 0.0009, 39.14/s/gpu Batch (t): 0.8175 LR: 0.000083 Step: 293750 Total Loss: 4.7010 Recon Loss: 4.6862 
[12/25 12:18:52 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000083 Step: 293800 Total Loss: 3.9536 Recon Loss: 3.9388 
[12/25 12:19:33 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000083 Step: 293850 Total Loss: 4.6359 Recon Loss: 4.6211 
[12/25 12:20:14 TiTok]: Data (t): 0.0010, 39.14/s/gpu Batch (t): 0.8177 LR: 0.000083 Step: 293900 Total Loss: 4.0028 Recon Loss: 3.9879 
[12/25 12:20:55 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000083 Step: 293950 Total Loss: 4.0043 Recon Loss: 3.9895 
[12/25 12:21:35 TiTok]: Data (t): 0.0010, 35.22/s/gpu Batch (t): 0.9085 LR: 0.000083 Step: 294000 Total Loss: 3.9498 Recon Loss: 3.9350 
[12/25 12:22:16 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000083 Step: 294050 Total Loss: 3.9452 Recon Loss: 3.9304 
[12/25 12:22:57 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000083 Step: 294100 Total Loss: 4.6465 Recon Loss: 4.6317 
[12/25 12:23:38 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8228 LR: 0.000083 Step: 294150 Total Loss: 4.6499 Recon Loss: 4.6351 
[12/25 12:24:19 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8206 LR: 0.000083 Step: 294200 Total Loss: 4.0117 Recon Loss: 3.9969 
[12/25 12:25:00 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8160 LR: 0.000083 Step: 294250 Total Loss: 6.0939 Recon Loss: 6.0791 
[12/25 12:25:41 TiTok]: Data (t): 0.0011, 39.36/s/gpu Batch (t): 0.8129 LR: 0.000083 Step: 294300 Total Loss: 3.9602 Recon Loss: 3.9454 
[12/25 12:26:21 TiTok]: Data (t): 0.0040, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000083 Step: 294350 Total Loss: 4.7023 Recon Loss: 4.6874 
[12/25 12:27:02 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8213 LR: 0.000083 Step: 294400 Total Loss: 3.9424 Recon Loss: 3.9276 
[12/25 12:27:43 TiTok]: Data (t): 0.0011, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000083 Step: 294450 Total Loss: 3.9667 Recon Loss: 3.9519 
[12/25 12:28:24 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000083 Step: 294500 Total Loss: 5.3734 Recon Loss: 5.3586 
[12/25 12:29:05 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000083 Step: 294550 Total Loss: 4.6699 Recon Loss: 4.6551 
[12/25 12:29:46 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000083 Step: 294600 Total Loss: 4.6620 Recon Loss: 4.6471 
[12/25 12:30:27 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000083 Step: 294650 Total Loss: 3.9409 Recon Loss: 3.9261 
[12/25 12:31:08 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000083 Step: 294700 Total Loss: 4.6599 Recon Loss: 4.6450 
[12/25 12:31:48 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000083 Step: 294750 Total Loss: 5.3573 Recon Loss: 5.3424 
[12/25 12:32:29 TiTok]: Data (t): 0.0011, 38.89/s/gpu Batch (t): 0.8229 LR: 0.000083 Step: 294800 Total Loss: 3.9604 Recon Loss: 3.9456 
[12/25 12:33:10 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000083 Step: 294850 Total Loss: 3.9467 Recon Loss: 3.9319 
[12/25 12:33:51 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000083 Step: 294900 Total Loss: 4.6736 Recon Loss: 4.6588 
[12/25 12:34:32 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000083 Step: 294950 Total Loss: 3.9594 Recon Loss: 3.9445 
[12/25 12:35:13 TiTok]: Data (t): 0.0011, 35.09/s/gpu Batch (t): 0.9120 LR: 0.000083 Step: 295000 Total Loss: 3.9545 Recon Loss: 3.9396 
[12/25 12:35:14 TiTok]: Reconstructing images...
[12/25 12:35:55 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000083 Step: 295050 Total Loss: 3.9249 Recon Loss: 3.9101 
[12/25 12:36:36 TiTok]: Data (t): 0.0010, 38.83/s/gpu Batch (t): 0.8240 LR: 0.000083 Step: 295100 Total Loss: 4.7213 Recon Loss: 4.7064 
[12/25 12:37:17 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000083 Step: 295150 Total Loss: 5.3618 Recon Loss: 5.3471 
[12/25 12:37:58 TiTok]: Data (t): 0.0012, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000083 Step: 295200 Total Loss: 3.9010 Recon Loss: 3.8861 
[12/25 12:38:39 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000083 Step: 295250 Total Loss: 4.6832 Recon Loss: 4.6685 
[12/25 12:39:20 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000083 Step: 295300 Total Loss: 4.7197 Recon Loss: 4.7049 
[12/25 12:40:01 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8162 LR: 0.000083 Step: 295350 Total Loss: 4.6635 Recon Loss: 4.6487 
[12/25 12:40:41 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000083 Step: 295400 Total Loss: 5.4215 Recon Loss: 5.4066 
[12/25 12:41:22 TiTok]: Data (t): 0.0010, 39.48/s/gpu Batch (t): 0.8105 LR: 0.000083 Step: 295450 Total Loss: 5.3938 Recon Loss: 5.3790 
[12/25 12:42:03 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000083 Step: 295500 Total Loss: 4.6650 Recon Loss: 4.6501 
[12/25 12:42:44 TiTok]: Data (t): 0.0011, 38.94/s/gpu Batch (t): 0.8217 LR: 0.000083 Step: 295550 Total Loss: 4.6534 Recon Loss: 4.6385 
[12/25 12:43:25 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000083 Step: 295600 Total Loss: 4.6741 Recon Loss: 4.6593 
[12/25 12:44:06 TiTok]: Data (t): 0.0010, 39.42/s/gpu Batch (t): 0.8119 LR: 0.000083 Step: 295650 Total Loss: 4.7081 Recon Loss: 4.6933 
[12/25 12:44:47 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000083 Step: 295700 Total Loss: 4.6735 Recon Loss: 4.6587 
[12/25 12:45:27 TiTok]: Data (t): 0.0011, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000083 Step: 295750 Total Loss: 4.7142 Recon Loss: 4.6994 
[12/25 12:46:08 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000083 Step: 295800 Total Loss: 4.6431 Recon Loss: 4.6282 
[12/25 12:46:49 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8187 LR: 0.000083 Step: 295850 Total Loss: 3.9595 Recon Loss: 3.9447 
[12/25 12:47:30 TiTok]: Data (t): 0.0011, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000083 Step: 295900 Total Loss: 3.8984 Recon Loss: 3.8836 
[12/25 12:48:11 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8152 LR: 0.000083 Step: 295950 Total Loss: 4.6464 Recon Loss: 4.6315 
[12/25 12:48:52 TiTok]: Data (t): 0.0011, 34.49/s/gpu Batch (t): 0.9277 LR: 0.000083 Step: 296000 Total Loss: 3.9947 Recon Loss: 3.9799 
[12/25 12:49:33 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000083 Step: 296050 Total Loss: 4.0205 Recon Loss: 4.0057 
[12/25 12:50:13 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000083 Step: 296100 Total Loss: 4.7065 Recon Loss: 4.6917 
[12/25 12:50:54 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8187 LR: 0.000083 Step: 296150 Total Loss: 3.9537 Recon Loss: 3.9390 
[12/25 12:51:35 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000083 Step: 296200 Total Loss: 5.3702 Recon Loss: 5.3554 
[12/25 12:52:16 TiTok]: Data (t): 0.0011, 38.50/s/gpu Batch (t): 0.8312 LR: 0.000083 Step: 296250 Total Loss: 3.9464 Recon Loss: 3.9316 
[12/25 12:52:57 TiTok]: Data (t): 0.0011, 39.26/s/gpu Batch (t): 0.8152 LR: 0.000083 Step: 296300 Total Loss: 4.7466 Recon Loss: 4.7317 
[12/25 12:53:38 TiTok]: Data (t): 0.0021, 39.08/s/gpu Batch (t): 0.8189 LR: 0.000083 Step: 296350 Total Loss: 4.6184 Recon Loss: 4.6035 
[12/25 12:54:19 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000083 Step: 296400 Total Loss: 4.6611 Recon Loss: 4.6462 
[12/25 12:54:59 TiTok]: Data (t): 0.0011, 39.13/s/gpu Batch (t): 0.8178 LR: 0.000083 Step: 296450 Total Loss: 4.6368 Recon Loss: 4.6220 
[12/25 12:55:40 TiTok]: Data (t): 0.0011, 38.95/s/gpu Batch (t): 0.8217 LR: 0.000083 Step: 296500 Total Loss: 4.6662 Recon Loss: 4.6514 
[12/25 12:56:21 TiTok]: Data (t): 0.0010, 38.81/s/gpu Batch (t): 0.8245 LR: 0.000083 Step: 296550 Total Loss: 3.9147 Recon Loss: 3.8999 
[12/25 12:57:02 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000083 Step: 296600 Total Loss: 3.9926 Recon Loss: 3.9778 
[12/25 12:57:43 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000083 Step: 296650 Total Loss: 4.6506 Recon Loss: 4.6357 
[12/25 12:58:24 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000083 Step: 296700 Total Loss: 4.6987 Recon Loss: 4.6838 
[12/25 12:59:05 TiTok]: Data (t): 0.0011, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000083 Step: 296750 Total Loss: 3.9077 Recon Loss: 3.8928 
[12/25 12:59:46 TiTok]: Data (t): 0.0011, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000083 Step: 296800 Total Loss: 3.9884 Recon Loss: 3.9735 
[12/25 13:00:26 TiTok]: Data (t): 0.0011, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000083 Step: 296850 Total Loss: 5.3779 Recon Loss: 5.3631 
[12/25 13:01:07 TiTok]: Data (t): 0.0011, 39.05/s/gpu Batch (t): 0.8196 LR: 0.000083 Step: 296900 Total Loss: 4.7033 Recon Loss: 4.6885 
[12/25 13:01:48 TiTok]: Data (t): 0.0010, 39.53/s/gpu Batch (t): 0.8096 LR: 0.000083 Step: 296950 Total Loss: 5.3758 Recon Loss: 5.3610 
[12/25 13:02:29 TiTok]: Data (t): 0.0010, 35.00/s/gpu Batch (t): 0.9144 LR: 0.000083 Step: 297000 Total Loss: 5.4012 Recon Loss: 5.3864 
[12/25 13:03:10 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8215 LR: 0.000083 Step: 297050 Total Loss: 4.6484 Recon Loss: 4.6336 
[12/25 13:03:51 TiTok]: Data (t): 0.0010, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000083 Step: 297100 Total Loss: 4.6037 Recon Loss: 4.5889 
[12/25 13:04:32 TiTok]: Data (t): 0.0010, 39.26/s/gpu Batch (t): 0.8151 LR: 0.000083 Step: 297150 Total Loss: 3.9985 Recon Loss: 3.9836 
[12/25 13:05:13 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000083 Step: 297200 Total Loss: 3.9142 Recon Loss: 3.8993 
[12/25 13:05:54 TiTok]: Data (t): 0.0011, 39.52/s/gpu Batch (t): 0.8097 LR: 0.000083 Step: 297250 Total Loss: 5.3191 Recon Loss: 5.3043 
[12/25 13:06:35 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000083 Step: 297300 Total Loss: 3.9329 Recon Loss: 3.9180 
[12/25 13:07:15 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000083 Step: 297350 Total Loss: 5.3520 Recon Loss: 5.3371 
[12/25 13:07:56 TiTok]: Data (t): 0.0011, 39.39/s/gpu Batch (t): 0.8123 LR: 0.000083 Step: 297400 Total Loss: 5.3564 Recon Loss: 5.3416 
[12/25 13:08:37 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000083 Step: 297450 Total Loss: 4.6171 Recon Loss: 4.6023 
[12/25 13:09:18 TiTok]: Data (t): 0.0011, 38.96/s/gpu Batch (t): 0.8214 LR: 0.000083 Step: 297500 Total Loss: 4.0002 Recon Loss: 3.9854 
[12/25 13:09:59 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000083 Step: 297550 Total Loss: 3.9950 Recon Loss: 3.9802 
[12/25 13:10:40 TiTok]: Data (t): 0.0010, 38.74/s/gpu Batch (t): 0.8260 LR: 0.000083 Step: 297600 Total Loss: 5.4149 Recon Loss: 5.4001 
[12/25 13:11:21 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000083 Step: 297650 Total Loss: 4.7122 Recon Loss: 4.6974 
[12/25 13:12:02 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8162 LR: 0.000083 Step: 297700 Total Loss: 4.7101 Recon Loss: 4.6952 
[12/25 13:12:42 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8196 LR: 0.000083 Step: 297750 Total Loss: 3.9659 Recon Loss: 3.9511 
[12/25 13:13:23 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000083 Step: 297800 Total Loss: 6.1125 Recon Loss: 6.0976 
[12/25 13:14:04 TiTok]: Data (t): 0.0011, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000083 Step: 297850 Total Loss: 4.6376 Recon Loss: 4.6227 
[12/25 13:14:45 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8226 LR: 0.000083 Step: 297900 Total Loss: 4.7038 Recon Loss: 4.6890 
[12/25 13:15:26 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000082 Step: 297950 Total Loss: 3.9627 Recon Loss: 3.9478 
[12/25 13:16:07 TiTok]: Data (t): 0.0010, 35.15/s/gpu Batch (t): 0.9103 LR: 0.000082 Step: 298000 Total Loss: 4.0057 Recon Loss: 3.9909 
[12/25 13:16:48 TiTok]: Data (t): 0.0011, 39.52/s/gpu Batch (t): 0.8097 LR: 0.000082 Step: 298050 Total Loss: 4.6312 Recon Loss: 4.6164 
[12/25 13:17:28 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000082 Step: 298100 Total Loss: 4.0143 Recon Loss: 3.9994 
[12/25 13:18:09 TiTok]: Data (t): 0.0011, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000082 Step: 298150 Total Loss: 4.6086 Recon Loss: 4.5937 
[12/25 13:18:50 TiTok]: Data (t): 0.0032, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000082 Step: 298200 Total Loss: 5.4181 Recon Loss: 5.4034 
[12/25 13:19:31 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000082 Step: 298250 Total Loss: 5.3756 Recon Loss: 5.3608 
[12/25 13:20:12 TiTok]: Data (t): 0.0011, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000082 Step: 298300 Total Loss: 4.6646 Recon Loss: 4.6498 
[12/25 13:20:53 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000082 Step: 298350 Total Loss: 3.9393 Recon Loss: 3.9245 
[12/25 13:21:33 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000082 Step: 298400 Total Loss: 5.3640 Recon Loss: 5.3492 
[12/25 13:22:14 TiTok]: Data (t): 0.0010, 39.40/s/gpu Batch (t): 0.8121 LR: 0.000082 Step: 298450 Total Loss: 5.4138 Recon Loss: 5.3990 
[12/25 13:22:55 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000082 Step: 298500 Total Loss: 5.4064 Recon Loss: 5.3916 
[12/25 13:23:36 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8136 LR: 0.000082 Step: 298550 Total Loss: 4.6345 Recon Loss: 4.6196 
[12/25 13:24:17 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000082 Step: 298600 Total Loss: 5.3938 Recon Loss: 5.3789 
[12/25 13:24:58 TiTok]: Data (t): 0.0010, 38.63/s/gpu Batch (t): 0.8283 LR: 0.000082 Step: 298650 Total Loss: 4.7157 Recon Loss: 4.7008 
[12/25 13:25:39 TiTok]: Data (t): 0.0011, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000082 Step: 298700 Total Loss: 4.7108 Recon Loss: 4.6960 
[12/25 13:26:19 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8156 LR: 0.000082 Step: 298750 Total Loss: 4.0046 Recon Loss: 3.9898 
[12/25 13:27:00 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8200 LR: 0.000082 Step: 298800 Total Loss: 4.6935 Recon Loss: 4.6787 
[12/25 13:27:41 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000082 Step: 298850 Total Loss: 4.6850 Recon Loss: 4.6701 
[12/25 13:28:22 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8164 LR: 0.000082 Step: 298900 Total Loss: 4.0350 Recon Loss: 4.0202 
[12/25 13:29:03 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000082 Step: 298950 Total Loss: 6.1395 Recon Loss: 6.1246 
[12/25 13:29:44 TiTok]: Data (t): 0.0010, 35.46/s/gpu Batch (t): 0.9024 LR: 0.000082 Step: 299000 Total Loss: 4.6562 Recon Loss: 4.6413 
[12/25 13:30:25 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000082 Step: 299050 Total Loss: 4.6794 Recon Loss: 4.6645 
[12/25 13:31:05 TiTok]: Data (t): 0.0011, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000082 Step: 299100 Total Loss: 4.6864 Recon Loss: 4.6716 
[12/25 13:31:46 TiTok]: Data (t): 0.0010, 39.46/s/gpu Batch (t): 0.8108 LR: 0.000082 Step: 299150 Total Loss: 5.4009 Recon Loss: 5.3861 
[12/25 13:32:27 TiTok]: Data (t): 0.0017, 39.33/s/gpu Batch (t): 0.8135 LR: 0.000082 Step: 299200 Total Loss: 4.7416 Recon Loss: 4.7267 
[12/25 13:33:08 TiTok]: Data (t): 0.0011, 39.47/s/gpu Batch (t): 0.8108 LR: 0.000082 Step: 299250 Total Loss: 3.8715 Recon Loss: 3.8567 
[12/25 13:33:49 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8138 LR: 0.000082 Step: 299300 Total Loss: 4.6522 Recon Loss: 4.6374 
[12/25 13:34:30 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000082 Step: 299350 Total Loss: 3.8819 Recon Loss: 3.8670 
[12/25 13:35:10 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000082 Step: 299400 Total Loss: 5.3458 Recon Loss: 5.3309 
[12/25 13:35:51 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8166 LR: 0.000082 Step: 299450 Total Loss: 3.9740 Recon Loss: 3.9592 
[12/25 13:36:32 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8202 LR: 0.000082 Step: 299500 Total Loss: 4.0179 Recon Loss: 4.0031 
[12/25 13:37:13 TiTok]: Data (t): 0.0011, 39.35/s/gpu Batch (t): 0.8131 LR: 0.000082 Step: 299550 Total Loss: 4.7268 Recon Loss: 4.7119 
[12/25 13:37:54 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8206 LR: 0.000082 Step: 299600 Total Loss: 3.9654 Recon Loss: 3.9506 
[12/25 13:38:35 TiTok]: Data (t): 0.0010, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000082 Step: 299650 Total Loss: 5.3890 Recon Loss: 5.3742 
[12/25 13:39:16 TiTok]: Data (t): 0.0011, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000082 Step: 299700 Total Loss: 4.6824 Recon Loss: 4.6675 
[12/25 13:39:56 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000082 Step: 299750 Total Loss: 3.9503 Recon Loss: 3.9354 
[12/25 13:40:37 TiTok]: Data (t): 0.0011, 38.69/s/gpu Batch (t): 0.8270 LR: 0.000082 Step: 299800 Total Loss: 4.6178 Recon Loss: 4.6030 
[12/25 13:41:18 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8158 LR: 0.000082 Step: 299850 Total Loss: 5.3466 Recon Loss: 5.3318 
[12/25 13:41:59 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000082 Step: 299900 Total Loss: 4.6577 Recon Loss: 4.6429 
[12/25 13:42:40 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8200 LR: 0.000082 Step: 299950 Total Loss: 4.6978 Recon Loss: 4.6830 
[12/25 13:43:21 TiTok]: Data (t): 0.0010, 35.97/s/gpu Batch (t): 0.8897 LR: 0.000082 Step: 300000 Total Loss: 4.7047 Recon Loss: 4.6899 
Model weights saved in titok_b64_stage1_run1/checkpoint-300000/unwrapped_model/pytorch_model.bin
[12/25 13:43:23 TiTok]: Saved state to titok_b64_stage1_run1/checkpoint-300000
Model weights saved in titok_b64_stage1_run1/checkpoint-300000/ema_model/pytorch_model.bin
[12/25 13:43:39 TiTok]: Reconstructing images...
[12/25 13:43:40 TiTok]: Computing metrics on the validation set.
[12/25 13:58:06 TiTok]: EMA EVALUATION Step: 300000 
[12/25 13:58:06 TiTok]: {'CodebookEntropy': tensor(11.6447, device='cuda:0', dtype=torch.float64),
 'CodebookUsage': 0.784912109375,
 'InceptionScore': 39.36518419487719,
 'rFID': 77.70921036914183}
[12/25 13:59:25 TiTok]: Data (t): 0.0011, 39.40/s/gpu Batch (t): 0.8122 LR: 0.000082 Step: 300050 Total Loss: 4.0055 Recon Loss: 3.9906 
[12/25 14:00:05 TiTok]: Data (t): 0.0011, 39.44/s/gpu Batch (t): 0.8114 LR: 0.000082 Step: 300100 Total Loss: 4.6674 Recon Loss: 4.6526 
[12/25 14:00:46 TiTok]: Data (t): 0.0010, 39.48/s/gpu Batch (t): 0.8106 LR: 0.000082 Step: 300150 Total Loss: 5.4218 Recon Loss: 5.4070 
[12/25 14:01:27 TiTok]: Data (t): 0.0011, 38.77/s/gpu Batch (t): 0.8254 LR: 0.000082 Step: 300200 Total Loss: 3.9874 Recon Loss: 3.9726 
[12/25 14:02:07 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000082 Step: 300250 Total Loss: 5.3707 Recon Loss: 5.3560 
[12/25 14:02:48 TiTok]: Data (t): 0.0011, 38.89/s/gpu Batch (t): 0.8228 LR: 0.000082 Step: 300300 Total Loss: 4.6227 Recon Loss: 4.6079 
[12/25 14:03:29 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8229 LR: 0.000082 Step: 300350 Total Loss: 5.4126 Recon Loss: 5.3978 
[12/25 14:04:10 TiTok]: Data (t): 0.0011, 39.21/s/gpu Batch (t): 0.8162 LR: 0.000082 Step: 300400 Total Loss: 4.6186 Recon Loss: 4.6038 
[12/25 14:04:51 TiTok]: Data (t): 0.0011, 38.91/s/gpu Batch (t): 0.8225 LR: 0.000082 Step: 300450 Total Loss: 4.6662 Recon Loss: 4.6514 
[12/25 14:05:32 TiTok]: Data (t): 0.0010, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000082 Step: 300500 Total Loss: 3.9875 Recon Loss: 3.9726 
[12/25 14:06:13 TiTok]: Data (t): 0.0010, 38.93/s/gpu Batch (t): 0.8220 LR: 0.000082 Step: 300550 Total Loss: 4.6134 Recon Loss: 4.5985 
[12/25 14:06:54 TiTok]: Data (t): 0.0011, 37.86/s/gpu Batch (t): 0.8451 LR: 0.000082 Step: 300600 Total Loss: 4.6663 Recon Loss: 4.6515 
Epoch 30/99 started.
[12/25 14:07:35 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000082 Step: 300650 Total Loss: 3.9132 Recon Loss: 3.8984 
[12/25 14:08:16 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8175 LR: 0.000082 Step: 300700 Total Loss: 3.9570 Recon Loss: 3.9422 
[12/25 14:08:57 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8230 LR: 0.000082 Step: 300750 Total Loss: 4.0298 Recon Loss: 4.0149 
[12/25 14:09:38 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000082 Step: 300800 Total Loss: 3.9189 Recon Loss: 3.9041 
[12/25 14:10:19 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000082 Step: 300850 Total Loss: 4.7554 Recon Loss: 4.7406 
[12/25 14:11:00 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000082 Step: 300900 Total Loss: 5.3893 Recon Loss: 5.3744 
[12/25 14:11:41 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8193 LR: 0.000082 Step: 300950 Total Loss: 3.9796 Recon Loss: 3.9647 
[12/25 14:12:22 TiTok]: Data (t): 0.0011, 35.17/s/gpu Batch (t): 0.9100 LR: 0.000082 Step: 301000 Total Loss: 4.6721 Recon Loss: 4.6572 
[12/25 14:13:03 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000082 Step: 301050 Total Loss: 4.7108 Recon Loss: 4.6959 
[12/25 14:13:43 TiTok]: Data (t): 0.0010, 38.75/s/gpu Batch (t): 0.8257 LR: 0.000082 Step: 301100 Total Loss: 4.6581 Recon Loss: 4.6433 
[12/25 14:14:24 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000082 Step: 301150 Total Loss: 4.6596 Recon Loss: 4.6448 
[12/25 14:15:05 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8202 LR: 0.000082 Step: 301200 Total Loss: 3.9066 Recon Loss: 3.8917 
[12/25 14:15:46 TiTok]: Data (t): 0.0010, 39.16/s/gpu Batch (t): 0.8173 LR: 0.000082 Step: 301250 Total Loss: 4.6766 Recon Loss: 4.6617 
[12/25 14:16:27 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8170 LR: 0.000082 Step: 301300 Total Loss: 3.9500 Recon Loss: 3.9353 
[12/25 14:17:08 TiTok]: Data (t): 0.0010, 39.45/s/gpu Batch (t): 0.8112 LR: 0.000082 Step: 301350 Total Loss: 5.3697 Recon Loss: 5.3548 
[12/25 14:17:49 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000082 Step: 301400 Total Loss: 4.6600 Recon Loss: 4.6451 
[12/25 14:18:30 TiTok]: Data (t): 0.0019, 39.16/s/gpu Batch (t): 0.8172 LR: 0.000082 Step: 301450 Total Loss: 3.8974 Recon Loss: 3.8825 
[12/25 14:19:10 TiTok]: Data (t): 0.0010, 39.25/s/gpu Batch (t): 0.8152 LR: 0.000082 Step: 301500 Total Loss: 3.9289 Recon Loss: 3.9140 
[12/25 14:19:51 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000082 Step: 301550 Total Loss: 6.1278 Recon Loss: 6.1129 
[12/25 14:20:32 TiTok]: Data (t): 0.0010, 39.32/s/gpu Batch (t): 0.8139 LR: 0.000082 Step: 301600 Total Loss: 5.4305 Recon Loss: 5.4158 
[12/25 14:21:13 TiTok]: Data (t): 0.0010, 38.77/s/gpu Batch (t): 0.8254 LR: 0.000082 Step: 301650 Total Loss: 4.7406 Recon Loss: 4.7258 
[12/25 14:21:54 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000082 Step: 301700 Total Loss: 3.9277 Recon Loss: 3.9129 
[12/25 14:22:35 TiTok]: Data (t): 0.0010, 38.87/s/gpu Batch (t): 0.8232 LR: 0.000082 Step: 301750 Total Loss: 3.9399 Recon Loss: 3.9251 
[12/25 14:23:16 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8204 LR: 0.000082 Step: 301800 Total Loss: 3.9280 Recon Loss: 3.9131 
[12/25 14:23:57 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8194 LR: 0.000082 Step: 301850 Total Loss: 3.9605 Recon Loss: 3.9457 
[12/25 14:24:38 TiTok]: Data (t): 0.0010, 38.83/s/gpu Batch (t): 0.8242 LR: 0.000082 Step: 301900 Total Loss: 4.7015 Recon Loss: 4.6867 
[12/25 14:25:18 TiTok]: Data (t): 0.0011, 39.29/s/gpu Batch (t): 0.8146 LR: 0.000082 Step: 301950 Total Loss: 5.3314 Recon Loss: 5.3165 
[12/25 14:25:59 TiTok]: Data (t): 0.0010, 35.07/s/gpu Batch (t): 0.9125 LR: 0.000082 Step: 302000 Total Loss: 4.6520 Recon Loss: 4.6371 
[12/25 14:26:40 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000082 Step: 302050 Total Loss: 4.7077 Recon Loss: 4.6928 
[12/25 14:27:21 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8201 LR: 0.000082 Step: 302100 Total Loss: 6.1252 Recon Loss: 6.1104 
[12/25 14:28:02 TiTok]: Data (t): 0.0010, 39.29/s/gpu Batch (t): 0.8145 LR: 0.000082 Step: 302150 Total Loss: 4.6683 Recon Loss: 4.6535 
[12/25 14:28:43 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8143 LR: 0.000082 Step: 302200 Total Loss: 4.7135 Recon Loss: 4.6988 
[12/25 14:29:24 TiTok]: Data (t): 0.0010, 39.41/s/gpu Batch (t): 0.8121 LR: 0.000082 Step: 302250 Total Loss: 5.3832 Recon Loss: 5.3682 
[12/25 14:30:05 TiTok]: Data (t): 0.0010, 38.74/s/gpu Batch (t): 0.8261 LR: 0.000082 Step: 302300 Total Loss: 5.3413 Recon Loss: 5.3264 
[12/25 14:30:46 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8166 LR: 0.000082 Step: 302350 Total Loss: 5.4136 Recon Loss: 5.3987 
[12/25 14:31:27 TiTok]: Data (t): 0.0012, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000082 Step: 302400 Total Loss: 5.4486 Recon Loss: 5.4338 
[12/25 14:32:07 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000082 Step: 302450 Total Loss: 3.9669 Recon Loss: 3.9521 
[12/25 14:32:48 TiTok]: Data (t): 0.0010, 39.27/s/gpu Batch (t): 0.8149 LR: 0.000082 Step: 302500 Total Loss: 5.4286 Recon Loss: 5.4138 
[12/25 14:33:29 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8205 LR: 0.000082 Step: 302550 Total Loss: 4.6552 Recon Loss: 4.6403 
[12/25 14:34:10 TiTok]: Data (t): 0.0012, 39.31/s/gpu Batch (t): 0.8141 LR: 0.000082 Step: 302600 Total Loss: 4.0063 Recon Loss: 3.9915 
[12/25 14:34:51 TiTok]: Data (t): 0.0010, 39.47/s/gpu Batch (t): 0.8108 LR: 0.000082 Step: 302650 Total Loss: 4.6848 Recon Loss: 4.6700 
[12/25 14:35:32 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000082 Step: 302700 Total Loss: 4.0022 Recon Loss: 3.9874 
[12/25 14:36:13 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000082 Step: 302750 Total Loss: 5.3993 Recon Loss: 5.3845 
[12/25 14:36:54 TiTok]: Data (t): 0.0011, 38.77/s/gpu Batch (t): 0.8254 LR: 0.000082 Step: 302800 Total Loss: 4.7450 Recon Loss: 4.7301 
[12/25 14:37:34 TiTok]: Data (t): 0.0010, 38.89/s/gpu Batch (t): 0.8228 LR: 0.000082 Step: 302850 Total Loss: 3.9855 Recon Loss: 3.9707 
[12/25 14:38:15 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8183 LR: 0.000082 Step: 302900 Total Loss: 4.0001 Recon Loss: 3.9852 
[12/25 14:38:56 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8125 LR: 0.000082 Step: 302950 Total Loss: 5.4006 Recon Loss: 5.3858 
[12/25 14:39:37 TiTok]: Data (t): 0.0010, 34.85/s/gpu Batch (t): 0.9181 LR: 0.000082 Step: 303000 Total Loss: 3.8943 Recon Loss: 3.8795 
[12/25 14:40:18 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8157 LR: 0.000082 Step: 303050 Total Loss: 4.7228 Recon Loss: 4.7079 
[12/25 14:40:59 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000082 Step: 303100 Total Loss: 3.9603 Recon Loss: 3.9456 
[12/25 14:41:40 TiTok]: Data (t): 0.0010, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000082 Step: 303150 Total Loss: 5.4221 Recon Loss: 5.4074 
[12/25 14:42:21 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000082 Step: 303200 Total Loss: 5.4030 Recon Loss: 5.3882 
[12/25 14:43:02 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8235 LR: 0.000082 Step: 303250 Total Loss: 4.7059 Recon Loss: 4.6911 
[12/25 14:43:42 TiTok]: Data (t): 0.0011, 39.00/s/gpu Batch (t): 0.8204 LR: 0.000082 Step: 303300 Total Loss: 4.7289 Recon Loss: 4.7140 
[12/25 14:44:23 TiTok]: Data (t): 0.0011, 38.82/s/gpu Batch (t): 0.8243 LR: 0.000082 Step: 303350 Total Loss: 4.6462 Recon Loss: 4.6315 
[12/25 14:45:04 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000082 Step: 303400 Total Loss: 6.0947 Recon Loss: 6.0798 
[12/25 14:45:45 TiTok]: Data (t): 0.0010, 39.01/s/gpu Batch (t): 0.8203 LR: 0.000082 Step: 303450 Total Loss: 5.4115 Recon Loss: 5.3967 
[12/25 14:46:26 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000082 Step: 303500 Total Loss: 4.6408 Recon Loss: 4.6260 
[12/25 14:47:07 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000082 Step: 303550 Total Loss: 3.9416 Recon Loss: 3.9269 
[12/25 14:47:48 TiTok]: Data (t): 0.0011, 39.50/s/gpu Batch (t): 0.8102 LR: 0.000082 Step: 303600 Total Loss: 4.6598 Recon Loss: 4.6450 
[12/25 14:48:29 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8218 LR: 0.000082 Step: 303650 Total Loss: 3.9417 Recon Loss: 3.9269 
[12/25 14:49:10 TiTok]: Data (t): 0.0010, 39.45/s/gpu Batch (t): 0.8111 LR: 0.000082 Step: 303700 Total Loss: 5.3667 Recon Loss: 5.3519 
[12/25 14:49:51 TiTok]: Data (t): 0.0010, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000082 Step: 303750 Total Loss: 4.6348 Recon Loss: 4.6200 
[12/25 14:50:31 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000082 Step: 303800 Total Loss: 4.6103 Recon Loss: 4.5955 
[12/25 14:51:12 TiTok]: Data (t): 0.0011, 38.98/s/gpu Batch (t): 0.8208 LR: 0.000082 Step: 303850 Total Loss: 4.0050 Recon Loss: 3.9902 
[12/25 14:51:53 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000082 Step: 303900 Total Loss: 5.3576 Recon Loss: 5.3428 
[12/25 14:52:34 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8160 LR: 0.000082 Step: 303950 Total Loss: 4.6773 Recon Loss: 4.6625 
[12/25 14:53:15 TiTok]: Data (t): 0.0010, 35.34/s/gpu Batch (t): 0.9055 LR: 0.000082 Step: 304000 Total Loss: 6.8226 Recon Loss: 6.8077 
[12/25 14:53:56 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000082 Step: 304050 Total Loss: 3.9312 Recon Loss: 3.9163 
[12/25 14:54:37 TiTok]: Data (t): 0.0011, 39.23/s/gpu Batch (t): 0.8158 LR: 0.000082 Step: 304100 Total Loss: 4.6613 Recon Loss: 4.6465 
[12/25 14:55:18 TiTok]: Data (t): 0.0010, 38.84/s/gpu Batch (t): 0.8238 LR: 0.000082 Step: 304150 Total Loss: 3.9608 Recon Loss: 3.9460 
[12/25 14:55:59 TiTok]: Data (t): 0.0011, 38.45/s/gpu Batch (t): 0.8322 LR: 0.000082 Step: 304200 Total Loss: 4.6616 Recon Loss: 4.6467 
[12/25 14:56:39 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000082 Step: 304250 Total Loss: 3.9462 Recon Loss: 3.9313 
[12/25 14:57:20 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8198 LR: 0.000082 Step: 304300 Total Loss: 3.9719 Recon Loss: 3.9571 
[12/25 14:58:01 TiTok]: Data (t): 0.0011, 39.25/s/gpu Batch (t): 0.8153 LR: 0.000082 Step: 304350 Total Loss: 4.6354 Recon Loss: 4.6205 
[12/25 14:58:42 TiTok]: Data (t): 0.0011, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000082 Step: 304400 Total Loss: 5.3469 Recon Loss: 5.3320 
[12/25 14:59:23 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8186 LR: 0.000082 Step: 304450 Total Loss: 4.0055 Recon Loss: 3.9906 
[12/25 15:00:04 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000082 Step: 304500 Total Loss: 3.9257 Recon Loss: 3.9109 
[12/25 15:00:45 TiTok]: Data (t): 0.0010, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000082 Step: 304550 Total Loss: 5.4047 Recon Loss: 5.3898 
[12/25 15:01:26 TiTok]: Data (t): 0.0018, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000082 Step: 304600 Total Loss: 4.0201 Recon Loss: 4.0052 
[12/25 15:02:07 TiTok]: Data (t): 0.0010, 39.23/s/gpu Batch (t): 0.8158 LR: 0.000082 Step: 304650 Total Loss: 4.6518 Recon Loss: 4.6370 
[12/25 15:02:47 TiTok]: Data (t): 0.0010, 39.24/s/gpu Batch (t): 0.8156 LR: 0.000082 Step: 304700 Total Loss: 4.0295 Recon Loss: 4.0147 
[12/25 15:03:28 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000082 Step: 304750 Total Loss: 4.6512 Recon Loss: 4.6363 
[12/25 15:04:09 TiTok]: Data (t): 0.0011, 39.46/s/gpu Batch (t): 0.8109 LR: 0.000082 Step: 304800 Total Loss: 4.6397 Recon Loss: 4.6249 
[12/25 15:04:50 TiTok]: Data (t): 0.0010, 39.43/s/gpu Batch (t): 0.8116 LR: 0.000082 Step: 304850 Total Loss: 4.6387 Recon Loss: 4.6240 
[12/25 15:05:31 TiTok]: Data (t): 0.0030, 38.72/s/gpu Batch (t): 0.8265 LR: 0.000082 Step: 304900 Total Loss: 3.9641 Recon Loss: 3.9494 
[12/25 15:06:12 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000082 Step: 304950 Total Loss: 4.6517 Recon Loss: 4.6369 
[12/25 15:06:53 TiTok]: Data (t): 0.0011, 35.32/s/gpu Batch (t): 0.9061 LR: 0.000082 Step: 305000 Total Loss: 4.6218 Recon Loss: 4.6071 
[12/25 15:06:54 TiTok]: Reconstructing images...
[12/25 15:07:35 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000082 Step: 305050 Total Loss: 3.9615 Recon Loss: 3.9466 
[12/25 15:08:16 TiTok]: Data (t): 0.0011, 39.46/s/gpu Batch (t): 0.8109 LR: 0.000082 Step: 305100 Total Loss: 5.3717 Recon Loss: 5.3568 
[12/25 15:08:57 TiTok]: Data (t): 0.0011, 39.12/s/gpu Batch (t): 0.8180 LR: 0.000082 Step: 305150 Total Loss: 3.9531 Recon Loss: 3.9383 
[12/25 15:09:38 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000082 Step: 305200 Total Loss: 5.4053 Recon Loss: 5.3904 
[12/25 15:10:19 TiTok]: Data (t): 0.0010, 38.90/s/gpu Batch (t): 0.8227 LR: 0.000082 Step: 305250 Total Loss: 4.6686 Recon Loss: 4.6538 
[12/25 15:11:00 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8184 LR: 0.000082 Step: 305300 Total Loss: 4.6803 Recon Loss: 4.6656 
[12/25 15:11:41 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8191 LR: 0.000082 Step: 305350 Total Loss: 5.3893 Recon Loss: 5.3745 
[12/25 15:12:22 TiTok]: Data (t): 0.0010, 38.95/s/gpu Batch (t): 0.8216 LR: 0.000082 Step: 305400 Total Loss: 3.9568 Recon Loss: 3.9419 
[12/25 15:13:02 TiTok]: Data (t): 0.0015, 39.28/s/gpu Batch (t): 0.8146 LR: 0.000082 Step: 305450 Total Loss: 4.6340 Recon Loss: 4.6192 
[12/25 15:13:43 TiTok]: Data (t): 0.0011, 38.91/s/gpu Batch (t): 0.8224 LR: 0.000082 Step: 305500 Total Loss: 5.3862 Recon Loss: 5.3714 
[12/25 15:14:24 TiTok]: Data (t): 0.0010, 39.09/s/gpu Batch (t): 0.8185 LR: 0.000082 Step: 305550 Total Loss: 4.6905 Recon Loss: 4.6756 
[12/25 15:15:05 TiTok]: Data (t): 0.0011, 39.27/s/gpu Batch (t): 0.8148 LR: 0.000082 Step: 305600 Total Loss: 4.6837 Recon Loss: 4.6688 
[12/25 15:15:46 TiTok]: Data (t): 0.0010, 39.19/s/gpu Batch (t): 0.8165 LR: 0.000082 Step: 305650 Total Loss: 4.7043 Recon Loss: 4.6894 
[12/25 15:16:27 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000082 Step: 305700 Total Loss: 3.9500 Recon Loss: 3.9351 
[12/25 15:17:08 TiTok]: Data (t): 0.0010, 39.49/s/gpu Batch (t): 0.8104 LR: 0.000082 Step: 305750 Total Loss: 4.6746 Recon Loss: 4.6597 
[12/25 15:17:49 TiTok]: Data (t): 0.0010, 38.88/s/gpu Batch (t): 0.8231 LR: 0.000082 Step: 305800 Total Loss: 4.0229 Recon Loss: 4.0081 
[12/25 15:18:30 TiTok]: Data (t): 0.0011, 39.24/s/gpu Batch (t): 0.8156 LR: 0.000082 Step: 305850 Total Loss: 4.0075 Recon Loss: 3.9926 
[12/25 15:19:10 TiTok]: Data (t): 0.0010, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000082 Step: 305900 Total Loss: 4.6917 Recon Loss: 4.6768 
[12/25 15:19:51 TiTok]: Data (t): 0.0010, 39.39/s/gpu Batch (t): 0.8123 LR: 0.000082 Step: 305950 Total Loss: 5.3586 Recon Loss: 5.3438 
[12/25 15:20:32 TiTok]: Data (t): 0.0011, 34.52/s/gpu Batch (t): 0.9270 LR: 0.000082 Step: 306000 Total Loss: 3.9957 Recon Loss: 3.9808 
[12/25 15:21:13 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000082 Step: 306050 Total Loss: 4.5920 Recon Loss: 4.5772 
[12/25 15:21:54 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000082 Step: 306100 Total Loss: 3.9448 Recon Loss: 3.9299 
[12/25 15:22:35 TiTok]: Data (t): 0.0010, 38.99/s/gpu Batch (t): 0.8208 LR: 0.000082 Step: 306150 Total Loss: 4.7216 Recon Loss: 4.7067 
[12/25 15:23:16 TiTok]: Data (t): 0.0010, 39.17/s/gpu Batch (t): 0.8169 LR: 0.000082 Step: 306200 Total Loss: 4.0012 Recon Loss: 3.9863 
[12/25 15:23:57 TiTok]: Data (t): 0.0010, 38.92/s/gpu Batch (t): 0.8223 LR: 0.000082 Step: 306250 Total Loss: 3.9466 Recon Loss: 3.9317 
[12/25 15:24:37 TiTok]: Data (t): 0.0011, 38.03/s/gpu Batch (t): 0.8415 LR: 0.000082 Step: 306300 Total Loss: 3.9486 Recon Loss: 3.9338 
[12/25 15:25:18 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8198 LR: 0.000082 Step: 306350 Total Loss: 4.6637 Recon Loss: 4.6489 
[12/25 15:25:59 TiTok]: Data (t): 0.0010, 39.18/s/gpu Batch (t): 0.8168 LR: 0.000082 Step: 306400 Total Loss: 6.1409 Recon Loss: 6.1260 
[12/25 15:26:40 TiTok]: Data (t): 0.0010, 39.06/s/gpu Batch (t): 0.8192 LR: 0.000082 Step: 306450 Total Loss: 4.7310 Recon Loss: 4.7161 
[12/25 15:27:21 TiTok]: Data (t): 0.0011, 39.35/s/gpu Batch (t): 0.8132 LR: 0.000082 Step: 306500 Total Loss: 3.9961 Recon Loss: 3.9812 
[12/25 15:28:02 TiTok]: Data (t): 0.0011, 39.57/s/gpu Batch (t): 0.8087 LR: 0.000082 Step: 306550 Total Loss: 6.0845 Recon Loss: 6.0697 
[12/25 15:28:43 TiTok]: Data (t): 0.0015, 38.83/s/gpu Batch (t): 0.8240 LR: 0.000082 Step: 306600 Total Loss: 4.6644 Recon Loss: 4.6495 
[12/25 15:29:24 TiTok]: Data (t): 0.0010, 39.04/s/gpu Batch (t): 0.8197 LR: 0.000082 Step: 306650 Total Loss: 5.3906 Recon Loss: 5.3757 
[12/25 15:30:04 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000081 Step: 306700 Total Loss: 3.9559 Recon Loss: 3.9411 
[12/25 15:30:45 TiTok]: Data (t): 0.0010, 38.96/s/gpu Batch (t): 0.8215 LR: 0.000081 Step: 306750 Total Loss: 4.0044 Recon Loss: 3.9895 
[12/25 15:31:26 TiTok]: Data (t): 0.0011, 39.38/s/gpu Batch (t): 0.8127 LR: 0.000081 Step: 306800 Total Loss: 6.0732 Recon Loss: 6.0584 
[12/25 15:32:07 TiTok]: Data (t): 0.0010, 39.07/s/gpu Batch (t): 0.8190 LR: 0.000081 Step: 306850 Total Loss: 4.6632 Recon Loss: 4.6485 
[12/25 15:32:48 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000081 Step: 306900 Total Loss: 4.6566 Recon Loss: 4.6418 
[12/25 15:33:29 TiTok]: Data (t): 0.0010, 39.52/s/gpu Batch (t): 0.8097 LR: 0.000081 Step: 306950 Total Loss: 4.6664 Recon Loss: 4.6515 
[12/25 15:34:10 TiTok]: Data (t): 0.0010, 35.20/s/gpu Batch (t): 0.9091 LR: 0.000081 Step: 307000 Total Loss: 5.3452 Recon Loss: 5.3304 
[12/25 15:34:51 TiTok]: Data (t): 0.0010, 39.40/s/gpu Batch (t): 0.8123 LR: 0.000081 Step: 307050 Total Loss: 5.3780 Recon Loss: 5.3632 
[12/25 15:35:32 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8161 LR: 0.000081 Step: 307100 Total Loss: 4.6840 Recon Loss: 4.6691 
[12/25 15:36:12 TiTok]: Data (t): 0.0010, 39.38/s/gpu Batch (t): 0.8126 LR: 0.000081 Step: 307150 Total Loss: 4.6986 Recon Loss: 4.6838 
[12/25 15:36:53 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8202 LR: 0.000081 Step: 307200 Total Loss: 4.7312 Recon Loss: 4.7164 
[12/25 15:37:34 TiTok]: Data (t): 0.0009, 39.12/s/gpu Batch (t): 0.8181 LR: 0.000081 Step: 307250 Total Loss: 4.6761 Recon Loss: 4.6613 
[12/25 15:38:15 TiTok]: Data (t): 0.0011, 39.30/s/gpu Batch (t): 0.8142 LR: 0.000081 Step: 307300 Total Loss: 4.6757 Recon Loss: 4.6609 
[12/25 15:38:56 TiTok]: Data (t): 0.0010, 39.39/s/gpu Batch (t): 0.8124 LR: 0.000081 Step: 307350 Total Loss: 3.8609 Recon Loss: 3.8462 
[12/25 15:39:37 TiTok]: Data (t): 0.0011, 38.80/s/gpu Batch (t): 0.8248 LR: 0.000081 Step: 307400 Total Loss: 4.0340 Recon Loss: 4.0191 
[12/25 15:40:18 TiTok]: Data (t): 0.0011, 38.09/s/gpu Batch (t): 0.8401 LR: 0.000081 Step: 307450 Total Loss: 5.4154 Recon Loss: 5.4005 
[12/25 15:40:59 TiTok]: Data (t): 0.0011, 39.02/s/gpu Batch (t): 0.8200 LR: 0.000081 Step: 307500 Total Loss: 3.9522 Recon Loss: 3.9373 
[12/25 15:41:40 TiTok]: Data (t): 0.0011, 38.99/s/gpu Batch (t): 0.8207 LR: 0.000081 Step: 307550 Total Loss: 4.6592 Recon Loss: 4.6444 
[12/25 15:42:20 TiTok]: Data (t): 0.0011, 39.04/s/gpu Batch (t): 0.8196 LR: 0.000081 Step: 307600 Total Loss: 5.3500 Recon Loss: 5.3352 
[12/25 15:43:01 TiTok]: Data (t): 0.0010, 39.33/s/gpu Batch (t): 0.8137 LR: 0.000081 Step: 307650 Total Loss: 6.0842 Recon Loss: 6.0695 
[12/25 15:43:42 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8173 LR: 0.000081 Step: 307700 Total Loss: 4.6482 Recon Loss: 4.6333 
[12/25 15:44:23 TiTok]: Data (t): 0.0010, 39.36/s/gpu Batch (t): 0.8130 LR: 0.000081 Step: 307750 Total Loss: 4.6567 Recon Loss: 4.6420 
[12/25 15:45:04 TiTok]: Data (t): 0.0010, 38.87/s/gpu Batch (t): 0.8232 LR: 0.000081 Step: 307800 Total Loss: 3.9702 Recon Loss: 3.9554 
[12/25 15:45:45 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8162 LR: 0.000081 Step: 307850 Total Loss: 3.9408 Recon Loss: 3.9260 
[12/25 15:46:26 TiTok]: Data (t): 0.0011, 39.41/s/gpu Batch (t): 0.8121 LR: 0.000081 Step: 307900 Total Loss: 3.9537 Recon Loss: 3.9388 
[12/25 15:47:07 TiTok]: Data (t): 0.0010, 39.21/s/gpu Batch (t): 0.8162 LR: 0.000081 Step: 307950 Total Loss: 3.9269 Recon Loss: 3.9121 
[12/25 15:47:48 TiTok]: Data (t): 0.0010, 34.96/s/gpu Batch (t): 0.9153 LR: 0.000081 Step: 308000 Total Loss: 5.3963 Recon Loss: 5.3815 
[12/25 15:48:28 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8164 LR: 0.000081 Step: 308050 Total Loss: 4.6463 Recon Loss: 4.6314 
[12/25 15:49:09 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8211 LR: 0.000081 Step: 308100 Total Loss: 4.6929 Recon Loss: 4.6782 
[12/25 15:49:50 TiTok]: Data (t): 0.0010, 38.91/s/gpu Batch (t): 0.8225 LR: 0.000081 Step: 308150 Total Loss: 3.8733 Recon Loss: 3.8584 
[12/25 15:50:31 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000081 Step: 308200 Total Loss: 4.6761 Recon Loss: 4.6613 
[12/25 15:51:12 TiTok]: Data (t): 0.0011, 39.15/s/gpu Batch (t): 0.8174 LR: 0.000081 Step: 308250 Total Loss: 5.3676 Recon Loss: 5.3528 
[12/25 15:51:53 TiTok]: Data (t): 0.0010, 38.94/s/gpu Batch (t): 0.8217 LR: 0.000081 Step: 308300 Total Loss: 4.7026 Recon Loss: 4.6878 
[12/25 15:52:34 TiTok]: Data (t): 0.0011, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000081 Step: 308350 Total Loss: 4.6812 Recon Loss: 4.6663 
[12/25 15:53:15 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000081 Step: 308400 Total Loss: 4.0291 Recon Loss: 4.0142 
[12/25 15:53:56 TiTok]: Data (t): 0.0010, 39.45/s/gpu Batch (t): 0.8111 LR: 0.000081 Step: 308450 Total Loss: 5.3922 Recon Loss: 5.3774 
[12/25 15:54:36 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8183 LR: 0.000081 Step: 308500 Total Loss: 5.4171 Recon Loss: 5.4023 
[12/25 15:55:17 TiTok]: Data (t): 0.0010, 39.44/s/gpu Batch (t): 0.8113 LR: 0.000081 Step: 308550 Total Loss: 4.5953 Recon Loss: 4.5804 
[12/25 15:55:58 TiTok]: Data (t): 0.0010, 38.63/s/gpu Batch (t): 0.8283 LR: 0.000081 Step: 308600 Total Loss: 6.1198 Recon Loss: 6.1049 
[12/25 15:56:39 TiTok]: Data (t): 0.0010, 39.28/s/gpu Batch (t): 0.8147 LR: 0.000081 Step: 308650 Total Loss: 4.7244 Recon Loss: 4.7096 
[12/25 15:57:20 TiTok]: Data (t): 0.0010, 39.05/s/gpu Batch (t): 0.8195 LR: 0.000081 Step: 308700 Total Loss: 3.9164 Recon Loss: 3.9016 
[12/25 15:58:01 TiTok]: Data (t): 0.0010, 39.11/s/gpu Batch (t): 0.8182 LR: 0.000081 Step: 308750 Total Loss: 4.6763 Recon Loss: 4.6615 
[12/25 15:58:42 TiTok]: Data (t): 0.0011, 39.08/s/gpu Batch (t): 0.8188 LR: 0.000081 Step: 308800 Total Loss: 5.3020 Recon Loss: 5.2871 
[12/25 15:59:22 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000081 Step: 308850 Total Loss: 3.9135 Recon Loss: 3.8987 
[12/25 16:00:03 TiTok]: Data (t): 0.0010, 38.86/s/gpu Batch (t): 0.8236 LR: 0.000081 Step: 308900 Total Loss: 4.6281 Recon Loss: 4.6132 
[12/25 16:00:44 TiTok]: Data (t): 0.0011, 39.34/s/gpu Batch (t): 0.8135 LR: 0.000081 Step: 308950 Total Loss: 3.8228 Recon Loss: 3.8080 
[12/25 16:01:25 TiTok]: Data (t): 0.0010, 35.42/s/gpu Batch (t): 0.9035 LR: 0.000081 Step: 309000 Total Loss: 4.6742 Recon Loss: 4.6594 
[12/25 16:02:06 TiTok]: Data (t): 0.0010, 38.75/s/gpu Batch (t): 0.8259 LR: 0.000081 Step: 309050 Total Loss: 4.6213 Recon Loss: 4.6065 
[12/25 16:02:47 TiTok]: Data (t): 0.0010, 39.02/s/gpu Batch (t): 0.8200 LR: 0.000081 Step: 309100 Total Loss: 5.4387 Recon Loss: 5.4238 
[12/25 16:03:28 TiTok]: Data (t): 0.0010, 39.10/s/gpu Batch (t): 0.8185 LR: 0.000081 Step: 309150 Total Loss: 4.6741 Recon Loss: 4.6591 
[12/25 16:04:09 TiTok]: Data (t): 0.0012, 39.18/s/gpu Batch (t): 0.8167 LR: 0.000081 Step: 309200 Total Loss: 4.6519 Recon Loss: 4.6370 
[12/25 16:04:50 TiTok]: Data (t): 0.0010, 39.03/s/gpu Batch (t): 0.8199 LR: 0.000081 Step: 309250 Total Loss: 5.3347 Recon Loss: 5.3198 
[12/25 16:05:30 TiTok]: Data (t): 0.0010, 39.20/s/gpu Batch (t): 0.8163 LR: 0.000081 Step: 309300 Total Loss: 3.9661 Recon Loss: 3.9512 
[12/25 16:06:11 TiTok]: Data (t): 0.0010, 39.12/s/gpu Batch (t): 0.8179 LR: 0.000081 Step: 309350 Total Loss: 4.7021 Recon Loss: 4.6872 
[12/25 16:06:52 TiTok]: Data (t): 0.0011, 39.22/s/gpu Batch (t): 0.8159 LR: 0.000081 Step: 309400 Total Loss: 4.6546 Recon Loss: 4.6398 
[12/25 16:07:33 TiTok]: Data (t): 0.0011, 38.97/s/gpu Batch (t): 0.8212 LR: 0.000081 Step: 309450 Total Loss: 3.9113 Recon Loss: 3.8965 
[12/25 16:08:14 TiTok]: Data (t): 0.0010, 39.00/s/gpu Batch (t): 0.8206 LR: 0.000081 Step: 309500 Total Loss: 4.6443 Recon Loss: 4.6294 
[12/25 16:08:55 TiTok]: Data (t): 0.0011, 39.34/s/gpu Batch (t): 0.8134 LR: 0.000081 Step: 309550 Total Loss: 5.4673 Recon Loss: 5.4525 
[12/25 16:09:36 TiTok]: Data (t): 0.0011, 38.92/s/gpu Batch (t): 0.8222 LR: 0.000081 Step: 309600 Total Loss: 3.9677 Recon Loss: 3.9528 
[12/25 16:10:17 TiTok]: Data (t): 0.0011, 38.87/s/gpu Batch (t): 0.8232 LR: 0.000081 Step: 309650 Total Loss: 3.9524 Recon Loss: 3.9375 
