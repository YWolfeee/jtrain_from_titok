Sun Jan  5 19:29:07 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.28.03              Driver Version: 560.28.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A40                     Off |   00000000:2D:00.0 Off |                    0 |
|  0%   27C    P8             21W /  300W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A40                     Off |   00000000:3A:00.0 Off |                    0 |
|  0%   26C    P8             21W /  300W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A40                     Off |   00000000:3B:00.0 Off |                    0 |
|  0%   27C    P8             21W /  300W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A40                     Off |   00000000:3C:00.0 Off |                    0 |
|  0%   27C    P8             22W /  300W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA A40                     Off |   00000000:AD:00.0 Off |                    0 |
|  0%   28C    P8             21W /  300W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA A40                     Off |   00000000:AE:00.0 Off |                    0 |
|  0%   28C    P8             20W /  300W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA A40                     Off |   00000000:BD:00.0 Off |                    0 |
|  0%   27C    P8             21W /  300W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA A40                     Off |   00000000:BE:00.0 Off |                    0 |
|  0%   28C    P8             27W /  300W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
attention mode is flashattention mode is flashattention mode is flashattention mode is flashattention mode is flashattention mode is flash
attention mode is flash





attention mode is flash
[01/05 19:29:49 TiTok]: Saving config to titok_s128_matryoshka_annealing_stage1_run1/config.yaml
[01/05 19:29:49 TiTok]: Config:
experiment:
  project: titok_s128_matryoshka_annealing_stage1
  name: titok_s128_matryoshka_annealing_stage1_run1
  output_dir: titok_s128_matryoshka_annealing_stage1_run1
  max_train_examples: 1281167
  save_every: 50000
  eval_every: 50000
  generate_every: 5000
  log_every: 50
  log_grad_norm_every: 1000
  resume: true
  init_weight: ''
  logging_dir: titok_s128_matryoshka_annealing_stage1_run1/logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: base
    vit_dec_model_size: base
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 128
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
  reconstruction_regularization:
    name: matryoshka
    mask_ratio_method: uniform
    max_mask_rate: 0.999
    annealing:
      time_start: 0.25
      time_end: 0.75
losses:
  quantizer_weight: 1.0
dataset:
  params:
    train_shards_path_or_url: /mnt/rdata8/imagenet_wds/imagenet-train-{000000..000252}.tar
    eval_shards_path_or_url: /mnt/rdata8/imagenet_wds/imagenet-val-{000000..000009}.tar
    num_workers_per_gpu: 12
  preprocessing:
    resize_shorter_edge: 256
    crop_size: 256
    random_crop: true
    random_flip: true
optimizer:
  name: adamw
  params:
    learning_rate: 0.0001
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 32
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: true
  use_ema: true
  seed: 42
  max_train_steps: 1000000
  num_generated_images: 2
  max_grad_norm: 1.0
config: configs/training/stage1/titok_s128_matryoshka_annealing.yaml

[01/05 19:29:58 TiTok]: Creating model and loss module.
self.train_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    RandomCrop(size=(256, 256), padding=None)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.eval_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(256, 256))
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.train_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    RandomCrop(size=(256, 256), padding=None)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.eval_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(256, 256))
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.train_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    RandomCrop(size=(256, 256), padding=None)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.eval_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(256, 256))
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.train_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    RandomCrop(size=(256, 256), padding=None)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.eval_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(256, 256))
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.train_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    RandomCrop(size=(256, 256), padding=None)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.eval_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(256, 256))
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.train_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    RandomCrop(size=(256, 256), padding=None)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.eval_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(256, 256))
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.train_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    RandomCrop(size=(256, 256), padding=None)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.eval_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(256, 256))
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
===================================================================================================================================================================================================
Layer (type:depth-idx)                        Input Shape               Output Shape              Param #                   Param %                   Kernel Shape              Mult-Adds
===================================================================================================================================================================================================
TiTok                                         [1, 3, 256, 256]          [1, 1024, 16, 16]         98,304                      0.06%                   --                        --
├─TiTokEncoder: 1-1                           --                        [1, 12, 1, 128]           296,448                     0.17%                   --                        --
│    └─Conv2d: 2-1                            [1, 3, 256, 256]          [1, 768, 16, 16]          590,592                     0.34%                   [16, 16]                  151,191,552
│    └─LayerNorm: 2-2                         [1, 385, 768]             [1, 385, 768]             1,536                       0.00%                   --                        1,536
│    └─ModuleList: 2-3                        --                        --                        --                             --                   --                        --
│    │    └─ResidualAttentionBlock: 3-1       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-1               [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-2      [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-3               [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-4              [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-1             [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-2               [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-3             [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-2       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-5               [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-6      [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-7               [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-8              [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-4             [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-5               [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-6             [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-3       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-9               [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-10     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-11              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-12             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-7             [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-8               [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-9             [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-4       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-13              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-14     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-15              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-16             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-10            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-11              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-12            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-5       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-17              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-18     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-19              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-20             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-13            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-14              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-15            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-6       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-21              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-22     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-23              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-24             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-16            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-17              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-18            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-7       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-25              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-26     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-27              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-28             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-19            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-20              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-21            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-8       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-29              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-30     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-31              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-32             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-22            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-23              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-24            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-9       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-33              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-34     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-35              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-36             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-25            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-26              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-27            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-10      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-37              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-38     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-39              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-40             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-28            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-29              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-30            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-11      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-41              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-42     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-43              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-44             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-31            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-32              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-33            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-12      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-45              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-46     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-47              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-48             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-34            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-35              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-36            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    └─LayerNorm: 2-4                         [1, 128, 768]             [1, 128, 768]             1,536                       0.00%                   --                        1,536
│    └─Conv2d: 2-5                            [1, 768, 128, 1]          [1, 12, 128, 1]           9,228                       0.01%                   [1, 1]                    1,181,184
├─VectorQuantizer: 1-2                        [1, 12, 1, 128]           [1, 12, 1, 128]           --                             --                   --                        --
│    └─Embedding: 2-6                         [128]                     [128, 12]                 49,152                      0.03%                   --                        6,291,456
├─TiTokDecoder: 1-3                           [1, 12, 1, 128]           [1, 1024, 16, 16]         297,216                     0.17%                   --                        --
│    └─Linear: 2-7                            [1, 128, 12]              [1, 128, 768]             9,984                       0.01%                   --                        9,984
│    └─LayerNorm: 2-8                         [1, 385, 768]             [1, 385, 768]             1,536                       0.00%                   --                        1,536
│    └─ModuleList: 2-9                        --                        --                        --                             --                   --                        --
│    │    └─ResidualAttentionBlock: 3-13      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-49              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-50     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-51              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-52             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-37            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-38              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-39            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-14      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-53              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-54     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-55              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-56             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-40            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-41              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-42            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-15      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-57              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-58     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-59              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-60             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-43            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-44              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-45            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-16      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-61              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-62     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-63              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-64             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-46            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-47              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-48            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-17      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-65              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-66     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-67              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-68             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-49            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-50              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-51            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-18      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-69              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-70     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-71              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-72             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-52            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-53              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-54            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-19      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-73              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-74     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-75              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-76             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-55            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-56              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-57            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-20      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-77              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-78     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-79              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-80             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-58            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-59              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-60            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-21      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-81              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-82     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-83              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-84             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-61            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-62              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-63            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-22      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-85              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-86     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-87              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-88             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-64            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-65              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-66            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-23      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-89              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-90     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-91              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-92             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-67            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-68              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-69            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-24      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-93              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-94     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-95              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-96             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-70            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-71              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-72            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    └─LayerNorm: 2-10                        [1, 256, 768]             [1, 256, 768]             1,536                       0.00%                   --                        1,536
│    └─Sequential: 2-11                       [1, 768, 16, 16]          [1, 1024, 16, 16]         --                             --                   --                        --
│    │    └─Conv2d: 3-25                      [1, 768, 16, 16]          [1, 1536, 16, 16]         1,181,184                   0.68%                   [1, 1]                    302,383,104
│    │    └─Tanh: 3-26                        [1, 1536, 16, 16]         [1, 1536, 16, 16]         --                             --                   --                        --
│    │    └─Conv2d: 3-27                      [1, 1536, 16, 16]         [1, 1024, 16, 16]         1,573,888                   0.90%                   [1, 1]                    402,915,328
│    └─Identity: 2-12                         [1, 1024, 16, 16]         [1, 1024, 16, 16]         --                             --                   --                        --
===================================================================================================================================================================================================
Total params: 174,221,068
Trainable params: 174,221,068
Non-trainable params: 0
Total mult-adds (G): 44.53
===================================================================================================================================================================================================
Input size (MB): 0.79
Forward/backward pass size (MB): 412.11
Params size (MB): 467.33
Estimated Total Size (MB): 880.23
===================================================================================================================================================================================================
[01/05 19:30:39 TiTok]: ===================================================================================================================================================================================================
Layer (type:depth-idx)                        Input Shape               Output Shape              Param #                   Param %                   Kernel Shape              Mult-Adds
===================================================================================================================================================================================================
TiTok                                         [1, 3, 256, 256]          [1, 1024, 16, 16]         98,304                      0.06%                   --                        --
├─TiTokEncoder: 1-1                           --                        [1, 12, 1, 128]           296,448                     0.17%                   --                        --
│    └─Conv2d: 2-1                            [1, 3, 256, 256]          [1, 768, 16, 16]          590,592                     0.34%                   [16, 16]                  151,191,552
│    └─LayerNorm: 2-2                         [1, 385, 768]             [1, 385, 768]             1,536                       0.00%                   --                        1,536
│    └─ModuleList: 2-3                        --                        --                        --                             --                   --                        --
│    │    └─ResidualAttentionBlock: 3-1       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-1               [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-2      [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-3               [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-4              [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-1             [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-2               [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-3             [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-2       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-5               [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-6      [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-7               [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-8              [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-4             [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-5               [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-6             [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-3       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-9               [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-10     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-11              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-12             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-7             [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-8               [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-9             [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-4       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-13              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-14     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-15              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-16             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-10            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-11              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-12            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-5       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-17              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-18     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-19              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-20             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-13            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-14              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-15            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-6       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-21              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-22     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-23              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-24             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-16            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-17              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-18            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-7       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-25              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-26     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-27              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-28             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-19            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-20              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-21            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-8       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-29              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-30     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-31              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-32             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-22            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-23              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-24            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-9       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-33              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-34     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-35              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-36             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-25            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-26              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-27            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-10      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-37              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-38     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-39              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-40             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-28            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-29              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-30            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-11      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-41              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-42     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-43              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-44             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-31            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-32              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-33            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-12      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-45              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-46     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-47              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-48             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-34            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-35              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-36            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    └─LayerNorm: 2-4                         [1, 128, 768]             [1, 128, 768]             1,536                       0.00%                   --                        1,536
│    └─Conv2d: 2-5                            [1, 768, 128, 1]          [1, 12, 128, 1]           9,228                       0.01%                   [1, 1]                    1,181,184
├─VectorQuantizer: 1-2                        [1, 12, 1, 128]           [1, 12, 1, 128]           --                             --                   --                        --
│    └─Embedding: 2-6                         [128]                     [128, 12]                 49,152                      0.03%                   --                        6,291,456
├─TiTokDecoder: 1-3                           [1, 12, 1, 128]           [1, 1024, 16, 16]         297,216                     0.17%                   --                        --
│    └─Linear: 2-7                            [1, 128, 12]              [1, 128, 768]             9,984                       0.01%                   --                        9,984
│    └─LayerNorm: 2-8                         [1, 385, 768]             [1, 385, 768]             1,536                       0.00%                   --                        1,536
│    └─ModuleList: 2-9                        --                        --                        --                             --                   --                        --
│    │    └─ResidualAttentionBlock: 3-13      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-49              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-50     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-51              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-52             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-37            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-38              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-39            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-14      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-53              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-54     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-55              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-56             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-40            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-41              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-42            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-15      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-57              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-58     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-59              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-60             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-43            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-44              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-45            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-16      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-61              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-62     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-63              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-64             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-46            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-47              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-48            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-17      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-65              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-66     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-67              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-68             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-49            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-50              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-51            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-18      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-69              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-70     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-71              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-72             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-52            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-53              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-54            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-19      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-73              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-74     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-75              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-76             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-55            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-56              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-57            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-20      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-77              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-78     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-79              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-80             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-58            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-59              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-60            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-21      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-81              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-82     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-83              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-84             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-61            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-62              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-63            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-22      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-85              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-86     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-87              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-88             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-64            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-65              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-66            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-23      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-89              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-90     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-91              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-92             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-67            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-68              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-69            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-24      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-93              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-94     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-95              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-96             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-70            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-71              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-72            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    └─LayerNorm: 2-10                        [1, 256, 768]             [1, 256, 768]             1,536                       0.00%                   --                        1,536
│    └─Sequential: 2-11                       [1, 768, 16, 16]          [1, 1024, 16, 16]         --                             --                   --                        --
│    │    └─Conv2d: 3-25                      [1, 768, 16, 16]          [1, 1536, 16, 16]         1,181,184                   0.68%                   [1, 1]                    302,383,104
│    │    └─Tanh: 3-26                        [1, 1536, 16, 16]         [1, 1536, 16, 16]         --                             --                   --                        --
│    │    └─Conv2d: 3-27                      [1, 1536, 16, 16]         [1, 1024, 16, 16]         1,573,888                   0.90%                   [1, 1]                    402,915,328
│    └─Identity: 2-12                         [1, 1024, 16, 16]         [1, 1024, 16, 16]         --                             --                   --                        --
===================================================================================================================================================================================================
Total params: 174,221,068
Trainable params: 174,221,068
Non-trainable params: 0
Total mult-adds (G): 44.53
===================================================================================================================================================================================================
Input size (MB): 0.79
Forward/backward pass size (MB): 412.11
Params size (MB): 467.33
Estimated Total Size (MB): 880.23
===================================================================================================================================================================================================
[01/05 19:30:39 TiTok]: Creating optimizers.
[01/05 19:30:39 TiTok]: Creating lr_schedulers.
[01/05 19:30:39 TiTok]: Creating dataloaders.
self.train_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    RandomCrop(size=(256, 256), padding=None)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.eval_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(256, 256))
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
[01/05 19:30:39 TiTok]: Creating evaluator.
[01/05 19:30:46 TiTok]: Preparing model, optimizer and dataloaders
[01/05 19:30:47 TiTok]: ***** Running training *****
[01/05 19:30:47 TiTok]:   Num training steps = 1000000
[01/05 19:30:47 TiTok]:   Gradient Accumulation steps = 1
[01/05 19:30:47 TiTok]:   Instantaneous batch size per gpu = 32
[01/05 19:30:47 TiTok]:   Total train batch size (w. parallel, distributed & accumulation) = 256
[01/05 19:30:47 TiTok]: All globbed checkpoints are: []
[01/05 19:30:47 TiTok]: Training from scratch.
Epoch 0/199 started.
[01/05 19:31:23 TiTok]: Data (t): 0.0007, 54.53/s/gpu Batch (t): 0.5868 LR: 0.000001 Step: 50 Total Loss: 6.9883 Recon Loss: 6.9735 
[01/05 19:31:52 TiTok]: Data (t): 0.0006, 54.54/s/gpu Batch (t): 0.5867 LR: 0.000001 Step: 100 Total Loss: 6.9299 Recon Loss: 6.9293 
[01/05 19:32:22 TiTok]: Data (t): 0.0006, 54.87/s/gpu Batch (t): 0.5832 LR: 0.000002 Step: 150 Total Loss: 6.9039 Recon Loss: 6.9037 
[01/05 19:32:51 TiTok]: Data (t): 0.0011, 54.81/s/gpu Batch (t): 0.5838 LR: 0.000002 Step: 200 Total Loss: 6.8837 Recon Loss: 6.8836 
[01/05 19:33:20 TiTok]: Data (t): 0.0006, 54.71/s/gpu Batch (t): 0.5849 LR: 0.000003 Step: 250 Total Loss: 6.8886 Recon Loss: 6.8885 
[01/05 19:33:49 TiTok]: Data (t): 0.0006, 54.36/s/gpu Batch (t): 0.5887 LR: 0.000003 Step: 300 Total Loss: 6.8867 Recon Loss: 6.8867 
[01/05 19:34:19 TiTok]: Data (t): 0.0006, 54.18/s/gpu Batch (t): 0.5907 LR: 0.000004 Step: 350 Total Loss: 6.8790 Recon Loss: 6.8790 
[01/05 19:34:48 TiTok]: Data (t): 0.0006, 54.54/s/gpu Batch (t): 0.5867 LR: 0.000004 Step: 400 Total Loss: 6.8706 Recon Loss: 6.8706 
[01/05 19:35:17 TiTok]: Data (t): 0.0011, 54.63/s/gpu Batch (t): 0.5858 LR: 0.000005 Step: 450 Total Loss: 6.8757 Recon Loss: 6.8756 
[01/05 19:35:46 TiTok]: Data (t): 0.0011, 55.19/s/gpu Batch (t): 0.5798 LR: 0.000005 Step: 500 Total Loss: 6.8804 Recon Loss: 6.8804 
[01/05 19:36:16 TiTok]: Data (t): 0.0007, 54.50/s/gpu Batch (t): 0.5872 LR: 0.000006 Step: 550 Total Loss: 6.8639 Recon Loss: 6.8639 
[01/05 19:36:45 TiTok]: Data (t): 0.0006, 54.83/s/gpu Batch (t): 0.5836 LR: 0.000006 Step: 600 Total Loss: 6.8698 Recon Loss: 6.8698 
[01/05 19:37:14 TiTok]: Data (t): 0.0006, 54.59/s/gpu Batch (t): 0.5861 LR: 0.000007 Step: 650 Total Loss: 6.8582 Recon Loss: 6.8581 
[01/05 19:37:44 TiTok]: Data (t): 0.0006, 54.66/s/gpu Batch (t): 0.5854 LR: 0.000007 Step: 700 Total Loss: 6.8581 Recon Loss: 6.8572 
[01/05 19:38:13 TiTok]: Data (t): 0.0006, 54.66/s/gpu Batch (t): 0.5854 LR: 0.000008 Step: 750 Total Loss: 6.8437 Recon Loss: 6.8398 
[01/05 19:38:42 TiTok]: Data (t): 0.0009, 54.14/s/gpu Batch (t): 0.5911 LR: 0.000008 Step: 800 Total Loss: 6.8119 Recon Loss: 6.8101 
[01/05 19:39:11 TiTok]: Data (t): 0.0006, 54.16/s/gpu Batch (t): 0.5909 LR: 0.000009 Step: 850 Total Loss: 6.8136 Recon Loss: 6.8119 
[01/05 19:39:41 TiTok]: Data (t): 0.0006, 54.70/s/gpu Batch (t): 0.5850 LR: 0.000009 Step: 900 Total Loss: 6.7962 Recon Loss: 6.7927 
[01/05 19:40:10 TiTok]: Data (t): 0.0006, 54.65/s/gpu Batch (t): 0.5856 LR: 0.000010 Step: 950 Total Loss: 6.7826 Recon Loss: 6.7801 
[01/05 19:40:40 TiTok]: Data (t): 0.0006, 44.55/s/gpu Batch (t): 0.7182 LR: 0.000010 Step: 1000 Total Loss: 6.7697 Recon Loss: 6.7673 
[01/05 19:41:09 TiTok]: Data (t): 0.0006, 54.20/s/gpu Batch (t): 0.5904 LR: 0.000010 Step: 1050 Total Loss: 6.7499 Recon Loss: 6.7476 
[01/05 19:41:38 TiTok]: Data (t): 0.0007, 54.72/s/gpu Batch (t): 0.5848 LR: 0.000011 Step: 1100 Total Loss: 6.7493 Recon Loss: 6.7467 
[01/05 19:42:07 TiTok]: Data (t): 0.0011, 55.23/s/gpu Batch (t): 0.5794 LR: 0.000012 Step: 1150 Total Loss: 6.7420 Recon Loss: 6.7396 
[01/05 19:42:37 TiTok]: Data (t): 0.0012, 54.63/s/gpu Batch (t): 0.5858 LR: 0.000012 Step: 1200 Total Loss: 6.7318 Recon Loss: 6.7295 
[01/05 19:43:06 TiTok]: Data (t): 0.0006, 54.72/s/gpu Batch (t): 0.5848 LR: 0.000013 Step: 1250 Total Loss: 6.7441 Recon Loss: 6.7417 
[01/05 19:43:35 TiTok]: Data (t): 0.0009, 54.47/s/gpu Batch (t): 0.5875 LR: 0.000013 Step: 1300 Total Loss: 6.7399 Recon Loss: 6.7371 
[01/05 19:44:05 TiTok]: Data (t): 0.0009, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000014 Step: 1350 Total Loss: 6.7415 Recon Loss: 6.7387 
[01/05 19:44:34 TiTok]: Data (t): 0.0006, 54.17/s/gpu Batch (t): 0.5908 LR: 0.000014 Step: 1400 Total Loss: 6.7143 Recon Loss: 6.7117 
[01/05 19:45:03 TiTok]: Data (t): 0.0011, 55.09/s/gpu Batch (t): 0.5808 LR: 0.000015 Step: 1450 Total Loss: 6.6809 Recon Loss: 6.6785 
[01/05 19:45:33 TiTok]: Data (t): 0.0008, 54.53/s/gpu Batch (t): 0.5868 LR: 0.000015 Step: 1500 Total Loss: 6.6926 Recon Loss: 6.6901 
[01/05 19:46:02 TiTok]: Data (t): 0.0006, 54.60/s/gpu Batch (t): 0.5860 LR: 0.000016 Step: 1550 Total Loss: 6.7031 Recon Loss: 6.7009 
[01/05 19:46:31 TiTok]: Data (t): 0.0006, 54.63/s/gpu Batch (t): 0.5857 LR: 0.000016 Step: 1600 Total Loss: 6.6738 Recon Loss: 6.6713 
[01/05 19:47:01 TiTok]: Data (t): 0.0011, 54.52/s/gpu Batch (t): 0.5869 LR: 0.000017 Step: 1650 Total Loss: 6.6744 Recon Loss: 6.6721 
[01/05 19:47:30 TiTok]: Data (t): 0.0006, 54.52/s/gpu Batch (t): 0.5869 LR: 0.000017 Step: 1700 Total Loss: 6.6854 Recon Loss: 6.6832 
[01/05 19:47:59 TiTok]: Data (t): 0.0006, 54.09/s/gpu Batch (t): 0.5916 LR: 0.000017 Step: 1750 Total Loss: 6.6884 Recon Loss: 6.6862 
[01/05 19:48:29 TiTok]: Data (t): 0.0006, 54.21/s/gpu Batch (t): 0.5903 LR: 0.000018 Step: 1800 Total Loss: 6.6690 Recon Loss: 6.6668 
[01/05 19:48:58 TiTok]: Data (t): 0.0006, 54.60/s/gpu Batch (t): 0.5861 LR: 0.000018 Step: 1850 Total Loss: 6.6497 Recon Loss: 6.6473 
[01/05 19:49:27 TiTok]: Data (t): 0.0011, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000019 Step: 1900 Total Loss: 6.6750 Recon Loss: 6.6726 
[01/05 19:49:57 TiTok]: Data (t): 0.0011, 55.21/s/gpu Batch (t): 0.5796 LR: 0.000020 Step: 1950 Total Loss: 6.6198 Recon Loss: 6.6175 
[01/05 19:50:26 TiTok]: Data (t): 0.0011, 45.88/s/gpu Batch (t): 0.6975 LR: 0.000020 Step: 2000 Total Loss: 6.6605 Recon Loss: 6.6582 
[01/05 19:50:56 TiTok]: Data (t): 0.0006, 54.43/s/gpu Batch (t): 0.5879 LR: 0.000021 Step: 2050 Total Loss: 6.6280 Recon Loss: 6.6257 
[01/05 19:51:25 TiTok]: Data (t): 0.0009, 54.38/s/gpu Batch (t): 0.5885 LR: 0.000021 Step: 2100 Total Loss: 6.6188 Recon Loss: 6.6164 
[01/05 19:51:54 TiTok]: Data (t): 0.0009, 54.51/s/gpu Batch (t): 0.5871 LR: 0.000022 Step: 2150 Total Loss: 6.6259 Recon Loss: 6.6236 
[01/05 19:52:24 TiTok]: Data (t): 0.0006, 54.58/s/gpu Batch (t): 0.5863 LR: 0.000022 Step: 2200 Total Loss: 6.6513 Recon Loss: 6.6490 
[01/05 19:52:53 TiTok]: Data (t): 0.0006, 54.12/s/gpu Batch (t): 0.5913 LR: 0.000023 Step: 2250 Total Loss: 6.6010 Recon Loss: 6.5985 
[01/05 19:53:22 TiTok]: Data (t): 0.0006, 54.60/s/gpu Batch (t): 0.5861 LR: 0.000023 Step: 2300 Total Loss: 6.6178 Recon Loss: 6.6155 
[01/05 19:53:52 TiTok]: Data (t): 0.0008, 54.68/s/gpu Batch (t): 0.5852 LR: 0.000023 Step: 2350 Total Loss: 6.5842 Recon Loss: 6.5819 
[01/05 19:54:21 TiTok]: Data (t): 0.0011, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000024 Step: 2400 Total Loss: 6.6046 Recon Loss: 6.6021 
[01/05 19:54:50 TiTok]: Data (t): 0.0011, 55.04/s/gpu Batch (t): 0.5814 LR: 0.000024 Step: 2450 Total Loss: 6.5723 Recon Loss: 6.5699 
[01/05 19:55:20 TiTok]: Data (t): 0.0006, 54.20/s/gpu Batch (t): 0.5905 LR: 0.000025 Step: 2500 Total Loss: 6.5804 Recon Loss: 6.5779 
[01/05 19:55:49 TiTok]: Data (t): 0.0014, 54.62/s/gpu Batch (t): 0.5859 LR: 0.000026 Step: 2550 Total Loss: 6.5787 Recon Loss: 6.5762 
[01/05 19:56:18 TiTok]: Data (t): 0.0011, 54.49/s/gpu Batch (t): 0.5873 LR: 0.000026 Step: 2600 Total Loss: 6.5475 Recon Loss: 6.5451 
[01/05 19:56:48 TiTok]: Data (t): 0.0006, 54.20/s/gpu Batch (t): 0.5904 LR: 0.000027 Step: 2650 Total Loss: 6.5597 Recon Loss: 6.5572 
[01/05 19:57:17 TiTok]: Data (t): 0.0011, 54.49/s/gpu Batch (t): 0.5872 LR: 0.000027 Step: 2700 Total Loss: 6.5661 Recon Loss: 6.5637 
[01/05 19:57:46 TiTok]: Data (t): 0.0006, 54.04/s/gpu Batch (t): 0.5922 LR: 0.000028 Step: 2750 Total Loss: 6.5455 Recon Loss: 6.5429 
[01/05 19:58:16 TiTok]: Data (t): 0.0006, 53.99/s/gpu Batch (t): 0.5927 LR: 0.000028 Step: 2800 Total Loss: 6.5463 Recon Loss: 6.5438 
[01/05 19:58:45 TiTok]: Data (t): 0.0006, 54.64/s/gpu Batch (t): 0.5857 LR: 0.000028 Step: 2850 Total Loss: 6.5441 Recon Loss: 6.5415 
[01/05 19:59:14 TiTok]: Data (t): 0.0007, 54.61/s/gpu Batch (t): 0.5859 LR: 0.000029 Step: 2900 Total Loss: 6.5489 Recon Loss: 6.5463 
[01/05 19:59:44 TiTok]: Data (t): 0.0011, 55.16/s/gpu Batch (t): 0.5802 LR: 0.000029 Step: 2950 Total Loss: 6.5455 Recon Loss: 6.5429 
[01/05 20:00:13 TiTok]: Data (t): 0.0011, 45.35/s/gpu Batch (t): 0.7057 LR: 0.000030 Step: 3000 Total Loss: 6.5258 Recon Loss: 6.5232 
[01/05 20:00:42 TiTok]: Data (t): 0.0006, 53.81/s/gpu Batch (t): 0.5947 LR: 0.000030 Step: 3050 Total Loss: 6.4736 Recon Loss: 6.4709 
[01/05 20:01:12 TiTok]: Data (t): 0.0006, 54.52/s/gpu Batch (t): 0.5869 LR: 0.000031 Step: 3100 Total Loss: 6.5199 Recon Loss: 6.5173 
[01/05 20:01:41 TiTok]: Data (t): 0.0011, 54.53/s/gpu Batch (t): 0.5868 LR: 0.000031 Step: 3150 Total Loss: 6.4873 Recon Loss: 6.4846 
[01/05 20:02:11 TiTok]: Data (t): 0.0011, 55.15/s/gpu Batch (t): 0.5803 LR: 0.000032 Step: 3200 Total Loss: 6.4779 Recon Loss: 6.4752 
[01/05 20:02:40 TiTok]: Data (t): 0.0011, 55.12/s/gpu Batch (t): 0.5806 LR: 0.000033 Step: 3250 Total Loss: 6.4776 Recon Loss: 6.4749 
[01/05 20:03:09 TiTok]: Data (t): 0.0007, 54.42/s/gpu Batch (t): 0.5880 LR: 0.000033 Step: 3300 Total Loss: 6.4932 Recon Loss: 6.4905 
[01/05 20:03:39 TiTok]: Data (t): 0.0008, 54.32/s/gpu Batch (t): 0.5891 LR: 0.000034 Step: 3350 Total Loss: 6.4735 Recon Loss: 6.4708 
[01/05 20:04:08 TiTok]: Data (t): 0.0006, 54.58/s/gpu Batch (t): 0.5863 LR: 0.000034 Step: 3400 Total Loss: 6.4664 Recon Loss: 6.4637 
[01/05 20:04:37 TiTok]: Data (t): 0.0011, 54.62/s/gpu Batch (t): 0.5859 LR: 0.000034 Step: 3450 Total Loss: 6.4518 Recon Loss: 6.4492 
[01/05 20:05:07 TiTok]: Data (t): 0.0006, 54.19/s/gpu Batch (t): 0.5906 LR: 0.000035 Step: 3500 Total Loss: 6.4478 Recon Loss: 6.4452 
[01/05 20:05:36 TiTok]: Data (t): 0.0014, 54.44/s/gpu Batch (t): 0.5878 LR: 0.000036 Step: 3550 Total Loss: 6.4436 Recon Loss: 6.4409 
[01/05 20:06:05 TiTok]: Data (t): 0.0006, 54.20/s/gpu Batch (t): 0.5904 LR: 0.000036 Step: 3600 Total Loss: 6.4412 Recon Loss: 6.4385 
[01/05 20:06:35 TiTok]: Data (t): 0.0006, 53.86/s/gpu Batch (t): 0.5942 LR: 0.000036 Step: 3650 Total Loss: 6.4276 Recon Loss: 6.4249 
[01/05 20:07:04 TiTok]: Data (t): 0.0006, 54.13/s/gpu Batch (t): 0.5911 LR: 0.000037 Step: 3700 Total Loss: 6.4193 Recon Loss: 6.4166 
[01/05 20:07:33 TiTok]: Data (t): 0.0014, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000038 Step: 3750 Total Loss: 6.4107 Recon Loss: 6.4079 
[01/05 20:08:03 TiTok]: Data (t): 0.0006, 53.99/s/gpu Batch (t): 0.5927 LR: 0.000038 Step: 3800 Total Loss: 6.4010 Recon Loss: 6.3982 
[01/05 20:08:32 TiTok]: Data (t): 0.0011, 54.67/s/gpu Batch (t): 0.5854 LR: 0.000039 Step: 3850 Total Loss: 6.4065 Recon Loss: 6.4037 
[01/05 20:09:01 TiTok]: Data (t): 0.0006, 54.53/s/gpu Batch (t): 0.5868 LR: 0.000039 Step: 3900 Total Loss: 6.4071 Recon Loss: 6.4043 
[01/05 20:09:31 TiTok]: Data (t): 0.0006, 54.24/s/gpu Batch (t): 0.5900 LR: 0.000040 Step: 3950 Total Loss: 6.3868 Recon Loss: 6.3840 
[01/05 20:10:00 TiTok]: Data (t): 0.0011, 45.87/s/gpu Batch (t): 0.6976 LR: 0.000040 Step: 4000 Total Loss: 6.3679 Recon Loss: 6.3649 
[01/05 20:10:30 TiTok]: Data (t): 0.0006, 54.15/s/gpu Batch (t): 0.5910 LR: 0.000041 Step: 4050 Total Loss: 6.3472 Recon Loss: 6.3442 
[01/05 20:10:59 TiTok]: Data (t): 0.0011, 54.66/s/gpu Batch (t): 0.5854 LR: 0.000041 Step: 4100 Total Loss: 6.3800 Recon Loss: 6.3770 
[01/05 20:11:28 TiTok]: Data (t): 0.0011, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000041 Step: 4150 Total Loss: 6.3630 Recon Loss: 6.3601 
[01/05 20:11:58 TiTok]: Data (t): 0.0006, 53.95/s/gpu Batch (t): 0.5932 LR: 0.000042 Step: 4200 Total Loss: 6.3525 Recon Loss: 6.3495 
[01/05 20:12:27 TiTok]: Data (t): 0.0011, 54.55/s/gpu Batch (t): 0.5866 LR: 0.000043 Step: 4250 Total Loss: 6.3669 Recon Loss: 6.3639 
[01/05 20:12:56 TiTok]: Data (t): 0.0011, 55.04/s/gpu Batch (t): 0.5814 LR: 0.000043 Step: 4300 Total Loss: 6.3373 Recon Loss: 6.3343 
[01/05 20:13:26 TiTok]: Data (t): 0.0006, 54.61/s/gpu Batch (t): 0.5860 LR: 0.000044 Step: 4350 Total Loss: 6.3326 Recon Loss: 6.3295 
[01/05 20:13:55 TiTok]: Data (t): 0.0006, 54.15/s/gpu Batch (t): 0.5909 LR: 0.000044 Step: 4400 Total Loss: 6.3100 Recon Loss: 6.3069 
[01/05 20:14:24 TiTok]: Data (t): 0.0013, 54.71/s/gpu Batch (t): 0.5849 LR: 0.000045 Step: 4450 Total Loss: 6.2936 Recon Loss: 6.2904 
[01/05 20:14:54 TiTok]: Data (t): 0.0009, 53.96/s/gpu Batch (t): 0.5930 LR: 0.000045 Step: 4500 Total Loss: 6.3270 Recon Loss: 6.3239 
[01/05 20:15:23 TiTok]: Data (t): 0.0011, 54.97/s/gpu Batch (t): 0.5821 LR: 0.000046 Step: 4550 Total Loss: 6.2947 Recon Loss: 6.2915 
[01/05 20:15:52 TiTok]: Data (t): 0.0007, 53.98/s/gpu Batch (t): 0.5928 LR: 0.000046 Step: 4600 Total Loss: 6.3050 Recon Loss: 6.3017 
[01/05 20:16:22 TiTok]: Data (t): 0.0011, 55.13/s/gpu Batch (t): 0.5805 LR: 0.000047 Step: 4650 Total Loss: 6.3043 Recon Loss: 6.3011 
[01/05 20:16:51 TiTok]: Data (t): 0.0006, 54.51/s/gpu Batch (t): 0.5870 LR: 0.000047 Step: 4700 Total Loss: 6.2899 Recon Loss: 6.2866 
[01/05 20:17:20 TiTok]: Data (t): 0.0011, 54.72/s/gpu Batch (t): 0.5847 LR: 0.000048 Step: 4750 Total Loss: 6.2714 Recon Loss: 6.2680 
[01/05 20:17:50 TiTok]: Data (t): 0.0006, 54.46/s/gpu Batch (t): 0.5876 LR: 0.000048 Step: 4800 Total Loss: 6.2591 Recon Loss: 6.2558 
[01/05 20:18:19 TiTok]: Data (t): 0.0011, 54.85/s/gpu Batch (t): 0.5835 LR: 0.000048 Step: 4850 Total Loss: 6.2847 Recon Loss: 6.2812 
[01/05 20:18:48 TiTok]: Data (t): 0.0011, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000049 Step: 4900 Total Loss: 6.2307 Recon Loss: 6.2272 
[01/05 20:19:18 TiTok]: Data (t): 0.0011, 54.46/s/gpu Batch (t): 0.5876 LR: 0.000050 Step: 4950 Total Loss: 6.2439 Recon Loss: 6.2405 
[01/05 20:19:47 TiTok]: Data (t): 0.0009, 46.57/s/gpu Batch (t): 0.6872 LR: 0.000050 Step: 5000 Total Loss: 6.2500 Recon Loss: 6.2465 
[01/05 20:19:49 TiTok]: Reconstructing images...
Epoch 1/199 started.
[01/05 20:20:27 TiTok]: Data (t): 0.0007, 54.71/s/gpu Batch (t): 0.5849 LR: 0.000051 Step: 5050 Total Loss: 6.2232 Recon Loss: 6.2196 
[01/05 20:20:56 TiTok]: Data (t): 0.0011, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000051 Step: 5100 Total Loss: 6.2082 Recon Loss: 6.2047 
[01/05 20:21:25 TiTok]: Data (t): 0.0007, 54.68/s/gpu Batch (t): 0.5852 LR: 0.000052 Step: 5150 Total Loss: 6.1775 Recon Loss: 6.1739 
[01/05 20:21:55 TiTok]: Data (t): 0.0011, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000052 Step: 5200 Total Loss: 6.2229 Recon Loss: 6.2193 
[01/05 20:22:24 TiTok]: Data (t): 0.0011, 55.14/s/gpu Batch (t): 0.5804 LR: 0.000053 Step: 5250 Total Loss: 6.2119 Recon Loss: 6.2083 
[01/05 20:22:53 TiTok]: Data (t): 0.0007, 54.63/s/gpu Batch (t): 0.5857 LR: 0.000053 Step: 5300 Total Loss: 6.1818 Recon Loss: 6.1782 
[01/05 20:23:23 TiTok]: Data (t): 0.0011, 54.62/s/gpu Batch (t): 0.5859 LR: 0.000054 Step: 5350 Total Loss: 6.1922 Recon Loss: 6.1886 
[01/05 20:23:52 TiTok]: Data (t): 0.0011, 54.53/s/gpu Batch (t): 0.5868 LR: 0.000054 Step: 5400 Total Loss: 6.1943 Recon Loss: 6.1906 
[01/05 20:24:21 TiTok]: Data (t): 0.0006, 54.66/s/gpu Batch (t): 0.5854 LR: 0.000055 Step: 5450 Total Loss: 6.1522 Recon Loss: 6.1486 
[01/05 20:24:51 TiTok]: Data (t): 0.0011, 54.61/s/gpu Batch (t): 0.5860 LR: 0.000055 Step: 5500 Total Loss: 6.1903 Recon Loss: 6.1867 
[01/05 20:25:20 TiTok]: Data (t): 0.0014, 55.23/s/gpu Batch (t): 0.5794 LR: 0.000056 Step: 5550 Total Loss: 6.1640 Recon Loss: 6.1603 
[01/05 20:25:49 TiTok]: Data (t): 0.0008, 53.66/s/gpu Batch (t): 0.5963 LR: 0.000056 Step: 5600 Total Loss: 6.1445 Recon Loss: 6.1408 
[01/05 20:26:19 TiTok]: Data (t): 0.0011, 54.46/s/gpu Batch (t): 0.5876 LR: 0.000056 Step: 5650 Total Loss: 6.1439 Recon Loss: 6.1402 
[01/05 20:26:48 TiTok]: Data (t): 0.0011, 54.32/s/gpu Batch (t): 0.5891 LR: 0.000057 Step: 5700 Total Loss: 6.1407 Recon Loss: 6.1369 
[01/05 20:27:17 TiTok]: Data (t): 0.0011, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000057 Step: 5750 Total Loss: 6.1101 Recon Loss: 6.1063 
[01/05 20:27:47 TiTok]: Data (t): 0.0011, 54.50/s/gpu Batch (t): 0.5872 LR: 0.000058 Step: 5800 Total Loss: 6.1040 Recon Loss: 6.1002 
[01/05 20:28:16 TiTok]: Data (t): 0.0011, 55.21/s/gpu Batch (t): 0.5796 LR: 0.000058 Step: 5850 Total Loss: 6.1038 Recon Loss: 6.0999 
[01/05 20:28:45 TiTok]: Data (t): 0.0011, 54.49/s/gpu Batch (t): 0.5872 LR: 0.000059 Step: 5900 Total Loss: 6.1252 Recon Loss: 6.1214 
[01/05 20:29:15 TiTok]: Data (t): 0.0011, 54.66/s/gpu Batch (t): 0.5855 LR: 0.000060 Step: 5950 Total Loss: 6.1032 Recon Loss: 6.0993 
[01/05 20:29:44 TiTok]: Data (t): 0.0012, 43.10/s/gpu Batch (t): 0.7424 LR: 0.000060 Step: 6000 Total Loss: 6.1048 Recon Loss: 6.1009 
[01/05 20:30:14 TiTok]: Data (t): 0.0011, 54.61/s/gpu Batch (t): 0.5859 LR: 0.000061 Step: 6050 Total Loss: 6.0418 Recon Loss: 6.0378 
[01/05 20:30:43 TiTok]: Data (t): 0.0014, 55.17/s/gpu Batch (t): 0.5801 LR: 0.000061 Step: 6100 Total Loss: 6.1180 Recon Loss: 6.1141 
[01/05 20:31:12 TiTok]: Data (t): 0.0011, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000062 Step: 6150 Total Loss: 6.0752 Recon Loss: 6.0713 
[01/05 20:31:42 TiTok]: Data (t): 0.0007, 54.11/s/gpu Batch (t): 0.5913 LR: 0.000062 Step: 6200 Total Loss: 6.0975 Recon Loss: 6.0936 
[01/05 20:32:11 TiTok]: Data (t): 0.0011, 54.66/s/gpu Batch (t): 0.5855 LR: 0.000063 Step: 6250 Total Loss: 6.0583 Recon Loss: 6.0543 
[01/05 20:32:40 TiTok]: Data (t): 0.0011, 55.05/s/gpu Batch (t): 0.5813 LR: 0.000063 Step: 6300 Total Loss: 6.0237 Recon Loss: 6.0196 
[01/05 20:33:10 TiTok]: Data (t): 0.0011, 55.03/s/gpu Batch (t): 0.5815 LR: 0.000063 Step: 6350 Total Loss: 6.0932 Recon Loss: 6.0891 
[01/05 20:33:39 TiTok]: Data (t): 0.0006, 54.51/s/gpu Batch (t): 0.5871 LR: 0.000064 Step: 6400 Total Loss: 6.0672 Recon Loss: 6.0631 
[01/05 20:34:08 TiTok]: Data (t): 0.0006, 54.18/s/gpu Batch (t): 0.5907 LR: 0.000064 Step: 6450 Total Loss: 6.0142 Recon Loss: 6.0102 
[01/05 20:34:38 TiTok]: Data (t): 0.0011, 54.46/s/gpu Batch (t): 0.5876 LR: 0.000065 Step: 6500 Total Loss: 6.0335 Recon Loss: 6.0294 
[01/05 20:35:07 TiTok]: Data (t): 0.0011, 54.93/s/gpu Batch (t): 0.5826 LR: 0.000065 Step: 6550 Total Loss: 6.0047 Recon Loss: 6.0006 
[01/05 20:35:36 TiTok]: Data (t): 0.0011, 54.69/s/gpu Batch (t): 0.5851 LR: 0.000066 Step: 6600 Total Loss: 6.0091 Recon Loss: 6.0049 
[01/05 20:36:06 TiTok]: Data (t): 0.0011, 55.04/s/gpu Batch (t): 0.5814 LR: 0.000066 Step: 6650 Total Loss: 6.0125 Recon Loss: 6.0082 
[01/05 20:36:35 TiTok]: Data (t): 0.0006, 54.10/s/gpu Batch (t): 0.5915 LR: 0.000067 Step: 6700 Total Loss: 5.9888 Recon Loss: 5.9844 
[01/05 20:37:04 TiTok]: Data (t): 0.0012, 54.44/s/gpu Batch (t): 0.5878 LR: 0.000067 Step: 6750 Total Loss: 5.9447 Recon Loss: 5.9402 
[01/05 20:37:34 TiTok]: Data (t): 0.0011, 54.41/s/gpu Batch (t): 0.5881 LR: 0.000068 Step: 6800 Total Loss: 6.0006 Recon Loss: 5.9960 
[01/05 20:38:03 TiTok]: Data (t): 0.0007, 54.06/s/gpu Batch (t): 0.5919 LR: 0.000068 Step: 6850 Total Loss: 5.9596 Recon Loss: 5.9550 
[01/05 20:38:32 TiTok]: Data (t): 0.0011, 54.28/s/gpu Batch (t): 0.5895 LR: 0.000069 Step: 6900 Total Loss: 5.9636 Recon Loss: 5.9590 
[01/05 20:39:02 TiTok]: Data (t): 0.0011, 54.50/s/gpu Batch (t): 0.5872 LR: 0.000069 Step: 6950 Total Loss: 5.9292 Recon Loss: 5.9244 
[01/05 20:39:31 TiTok]: Data (t): 0.0011, 45.46/s/gpu Batch (t): 0.7039 LR: 0.000070 Step: 7000 Total Loss: 5.9609 Recon Loss: 5.9561 
[01/05 20:40:00 TiTok]: Data (t): 0.0014, 54.94/s/gpu Batch (t): 0.5824 LR: 0.000070 Step: 7050 Total Loss: 5.8956 Recon Loss: 5.8908 
[01/05 20:40:30 TiTok]: Data (t): 0.0007, 54.21/s/gpu Batch (t): 0.5903 LR: 0.000071 Step: 7100 Total Loss: 5.9632 Recon Loss: 5.9582 
[01/05 20:40:59 TiTok]: Data (t): 0.0011, 55.16/s/gpu Batch (t): 0.5802 LR: 0.000071 Step: 7150 Total Loss: 5.9538 Recon Loss: 5.9489 
[01/05 20:41:28 TiTok]: Data (t): 0.0011, 54.53/s/gpu Batch (t): 0.5868 LR: 0.000072 Step: 7200 Total Loss: 5.9338 Recon Loss: 5.9287 
[01/05 20:41:58 TiTok]: Data (t): 0.0006, 54.52/s/gpu Batch (t): 0.5869 LR: 0.000072 Step: 7250 Total Loss: 5.9192 Recon Loss: 5.9141 
[01/05 20:42:27 TiTok]: Data (t): 0.0013, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000073 Step: 7300 Total Loss: 5.8879 Recon Loss: 5.8826 
[01/05 20:42:57 TiTok]: Data (t): 0.0011, 54.52/s/gpu Batch (t): 0.5869 LR: 0.000073 Step: 7350 Total Loss: 5.8845 Recon Loss: 5.8794 
[01/05 20:43:26 TiTok]: Data (t): 0.0006, 54.54/s/gpu Batch (t): 0.5867 LR: 0.000074 Step: 7400 Total Loss: 5.9046 Recon Loss: 5.8993 
[01/05 20:43:55 TiTok]: Data (t): 0.0011, 54.72/s/gpu Batch (t): 0.5848 LR: 0.000074 Step: 7450 Total Loss: 5.8789 Recon Loss: 5.8737 
[01/05 20:44:25 TiTok]: Data (t): 0.0006, 54.17/s/gpu Batch (t): 0.5907 LR: 0.000075 Step: 7500 Total Loss: 5.8535 Recon Loss: 5.8480 
[01/05 20:44:54 TiTok]: Data (t): 0.0011, 54.05/s/gpu Batch (t): 0.5921 LR: 0.000075 Step: 7550 Total Loss: 5.8909 Recon Loss: 5.8855 
[01/05 20:45:23 TiTok]: Data (t): 0.0008, 54.87/s/gpu Batch (t): 0.5832 LR: 0.000076 Step: 7600 Total Loss: 5.8128 Recon Loss: 5.8072 
[01/05 20:45:53 TiTok]: Data (t): 0.0011, 55.05/s/gpu Batch (t): 0.5813 LR: 0.000076 Step: 7650 Total Loss: 5.8632 Recon Loss: 5.8575 
[01/05 20:46:22 TiTok]: Data (t): 0.0011, 54.60/s/gpu Batch (t): 0.5861 LR: 0.000077 Step: 7700 Total Loss: 5.8028 Recon Loss: 5.7972 
[01/05 20:46:51 TiTok]: Data (t): 0.0011, 54.48/s/gpu Batch (t): 0.5873 LR: 0.000077 Step: 7750 Total Loss: 5.8156 Recon Loss: 5.8098 
[01/05 20:47:21 TiTok]: Data (t): 0.0011, 54.54/s/gpu Batch (t): 0.5868 LR: 0.000078 Step: 7800 Total Loss: 5.8518 Recon Loss: 5.8460 
[01/05 20:47:50 TiTok]: Data (t): 0.0011, 54.55/s/gpu Batch (t): 0.5867 LR: 0.000078 Step: 7850 Total Loss: 5.8219 Recon Loss: 5.8161 
[01/05 20:48:19 TiTok]: Data (t): 0.0011, 54.40/s/gpu Batch (t): 0.5882 LR: 0.000079 Step: 7900 Total Loss: 5.7855 Recon Loss: 5.7796 
[01/05 20:48:49 TiTok]: Data (t): 0.0011, 54.53/s/gpu Batch (t): 0.5869 LR: 0.000079 Step: 7950 Total Loss: 5.7717 Recon Loss: 5.7657 
[01/05 20:49:18 TiTok]: Data (t): 0.0011, 45.09/s/gpu Batch (t): 0.7097 LR: 0.000080 Step: 8000 Total Loss: 5.7655 Recon Loss: 5.7594 
[01/05 20:49:47 TiTok]: Data (t): 0.0011, 54.50/s/gpu Batch (t): 0.5871 LR: 0.000080 Step: 8050 Total Loss: 5.7166 Recon Loss: 5.7102 
[01/05 20:50:17 TiTok]: Data (t): 0.0012, 55.06/s/gpu Batch (t): 0.5812 LR: 0.000081 Step: 8100 Total Loss: 5.7256 Recon Loss: 5.7191 
[01/05 20:50:46 TiTok]: Data (t): 0.0011, 54.37/s/gpu Batch (t): 0.5885 LR: 0.000081 Step: 8150 Total Loss: 5.7376 Recon Loss: 5.7307 
[01/05 20:51:15 TiTok]: Data (t): 0.0011, 53.22/s/gpu Batch (t): 0.6012 LR: 0.000082 Step: 8200 Total Loss: 5.7597 Recon Loss: 5.7525 
[01/05 20:51:45 TiTok]: Data (t): 0.0011, 54.63/s/gpu Batch (t): 0.5858 LR: 0.000082 Step: 8250 Total Loss: 5.7142 Recon Loss: 5.7068 
[01/05 20:52:14 TiTok]: Data (t): 0.0006, 54.60/s/gpu Batch (t): 0.5861 LR: 0.000083 Step: 8300 Total Loss: 5.6980 Recon Loss: 5.6904 
[01/05 20:52:43 TiTok]: Data (t): 0.0011, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000083 Step: 8350 Total Loss: 5.6589 Recon Loss: 5.6512 
[01/05 20:53:13 TiTok]: Data (t): 0.0006, 54.60/s/gpu Batch (t): 0.5860 LR: 0.000084 Step: 8400 Total Loss: 5.6651 Recon Loss: 5.6574 
[01/05 20:53:42 TiTok]: Data (t): 0.0006, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000084 Step: 8450 Total Loss: 5.6374 Recon Loss: 5.6296 
[01/05 20:54:12 TiTok]: Data (t): 0.0011, 54.50/s/gpu Batch (t): 0.5871 LR: 0.000085 Step: 8500 Total Loss: 5.5991 Recon Loss: 5.5912 
[01/05 20:54:41 TiTok]: Data (t): 0.0006, 54.74/s/gpu Batch (t): 0.5846 LR: 0.000085 Step: 8550 Total Loss: 5.6048 Recon Loss: 5.5967 
[01/05 20:55:10 TiTok]: Data (t): 0.0014, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000086 Step: 8600 Total Loss: 5.6052 Recon Loss: 5.5973 
[01/05 20:55:40 TiTok]: Data (t): 0.0007, 54.09/s/gpu Batch (t): 0.5916 LR: 0.000086 Step: 8650 Total Loss: 5.5977 Recon Loss: 5.5894 
[01/05 20:56:09 TiTok]: Data (t): 0.0011, 54.38/s/gpu Batch (t): 0.5884 LR: 0.000087 Step: 8700 Total Loss: 5.5765 Recon Loss: 5.5684 
[01/05 20:56:38 TiTok]: Data (t): 0.0006, 54.12/s/gpu Batch (t): 0.5912 LR: 0.000087 Step: 8750 Total Loss: 5.5580 Recon Loss: 5.5499 
[01/05 20:57:08 TiTok]: Data (t): 0.0006, 53.98/s/gpu Batch (t): 0.5928 LR: 0.000088 Step: 8800 Total Loss: 5.5423 Recon Loss: 5.5341 
[01/05 20:57:37 TiTok]: Data (t): 0.0012, 54.64/s/gpu Batch (t): 0.5857 LR: 0.000088 Step: 8850 Total Loss: 5.5581 Recon Loss: 5.5499 
[01/05 20:58:07 TiTok]: Data (t): 0.0011, 54.47/s/gpu Batch (t): 0.5874 LR: 0.000089 Step: 8900 Total Loss: 5.5380 Recon Loss: 5.5298 
[01/05 20:58:36 TiTok]: Data (t): 0.0011, 54.58/s/gpu Batch (t): 0.5863 LR: 0.000089 Step: 8950 Total Loss: 5.5208 Recon Loss: 5.5126 
[01/05 20:59:06 TiTok]: Data (t): 0.0011, 45.75/s/gpu Batch (t): 0.6994 LR: 0.000090 Step: 9000 Total Loss: 5.5119 Recon Loss: 5.5034 
[01/05 20:59:35 TiTok]: Data (t): 0.0011, 54.46/s/gpu Batch (t): 0.5876 LR: 0.000090 Step: 9050 Total Loss: 5.5374 Recon Loss: 5.5290 
[01/05 21:00:04 TiTok]: Data (t): 0.0011, 54.82/s/gpu Batch (t): 0.5837 LR: 0.000091 Step: 9100 Total Loss: 5.4950 Recon Loss: 5.4867 
[01/05 21:00:34 TiTok]: Data (t): 0.0011, 54.40/s/gpu Batch (t): 0.5883 LR: 0.000091 Step: 9150 Total Loss: 5.4533 Recon Loss: 5.4448 
[01/05 21:01:03 TiTok]: Data (t): 0.0014, 54.54/s/gpu Batch (t): 0.5867 LR: 0.000092 Step: 9200 Total Loss: 5.4888 Recon Loss: 5.4803 
[01/05 21:01:33 TiTok]: Data (t): 0.0011, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000092 Step: 9250 Total Loss: 5.4166 Recon Loss: 5.4078 
[01/05 21:02:02 TiTok]: Data (t): 0.0006, 54.51/s/gpu Batch (t): 0.5871 LR: 0.000093 Step: 9300 Total Loss: 5.4251 Recon Loss: 5.4164 
[01/05 21:02:31 TiTok]: Data (t): 0.0006, 54.17/s/gpu Batch (t): 0.5908 LR: 0.000093 Step: 9350 Total Loss: 5.4604 Recon Loss: 5.4516 
[01/05 21:03:01 TiTok]: Data (t): 0.0006, 54.15/s/gpu Batch (t): 0.5910 LR: 0.000094 Step: 9400 Total Loss: 5.4324 Recon Loss: 5.4234 
[01/05 21:03:30 TiTok]: Data (t): 0.0011, 54.63/s/gpu Batch (t): 0.5858 LR: 0.000094 Step: 9450 Total Loss: 5.3771 Recon Loss: 5.3681 
[01/05 21:03:59 TiTok]: Data (t): 0.0008, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000095 Step: 9500 Total Loss: 5.3985 Recon Loss: 5.3895 
[01/05 21:04:29 TiTok]: Data (t): 0.0011, 54.54/s/gpu Batch (t): 0.5868 LR: 0.000095 Step: 9550 Total Loss: 5.3881 Recon Loss: 5.3787 
[01/05 21:04:58 TiTok]: Data (t): 0.0012, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000096 Step: 9600 Total Loss: 5.3653 Recon Loss: 5.3557 
[01/05 21:05:28 TiTok]: Data (t): 0.0006, 53.92/s/gpu Batch (t): 0.5935 LR: 0.000096 Step: 9650 Total Loss: 5.3255 Recon Loss: 5.3157 
[01/05 21:05:57 TiTok]: Data (t): 0.0011, 54.50/s/gpu Batch (t): 0.5871 LR: 0.000097 Step: 9700 Total Loss: 5.3571 Recon Loss: 5.3472 
[01/05 21:06:26 TiTok]: Data (t): 0.0011, 54.50/s/gpu Batch (t): 0.5872 LR: 0.000097 Step: 9750 Total Loss: 5.3112 Recon Loss: 5.3013 
[01/05 21:06:56 TiTok]: Data (t): 0.0012, 55.10/s/gpu Batch (t): 0.5808 LR: 0.000098 Step: 9800 Total Loss: 5.3717 Recon Loss: 5.3617 
[01/05 21:07:25 TiTok]: Data (t): 0.0006, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000098 Step: 9850 Total Loss: 5.3092 Recon Loss: 5.2990 
[01/05 21:07:54 TiTok]: Data (t): 0.0009, 54.43/s/gpu Batch (t): 0.5879 LR: 0.000099 Step: 9900 Total Loss: 5.2594 Recon Loss: 5.2490 
[01/05 21:08:24 TiTok]: Data (t): 0.0011, 54.97/s/gpu Batch (t): 0.5822 LR: 0.000099 Step: 9950 Total Loss: 5.2601 Recon Loss: 5.2495 
[01/05 21:08:53 TiTok]: Data (t): 0.0011, 45.65/s/gpu Batch (t): 0.7010 LR: 0.000100 Step: 10000 Total Loss: 5.2494 Recon Loss: 5.2389 
[01/05 21:08:54 TiTok]: Reconstructing images...
Epoch 2/199 started.
[01/05 21:09:31 TiTok]: Data (t): 0.0011, 54.54/s/gpu Batch (t): 0.5868 LR: 0.000100 Step: 10050 Total Loss: 5.1943 Recon Loss: 5.1836 
[01/05 21:10:00 TiTok]: Data (t): 0.0011, 54.50/s/gpu Batch (t): 0.5871 LR: 0.000100 Step: 10100 Total Loss: 5.1853 Recon Loss: 5.1745 
[01/05 21:10:30 TiTok]: Data (t): 0.0011, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 10150 Total Loss: 5.2372 Recon Loss: 5.2264 
[01/05 21:10:59 TiTok]: Data (t): 0.0011, 55.01/s/gpu Batch (t): 0.5817 LR: 0.000100 Step: 10200 Total Loss: 5.2241 Recon Loss: 5.2133 
[01/05 21:11:28 TiTok]: Data (t): 0.0009, 54.14/s/gpu Batch (t): 0.5910 LR: 0.000100 Step: 10250 Total Loss: 5.1809 Recon Loss: 5.1700 
[01/05 21:11:58 TiTok]: Data (t): 0.0007, 53.97/s/gpu Batch (t): 0.5930 LR: 0.000100 Step: 10300 Total Loss: 5.1909 Recon Loss: 5.1801 
[01/05 21:12:27 TiTok]: Data (t): 0.0006, 54.06/s/gpu Batch (t): 0.5919 LR: 0.000100 Step: 10350 Total Loss: 5.1524 Recon Loss: 5.1414 
[01/05 21:12:56 TiTok]: Data (t): 0.0017, 54.98/s/gpu Batch (t): 0.5820 LR: 0.000100 Step: 10400 Total Loss: 5.1455 Recon Loss: 5.1346 
[01/05 21:13:26 TiTok]: Data (t): 0.0011, 54.61/s/gpu Batch (t): 0.5859 LR: 0.000100 Step: 10450 Total Loss: 5.1564 Recon Loss: 5.1455 
[01/05 21:13:55 TiTok]: Data (t): 0.0006, 54.37/s/gpu Batch (t): 0.5886 LR: 0.000100 Step: 10500 Total Loss: 5.1174 Recon Loss: 5.1066 
[01/05 21:14:25 TiTok]: Data (t): 0.0011, 54.66/s/gpu Batch (t): 0.5855 LR: 0.000100 Step: 10550 Total Loss: 5.1219 Recon Loss: 5.1109 
[01/05 21:14:54 TiTok]: Data (t): 0.0007, 53.97/s/gpu Batch (t): 0.5929 LR: 0.000100 Step: 10600 Total Loss: 5.1174 Recon Loss: 5.1066 
[01/05 21:15:23 TiTok]: Data (t): 0.0006, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000100 Step: 10650 Total Loss: 5.0924 Recon Loss: 5.0815 
[01/05 21:15:53 TiTok]: Data (t): 0.0006, 54.24/s/gpu Batch (t): 0.5900 LR: 0.000100 Step: 10700 Total Loss: 5.0902 Recon Loss: 5.0794 
[01/05 21:16:22 TiTok]: Data (t): 0.0014, 54.44/s/gpu Batch (t): 0.5878 LR: 0.000100 Step: 10750 Total Loss: 5.0661 Recon Loss: 5.0552 
[01/05 21:16:51 TiTok]: Data (t): 0.0014, 54.64/s/gpu Batch (t): 0.5857 LR: 0.000100 Step: 10800 Total Loss: 5.0606 Recon Loss: 5.0498 
[01/05 21:17:21 TiTok]: Data (t): 0.0013, 54.45/s/gpu Batch (t): 0.5877 LR: 0.000100 Step: 10850 Total Loss: 5.0135 Recon Loss: 5.0028 
[01/05 21:17:50 TiTok]: Data (t): 0.0011, 55.08/s/gpu Batch (t): 0.5810 LR: 0.000100 Step: 10900 Total Loss: 5.0295 Recon Loss: 5.0187 
[01/05 21:18:19 TiTok]: Data (t): 0.0006, 54.62/s/gpu Batch (t): 0.5859 LR: 0.000100 Step: 10950 Total Loss: 5.0063 Recon Loss: 4.9954 
[01/05 21:18:49 TiTok]: Data (t): 0.0006, 44.49/s/gpu Batch (t): 0.7193 LR: 0.000100 Step: 11000 Total Loss: 5.0540 Recon Loss: 5.0431 
[01/05 21:19:18 TiTok]: Data (t): 0.0011, 54.60/s/gpu Batch (t): 0.5860 LR: 0.000100 Step: 11050 Total Loss: 5.0124 Recon Loss: 5.0016 
[01/05 21:19:48 TiTok]: Data (t): 0.0007, 54.12/s/gpu Batch (t): 0.5913 LR: 0.000100 Step: 11100 Total Loss: 5.0421 Recon Loss: 5.0312 
[01/05 21:20:17 TiTok]: Data (t): 0.0006, 54.14/s/gpu Batch (t): 0.5910 LR: 0.000100 Step: 11150 Total Loss: 5.0232 Recon Loss: 5.0124 
[01/05 21:20:46 TiTok]: Data (t): 0.0007, 54.11/s/gpu Batch (t): 0.5914 LR: 0.000100 Step: 11200 Total Loss: 4.9748 Recon Loss: 4.9639 
[01/05 21:21:16 TiTok]: Data (t): 0.0011, 54.44/s/gpu Batch (t): 0.5878 LR: 0.000100 Step: 11250 Total Loss: 4.9981 Recon Loss: 4.9872 
[01/05 21:21:45 TiTok]: Data (t): 0.0006, 54.47/s/gpu Batch (t): 0.5875 LR: 0.000100 Step: 11300 Total Loss: 4.9506 Recon Loss: 4.9397 
[01/05 21:22:14 TiTok]: Data (t): 0.0011, 54.35/s/gpu Batch (t): 0.5887 LR: 0.000100 Step: 11350 Total Loss: 4.9721 Recon Loss: 4.9612 
[01/05 21:22:44 TiTok]: Data (t): 0.0011, 55.11/s/gpu Batch (t): 0.5807 LR: 0.000100 Step: 11400 Total Loss: 4.9686 Recon Loss: 4.9577 
[01/05 21:23:13 TiTok]: Data (t): 0.0013, 54.66/s/gpu Batch (t): 0.5854 LR: 0.000100 Step: 11450 Total Loss: 4.9593 Recon Loss: 4.9484 
[01/05 21:23:42 TiTok]: Data (t): 0.0006, 54.10/s/gpu Batch (t): 0.5915 LR: 0.000100 Step: 11500 Total Loss: 4.9706 Recon Loss: 4.9597 
[01/05 21:24:12 TiTok]: Data (t): 0.0007, 54.54/s/gpu Batch (t): 0.5867 LR: 0.000100 Step: 11550 Total Loss: 4.9329 Recon Loss: 4.9220 
[01/05 21:24:41 TiTok]: Data (t): 0.0007, 54.47/s/gpu Batch (t): 0.5875 LR: 0.000100 Step: 11600 Total Loss: 4.9830 Recon Loss: 4.9721 
[01/05 21:25:10 TiTok]: Data (t): 0.0011, 54.55/s/gpu Batch (t): 0.5867 LR: 0.000100 Step: 11650 Total Loss: 4.9321 Recon Loss: 4.9213 
[01/05 21:25:40 TiTok]: Data (t): 0.0007, 53.95/s/gpu Batch (t): 0.5932 LR: 0.000100 Step: 11700 Total Loss: 4.8801 Recon Loss: 4.8692 
[01/05 21:26:09 TiTok]: Data (t): 0.0011, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000100 Step: 11750 Total Loss: 4.9059 Recon Loss: 4.8950 
[01/05 21:26:39 TiTok]: Data (t): 0.0006, 54.61/s/gpu Batch (t): 0.5860 LR: 0.000100 Step: 11800 Total Loss: 4.8928 Recon Loss: 4.8819 
[01/05 21:27:08 TiTok]: Data (t): 0.0014, 54.52/s/gpu Batch (t): 0.5869 LR: 0.000100 Step: 11850 Total Loss: 4.8481 Recon Loss: 4.8373 
[01/05 21:27:37 TiTok]: Data (t): 0.0011, 54.52/s/gpu Batch (t): 0.5869 LR: 0.000100 Step: 11900 Total Loss: 4.8885 Recon Loss: 4.8776 
[01/05 21:28:07 TiTok]: Data (t): 0.0006, 54.48/s/gpu Batch (t): 0.5873 LR: 0.000100 Step: 11950 Total Loss: 4.8555 Recon Loss: 4.8447 
[01/05 21:28:36 TiTok]: Data (t): 0.0011, 45.15/s/gpu Batch (t): 0.7088 LR: 0.000100 Step: 12000 Total Loss: 4.8238 Recon Loss: 4.8130 
[01/05 21:29:05 TiTok]: Data (t): 0.0011, 54.68/s/gpu Batch (t): 0.5852 LR: 0.000100 Step: 12050 Total Loss: 4.8572 Recon Loss: 4.8463 
[01/05 21:29:35 TiTok]: Data (t): 0.0006, 54.11/s/gpu Batch (t): 0.5914 LR: 0.000100 Step: 12100 Total Loss: 4.8236 Recon Loss: 4.8127 
[01/05 21:30:04 TiTok]: Data (t): 0.0011, 54.98/s/gpu Batch (t): 0.5821 LR: 0.000100 Step: 12150 Total Loss: 4.8550 Recon Loss: 4.8441 
[01/05 21:30:34 TiTok]: Data (t): 0.0006, 54.49/s/gpu Batch (t): 0.5873 LR: 0.000100 Step: 12200 Total Loss: 4.8517 Recon Loss: 4.8408 
[01/05 21:31:03 TiTok]: Data (t): 0.0011, 55.10/s/gpu Batch (t): 0.5808 LR: 0.000100 Step: 12250 Total Loss: 4.8227 Recon Loss: 4.8117 
[01/05 21:31:32 TiTok]: Data (t): 0.0011, 54.58/s/gpu Batch (t): 0.5863 LR: 0.000100 Step: 12300 Total Loss: 4.8630 Recon Loss: 4.8521 
[01/05 21:32:02 TiTok]: Data (t): 0.0011, 54.04/s/gpu Batch (t): 0.5921 LR: 0.000100 Step: 12350 Total Loss: 4.8531 Recon Loss: 4.8422 
[01/05 21:32:31 TiTok]: Data (t): 0.0011, 54.40/s/gpu Batch (t): 0.5882 LR: 0.000100 Step: 12400 Total Loss: 4.7818 Recon Loss: 4.7709 
[01/05 21:33:00 TiTok]: Data (t): 0.0006, 54.32/s/gpu Batch (t): 0.5891 LR: 0.000100 Step: 12450 Total Loss: 4.8426 Recon Loss: 4.8317 
[01/05 21:33:30 TiTok]: Data (t): 0.0011, 53.66/s/gpu Batch (t): 0.5963 LR: 0.000100 Step: 12500 Total Loss: 4.7952 Recon Loss: 4.7843 
[01/05 21:33:59 TiTok]: Data (t): 0.0011, 54.55/s/gpu Batch (t): 0.5867 LR: 0.000100 Step: 12550 Total Loss: 4.8254 Recon Loss: 4.8144 
[01/05 21:34:28 TiTok]: Data (t): 0.0011, 54.64/s/gpu Batch (t): 0.5856 LR: 0.000100 Step: 12600 Total Loss: 4.7721 Recon Loss: 4.7611 
[01/05 21:34:58 TiTok]: Data (t): 0.0006, 54.11/s/gpu Batch (t): 0.5913 LR: 0.000100 Step: 12650 Total Loss: 4.7522 Recon Loss: 4.7412 
[01/05 21:35:27 TiTok]: Data (t): 0.0007, 54.63/s/gpu Batch (t): 0.5858 LR: 0.000100 Step: 12700 Total Loss: 4.7849 Recon Loss: 4.7740 
[01/05 21:35:57 TiTok]: Data (t): 0.0011, 54.60/s/gpu Batch (t): 0.5861 LR: 0.000100 Step: 12750 Total Loss: 4.7729 Recon Loss: 4.7620 
[01/05 21:36:26 TiTok]: Data (t): 0.0011, 54.50/s/gpu Batch (t): 0.5871 LR: 0.000100 Step: 12800 Total Loss: 4.7507 Recon Loss: 4.7397 
[01/05 21:36:55 TiTok]: Data (t): 0.0011, 54.53/s/gpu Batch (t): 0.5868 LR: 0.000100 Step: 12850 Total Loss: 4.8003 Recon Loss: 4.7893 
[01/05 21:37:25 TiTok]: Data (t): 0.0007, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 12900 Total Loss: 4.7676 Recon Loss: 4.7566 
[01/05 21:37:54 TiTok]: Data (t): 0.0006, 54.61/s/gpu Batch (t): 0.5860 LR: 0.000100 Step: 12950 Total Loss: 4.7503 Recon Loss: 4.7393 
[01/05 21:38:23 TiTok]: Data (t): 0.0012, 45.80/s/gpu Batch (t): 0.6987 LR: 0.000100 Step: 13000 Total Loss: 4.7311 Recon Loss: 4.7201 
[01/05 21:38:53 TiTok]: Data (t): 0.0011, 54.39/s/gpu Batch (t): 0.5883 LR: 0.000100 Step: 13050 Total Loss: 4.7167 Recon Loss: 4.7058 
[01/05 21:39:22 TiTok]: Data (t): 0.0007, 53.92/s/gpu Batch (t): 0.5935 LR: 0.000100 Step: 13100 Total Loss: 4.7333 Recon Loss: 4.7223 
[01/05 21:39:51 TiTok]: Data (t): 0.0007, 54.74/s/gpu Batch (t): 0.5846 LR: 0.000100 Step: 13150 Total Loss: 4.7481 Recon Loss: 4.7371 
[01/05 21:40:21 TiTok]: Data (t): 0.0006, 54.13/s/gpu Batch (t): 0.5911 LR: 0.000100 Step: 13200 Total Loss: 4.7234 Recon Loss: 4.7123 
[01/05 21:40:50 TiTok]: Data (t): 0.0010, 54.52/s/gpu Batch (t): 0.5869 LR: 0.000100 Step: 13250 Total Loss: 4.7317 Recon Loss: 4.7206 
[01/05 21:41:20 TiTok]: Data (t): 0.0006, 54.08/s/gpu Batch (t): 0.5918 LR: 0.000100 Step: 13300 Total Loss: 4.7090 Recon Loss: 4.6978 
[01/05 21:41:49 TiTok]: Data (t): 0.0006, 54.38/s/gpu Batch (t): 0.5884 LR: 0.000100 Step: 13350 Total Loss: 4.7098 Recon Loss: 4.6987 
[01/05 21:42:18 TiTok]: Data (t): 0.0011, 54.47/s/gpu Batch (t): 0.5875 LR: 0.000100 Step: 13400 Total Loss: 4.6898 Recon Loss: 4.6788 
[01/05 21:42:48 TiTok]: Data (t): 0.0011, 54.68/s/gpu Batch (t): 0.5852 LR: 0.000100 Step: 13450 Total Loss: 4.6704 Recon Loss: 4.6593 
[01/05 21:43:17 TiTok]: Data (t): 0.0007, 54.14/s/gpu Batch (t): 0.5910 LR: 0.000100 Step: 13500 Total Loss: 4.6878 Recon Loss: 4.6766 
[01/05 21:43:46 TiTok]: Data (t): 0.0006, 54.50/s/gpu Batch (t): 0.5872 LR: 0.000100 Step: 13550 Total Loss: 4.7172 Recon Loss: 4.7060 
[01/05 21:44:16 TiTok]: Data (t): 0.0013, 54.95/s/gpu Batch (t): 0.5824 LR: 0.000100 Step: 13600 Total Loss: 4.6484 Recon Loss: 4.6371 
[01/05 21:44:45 TiTok]: Data (t): 0.0006, 54.47/s/gpu Batch (t): 0.5875 LR: 0.000100 Step: 13650 Total Loss: 4.6731 Recon Loss: 4.6620 
[01/05 21:45:14 TiTok]: Data (t): 0.0013, 54.49/s/gpu Batch (t): 0.5873 LR: 0.000100 Step: 13700 Total Loss: 4.6853 Recon Loss: 4.6741 
[01/05 21:45:44 TiTok]: Data (t): 0.0006, 53.86/s/gpu Batch (t): 0.5941 LR: 0.000100 Step: 13750 Total Loss: 4.6201 Recon Loss: 4.6089 
[01/05 21:46:13 TiTok]: Data (t): 0.0014, 55.06/s/gpu Batch (t): 0.5812 LR: 0.000100 Step: 13800 Total Loss: 4.6380 Recon Loss: 4.6269 
[01/05 21:46:42 TiTok]: Data (t): 0.0014, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000100 Step: 13850 Total Loss: 4.6341 Recon Loss: 4.6229 
[01/05 21:47:12 TiTok]: Data (t): 0.0011, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 13900 Total Loss: 4.6041 Recon Loss: 4.5929 
[01/05 21:47:41 TiTok]: Data (t): 0.0009, 54.01/s/gpu Batch (t): 0.5925 LR: 0.000100 Step: 13950 Total Loss: 4.5972 Recon Loss: 4.5859 
[01/05 21:48:11 TiTok]: Data (t): 0.0006, 45.72/s/gpu Batch (t): 0.6999 LR: 0.000100 Step: 14000 Total Loss: 4.6188 Recon Loss: 4.6077 
[01/05 21:48:40 TiTok]: Data (t): 0.0009, 54.42/s/gpu Batch (t): 0.5880 LR: 0.000100 Step: 14050 Total Loss: 4.6648 Recon Loss: 4.6536 
[01/05 21:49:09 TiTok]: Data (t): 0.0006, 54.45/s/gpu Batch (t): 0.5877 LR: 0.000100 Step: 14100 Total Loss: 4.6269 Recon Loss: 4.6156 
[01/05 21:49:39 TiTok]: Data (t): 0.0006, 54.49/s/gpu Batch (t): 0.5872 LR: 0.000100 Step: 14150 Total Loss: 4.6102 Recon Loss: 4.5990 
[01/05 21:50:08 TiTok]: Data (t): 0.0011, 54.48/s/gpu Batch (t): 0.5873 LR: 0.000100 Step: 14200 Total Loss: 4.6064 Recon Loss: 4.5951 
[01/05 21:50:37 TiTok]: Data (t): 0.0011, 54.38/s/gpu Batch (t): 0.5885 LR: 0.000100 Step: 14250 Total Loss: 4.5497 Recon Loss: 4.5384 
[01/05 21:51:07 TiTok]: Data (t): 0.0014, 55.05/s/gpu Batch (t): 0.5813 LR: 0.000100 Step: 14300 Total Loss: 4.6009 Recon Loss: 4.5897 
[01/05 21:51:36 TiTok]: Data (t): 0.0011, 54.65/s/gpu Batch (t): 0.5856 LR: 0.000100 Step: 14350 Total Loss: 4.6384 Recon Loss: 4.6270 
[01/05 21:52:05 TiTok]: Data (t): 0.0011, 54.95/s/gpu Batch (t): 0.5824 LR: 0.000100 Step: 14400 Total Loss: 4.5973 Recon Loss: 4.5860 
[01/05 21:52:35 TiTok]: Data (t): 0.0011, 54.46/s/gpu Batch (t): 0.5876 LR: 0.000100 Step: 14450 Total Loss: 4.5920 Recon Loss: 4.5806 
[01/05 21:53:04 TiTok]: Data (t): 0.0011, 54.65/s/gpu Batch (t): 0.5856 LR: 0.000100 Step: 14500 Total Loss: 4.6324 Recon Loss: 4.6210 
[01/05 21:53:34 TiTok]: Data (t): 0.0006, 54.42/s/gpu Batch (t): 0.5880 LR: 0.000100 Step: 14550 Total Loss: 4.5547 Recon Loss: 4.5433 
[01/05 21:54:03 TiTok]: Data (t): 0.0011, 54.48/s/gpu Batch (t): 0.5873 LR: 0.000100 Step: 14600 Total Loss: 4.5390 Recon Loss: 4.5276 
[01/05 21:54:32 TiTok]: Data (t): 0.0011, 54.92/s/gpu Batch (t): 0.5827 LR: 0.000100 Step: 14650 Total Loss: 4.5940 Recon Loss: 4.5827 
[01/05 21:55:02 TiTok]: Data (t): 0.0013, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000100 Step: 14700 Total Loss: 4.5588 Recon Loss: 4.5474 
[01/05 21:55:31 TiTok]: Data (t): 0.0013, 55.07/s/gpu Batch (t): 0.5811 LR: 0.000100 Step: 14750 Total Loss: 4.5510 Recon Loss: 4.5396 
[01/05 21:56:00 TiTok]: Data (t): 0.0011, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 14800 Total Loss: 4.5490 Recon Loss: 4.5376 
[01/05 21:56:30 TiTok]: Data (t): 0.0009, 54.64/s/gpu Batch (t): 0.5857 LR: 0.000100 Step: 14850 Total Loss: 4.5748 Recon Loss: 4.5633 
[01/05 21:56:59 TiTok]: Data (t): 0.0011, 54.50/s/gpu Batch (t): 0.5872 LR: 0.000100 Step: 14900 Total Loss: 4.5618 Recon Loss: 4.5504 
[01/05 21:57:28 TiTok]: Data (t): 0.0011, 54.53/s/gpu Batch (t): 0.5868 LR: 0.000100 Step: 14950 Total Loss: 4.5631 Recon Loss: 4.5517 
[01/05 21:57:58 TiTok]: Data (t): 0.0011, 44.93/s/gpu Batch (t): 0.7122 LR: 0.000100 Step: 15000 Total Loss: 4.5572 Recon Loss: 4.5457 
[01/05 21:57:59 TiTok]: Reconstructing images...
Epoch 3/199 started.
[01/05 21:58:34 TiTok]: Data (t): 0.0009, 54.46/s/gpu Batch (t): 0.5876 LR: 0.000100 Step: 15050 Total Loss: 4.5604 Recon Loss: 4.5490 
[01/05 21:59:04 TiTok]: Data (t): 0.0011, 54.61/s/gpu Batch (t): 0.5860 LR: 0.000100 Step: 15100 Total Loss: 4.5155 Recon Loss: 4.5040 
[01/05 21:59:33 TiTok]: Data (t): 0.0008, 54.51/s/gpu Batch (t): 0.5870 LR: 0.000100 Step: 15150 Total Loss: 4.5330 Recon Loss: 4.5215 
[01/05 22:00:02 TiTok]: Data (t): 0.0011, 54.50/s/gpu Batch (t): 0.5872 LR: 0.000100 Step: 15200 Total Loss: 4.4681 Recon Loss: 4.4566 
[01/05 22:00:31 TiTok]: Data (t): 0.0006, 54.53/s/gpu Batch (t): 0.5868 LR: 0.000100 Step: 15250 Total Loss: 4.4865 Recon Loss: 4.4749 
[01/05 22:01:01 TiTok]: Data (t): 0.0011, 54.61/s/gpu Batch (t): 0.5859 LR: 0.000100 Step: 15300 Total Loss: 4.4967 Recon Loss: 4.4850 
[01/05 22:01:30 TiTok]: Data (t): 0.0006, 54.09/s/gpu Batch (t): 0.5916 LR: 0.000100 Step: 15350 Total Loss: 4.4639 Recon Loss: 4.4522 
[01/05 22:02:00 TiTok]: Data (t): 0.0011, 55.02/s/gpu Batch (t): 0.5816 LR: 0.000100 Step: 15400 Total Loss: 4.4445 Recon Loss: 4.4328 
[01/05 22:02:29 TiTok]: Data (t): 0.0011, 54.55/s/gpu Batch (t): 0.5866 LR: 0.000100 Step: 15450 Total Loss: 4.4680 Recon Loss: 4.4561 
[01/05 22:02:58 TiTok]: Data (t): 0.0007, 54.47/s/gpu Batch (t): 0.5874 LR: 0.000100 Step: 15500 Total Loss: 4.4714 Recon Loss: 4.4596 
[01/05 22:03:28 TiTok]: Data (t): 0.0006, 54.12/s/gpu Batch (t): 0.5913 LR: 0.000100 Step: 15550 Total Loss: 4.5039 Recon Loss: 4.4919 
[01/05 22:03:57 TiTok]: Data (t): 0.0007, 53.97/s/gpu Batch (t): 0.5929 LR: 0.000100 Step: 15600 Total Loss: 4.4783 Recon Loss: 4.4662 
[01/05 22:04:26 TiTok]: Data (t): 0.0011, 54.94/s/gpu Batch (t): 0.5825 LR: 0.000100 Step: 15650 Total Loss: 4.4761 Recon Loss: 4.4640 
[01/05 22:04:56 TiTok]: Data (t): 0.0011, 54.54/s/gpu Batch (t): 0.5867 LR: 0.000100 Step: 15700 Total Loss: 4.4429 Recon Loss: 4.4308 
[01/05 22:05:25 TiTok]: Data (t): 0.0011, 54.58/s/gpu Batch (t): 0.5863 LR: 0.000100 Step: 15750 Total Loss: 4.4710 Recon Loss: 4.4588 
[01/05 22:05:54 TiTok]: Data (t): 0.0007, 54.21/s/gpu Batch (t): 0.5903 LR: 0.000100 Step: 15800 Total Loss: 4.4899 Recon Loss: 4.4777 
[01/05 22:06:24 TiTok]: Data (t): 0.0006, 54.64/s/gpu Batch (t): 0.5857 LR: 0.000100 Step: 15850 Total Loss: 4.4748 Recon Loss: 4.4626 
[01/05 22:06:53 TiTok]: Data (t): 0.0011, 54.58/s/gpu Batch (t): 0.5863 LR: 0.000100 Step: 15900 Total Loss: 4.4836 Recon Loss: 4.4714 
[01/05 22:07:22 TiTok]: Data (t): 0.0011, 54.63/s/gpu Batch (t): 0.5858 LR: 0.000100 Step: 15950 Total Loss: 4.5173 Recon Loss: 4.5049 
[01/05 22:07:52 TiTok]: Data (t): 0.0006, 45.13/s/gpu Batch (t): 0.7091 LR: 0.000100 Step: 16000 Total Loss: 4.4692 Recon Loss: 4.4567 
[01/05 22:08:21 TiTok]: Data (t): 0.0011, 54.60/s/gpu Batch (t): 0.5861 LR: 0.000100 Step: 16050 Total Loss: 4.4773 Recon Loss: 4.4648 
[01/05 22:08:50 TiTok]: Data (t): 0.0011, 54.66/s/gpu Batch (t): 0.5854 LR: 0.000100 Step: 16100 Total Loss: 4.4437 Recon Loss: 4.4310 
[01/05 22:09:20 TiTok]: Data (t): 0.0011, 54.51/s/gpu Batch (t): 0.5871 LR: 0.000100 Step: 16150 Total Loss: 4.4297 Recon Loss: 4.4168 
[01/05 22:09:49 TiTok]: Data (t): 0.0009, 54.63/s/gpu Batch (t): 0.5857 LR: 0.000100 Step: 16200 Total Loss: 4.5005 Recon Loss: 4.4874 
[01/05 22:10:18 TiTok]: Data (t): 0.0011, 54.55/s/gpu Batch (t): 0.5867 LR: 0.000100 Step: 16250 Total Loss: 4.4071 Recon Loss: 4.3939 
[01/05 22:10:48 TiTok]: Data (t): 0.0007, 54.08/s/gpu Batch (t): 0.5917 LR: 0.000100 Step: 16300 Total Loss: 4.4389 Recon Loss: 4.4253 
[01/05 22:11:20 TiTok]: Data (t): 0.0011, 54.54/s/gpu Batch (t): 0.5867 LR: 0.000100 Step: 16350 Total Loss: 4.4423 Recon Loss: 4.4286 
[01/05 22:11:50 TiTok]: Data (t): 0.0012, 54.55/s/gpu Batch (t): 0.5866 LR: 0.000100 Step: 16400 Total Loss: 4.4360 Recon Loss: 4.4221 
[01/05 22:12:19 TiTok]: Data (t): 0.0008, 54.10/s/gpu Batch (t): 0.5915 LR: 0.000100 Step: 16450 Total Loss: 4.4670 Recon Loss: 4.4530 
[01/05 22:12:48 TiTok]: Data (t): 0.0011, 54.55/s/gpu Batch (t): 0.5867 LR: 0.000100 Step: 16500 Total Loss: 4.4073 Recon Loss: 4.3933 
[01/05 22:13:18 TiTok]: Data (t): 0.0011, 55.12/s/gpu Batch (t): 0.5806 LR: 0.000100 Step: 16550 Total Loss: 4.3870 Recon Loss: 4.3730 
[01/05 22:13:47 TiTok]: Data (t): 0.0011, 54.67/s/gpu Batch (t): 0.5853 LR: 0.000100 Step: 16600 Total Loss: 4.4286 Recon Loss: 4.4146 
[01/05 22:14:16 TiTok]: Data (t): 0.0011, 55.06/s/gpu Batch (t): 0.5812 LR: 0.000100 Step: 16650 Total Loss: 4.3941 Recon Loss: 4.3802 
[01/05 22:14:46 TiTok]: Data (t): 0.0006, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 16700 Total Loss: 4.4386 Recon Loss: 4.4245 
[01/05 22:15:15 TiTok]: Data (t): 0.0011, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 16750 Total Loss: 4.3704 Recon Loss: 4.3563 
[01/05 22:15:44 TiTok]: Data (t): 0.0011, 54.53/s/gpu Batch (t): 0.5868 LR: 0.000100 Step: 16800 Total Loss: 4.3816 Recon Loss: 4.3676 
[01/05 22:16:14 TiTok]: Data (t): 0.0014, 54.61/s/gpu Batch (t): 0.5860 LR: 0.000100 Step: 16850 Total Loss: 4.4026 Recon Loss: 4.3885 
[01/05 22:16:43 TiTok]: Data (t): 0.0011, 54.61/s/gpu Batch (t): 0.5859 LR: 0.000100 Step: 16900 Total Loss: 4.3517 Recon Loss: 4.3376 
[01/05 22:17:12 TiTok]: Data (t): 0.0011, 54.60/s/gpu Batch (t): 0.5861 LR: 0.000100 Step: 16950 Total Loss: 4.3921 Recon Loss: 4.3780 
[01/05 22:17:42 TiTok]: Data (t): 0.0011, 46.98/s/gpu Batch (t): 0.6811 LR: 0.000100 Step: 17000 Total Loss: 4.3388 Recon Loss: 4.3247 
[01/05 22:18:11 TiTok]: Data (t): 0.0006, 54.52/s/gpu Batch (t): 0.5869 LR: 0.000100 Step: 17050 Total Loss: 4.3796 Recon Loss: 4.3655 
[01/05 22:18:40 TiTok]: Data (t): 0.0006, 54.11/s/gpu Batch (t): 0.5914 LR: 0.000100 Step: 17100 Total Loss: 4.3385 Recon Loss: 4.3244 
[01/05 22:19:10 TiTok]: Data (t): 0.0011, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000100 Step: 17150 Total Loss: 4.3685 Recon Loss: 4.3544 
[01/05 22:19:39 TiTok]: Data (t): 0.0011, 54.46/s/gpu Batch (t): 0.5876 LR: 0.000100 Step: 17200 Total Loss: 4.3573 Recon Loss: 4.3432 
[01/05 22:20:08 TiTok]: Data (t): 0.0011, 55.11/s/gpu Batch (t): 0.5807 LR: 0.000100 Step: 17250 Total Loss: 4.3520 Recon Loss: 4.3379 
[01/05 22:20:38 TiTok]: Data (t): 0.0006, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 17300 Total Loss: 4.3119 Recon Loss: 4.2978 
[01/05 22:21:07 TiTok]: Data (t): 0.0011, 54.41/s/gpu Batch (t): 0.5881 LR: 0.000100 Step: 17350 Total Loss: 4.3247 Recon Loss: 4.3106 
[01/05 22:21:36 TiTok]: Data (t): 0.0011, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000100 Step: 17400 Total Loss: 4.3603 Recon Loss: 4.3462 
[01/05 22:22:06 TiTok]: Data (t): 0.0011, 54.47/s/gpu Batch (t): 0.5875 LR: 0.000100 Step: 17450 Total Loss: 4.3421 Recon Loss: 4.3280 
[01/05 22:22:35 TiTok]: Data (t): 0.0007, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 17500 Total Loss: 4.3007 Recon Loss: 4.2865 
[01/05 22:23:04 TiTok]: Data (t): 0.0007, 54.02/s/gpu Batch (t): 0.5924 LR: 0.000100 Step: 17550 Total Loss: 4.3535 Recon Loss: 4.3393 
[01/05 22:23:34 TiTok]: Data (t): 0.0007, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 17600 Total Loss: 4.2701 Recon Loss: 4.2560 
[01/05 22:24:03 TiTok]: Data (t): 0.0011, 54.65/s/gpu Batch (t): 0.5856 LR: 0.000100 Step: 17650 Total Loss: 4.2710 Recon Loss: 4.2568 
[01/05 22:24:32 TiTok]: Data (t): 0.0011, 55.10/s/gpu Batch (t): 0.5808 LR: 0.000100 Step: 17700 Total Loss: 4.2881 Recon Loss: 4.2739 
[01/05 22:25:02 TiTok]: Data (t): 0.0011, 54.60/s/gpu Batch (t): 0.5860 LR: 0.000100 Step: 17750 Total Loss: 4.2917 Recon Loss: 4.2775 
[01/05 22:25:31 TiTok]: Data (t): 0.0006, 54.09/s/gpu Batch (t): 0.5916 LR: 0.000100 Step: 17800 Total Loss: 4.2709 Recon Loss: 4.2568 
[01/05 22:26:00 TiTok]: Data (t): 0.0007, 54.60/s/gpu Batch (t): 0.5861 LR: 0.000100 Step: 17850 Total Loss: 4.2682 Recon Loss: 4.2541 
[01/05 22:26:30 TiTok]: Data (t): 0.0011, 54.55/s/gpu Batch (t): 0.5867 LR: 0.000100 Step: 17900 Total Loss: 4.2781 Recon Loss: 4.2640 
[01/05 22:26:59 TiTok]: Data (t): 0.0011, 55.04/s/gpu Batch (t): 0.5814 LR: 0.000100 Step: 17950 Total Loss: 4.3063 Recon Loss: 4.2922 
[01/05 22:27:29 TiTok]: Data (t): 0.0011, 46.72/s/gpu Batch (t): 0.6849 LR: 0.000100 Step: 18000 Total Loss: 4.2211 Recon Loss: 4.2070 
[01/05 22:27:58 TiTok]: Data (t): 0.0011, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000100 Step: 18050 Total Loss: 4.2530 Recon Loss: 4.2389 
[01/05 22:28:27 TiTok]: Data (t): 0.0012, 54.38/s/gpu Batch (t): 0.5884 LR: 0.000100 Step: 18100 Total Loss: 4.1968 Recon Loss: 4.1826 
[01/05 22:28:57 TiTok]: Data (t): 0.0006, 54.13/s/gpu Batch (t): 0.5912 LR: 0.000100 Step: 18150 Total Loss: 4.2271 Recon Loss: 4.2130 
[01/05 22:29:26 TiTok]: Data (t): 0.0011, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000100 Step: 18200 Total Loss: 4.2526 Recon Loss: 4.2384 
[01/05 22:29:55 TiTok]: Data (t): 0.0011, 54.60/s/gpu Batch (t): 0.5861 LR: 0.000100 Step: 18250 Total Loss: 4.2614 Recon Loss: 4.2472 
[01/05 22:30:25 TiTok]: Data (t): 0.0011, 54.63/s/gpu Batch (t): 0.5858 LR: 0.000100 Step: 18300 Total Loss: 4.2677 Recon Loss: 4.2536 
[01/05 22:30:54 TiTok]: Data (t): 0.0011, 54.39/s/gpu Batch (t): 0.5883 LR: 0.000100 Step: 18350 Total Loss: 4.2710 Recon Loss: 4.2569 
[01/05 22:31:24 TiTok]: Data (t): 0.0011, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000100 Step: 18400 Total Loss: 4.2378 Recon Loss: 4.2236 
[01/05 22:31:53 TiTok]: Data (t): 0.0013, 54.58/s/gpu Batch (t): 0.5863 LR: 0.000100 Step: 18450 Total Loss: 4.2424 Recon Loss: 4.2282 
[01/05 22:32:22 TiTok]: Data (t): 0.0011, 55.06/s/gpu Batch (t): 0.5811 LR: 0.000100 Step: 18500 Total Loss: 4.1983 Recon Loss: 4.1842 
[01/05 22:32:52 TiTok]: Data (t): 0.0011, 54.53/s/gpu Batch (t): 0.5868 LR: 0.000100 Step: 18550 Total Loss: 4.2220 Recon Loss: 4.2078 
[01/05 22:33:21 TiTok]: Data (t): 0.0007, 54.67/s/gpu Batch (t): 0.5854 LR: 0.000100 Step: 18600 Total Loss: 4.1864 Recon Loss: 4.1722 
[01/05 22:33:50 TiTok]: Data (t): 0.0011, 54.44/s/gpu Batch (t): 0.5878 LR: 0.000100 Step: 18650 Total Loss: 4.2037 Recon Loss: 4.1896 
[01/05 22:34:20 TiTok]: Data (t): 0.0012, 54.49/s/gpu Batch (t): 0.5873 LR: 0.000100 Step: 18700 Total Loss: 4.1915 Recon Loss: 4.1773 
[01/05 22:34:49 TiTok]: Data (t): 0.0011, 54.50/s/gpu Batch (t): 0.5872 LR: 0.000100 Step: 18750 Total Loss: 4.2313 Recon Loss: 4.2171 
[01/05 22:35:18 TiTok]: Data (t): 0.0013, 54.97/s/gpu Batch (t): 0.5821 LR: 0.000100 Step: 18800 Total Loss: 4.2207 Recon Loss: 4.2066 
[01/05 22:35:48 TiTok]: Data (t): 0.0011, 54.61/s/gpu Batch (t): 0.5860 LR: 0.000100 Step: 18850 Total Loss: 4.1844 Recon Loss: 4.1703 
[01/05 22:36:17 TiTok]: Data (t): 0.0009, 54.71/s/gpu Batch (t): 0.5849 LR: 0.000100 Step: 18900 Total Loss: 4.1929 Recon Loss: 4.1788 
[01/05 22:36:46 TiTok]: Data (t): 0.0011, 54.60/s/gpu Batch (t): 0.5861 LR: 0.000100 Step: 18950 Total Loss: 4.2065 Recon Loss: 4.1923 
[01/05 22:37:16 TiTok]: Data (t): 0.0011, 46.52/s/gpu Batch (t): 0.6879 LR: 0.000100 Step: 19000 Total Loss: 4.2017 Recon Loss: 4.1876 
[01/05 22:37:45 TiTok]: Data (t): 0.0006, 54.58/s/gpu Batch (t): 0.5863 LR: 0.000100 Step: 19050 Total Loss: 4.2105 Recon Loss: 4.1964 
[01/05 22:38:14 TiTok]: Data (t): 0.0011, 54.65/s/gpu Batch (t): 0.5856 LR: 0.000100 Step: 19100 Total Loss: 4.1339 Recon Loss: 4.1198 
[01/05 22:38:44 TiTok]: Data (t): 0.0009, 54.51/s/gpu Batch (t): 0.5870 LR: 0.000100 Step: 19150 Total Loss: 4.1447 Recon Loss: 4.1305 
[01/05 22:39:13 TiTok]: Data (t): 0.0011, 54.58/s/gpu Batch (t): 0.5863 LR: 0.000100 Step: 19200 Total Loss: 4.1457 Recon Loss: 4.1315 
[01/05 22:39:42 TiTok]: Data (t): 0.0007, 54.18/s/gpu Batch (t): 0.5906 LR: 0.000100 Step: 19250 Total Loss: 4.1427 Recon Loss: 4.1286 
[01/05 22:40:12 TiTok]: Data (t): 0.0011, 54.58/s/gpu Batch (t): 0.5862 LR: 0.000100 Step: 19300 Total Loss: 4.1484 Recon Loss: 4.1343 
[01/05 22:40:41 TiTok]: Data (t): 0.0011, 54.60/s/gpu Batch (t): 0.5861 LR: 0.000100 Step: 19350 Total Loss: 4.1300 Recon Loss: 4.1159 
[01/05 22:41:10 TiTok]: Data (t): 0.0011, 54.62/s/gpu Batch (t): 0.5858 LR: 0.000100 Step: 19400 Total Loss: 4.0959 Recon Loss: 4.0818 
[01/05 22:41:40 TiTok]: Data (t): 0.0011, 54.61/s/gpu Batch (t): 0.5860 LR: 0.000100 Step: 19450 Total Loss: 4.1536 Recon Loss: 4.1395 
[01/05 22:42:09 TiTok]: Data (t): 0.0011, 54.53/s/gpu Batch (t): 0.5868 LR: 0.000100 Step: 19500 Total Loss: 4.1097 Recon Loss: 4.0956 
[01/05 22:42:38 TiTok]: Data (t): 0.0012, 54.42/s/gpu Batch (t): 0.5880 LR: 0.000100 Step: 19550 Total Loss: 4.1724 Recon Loss: 4.1582 
[01/05 22:43:08 TiTok]: Data (t): 0.0011, 54.53/s/gpu Batch (t): 0.5868 LR: 0.000100 Step: 19600 Total Loss: 4.1221 Recon Loss: 4.1080 
[01/05 22:43:37 TiTok]: Data (t): 0.0011, 54.61/s/gpu Batch (t): 0.5860 LR: 0.000100 Step: 19650 Total Loss: 4.1051 Recon Loss: 4.0910 
[01/05 22:44:06 TiTok]: Data (t): 0.0011, 55.06/s/gpu Batch (t): 0.5812 LR: 0.000100 Step: 19700 Total Loss: 4.1179 Recon Loss: 4.1038 
[01/05 22:44:36 TiTok]: Data (t): 0.0011, 54.54/s/gpu Batch (t): 0.5867 LR: 0.000100 Step: 19750 Total Loss: 4.1198 Recon Loss: 4.1057 
[01/05 22:45:05 TiTok]: Data (t): 0.0007, 54.20/s/gpu Batch (t): 0.5904 LR: 0.000100 Step: 19800 Total Loss: 4.0749 Recon Loss: 4.0608 
[01/05 22:45:34 TiTok]: Data (t): 0.0006, 54.26/s/gpu Batch (t): 0.5898 LR: 0.000100 Step: 19850 Total Loss: 4.1124 Recon Loss: 4.0983 
[01/05 22:46:04 TiTok]: Data (t): 0.0011, 54.90/s/gpu Batch (t): 0.5829 LR: 0.000100 Step: 19900 Total Loss: 4.1313 Recon Loss: 4.1171 
[01/05 22:46:33 TiTok]: Data (t): 0.0006, 54.13/s/gpu Batch (t): 0.5912 LR: 0.000100 Step: 19950 Total Loss: 4.1342 Recon Loss: 4.1201 
[01/05 22:47:02 TiTok]: Data (t): 0.0007, 46.89/s/gpu Batch (t): 0.6824 LR: 0.000100 Step: 20000 Total Loss: 4.0588 Recon Loss: 4.0447 
[01/05 22:47:04 TiTok]: Reconstructing images...
[01/05 22:47:37 TiTok]: Data (t): 0.0011, 55.13/s/gpu Batch (t): 0.5805 LR: 0.000100 Step: 20050 Total Loss: 4.0841 Recon Loss: 4.0700 
Epoch 4/199 started.
[01/05 22:48:08 TiTok]: Data (t): 0.0014, 55.05/s/gpu Batch (t): 0.5813 LR: 0.000100 Step: 20100 Total Loss: 4.0143 Recon Loss: 4.0002 
[01/05 22:48:38 TiTok]: Data (t): 0.0006, 54.48/s/gpu Batch (t): 0.5874 LR: 0.000100 Step: 20150 Total Loss: 4.0725 Recon Loss: 4.0585 
[01/05 22:49:07 TiTok]: Data (t): 0.0006, 54.05/s/gpu Batch (t): 0.5921 LR: 0.000100 Step: 20200 Total Loss: 4.1022 Recon Loss: 4.0881 
[01/05 22:49:36 TiTok]: Data (t): 0.0012, 55.15/s/gpu Batch (t): 0.5803 LR: 0.000100 Step: 20250 Total Loss: 4.0637 Recon Loss: 4.0496 
[01/05 22:50:06 TiTok]: Data (t): 0.0007, 54.52/s/gpu Batch (t): 0.5870 LR: 0.000100 Step: 20300 Total Loss: 4.1117 Recon Loss: 4.0975 
[01/05 22:50:35 TiTok]: Data (t): 0.0011, 54.43/s/gpu Batch (t): 0.5879 LR: 0.000100 Step: 20350 Total Loss: 4.0580 Recon Loss: 4.0439 
[01/05 22:51:04 TiTok]: Data (t): 0.0012, 54.69/s/gpu Batch (t): 0.5851 LR: 0.000100 Step: 20400 Total Loss: 4.0210 Recon Loss: 4.0069 
[01/05 22:51:34 TiTok]: Data (t): 0.0011, 54.54/s/gpu Batch (t): 0.5867 LR: 0.000100 Step: 20450 Total Loss: 4.0654 Recon Loss: 4.0513 
[01/05 22:52:03 TiTok]: Data (t): 0.0011, 54.86/s/gpu Batch (t): 0.5833 LR: 0.000100 Step: 20500 Total Loss: 4.0506 Recon Loss: 4.0365 
[01/05 22:52:32 TiTok]: Data (t): 0.0011, 54.99/s/gpu Batch (t): 0.5819 LR: 0.000100 Step: 20550 Total Loss: 4.0262 Recon Loss: 4.0121 
[01/05 22:53:02 TiTok]: Data (t): 0.0012, 54.98/s/gpu Batch (t): 0.5820 LR: 0.000100 Step: 20600 Total Loss: 4.0810 Recon Loss: 4.0669 
[01/05 22:53:31 TiTok]: Data (t): 0.0007, 54.08/s/gpu Batch (t): 0.5917 LR: 0.000100 Step: 20650 Total Loss: 4.0229 Recon Loss: 4.0088 
[01/05 22:54:00 TiTok]: Data (t): 0.0007, 54.50/s/gpu Batch (t): 0.5871 LR: 0.000100 Step: 20700 Total Loss: 4.0379 Recon Loss: 4.0238 
[01/05 22:54:30 TiTok]: Data (t): 0.0011, 54.68/s/gpu Batch (t): 0.5853 LR: 0.000100 Step: 20750 Total Loss: 4.0237 Recon Loss: 4.0095 
[01/05 22:54:59 TiTok]: Data (t): 0.0011, 54.53/s/gpu Batch (t): 0.5869 LR: 0.000100 Step: 20800 Total Loss: 4.0342 Recon Loss: 4.0201 
[01/05 22:55:28 TiTok]: Data (t): 0.0009, 54.19/s/gpu Batch (t): 0.5905 LR: 0.000100 Step: 20850 Total Loss: 4.0421 Recon Loss: 4.0280 
[01/05 22:55:58 TiTok]: Data (t): 0.0006, 54.69/s/gpu Batch (t): 0.5851 LR: 0.000100 Step: 20900 Total Loss: 4.0105 Recon Loss: 3.9964 
[01/05 22:56:27 TiTok]: Data (t): 0.0011, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000100 Step: 20950 Total Loss: 4.0099 Recon Loss: 3.9957 
[01/05 22:56:57 TiTok]: Data (t): 0.0007, 44.48/s/gpu Batch (t): 0.7194 LR: 0.000100 Step: 21000 Total Loss: 4.0043 Recon Loss: 3.9902 
[01/05 22:57:26 TiTok]: Data (t): 0.0011, 55.06/s/gpu Batch (t): 0.5811 LR: 0.000100 Step: 21050 Total Loss: 4.0331 Recon Loss: 4.0190 
[01/05 22:57:55 TiTok]: Data (t): 0.0011, 55.06/s/gpu Batch (t): 0.5811 LR: 0.000100 Step: 21100 Total Loss: 4.0149 Recon Loss: 4.0008 
[01/05 22:58:25 TiTok]: Data (t): 0.0007, 54.48/s/gpu Batch (t): 0.5874 LR: 0.000100 Step: 21150 Total Loss: 4.0134 Recon Loss: 3.9993 
[01/05 22:58:54 TiTok]: Data (t): 0.0011, 54.62/s/gpu Batch (t): 0.5859 LR: 0.000100 Step: 21200 Total Loss: 4.0175 Recon Loss: 4.0033 
[01/05 22:59:23 TiTok]: Data (t): 0.0011, 54.64/s/gpu Batch (t): 0.5856 LR: 0.000100 Step: 21250 Total Loss: 3.9980 Recon Loss: 3.9839 
[01/05 22:59:53 TiTok]: Data (t): 0.0011, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 21300 Total Loss: 4.0071 Recon Loss: 3.9930 
[01/05 23:00:22 TiTok]: Data (t): 0.0009, 53.60/s/gpu Batch (t): 0.5970 LR: 0.000100 Step: 21350 Total Loss: 3.9750 Recon Loss: 3.9609 
[01/05 23:00:51 TiTok]: Data (t): 0.0009, 54.43/s/gpu Batch (t): 0.5879 LR: 0.000100 Step: 21400 Total Loss: 4.0166 Recon Loss: 4.0025 
[01/05 23:01:21 TiTok]: Data (t): 0.0011, 54.94/s/gpu Batch (t): 0.5824 LR: 0.000100 Step: 21450 Total Loss: 4.0389 Recon Loss: 4.0248 
[01/05 23:01:50 TiTok]: Data (t): 0.0016, 55.12/s/gpu Batch (t): 0.5806 LR: 0.000100 Step: 21500 Total Loss: 4.0093 Recon Loss: 3.9952 
[01/05 23:02:19 TiTok]: Data (t): 0.0006, 54.73/s/gpu Batch (t): 0.5847 LR: 0.000100 Step: 21550 Total Loss: 4.0269 Recon Loss: 4.0127 
[01/05 23:02:49 TiTok]: Data (t): 0.0011, 54.62/s/gpu Batch (t): 0.5858 LR: 0.000100 Step: 21600 Total Loss: 3.9895 Recon Loss: 3.9754 
[01/05 23:03:18 TiTok]: Data (t): 0.0014, 54.41/s/gpu Batch (t): 0.5881 LR: 0.000100 Step: 21650 Total Loss: 3.9767 Recon Loss: 3.9626 
[01/05 23:03:47 TiTok]: Data (t): 0.0006, 54.53/s/gpu Batch (t): 0.5868 LR: 0.000100 Step: 21700 Total Loss: 3.9718 Recon Loss: 3.9577 
[01/05 23:04:17 TiTok]: Data (t): 0.0007, 54.06/s/gpu Batch (t): 0.5919 LR: 0.000100 Step: 21750 Total Loss: 3.9761 Recon Loss: 3.9620 
[01/05 23:04:46 TiTok]: Data (t): 0.0011, 54.58/s/gpu Batch (t): 0.5863 LR: 0.000100 Step: 21800 Total Loss: 4.0247 Recon Loss: 4.0106 
[01/05 23:05:15 TiTok]: Data (t): 0.0012, 54.52/s/gpu Batch (t): 0.5869 LR: 0.000100 Step: 21850 Total Loss: 4.0032 Recon Loss: 3.9891 
[01/05 23:05:45 TiTok]: Data (t): 0.0011, 54.37/s/gpu Batch (t): 0.5885 LR: 0.000100 Step: 21900 Total Loss: 3.9957 Recon Loss: 3.9816 
[01/05 23:06:14 TiTok]: Data (t): 0.0011, 54.61/s/gpu Batch (t): 0.5860 LR: 0.000100 Step: 21950 Total Loss: 3.9738 Recon Loss: 3.9597 
[01/05 23:06:43 TiTok]: Data (t): 0.0007, 46.14/s/gpu Batch (t): 0.6935 LR: 0.000100 Step: 22000 Total Loss: 4.0047 Recon Loss: 3.9906 
[01/05 23:07:13 TiTok]: Data (t): 0.0006, 54.11/s/gpu Batch (t): 0.5914 LR: 0.000100 Step: 22050 Total Loss: 3.9459 Recon Loss: 3.9318 
[01/05 23:07:42 TiTok]: Data (t): 0.0007, 54.16/s/gpu Batch (t): 0.5908 LR: 0.000100 Step: 22100 Total Loss: 3.9873 Recon Loss: 3.9731 
[01/05 23:08:11 TiTok]: Data (t): 0.0014, 55.08/s/gpu Batch (t): 0.5810 LR: 0.000100 Step: 22150 Total Loss: 3.9688 Recon Loss: 3.9547 
[01/05 23:08:41 TiTok]: Data (t): 0.0009, 54.11/s/gpu Batch (t): 0.5914 LR: 0.000100 Step: 22200 Total Loss: 3.9562 Recon Loss: 3.9420 
[01/05 23:09:10 TiTok]: Data (t): 0.0011, 54.48/s/gpu Batch (t): 0.5874 LR: 0.000100 Step: 22250 Total Loss: 3.9507 Recon Loss: 3.9366 
[01/05 23:09:39 TiTok]: Data (t): 0.0011, 55.02/s/gpu Batch (t): 0.5816 LR: 0.000100 Step: 22300 Total Loss: 3.9805 Recon Loss: 3.9664 
[01/05 23:10:09 TiTok]: Data (t): 0.0011, 54.63/s/gpu Batch (t): 0.5858 LR: 0.000100 Step: 22350 Total Loss: 3.9466 Recon Loss: 3.9325 
[01/05 23:10:38 TiTok]: Data (t): 0.0011, 54.49/s/gpu Batch (t): 0.5872 LR: 0.000100 Step: 22400 Total Loss: 3.9540 Recon Loss: 3.9399 
[01/05 23:11:07 TiTok]: Data (t): 0.0011, 54.44/s/gpu Batch (t): 0.5878 LR: 0.000100 Step: 22450 Total Loss: 3.9419 Recon Loss: 3.9277 
[01/05 23:11:37 TiTok]: Data (t): 0.0011, 54.58/s/gpu Batch (t): 0.5863 LR: 0.000100 Step: 22500 Total Loss: 3.9585 Recon Loss: 3.9444 
[01/05 23:12:06 TiTok]: Data (t): 0.0011, 54.47/s/gpu Batch (t): 0.5874 LR: 0.000100 Step: 22550 Total Loss: 3.9624 Recon Loss: 3.9483 
[01/05 23:12:35 TiTok]: Data (t): 0.0011, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000100 Step: 22600 Total Loss: 3.9293 Recon Loss: 3.9151 
[01/05 23:13:05 TiTok]: Data (t): 0.0006, 54.56/s/gpu Batch (t): 0.5866 LR: 0.000100 Step: 22650 Total Loss: 3.9494 Recon Loss: 3.9353 
[01/05 23:13:34 TiTok]: Data (t): 0.0011, 54.61/s/gpu Batch (t): 0.5860 LR: 0.000100 Step: 22700 Total Loss: 3.9310 Recon Loss: 3.9169 
[01/05 23:14:03 TiTok]: Data (t): 0.0011, 53.94/s/gpu Batch (t): 0.5932 LR: 0.000100 Step: 22750 Total Loss: 3.8958 Recon Loss: 3.8817 
[01/05 23:14:33 TiTok]: Data (t): 0.0006, 54.15/s/gpu Batch (t): 0.5910 LR: 0.000100 Step: 22800 Total Loss: 3.9594 Recon Loss: 3.9453 
[01/05 23:15:02 TiTok]: Data (t): 0.0007, 54.53/s/gpu Batch (t): 0.5868 LR: 0.000100 Step: 22850 Total Loss: 3.8902 Recon Loss: 3.8760 
[01/05 23:15:31 TiTok]: Data (t): 0.0011, 54.55/s/gpu Batch (t): 0.5867 LR: 0.000100 Step: 22900 Total Loss: 3.9203 Recon Loss: 3.9062 
[01/05 23:16:01 TiTok]: Data (t): 0.0007, 54.09/s/gpu Batch (t): 0.5916 LR: 0.000100 Step: 22950 Total Loss: 3.8805 Recon Loss: 3.8664 
[01/05 23:16:30 TiTok]: Data (t): 0.0011, 44.49/s/gpu Batch (t): 0.7193 LR: 0.000100 Step: 23000 Total Loss: 3.9622 Recon Loss: 3.9480 
[01/05 23:17:00 TiTok]: Data (t): 0.0007, 54.48/s/gpu Batch (t): 0.5874 LR: 0.000100 Step: 23050 Total Loss: 3.9298 Recon Loss: 3.9157 
[01/05 23:17:29 TiTok]: Data (t): 0.0012, 54.63/s/gpu Batch (t): 0.5857 LR: 0.000100 Step: 23100 Total Loss: 3.9125 Recon Loss: 3.8984 
[01/05 23:17:58 TiTok]: Data (t): 0.0011, 54.73/s/gpu Batch (t): 0.5847 LR: 0.000100 Step: 23150 Total Loss: 3.9305 Recon Loss: 3.9164 
[01/05 23:18:28 TiTok]: Data (t): 0.0011, 54.96/s/gpu Batch (t): 0.5823 LR: 0.000100 Step: 23200 Total Loss: 3.8959 Recon Loss: 3.8818 
[01/05 23:18:57 TiTok]: Data (t): 0.0006, 54.64/s/gpu Batch (t): 0.5857 LR: 0.000100 Step: 23250 Total Loss: 3.9164 Recon Loss: 3.9023 
[01/05 23:19:26 TiTok]: Data (t): 0.0008, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 23300 Total Loss: 3.9534 Recon Loss: 3.9392 
[01/05 23:19:56 TiTok]: Data (t): 0.0011, 54.60/s/gpu Batch (t): 0.5861 LR: 0.000100 Step: 23350 Total Loss: 3.8710 Recon Loss: 3.8569 
[01/05 23:20:25 TiTok]: Data (t): 0.0006, 54.55/s/gpu Batch (t): 0.5866 LR: 0.000100 Step: 23400 Total Loss: 3.9019 Recon Loss: 3.8878 
[01/05 23:20:54 TiTok]: Data (t): 0.0012, 55.00/s/gpu Batch (t): 0.5819 LR: 0.000100 Step: 23450 Total Loss: 3.9234 Recon Loss: 3.9093 
[01/05 23:21:24 TiTok]: Data (t): 0.0011, 54.39/s/gpu Batch (t): 0.5883 LR: 0.000100 Step: 23500 Total Loss: 3.9100 Recon Loss: 3.8958 
[01/05 23:21:53 TiTok]: Data (t): 0.0011, 54.52/s/gpu Batch (t): 0.5869 LR: 0.000100 Step: 23550 Total Loss: 3.9201 Recon Loss: 3.9060 
[01/05 23:22:23 TiTok]: Data (t): 0.0014, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 23600 Total Loss: 3.8829 Recon Loss: 3.8688 
[01/05 23:22:52 TiTok]: Data (t): 0.0006, 54.53/s/gpu Batch (t): 0.5869 LR: 0.000100 Step: 23650 Total Loss: 3.9145 Recon Loss: 3.9004 
[01/05 23:23:21 TiTok]: Data (t): 0.0011, 54.52/s/gpu Batch (t): 0.5869 LR: 0.000100 Step: 23700 Total Loss: 3.8858 Recon Loss: 3.8717 
[01/05 23:23:51 TiTok]: Data (t): 0.0011, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 23750 Total Loss: 3.8368 Recon Loss: 3.8228 
[01/05 23:24:20 TiTok]: Data (t): 0.0011, 55.10/s/gpu Batch (t): 0.5807 LR: 0.000100 Step: 23800 Total Loss: 3.9375 Recon Loss: 3.9234 
[01/05 23:24:49 TiTok]: Data (t): 0.0011, 53.99/s/gpu Batch (t): 0.5928 LR: 0.000100 Step: 23850 Total Loss: 3.8844 Recon Loss: 3.8703 
[01/05 23:25:19 TiTok]: Data (t): 0.0011, 54.98/s/gpu Batch (t): 0.5821 LR: 0.000100 Step: 23900 Total Loss: 3.8911 Recon Loss: 3.8770 
[01/05 23:25:48 TiTok]: Data (t): 0.0013, 54.67/s/gpu Batch (t): 0.5853 LR: 0.000100 Step: 23950 Total Loss: 3.8915 Recon Loss: 3.8774 
[01/05 23:26:18 TiTok]: Data (t): 0.0006, 45.40/s/gpu Batch (t): 0.7048 LR: 0.000100 Step: 24000 Total Loss: 3.8697 Recon Loss: 3.8557 
[01/05 23:26:47 TiTok]: Data (t): 0.0011, 54.58/s/gpu Batch (t): 0.5863 LR: 0.000100 Step: 24050 Total Loss: 3.8957 Recon Loss: 3.8816 
[01/05 23:27:16 TiTok]: Data (t): 0.0007, 54.22/s/gpu Batch (t): 0.5902 LR: 0.000100 Step: 24100 Total Loss: 3.8729 Recon Loss: 3.8588 
[01/05 23:27:46 TiTok]: Data (t): 0.0011, 54.90/s/gpu Batch (t): 0.5829 LR: 0.000100 Step: 24150 Total Loss: 3.9065 Recon Loss: 3.8924 
[01/05 23:28:15 TiTok]: Data (t): 0.0007, 54.09/s/gpu Batch (t): 0.5916 LR: 0.000100 Step: 24200 Total Loss: 3.8795 Recon Loss: 3.8654 
[01/05 23:28:44 TiTok]: Data (t): 0.0011, 55.00/s/gpu Batch (t): 0.5818 LR: 0.000100 Step: 24250 Total Loss: 3.8519 Recon Loss: 3.8378 
[01/05 23:29:14 TiTok]: Data (t): 0.0007, 54.10/s/gpu Batch (t): 0.5915 LR: 0.000100 Step: 24300 Total Loss: 3.8747 Recon Loss: 3.8606 
[01/05 23:29:43 TiTok]: Data (t): 0.0006, 54.49/s/gpu Batch (t): 0.5872 LR: 0.000100 Step: 24350 Total Loss: 3.8816 Recon Loss: 3.8675 
[01/05 23:30:12 TiTok]: Data (t): 0.0011, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 24400 Total Loss: 3.8494 Recon Loss: 3.8353 
[01/05 23:30:42 TiTok]: Data (t): 0.0011, 54.99/s/gpu Batch (t): 0.5819 LR: 0.000100 Step: 24450 Total Loss: 3.8620 Recon Loss: 3.8479 
[01/05 23:31:11 TiTok]: Data (t): 0.0007, 54.10/s/gpu Batch (t): 0.5915 LR: 0.000100 Step: 24500 Total Loss: 3.8279 Recon Loss: 3.8138 
[01/05 23:31:41 TiTok]: Data (t): 0.0011, 54.65/s/gpu Batch (t): 0.5855 LR: 0.000100 Step: 24550 Total Loss: 3.8422 Recon Loss: 3.8281 
[01/05 23:32:10 TiTok]: Data (t): 0.0011, 54.50/s/gpu Batch (t): 0.5871 LR: 0.000100 Step: 24600 Total Loss: 3.8555 Recon Loss: 3.8414 
[01/05 23:32:39 TiTok]: Data (t): 0.0011, 55.09/s/gpu Batch (t): 0.5808 LR: 0.000100 Step: 24650 Total Loss: 3.8692 Recon Loss: 3.8551 
[01/05 23:33:09 TiTok]: Data (t): 0.0011, 54.59/s/gpu Batch (t): 0.5861 LR: 0.000100 Step: 24700 Total Loss: 3.8433 Recon Loss: 3.8292 
[01/05 23:33:38 TiTok]: Data (t): 0.0013, 54.82/s/gpu Batch (t): 0.5838 LR: 0.000100 Step: 24750 Total Loss: 3.8583 Recon Loss: 3.8442 
[01/05 23:34:07 TiTok]: Data (t): 0.0007, 53.69/s/gpu Batch (t): 0.5960 LR: 0.000100 Step: 24800 Total Loss: 3.8099 Recon Loss: 3.7958 
[01/05 23:34:37 TiTok]: Data (t): 0.0011, 54.39/s/gpu Batch (t): 0.5883 LR: 0.000100 Step: 24850 Total Loss: 3.8556 Recon Loss: 3.8415 
[01/05 23:35:06 TiTok]: Data (t): 0.0006, 54.15/s/gpu Batch (t): 0.5909 LR: 0.000100 Step: 24900 Total Loss: 3.8818 Recon Loss: 3.8677 
[01/05 23:35:35 TiTok]: Data (t): 0.0012, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 24950 Total Loss: 3.8391 Recon Loss: 3.8250 
[01/05 23:36:05 TiTok]: Data (t): 0.0011, 45.17/s/gpu Batch (t): 0.7084 LR: 0.000100 Step: 25000 Total Loss: 3.8308 Recon Loss: 3.8167 
[01/05 23:36:06 TiTok]: Reconstructing images...
[01/05 23:36:42 TiTok]: Data (t): 0.0007, 54.69/s/gpu Batch (t): 0.5851 LR: 0.000100 Step: 25050 Total Loss: 3.8432 Recon Loss: 3.8290 
Epoch 5/199 started.
[01/05 23:37:13 TiTok]: Data (t): 0.0006, 54.64/s/gpu Batch (t): 0.5857 LR: 0.000100 Step: 25100 Total Loss: 3.8261 Recon Loss: 3.8119 
[01/05 23:37:42 TiTok]: Data (t): 0.0011, 54.51/s/gpu Batch (t): 0.5870 LR: 0.000100 Step: 25150 Total Loss: 3.8650 Recon Loss: 3.8509 
[01/05 23:38:11 TiTok]: Data (t): 0.0011, 54.60/s/gpu Batch (t): 0.5861 LR: 0.000100 Step: 25200 Total Loss: 3.8423 Recon Loss: 3.8281 
[01/05 23:38:41 TiTok]: Data (t): 0.0011, 55.10/s/gpu Batch (t): 0.5808 LR: 0.000100 Step: 25250 Total Loss: 3.8145 Recon Loss: 3.8004 
[01/05 23:39:10 TiTok]: Data (t): 0.0006, 54.52/s/gpu Batch (t): 0.5870 LR: 0.000100 Step: 25300 Total Loss: 3.8063 Recon Loss: 3.7921 
[01/05 23:39:39 TiTok]: Data (t): 0.0007, 54.53/s/gpu Batch (t): 0.5869 LR: 0.000100 Step: 25350 Total Loss: 3.8127 Recon Loss: 3.7986 
[01/05 23:40:09 TiTok]: Data (t): 0.0012, 54.65/s/gpu Batch (t): 0.5855 LR: 0.000100 Step: 25400 Total Loss: 3.8412 Recon Loss: 3.8271 
[01/05 23:40:38 TiTok]: Data (t): 0.0011, 54.64/s/gpu Batch (t): 0.5857 LR: 0.000100 Step: 25450 Total Loss: 3.8712 Recon Loss: 3.8571 
[01/05 23:41:07 TiTok]: Data (t): 0.0006, 54.49/s/gpu Batch (t): 0.5872 LR: 0.000100 Step: 25500 Total Loss: 3.7963 Recon Loss: 3.7822 
[01/05 23:41:37 TiTok]: Data (t): 0.0011, 54.67/s/gpu Batch (t): 0.5853 LR: 0.000100 Step: 25550 Total Loss: 3.8408 Recon Loss: 3.8267 
[01/05 23:42:06 TiTok]: Data (t): 0.0011, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000100 Step: 25600 Total Loss: 3.8528 Recon Loss: 3.8387 
[01/05 23:42:35 TiTok]: Data (t): 0.0007, 54.26/s/gpu Batch (t): 0.5897 LR: 0.000100 Step: 25650 Total Loss: 3.8473 Recon Loss: 3.8332 
[01/05 23:43:05 TiTok]: Data (t): 0.0011, 54.61/s/gpu Batch (t): 0.5859 LR: 0.000100 Step: 25700 Total Loss: 3.8751 Recon Loss: 3.8610 
[01/05 23:43:34 TiTok]: Data (t): 0.0011, 54.50/s/gpu Batch (t): 0.5872 LR: 0.000100 Step: 25750 Total Loss: 3.8145 Recon Loss: 3.8004 
[01/05 23:44:03 TiTok]: Data (t): 0.0011, 55.06/s/gpu Batch (t): 0.5812 LR: 0.000100 Step: 25800 Total Loss: 3.8434 Recon Loss: 3.8293 
[01/05 23:44:33 TiTok]: Data (t): 0.0011, 54.65/s/gpu Batch (t): 0.5856 LR: 0.000100 Step: 25850 Total Loss: 3.8385 Recon Loss: 3.8244 
[01/05 23:45:02 TiTok]: Data (t): 0.0011, 54.60/s/gpu Batch (t): 0.5861 LR: 0.000100 Step: 25900 Total Loss: 3.8372 Recon Loss: 3.8232 
[01/05 23:45:31 TiTok]: Data (t): 0.0009, 54.16/s/gpu Batch (t): 0.5908 LR: 0.000100 Step: 25950 Total Loss: 3.8290 Recon Loss: 3.8148 
[01/05 23:46:01 TiTok]: Data (t): 0.0007, 46.89/s/gpu Batch (t): 0.6825 LR: 0.000100 Step: 26000 Total Loss: 3.8301 Recon Loss: 3.8159 
[01/05 23:46:30 TiTok]: Data (t): 0.0011, 54.54/s/gpu Batch (t): 0.5867 LR: 0.000100 Step: 26050 Total Loss: 3.7965 Recon Loss: 3.7824 
[01/05 23:46:59 TiTok]: Data (t): 0.0011, 54.70/s/gpu Batch (t): 0.5850 LR: 0.000100 Step: 26100 Total Loss: 3.7930 Recon Loss: 3.7789 
[01/05 23:47:29 TiTok]: Data (t): 0.0012, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000100 Step: 26150 Total Loss: 3.8001 Recon Loss: 3.7859 
[01/05 23:47:58 TiTok]: Data (t): 0.0011, 54.57/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 26200 Total Loss: 3.7438 Recon Loss: 3.7297 
[01/05 23:48:27 TiTok]: Data (t): 0.0013, 54.59/s/gpu Batch (t): 0.5861 LR: 0.000100 Step: 26250 Total Loss: 3.7851 Recon Loss: 3.7710 
[01/05 23:48:57 TiTok]: Data (t): 0.0012, 54.60/s/gpu Batch (t): 0.5861 LR: 0.000100 Step: 26300 Total Loss: 3.8156 Recon Loss: 3.8015 
[01/05 23:49:26 TiTok]: Data (t): 0.0011, 54.63/s/gpu Batch (t): 0.5857 LR: 0.000100 Step: 26350 Total Loss: 3.8297 Recon Loss: 3.8155 
[01/05 23:49:55 TiTok]: Data (t): 0.0008, 54.16/s/gpu Batch (t): 0.5908 LR: 0.000100 Step: 26400 Total Loss: 3.8060 Recon Loss: 3.7919 
[01/05 23:50:25 TiTok]: Data (t): 0.0011, 54.48/s/gpu Batch (t): 0.5874 LR: 0.000100 Step: 26450 Total Loss: 3.7705 Recon Loss: 3.7564 
[01/05 23:50:54 TiTok]: Data (t): 0.0011, 54.50/s/gpu Batch (t): 0.5871 LR: 0.000100 Step: 26500 Total Loss: 3.8027 Recon Loss: 3.7886 
[01/05 23:51:23 TiTok]: Data (t): 0.0006, 54.20/s/gpu Batch (t): 0.5904 LR: 0.000100 Step: 26550 Total Loss: 3.7938 Recon Loss: 3.7797 
[01/05 23:51:53 TiTok]: Data (t): 0.0011, 54.85/s/gpu Batch (t): 0.5834 LR: 0.000100 Step: 26600 Total Loss: 3.7931 Recon Loss: 3.7789 
[01/05 23:52:22 TiTok]: Data (t): 0.0006, 54.17/s/gpu Batch (t): 0.5907 LR: 0.000100 Step: 26650 Total Loss: 3.7896 Recon Loss: 3.7755 
[01/05 23:52:51 TiTok]: Data (t): 0.0012, 54.66/s/gpu Batch (t): 0.5854 LR: 0.000100 Step: 26700 Total Loss: 3.7895 Recon Loss: 3.7754 
[01/05 23:53:21 TiTok]: Data (t): 0.0011, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000100 Step: 26750 Total Loss: 3.7519 Recon Loss: 3.7378 
[01/05 23:53:50 TiTok]: Data (t): 0.0006, 54.61/s/gpu Batch (t): 0.5860 LR: 0.000100 Step: 26800 Total Loss: 3.7543 Recon Loss: 3.7401 
[01/05 23:54:19 TiTok]: Data (t): 0.0007, 54.55/s/gpu Batch (t): 0.5866 LR: 0.000100 Step: 26850 Total Loss: 3.8018 Recon Loss: 3.7877 
[01/05 23:54:49 TiTok]: Data (t): 0.0008, 54.60/s/gpu Batch (t): 0.5861 LR: 0.000100 Step: 26900 Total Loss: 3.7787 Recon Loss: 3.7646 
[01/05 23:55:18 TiTok]: Data (t): 0.0011, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 26950 Total Loss: 3.7731 Recon Loss: 3.7589 
[01/05 23:55:48 TiTok]: Data (t): 0.0006, 47.15/s/gpu Batch (t): 0.6787 LR: 0.000100 Step: 27000 Total Loss: 3.7714 Recon Loss: 3.7573 
[01/05 23:56:17 TiTok]: Data (t): 0.0006, 54.07/s/gpu Batch (t): 0.5919 LR: 0.000100 Step: 27050 Total Loss: 3.7647 Recon Loss: 3.7505 
[01/05 23:56:46 TiTok]: Data (t): 0.0011, 54.64/s/gpu Batch (t): 0.5857 LR: 0.000100 Step: 27100 Total Loss: 3.7334 Recon Loss: 3.7193 
[01/05 23:57:16 TiTok]: Data (t): 0.0011, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000100 Step: 27150 Total Loss: 3.8392 Recon Loss: 3.8251 
[01/05 23:57:45 TiTok]: Data (t): 0.0007, 54.60/s/gpu Batch (t): 0.5861 LR: 0.000100 Step: 27200 Total Loss: 3.7868 Recon Loss: 3.7727 
[01/05 23:58:14 TiTok]: Data (t): 0.0013, 54.63/s/gpu Batch (t): 0.5857 LR: 0.000100 Step: 27250 Total Loss: 3.7808 Recon Loss: 3.7667 
[01/05 23:58:44 TiTok]: Data (t): 0.0006, 54.06/s/gpu Batch (t): 0.5919 LR: 0.000100 Step: 27300 Total Loss: 3.7358 Recon Loss: 3.7218 
[01/05 23:59:13 TiTok]: Data (t): 0.0011, 54.54/s/gpu Batch (t): 0.5867 LR: 0.000100 Step: 27350 Total Loss: 3.7945 Recon Loss: 3.7803 
[01/05 23:59:43 TiTok]: Data (t): 0.0006, 54.66/s/gpu Batch (t): 0.5854 LR: 0.000100 Step: 27400 Total Loss: 3.7661 Recon Loss: 3.7519 
[01/06 00:00:12 TiTok]: Data (t): 0.0012, 54.93/s/gpu Batch (t): 0.5825 LR: 0.000100 Step: 27450 Total Loss: 3.8015 Recon Loss: 3.7873 
[01/06 00:00:41 TiTok]: Data (t): 0.0007, 54.54/s/gpu Batch (t): 0.5867 LR: 0.000100 Step: 27500 Total Loss: 3.7482 Recon Loss: 3.7341 
[01/06 00:01:11 TiTok]: Data (t): 0.0011, 54.43/s/gpu Batch (t): 0.5879 LR: 0.000100 Step: 27550 Total Loss: 3.7778 Recon Loss: 3.7637 
[01/06 00:01:40 TiTok]: Data (t): 0.0011, 54.28/s/gpu Batch (t): 0.5895 LR: 0.000100 Step: 27600 Total Loss: 3.7633 Recon Loss: 3.7491 
[01/06 00:02:09 TiTok]: Data (t): 0.0008, 54.11/s/gpu Batch (t): 0.5914 LR: 0.000100 Step: 27650 Total Loss: 3.7872 Recon Loss: 3.7731 
[01/06 00:02:39 TiTok]: Data (t): 0.0011, 54.46/s/gpu Batch (t): 0.5876 LR: 0.000100 Step: 27700 Total Loss: 3.7110 Recon Loss: 3.6969 
[01/06 00:03:08 TiTok]: Data (t): 0.0006, 53.98/s/gpu Batch (t): 0.5928 LR: 0.000100 Step: 27750 Total Loss: 3.7283 Recon Loss: 3.7141 
[01/06 00:03:38 TiTok]: Data (t): 0.0011, 54.90/s/gpu Batch (t): 0.5828 LR: 0.000100 Step: 27800 Total Loss: 3.7291 Recon Loss: 3.7150 
[01/06 00:04:07 TiTok]: Data (t): 0.0011, 54.62/s/gpu Batch (t): 0.5859 LR: 0.000100 Step: 27850 Total Loss: 3.7501 Recon Loss: 3.7360 
[01/06 00:04:36 TiTok]: Data (t): 0.0011, 54.66/s/gpu Batch (t): 0.5855 LR: 0.000100 Step: 27900 Total Loss: 3.7179 Recon Loss: 3.7038 
[01/06 00:05:06 TiTok]: Data (t): 0.0006, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 27950 Total Loss: 3.6688 Recon Loss: 3.6547 
[01/06 00:05:35 TiTok]: Data (t): 0.0012, 43.06/s/gpu Batch (t): 0.7431 LR: 0.000100 Step: 28000 Total Loss: 3.7296 Recon Loss: 3.7155 
[01/06 00:06:05 TiTok]: Data (t): 0.0008, 54.04/s/gpu Batch (t): 0.5921 LR: 0.000100 Step: 28050 Total Loss: 3.7020 Recon Loss: 3.6879 
[01/06 00:06:34 TiTok]: Data (t): 0.0007, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000100 Step: 28100 Total Loss: 3.7026 Recon Loss: 3.6885 
[01/06 00:07:03 TiTok]: Data (t): 0.0007, 54.61/s/gpu Batch (t): 0.5860 LR: 0.000100 Step: 28150 Total Loss: 3.7254 Recon Loss: 3.7114 
[01/06 00:07:33 TiTok]: Data (t): 0.0011, 54.62/s/gpu Batch (t): 0.5859 LR: 0.000100 Step: 28200 Total Loss: 3.7360 Recon Loss: 3.7219 
[01/06 00:08:02 TiTok]: Data (t): 0.0011, 54.51/s/gpu Batch (t): 0.5871 LR: 0.000100 Step: 28250 Total Loss: 3.7308 Recon Loss: 3.7167 
[01/06 00:08:31 TiTok]: Data (t): 0.0011, 54.50/s/gpu Batch (t): 0.5871 LR: 0.000100 Step: 28300 Total Loss: 3.7410 Recon Loss: 3.7268 
[01/06 00:09:01 TiTok]: Data (t): 0.0006, 54.65/s/gpu Batch (t): 0.5855 LR: 0.000100 Step: 28350 Total Loss: 3.7295 Recon Loss: 3.7154 
[01/06 00:09:30 TiTok]: Data (t): 0.0011, 54.67/s/gpu Batch (t): 0.5853 LR: 0.000100 Step: 28400 Total Loss: 3.7104 Recon Loss: 3.6962 
[01/06 00:09:59 TiTok]: Data (t): 0.0014, 54.50/s/gpu Batch (t): 0.5872 LR: 0.000100 Step: 28450 Total Loss: 3.7396 Recon Loss: 3.7254 
[01/06 00:10:29 TiTok]: Data (t): 0.0011, 54.93/s/gpu Batch (t): 0.5826 LR: 0.000100 Step: 28500 Total Loss: 3.7284 Recon Loss: 3.7143 
[01/06 00:10:58 TiTok]: Data (t): 0.0011, 54.33/s/gpu Batch (t): 0.5890 LR: 0.000100 Step: 28550 Total Loss: 3.7539 Recon Loss: 3.7397 
[01/06 00:11:28 TiTok]: Data (t): 0.0007, 54.11/s/gpu Batch (t): 0.5914 LR: 0.000100 Step: 28600 Total Loss: 3.7289 Recon Loss: 3.7147 
[01/06 00:11:57 TiTok]: Data (t): 0.0007, 53.96/s/gpu Batch (t): 0.5930 LR: 0.000100 Step: 28650 Total Loss: 3.7291 Recon Loss: 3.7150 
[01/06 00:12:26 TiTok]: Data (t): 0.0011, 54.61/s/gpu Batch (t): 0.5860 LR: 0.000100 Step: 28700 Total Loss: 3.7138 Recon Loss: 3.6996 
[01/06 00:12:56 TiTok]: Data (t): 0.0007, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000100 Step: 28750 Total Loss: 3.7482 Recon Loss: 3.7340 
[01/06 00:13:25 TiTok]: Data (t): 0.0014, 54.50/s/gpu Batch (t): 0.5871 LR: 0.000100 Step: 28800 Total Loss: 3.7330 Recon Loss: 3.7188 
[01/06 00:13:54 TiTok]: Data (t): 0.0009, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000100 Step: 28850 Total Loss: 3.7320 Recon Loss: 3.7178 
[01/06 00:14:24 TiTok]: Data (t): 0.0007, 54.11/s/gpu Batch (t): 0.5914 LR: 0.000100 Step: 28900 Total Loss: 3.7078 Recon Loss: 3.6937 
[01/06 00:14:53 TiTok]: Data (t): 0.0011, 54.65/s/gpu Batch (t): 0.5856 LR: 0.000100 Step: 28950 Total Loss: 3.6881 Recon Loss: 3.6739 
[01/06 00:15:23 TiTok]: Data (t): 0.0011, 45.38/s/gpu Batch (t): 0.7052 LR: 0.000100 Step: 29000 Total Loss: 3.7537 Recon Loss: 3.7396 
[01/06 00:15:52 TiTok]: Data (t): 0.0011, 54.52/s/gpu Batch (t): 0.5869 LR: 0.000100 Step: 29050 Total Loss: 3.7145 Recon Loss: 3.7004 
[01/06 00:16:21 TiTok]: Data (t): 0.0011, 54.43/s/gpu Batch (t): 0.5879 LR: 0.000100 Step: 29100 Total Loss: 3.6574 Recon Loss: 3.6432 
[01/06 00:16:51 TiTok]: Data (t): 0.0007, 54.08/s/gpu Batch (t): 0.5917 LR: 0.000100 Step: 29150 Total Loss: 3.7409 Recon Loss: 3.7267 
[01/06 00:17:20 TiTok]: Data (t): 0.0007, 54.64/s/gpu Batch (t): 0.5857 LR: 0.000100 Step: 29200 Total Loss: 3.7145 Recon Loss: 3.7004 
[01/06 00:17:49 TiTok]: Data (t): 0.0006, 54.08/s/gpu Batch (t): 0.5918 LR: 0.000100 Step: 29250 Total Loss: 3.7196 Recon Loss: 3.7055 
[01/06 00:18:19 TiTok]: Data (t): 0.0014, 54.50/s/gpu Batch (t): 0.5871 LR: 0.000100 Step: 29300 Total Loss: 3.7347 Recon Loss: 3.7206 
[01/06 00:18:48 TiTok]: Data (t): 0.0006, 54.36/s/gpu Batch (t): 0.5887 LR: 0.000100 Step: 29350 Total Loss: 3.7070 Recon Loss: 3.6928 
[01/06 00:19:17 TiTok]: Data (t): 0.0006, 54.40/s/gpu Batch (t): 0.5883 LR: 0.000100 Step: 29400 Total Loss: 3.7086 Recon Loss: 3.6944 
[01/06 00:19:47 TiTok]: Data (t): 0.0006, 54.48/s/gpu Batch (t): 0.5873 LR: 0.000100 Step: 29450 Total Loss: 3.7416 Recon Loss: 3.7274 
[01/06 00:20:16 TiTok]: Data (t): 0.0011, 54.48/s/gpu Batch (t): 0.5873 LR: 0.000100 Step: 29500 Total Loss: 3.7263 Recon Loss: 3.7121 
[01/06 00:20:45 TiTok]: Data (t): 0.0006, 54.09/s/gpu Batch (t): 0.5917 LR: 0.000100 Step: 29550 Total Loss: 3.6886 Recon Loss: 3.6744 
[01/06 00:21:15 TiTok]: Data (t): 0.0012, 54.91/s/gpu Batch (t): 0.5828 LR: 0.000100 Step: 29600 Total Loss: 3.6868 Recon Loss: 3.6727 
[01/06 00:21:44 TiTok]: Data (t): 0.0011, 54.58/s/gpu Batch (t): 0.5863 LR: 0.000100 Step: 29650 Total Loss: 3.7174 Recon Loss: 3.7033 
[01/06 00:22:14 TiTok]: Data (t): 0.0012, 55.04/s/gpu Batch (t): 0.5814 LR: 0.000100 Step: 29700 Total Loss: 3.7063 Recon Loss: 3.6922 
[01/06 00:22:43 TiTok]: Data (t): 0.0011, 54.43/s/gpu Batch (t): 0.5879 LR: 0.000100 Step: 29750 Total Loss: 3.7302 Recon Loss: 3.7161 
[01/06 00:23:12 TiTok]: Data (t): 0.0006, 54.06/s/gpu Batch (t): 0.5919 LR: 0.000100 Step: 29800 Total Loss: 3.6532 Recon Loss: 3.6391 
[01/06 00:23:42 TiTok]: Data (t): 0.0010, 54.40/s/gpu Batch (t): 0.5882 LR: 0.000100 Step: 29850 Total Loss: 3.6553 Recon Loss: 3.6412 
[01/06 00:24:11 TiTok]: Data (t): 0.0006, 54.50/s/gpu Batch (t): 0.5872 LR: 0.000100 Step: 29900 Total Loss: 3.7087 Recon Loss: 3.6945 
[01/06 00:24:40 TiTok]: Data (t): 0.0009, 54.48/s/gpu Batch (t): 0.5873 LR: 0.000100 Step: 29950 Total Loss: 3.7089 Recon Loss: 3.6947 
[01/06 00:25:10 TiTok]: Data (t): 0.0011, 45.26/s/gpu Batch (t): 0.7070 LR: 0.000100 Step: 30000 Total Loss: 3.6727 Recon Loss: 3.6586 
[01/06 00:25:11 TiTok]: Reconstructing images...
[01/06 00:25:44 TiTok]: Data (t): 0.0011, 54.76/s/gpu Batch (t): 0.5844 LR: 0.000100 Step: 30050 Total Loss: 3.6956 Recon Loss: 3.6814 
Epoch 6/199 started.
[01/06 00:26:16 TiTok]: Data (t): 0.0006, 54.67/s/gpu Batch (t): 0.5853 LR: 0.000100 Step: 30100 Total Loss: 3.6616 Recon Loss: 3.6474 
[01/06 00:26:45 TiTok]: Data (t): 0.0011, 54.35/s/gpu Batch (t): 0.5888 LR: 0.000100 Step: 30150 Total Loss: 3.7314 Recon Loss: 3.7173 
[01/06 00:27:15 TiTok]: Data (t): 0.0014, 54.95/s/gpu Batch (t): 0.5824 LR: 0.000100 Step: 30200 Total Loss: 3.6961 Recon Loss: 3.6820 
[01/06 00:27:44 TiTok]: Data (t): 0.0007, 54.54/s/gpu Batch (t): 0.5868 LR: 0.000100 Step: 30250 Total Loss: 3.6377 Recon Loss: 3.6236 
[01/06 00:28:13 TiTok]: Data (t): 0.0006, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 30300 Total Loss: 3.6866 Recon Loss: 3.6724 
[01/06 00:28:43 TiTok]: Data (t): 0.0014, 54.56/s/gpu Batch (t): 0.5866 LR: 0.000100 Step: 30350 Total Loss: 3.6702 Recon Loss: 3.6561 
[01/06 00:29:12 TiTok]: Data (t): 0.0011, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000100 Step: 30400 Total Loss: 3.6976 Recon Loss: 3.6834 
[01/06 00:29:41 TiTok]: Data (t): 0.0011, 54.60/s/gpu Batch (t): 0.5861 LR: 0.000100 Step: 30450 Total Loss: 3.6854 Recon Loss: 3.6713 
[01/06 00:30:11 TiTok]: Data (t): 0.0006, 54.11/s/gpu Batch (t): 0.5914 LR: 0.000100 Step: 30500 Total Loss: 3.6828 Recon Loss: 3.6687 
[01/06 00:30:40 TiTok]: Data (t): 0.0011, 54.60/s/gpu Batch (t): 0.5860 LR: 0.000100 Step: 30550 Total Loss: 3.7095 Recon Loss: 3.6954 
[01/06 00:31:09 TiTok]: Data (t): 0.0012, 54.72/s/gpu Batch (t): 0.5847 LR: 0.000100 Step: 30600 Total Loss: 3.6804 Recon Loss: 3.6663 
[01/06 00:31:39 TiTok]: Data (t): 0.0011, 54.48/s/gpu Batch (t): 0.5874 LR: 0.000100 Step: 30650 Total Loss: 3.6529 Recon Loss: 3.6387 
[01/06 00:32:08 TiTok]: Data (t): 0.0006, 54.18/s/gpu Batch (t): 0.5906 LR: 0.000100 Step: 30700 Total Loss: 3.6952 Recon Loss: 3.6810 
[01/06 00:32:37 TiTok]: Data (t): 0.0011, 54.51/s/gpu Batch (t): 0.5870 LR: 0.000100 Step: 30750 Total Loss: 3.6740 Recon Loss: 3.6599 
[01/06 00:33:07 TiTok]: Data (t): 0.0006, 54.19/s/gpu Batch (t): 0.5905 LR: 0.000100 Step: 30800 Total Loss: 3.7008 Recon Loss: 3.6866 
[01/06 00:33:36 TiTok]: Data (t): 0.0012, 54.67/s/gpu Batch (t): 0.5854 LR: 0.000100 Step: 30850 Total Loss: 3.6459 Recon Loss: 3.6318 
[01/06 00:34:05 TiTok]: Data (t): 0.0008, 54.11/s/gpu Batch (t): 0.5913 LR: 0.000100 Step: 30900 Total Loss: 3.6413 Recon Loss: 3.6271 
[01/06 00:34:35 TiTok]: Data (t): 0.0006, 54.13/s/gpu Batch (t): 0.5911 LR: 0.000100 Step: 30950 Total Loss: 3.5946 Recon Loss: 3.5805 
[01/06 00:35:04 TiTok]: Data (t): 0.0011, 47.79/s/gpu Batch (t): 0.6695 LR: 0.000100 Step: 31000 Total Loss: 3.6251 Recon Loss: 3.6109 
[01/06 00:35:33 TiTok]: Data (t): 0.0011, 55.08/s/gpu Batch (t): 0.5810 LR: 0.000100 Step: 31050 Total Loss: 3.6684 Recon Loss: 3.6542 
[01/06 00:36:03 TiTok]: Data (t): 0.0006, 54.62/s/gpu Batch (t): 0.5859 LR: 0.000100 Step: 31100 Total Loss: 3.6512 Recon Loss: 3.6370 
[01/06 00:36:32 TiTok]: Data (t): 0.0007, 54.51/s/gpu Batch (t): 0.5871 LR: 0.000100 Step: 31150 Total Loss: 3.6096 Recon Loss: 3.5954 
[01/06 00:37:02 TiTok]: Data (t): 0.0007, 54.05/s/gpu Batch (t): 0.5920 LR: 0.000100 Step: 31200 Total Loss: 3.6867 Recon Loss: 3.6725 
[01/06 00:37:31 TiTok]: Data (t): 0.0006, 54.28/s/gpu Batch (t): 0.5896 LR: 0.000100 Step: 31250 Total Loss: 3.6369 Recon Loss: 3.6227 
[01/06 00:38:00 TiTok]: Data (t): 0.0006, 54.63/s/gpu Batch (t): 0.5858 LR: 0.000100 Step: 31300 Total Loss: 3.6645 Recon Loss: 3.6503 
[01/06 00:38:30 TiTok]: Data (t): 0.0011, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 31350 Total Loss: 3.6323 Recon Loss: 3.6181 
[01/06 00:38:59 TiTok]: Data (t): 0.0008, 54.18/s/gpu Batch (t): 0.5906 LR: 0.000100 Step: 31400 Total Loss: 3.6142 Recon Loss: 3.6000 
[01/06 00:39:28 TiTok]: Data (t): 0.0011, 54.93/s/gpu Batch (t): 0.5825 LR: 0.000100 Step: 31450 Total Loss: 3.6674 Recon Loss: 3.6532 
[01/06 00:39:58 TiTok]: Data (t): 0.0015, 54.53/s/gpu Batch (t): 0.5868 LR: 0.000100 Step: 31500 Total Loss: 3.6407 Recon Loss: 3.6266 
[01/06 00:40:27 TiTok]: Data (t): 0.0011, 54.47/s/gpu Batch (t): 0.5875 LR: 0.000100 Step: 31550 Total Loss: 3.6124 Recon Loss: 3.5982 
[01/06 00:40:56 TiTok]: Data (t): 0.0006, 54.15/s/gpu Batch (t): 0.5910 LR: 0.000100 Step: 31600 Total Loss: 3.6356 Recon Loss: 3.6215 
[01/06 00:41:26 TiTok]: Data (t): 0.0006, 54.12/s/gpu Batch (t): 0.5913 LR: 0.000100 Step: 31650 Total Loss: 3.6112 Recon Loss: 3.5970 
[01/06 00:41:55 TiTok]: Data (t): 0.0011, 54.62/s/gpu Batch (t): 0.5859 LR: 0.000100 Step: 31700 Total Loss: 3.6352 Recon Loss: 3.6210 
[01/06 00:42:24 TiTok]: Data (t): 0.0006, 54.62/s/gpu Batch (t): 0.5859 LR: 0.000100 Step: 31750 Total Loss: 3.6266 Recon Loss: 3.6125 
[01/06 00:42:54 TiTok]: Data (t): 0.0012, 54.51/s/gpu Batch (t): 0.5871 LR: 0.000100 Step: 31800 Total Loss: 3.6483 Recon Loss: 3.6342 
[01/06 00:43:23 TiTok]: Data (t): 0.0008, 54.14/s/gpu Batch (t): 0.5911 LR: 0.000100 Step: 31850 Total Loss: 3.6687 Recon Loss: 3.6546 
[01/06 00:43:52 TiTok]: Data (t): 0.0011, 54.64/s/gpu Batch (t): 0.5856 LR: 0.000100 Step: 31900 Total Loss: 3.6369 Recon Loss: 3.6227 
[01/06 00:44:22 TiTok]: Data (t): 0.0011, 54.63/s/gpu Batch (t): 0.5857 LR: 0.000100 Step: 31950 Total Loss: 3.6338 Recon Loss: 3.6197 
[01/06 00:44:51 TiTok]: Data (t): 0.0011, 45.16/s/gpu Batch (t): 0.7086 LR: 0.000100 Step: 32000 Total Loss: 3.6220 Recon Loss: 3.6078 
[01/06 00:45:20 TiTok]: Data (t): 0.0013, 54.48/s/gpu Batch (t): 0.5874 LR: 0.000100 Step: 32050 Total Loss: 3.6363 Recon Loss: 3.6221 
[01/06 00:45:50 TiTok]: Data (t): 0.0006, 53.56/s/gpu Batch (t): 0.5975 LR: 0.000100 Step: 32100 Total Loss: 3.6231 Recon Loss: 3.6090 
[01/06 00:46:19 TiTok]: Data (t): 0.0006, 54.26/s/gpu Batch (t): 0.5897 LR: 0.000100 Step: 32150 Total Loss: 3.6318 Recon Loss: 3.6176 
[01/06 00:46:49 TiTok]: Data (t): 0.0011, 54.54/s/gpu Batch (t): 0.5867 LR: 0.000100 Step: 32200 Total Loss: 3.6451 Recon Loss: 3.6309 
[01/06 00:47:18 TiTok]: Data (t): 0.0011, 54.97/s/gpu Batch (t): 0.5822 LR: 0.000100 Step: 32250 Total Loss: 3.6588 Recon Loss: 3.6446 
[01/06 00:47:47 TiTok]: Data (t): 0.0014, 54.51/s/gpu Batch (t): 0.5870 LR: 0.000100 Step: 32300 Total Loss: 3.5984 Recon Loss: 3.5843 
[01/06 00:48:17 TiTok]: Data (t): 0.0006, 54.59/s/gpu Batch (t): 0.5861 LR: 0.000100 Step: 32350 Total Loss: 3.6137 Recon Loss: 3.5995 
[01/06 00:48:46 TiTok]: Data (t): 0.0011, 54.51/s/gpu Batch (t): 0.5871 LR: 0.000100 Step: 32400 Total Loss: 3.5834 Recon Loss: 3.5692 
[01/06 00:49:15 TiTok]: Data (t): 0.0006, 54.00/s/gpu Batch (t): 0.5925 LR: 0.000100 Step: 32450 Total Loss: 3.6657 Recon Loss: 3.6516 
[01/06 00:49:45 TiTok]: Data (t): 0.0013, 54.11/s/gpu Batch (t): 0.5914 LR: 0.000100 Step: 32500 Total Loss: 3.6025 Recon Loss: 3.5884 
[01/06 00:50:14 TiTok]: Data (t): 0.0011, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 32550 Total Loss: 3.6479 Recon Loss: 3.6338 
[01/06 00:50:43 TiTok]: Data (t): 0.0011, 54.91/s/gpu Batch (t): 0.5828 LR: 0.000100 Step: 32600 Total Loss: 3.6291 Recon Loss: 3.6149 
[01/06 00:51:13 TiTok]: Data (t): 0.0011, 55.02/s/gpu Batch (t): 0.5816 LR: 0.000100 Step: 32650 Total Loss: 3.6282 Recon Loss: 3.6140 
[01/06 00:51:42 TiTok]: Data (t): 0.0007, 54.12/s/gpu Batch (t): 0.5912 LR: 0.000100 Step: 32700 Total Loss: 3.6619 Recon Loss: 3.6477 
[01/06 00:52:11 TiTok]: Data (t): 0.0008, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 32750 Total Loss: 3.6148 Recon Loss: 3.6006 
[01/06 00:52:41 TiTok]: Data (t): 0.0012, 55.03/s/gpu Batch (t): 0.5815 LR: 0.000100 Step: 32800 Total Loss: 3.5895 Recon Loss: 3.5753 
[01/06 00:53:10 TiTok]: Data (t): 0.0006, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000100 Step: 32850 Total Loss: 3.6022 Recon Loss: 3.5881 
[01/06 00:53:39 TiTok]: Data (t): 0.0007, 54.35/s/gpu Batch (t): 0.5887 LR: 0.000100 Step: 32900 Total Loss: 3.6390 Recon Loss: 3.6248 
[01/06 00:54:09 TiTok]: Data (t): 0.0011, 54.70/s/gpu Batch (t): 0.5850 LR: 0.000100 Step: 32950 Total Loss: 3.6131 Recon Loss: 3.5990 
[01/06 00:54:38 TiTok]: Data (t): 0.0011, 45.14/s/gpu Batch (t): 0.7088 LR: 0.000100 Step: 33000 Total Loss: 3.6234 Recon Loss: 3.6092 
[01/06 00:55:08 TiTok]: Data (t): 0.0012, 54.61/s/gpu Batch (t): 0.5859 LR: 0.000100 Step: 33050 Total Loss: 3.5962 Recon Loss: 3.5820 
[01/06 00:55:37 TiTok]: Data (t): 0.0011, 54.55/s/gpu Batch (t): 0.5866 LR: 0.000100 Step: 33100 Total Loss: 3.6399 Recon Loss: 3.6257 
[01/06 00:56:06 TiTok]: Data (t): 0.0011, 55.06/s/gpu Batch (t): 0.5812 LR: 0.000100 Step: 33150 Total Loss: 3.6299 Recon Loss: 3.6158 
[01/06 00:56:36 TiTok]: Data (t): 0.0011, 54.96/s/gpu Batch (t): 0.5823 LR: 0.000100 Step: 33200 Total Loss: 3.6335 Recon Loss: 3.6193 
[01/06 00:57:05 TiTok]: Data (t): 0.0006, 53.99/s/gpu Batch (t): 0.5927 LR: 0.000100 Step: 33250 Total Loss: 3.5596 Recon Loss: 3.5454 
[01/06 00:57:34 TiTok]: Data (t): 0.0013, 54.51/s/gpu Batch (t): 0.5871 LR: 0.000100 Step: 33300 Total Loss: 3.5848 Recon Loss: 3.5706 
[01/06 00:58:04 TiTok]: Data (t): 0.0013, 54.53/s/gpu Batch (t): 0.5868 LR: 0.000100 Step: 33350 Total Loss: 3.5862 Recon Loss: 3.5720 
[01/06 00:58:33 TiTok]: Data (t): 0.0011, 54.44/s/gpu Batch (t): 0.5878 LR: 0.000100 Step: 33400 Total Loss: 3.5945 Recon Loss: 3.5804 
[01/06 00:59:02 TiTok]: Data (t): 0.0008, 54.48/s/gpu Batch (t): 0.5873 LR: 0.000100 Step: 33450 Total Loss: 3.5883 Recon Loss: 3.5741 
[01/06 00:59:32 TiTok]: Data (t): 0.0006, 53.50/s/gpu Batch (t): 0.5982 LR: 0.000100 Step: 33500 Total Loss: 3.5957 Recon Loss: 3.5816 
[01/06 01:00:01 TiTok]: Data (t): 0.0007, 54.14/s/gpu Batch (t): 0.5910 LR: 0.000100 Step: 33550 Total Loss: 3.6173 Recon Loss: 3.6031 
[01/06 01:00:30 TiTok]: Data (t): 0.0012, 54.57/s/gpu Batch (t): 0.5863 LR: 0.000100 Step: 33600 Total Loss: 3.6075 Recon Loss: 3.5934 
[01/06 01:01:00 TiTok]: Data (t): 0.0011, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 33650 Total Loss: 3.6277 Recon Loss: 3.6136 
[01/06 01:01:29 TiTok]: Data (t): 0.0011, 54.51/s/gpu Batch (t): 0.5870 LR: 0.000100 Step: 33700 Total Loss: 3.5840 Recon Loss: 3.5699 
[01/06 01:01:58 TiTok]: Data (t): 0.0011, 54.55/s/gpu Batch (t): 0.5866 LR: 0.000100 Step: 33750 Total Loss: 3.6196 Recon Loss: 3.6054 
[01/06 01:02:28 TiTok]: Data (t): 0.0011, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 33800 Total Loss: 3.5819 Recon Loss: 3.5677 
[01/06 01:02:57 TiTok]: Data (t): 0.0009, 54.60/s/gpu Batch (t): 0.5861 LR: 0.000100 Step: 33850 Total Loss: 3.6275 Recon Loss: 3.6134 
[01/06 01:03:27 TiTok]: Data (t): 0.0011, 54.52/s/gpu Batch (t): 0.5869 LR: 0.000100 Step: 33900 Total Loss: 3.6228 Recon Loss: 3.6086 
[01/06 01:03:56 TiTok]: Data (t): 0.0011, 55.08/s/gpu Batch (t): 0.5810 LR: 0.000100 Step: 33950 Total Loss: 3.5942 Recon Loss: 3.5800 
[01/06 01:04:25 TiTok]: Data (t): 0.0011, 45.42/s/gpu Batch (t): 0.7045 LR: 0.000100 Step: 34000 Total Loss: 3.5716 Recon Loss: 3.5574 
[01/06 01:04:55 TiTok]: Data (t): 0.0006, 54.15/s/gpu Batch (t): 0.5910 LR: 0.000100 Step: 34050 Total Loss: 3.5861 Recon Loss: 3.5719 
[01/06 01:05:24 TiTok]: Data (t): 0.0011, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 34100 Total Loss: 3.5946 Recon Loss: 3.5804 
[01/06 01:05:53 TiTok]: Data (t): 0.0011, 54.93/s/gpu Batch (t): 0.5826 LR: 0.000100 Step: 34150 Total Loss: 3.6104 Recon Loss: 3.5962 
[01/06 01:06:23 TiTok]: Data (t): 0.0011, 55.09/s/gpu Batch (t): 0.5808 LR: 0.000100 Step: 34200 Total Loss: 3.6051 Recon Loss: 3.5909 
[01/06 01:06:52 TiTok]: Data (t): 0.0006, 54.69/s/gpu Batch (t): 0.5852 LR: 0.000100 Step: 34250 Total Loss: 3.5765 Recon Loss: 3.5623 
[01/06 01:07:21 TiTok]: Data (t): 0.0011, 54.30/s/gpu Batch (t): 0.5893 LR: 0.000100 Step: 34300 Total Loss: 3.6140 Recon Loss: 3.5998 
[01/06 01:07:51 TiTok]: Data (t): 0.0011, 55.05/s/gpu Batch (t): 0.5813 LR: 0.000100 Step: 34350 Total Loss: 3.5731 Recon Loss: 3.5589 
[01/06 01:08:20 TiTok]: Data (t): 0.0007, 54.14/s/gpu Batch (t): 0.5911 LR: 0.000100 Step: 34400 Total Loss: 3.6008 Recon Loss: 3.5866 
[01/06 01:08:49 TiTok]: Data (t): 0.0006, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 34450 Total Loss: 3.5987 Recon Loss: 3.5845 
[01/06 01:09:19 TiTok]: Data (t): 0.0006, 54.58/s/gpu Batch (t): 0.5863 LR: 0.000100 Step: 34500 Total Loss: 3.5382 Recon Loss: 3.5241 
[01/06 01:09:48 TiTok]: Data (t): 0.0011, 54.54/s/gpu Batch (t): 0.5867 LR: 0.000100 Step: 34550 Total Loss: 3.5757 Recon Loss: 3.5615 
[01/06 01:10:18 TiTok]: Data (t): 0.0011, 54.62/s/gpu Batch (t): 0.5858 LR: 0.000100 Step: 34600 Total Loss: 3.5771 Recon Loss: 3.5630 
[01/06 01:10:47 TiTok]: Data (t): 0.0011, 54.64/s/gpu Batch (t): 0.5857 LR: 0.000100 Step: 34650 Total Loss: 3.5802 Recon Loss: 3.5660 
[01/06 01:11:16 TiTok]: Data (t): 0.0014, 54.62/s/gpu Batch (t): 0.5859 LR: 0.000100 Step: 34700 Total Loss: 3.5743 Recon Loss: 3.5602 
[01/06 01:11:46 TiTok]: Data (t): 0.0006, 54.53/s/gpu Batch (t): 0.5868 LR: 0.000100 Step: 34750 Total Loss: 3.5714 Recon Loss: 3.5571 
[01/06 01:12:15 TiTok]: Data (t): 0.0011, 53.43/s/gpu Batch (t): 0.5989 LR: 0.000100 Step: 34800 Total Loss: 3.5737 Recon Loss: 3.5595 
[01/06 01:12:44 TiTok]: Data (t): 0.0011, 55.07/s/gpu Batch (t): 0.5811 LR: 0.000100 Step: 34850 Total Loss: 3.5718 Recon Loss: 3.5576 
[01/06 01:13:14 TiTok]: Data (t): 0.0011, 54.17/s/gpu Batch (t): 0.5908 LR: 0.000100 Step: 34900 Total Loss: 3.5582 Recon Loss: 3.5440 
[01/06 01:13:43 TiTok]: Data (t): 0.0007, 54.68/s/gpu Batch (t): 0.5853 LR: 0.000100 Step: 34950 Total Loss: 3.5821 Recon Loss: 3.5679 
[01/06 01:14:12 TiTok]: Data (t): 0.0011, 46.87/s/gpu Batch (t): 0.6828 LR: 0.000100 Step: 35000 Total Loss: 3.5748 Recon Loss: 3.5607 
[01/06 01:14:14 TiTok]: Reconstructing images...
[01/06 01:14:49 TiTok]: Data (t): 0.0011, 55.03/s/gpu Batch (t): 0.5815 LR: 0.000100 Step: 35050 Total Loss: 3.5863 Recon Loss: 3.5721 
[01/06 01:15:18 TiTok]: Data (t): 0.0006, 54.20/s/gpu Batch (t): 0.5905 LR: 0.000100 Step: 35100 Total Loss: 3.5699 Recon Loss: 3.5557 
Epoch 7/199 started.
[01/06 01:15:49 TiTok]: Data (t): 0.0011, 54.65/s/gpu Batch (t): 0.5856 LR: 0.000100 Step: 35150 Total Loss: 3.6067 Recon Loss: 3.5925 
[01/06 01:16:19 TiTok]: Data (t): 0.0007, 54.07/s/gpu Batch (t): 0.5918 LR: 0.000100 Step: 35200 Total Loss: 3.5567 Recon Loss: 3.5425 
[01/06 01:16:48 TiTok]: Data (t): 0.0012, 54.55/s/gpu Batch (t): 0.5866 LR: 0.000100 Step: 35250 Total Loss: 3.5690 Recon Loss: 3.5548 
[01/06 01:17:17 TiTok]: Data (t): 0.0009, 54.16/s/gpu Batch (t): 0.5909 LR: 0.000100 Step: 35300 Total Loss: 3.5924 Recon Loss: 3.5783 
[01/06 01:17:47 TiTok]: Data (t): 0.0011, 54.82/s/gpu Batch (t): 0.5837 LR: 0.000100 Step: 35350 Total Loss: 3.5579 Recon Loss: 3.5437 
[01/06 01:18:16 TiTok]: Data (t): 0.0006, 54.03/s/gpu Batch (t): 0.5923 LR: 0.000100 Step: 35400 Total Loss: 3.5425 Recon Loss: 3.5283 
[01/06 01:18:45 TiTok]: Data (t): 0.0011, 54.88/s/gpu Batch (t): 0.5831 LR: 0.000100 Step: 35450 Total Loss: 3.5950 Recon Loss: 3.5808 
[01/06 01:19:15 TiTok]: Data (t): 0.0012, 55.09/s/gpu Batch (t): 0.5809 LR: 0.000100 Step: 35500 Total Loss: 3.5625 Recon Loss: 3.5483 
[01/06 01:19:44 TiTok]: Data (t): 0.0011, 54.58/s/gpu Batch (t): 0.5863 LR: 0.000100 Step: 35550 Total Loss: 3.5837 Recon Loss: 3.5696 
[01/06 01:20:13 TiTok]: Data (t): 0.0011, 54.40/s/gpu Batch (t): 0.5882 LR: 0.000100 Step: 35600 Total Loss: 3.5631 Recon Loss: 3.5490 
[01/06 01:20:43 TiTok]: Data (t): 0.0011, 54.43/s/gpu Batch (t): 0.5879 LR: 0.000100 Step: 35650 Total Loss: 3.5353 Recon Loss: 3.5211 
[01/06 01:21:12 TiTok]: Data (t): 0.0009, 54.17/s/gpu Batch (t): 0.5908 LR: 0.000100 Step: 35700 Total Loss: 3.5185 Recon Loss: 3.5043 
[01/06 01:21:41 TiTok]: Data (t): 0.0011, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 35750 Total Loss: 3.6033 Recon Loss: 3.5891 
[01/06 01:22:11 TiTok]: Data (t): 0.0006, 54.60/s/gpu Batch (t): 0.5860 LR: 0.000100 Step: 35800 Total Loss: 3.5766 Recon Loss: 3.5624 
[01/06 01:22:40 TiTok]: Data (t): 0.0011, 54.54/s/gpu Batch (t): 0.5867 LR: 0.000100 Step: 35850 Total Loss: 3.5627 Recon Loss: 3.5485 
[01/06 01:23:09 TiTok]: Data (t): 0.0011, 54.61/s/gpu Batch (t): 0.5860 LR: 0.000100 Step: 35900 Total Loss: 3.5515 Recon Loss: 3.5373 
[01/06 01:23:39 TiTok]: Data (t): 0.0007, 54.22/s/gpu Batch (t): 0.5902 LR: 0.000100 Step: 35950 Total Loss: 3.5823 Recon Loss: 3.5681 
[01/06 01:24:08 TiTok]: Data (t): 0.0011, 43.94/s/gpu Batch (t): 0.7282 LR: 0.000100 Step: 36000 Total Loss: 3.5460 Recon Loss: 3.5318 
[01/06 01:24:38 TiTok]: Data (t): 0.0006, 54.53/s/gpu Batch (t): 0.5869 LR: 0.000100 Step: 36050 Total Loss: 3.5922 Recon Loss: 3.5779 
[01/06 01:25:07 TiTok]: Data (t): 0.0011, 54.61/s/gpu Batch (t): 0.5859 LR: 0.000100 Step: 36100 Total Loss: 3.5933 Recon Loss: 3.5791 
[01/06 01:25:36 TiTok]: Data (t): 0.0013, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 36150 Total Loss: 3.5527 Recon Loss: 3.5386 
[01/06 01:26:06 TiTok]: Data (t): 0.0012, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 36200 Total Loss: 3.5404 Recon Loss: 3.5262 
[01/06 01:26:35 TiTok]: Data (t): 0.0013, 54.60/s/gpu Batch (t): 0.5860 LR: 0.000100 Step: 36250 Total Loss: 3.5847 Recon Loss: 3.5705 
[01/06 01:27:04 TiTok]: Data (t): 0.0011, 54.49/s/gpu Batch (t): 0.5872 LR: 0.000100 Step: 36300 Total Loss: 3.5450 Recon Loss: 3.5308 
[01/06 01:27:34 TiTok]: Data (t): 0.0006, 54.68/s/gpu Batch (t): 0.5852 LR: 0.000100 Step: 36350 Total Loss: 3.5194 Recon Loss: 3.5052 
[01/06 01:28:03 TiTok]: Data (t): 0.0007, 54.09/s/gpu Batch (t): 0.5916 LR: 0.000100 Step: 36400 Total Loss: 3.5505 Recon Loss: 3.5363 
[01/06 01:28:32 TiTok]: Data (t): 0.0011, 54.56/s/gpu Batch (t): 0.5866 LR: 0.000100 Step: 36450 Total Loss: 3.5524 Recon Loss: 3.5382 
[01/06 01:29:02 TiTok]: Data (t): 0.0011, 54.54/s/gpu Batch (t): 0.5868 LR: 0.000100 Step: 36500 Total Loss: 3.5669 Recon Loss: 3.5527 
[01/06 01:29:31 TiTok]: Data (t): 0.0011, 54.60/s/gpu Batch (t): 0.5861 LR: 0.000100 Step: 36550 Total Loss: 3.5312 Recon Loss: 3.5170 
[01/06 01:30:01 TiTok]: Data (t): 0.0011, 54.58/s/gpu Batch (t): 0.5863 LR: 0.000100 Step: 36600 Total Loss: 3.5309 Recon Loss: 3.5167 
[01/06 01:30:30 TiTok]: Data (t): 0.0011, 54.63/s/gpu Batch (t): 0.5857 LR: 0.000100 Step: 36650 Total Loss: 3.5443 Recon Loss: 3.5302 
[01/06 01:30:59 TiTok]: Data (t): 0.0011, 55.05/s/gpu Batch (t): 0.5813 LR: 0.000100 Step: 36700 Total Loss: 3.6065 Recon Loss: 3.5923 
[01/06 01:31:29 TiTok]: Data (t): 0.0011, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 36750 Total Loss: 3.5769 Recon Loss: 3.5627 
[01/06 01:31:58 TiTok]: Data (t): 0.0011, 54.39/s/gpu Batch (t): 0.5884 LR: 0.000100 Step: 36800 Total Loss: 3.5389 Recon Loss: 3.5247 
[01/06 01:32:28 TiTok]: Data (t): 0.0007, 54.65/s/gpu Batch (t): 0.5855 LR: 0.000100 Step: 36850 Total Loss: 3.5202 Recon Loss: 3.5061 
[01/06 01:32:57 TiTok]: Data (t): 0.0011, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 36900 Total Loss: 3.5168 Recon Loss: 3.5026 
[01/06 01:33:26 TiTok]: Data (t): 0.0011, 54.47/s/gpu Batch (t): 0.5875 LR: 0.000100 Step: 36950 Total Loss: 3.5300 Recon Loss: 3.5159 
[01/06 01:33:56 TiTok]: Data (t): 0.0011, 44.81/s/gpu Batch (t): 0.7141 LR: 0.000100 Step: 37000 Total Loss: 3.5633 Recon Loss: 3.5492 
[01/06 01:34:25 TiTok]: Data (t): 0.0011, 54.65/s/gpu Batch (t): 0.5856 LR: 0.000100 Step: 37050 Total Loss: 3.5531 Recon Loss: 3.5389 
[01/06 01:34:54 TiTok]: Data (t): 0.0006, 54.19/s/gpu Batch (t): 0.5905 LR: 0.000100 Step: 37100 Total Loss: 3.5013 Recon Loss: 3.4871 
[01/06 01:35:24 TiTok]: Data (t): 0.0011, 54.53/s/gpu Batch (t): 0.5869 LR: 0.000100 Step: 37150 Total Loss: 3.5696 Recon Loss: 3.5554 
[01/06 01:35:53 TiTok]: Data (t): 0.0008, 53.81/s/gpu Batch (t): 0.5946 LR: 0.000100 Step: 37200 Total Loss: 3.5439 Recon Loss: 3.5297 
[01/06 01:36:22 TiTok]: Data (t): 0.0014, 55.01/s/gpu Batch (t): 0.5817 LR: 0.000100 Step: 37250 Total Loss: 3.5490 Recon Loss: 3.5348 
[01/06 01:36:52 TiTok]: Data (t): 0.0011, 54.43/s/gpu Batch (t): 0.5879 LR: 0.000100 Step: 37300 Total Loss: 3.5249 Recon Loss: 3.5107 
[01/06 01:37:21 TiTok]: Data (t): 0.0013, 55.09/s/gpu Batch (t): 0.5809 LR: 0.000100 Step: 37350 Total Loss: 3.5325 Recon Loss: 3.5183 
[01/06 01:37:51 TiTok]: Data (t): 0.0006, 54.03/s/gpu Batch (t): 0.5922 LR: 0.000100 Step: 37400 Total Loss: 3.5399 Recon Loss: 3.5258 
[01/06 01:38:20 TiTok]: Data (t): 0.0011, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 37450 Total Loss: 3.5568 Recon Loss: 3.5426 
[01/06 01:38:49 TiTok]: Data (t): 0.0011, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 37500 Total Loss: 3.5239 Recon Loss: 3.5097 
[01/06 01:39:19 TiTok]: Data (t): 0.0011, 55.00/s/gpu Batch (t): 0.5819 LR: 0.000100 Step: 37550 Total Loss: 3.5521 Recon Loss: 3.5378 
[01/06 01:39:48 TiTok]: Data (t): 0.0011, 54.56/s/gpu Batch (t): 0.5866 LR: 0.000100 Step: 37600 Total Loss: 3.5580 Recon Loss: 3.5438 
[01/06 01:40:17 TiTok]: Data (t): 0.0011, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 37650 Total Loss: 3.5188 Recon Loss: 3.5046 
[01/06 01:40:47 TiTok]: Data (t): 0.0011, 54.34/s/gpu Batch (t): 0.5889 LR: 0.000100 Step: 37700 Total Loss: 3.5024 Recon Loss: 3.4883 
[01/06 01:41:16 TiTok]: Data (t): 0.0011, 55.01/s/gpu Batch (t): 0.5817 LR: 0.000100 Step: 37750 Total Loss: 3.5215 Recon Loss: 3.5073 
[01/06 01:41:45 TiTok]: Data (t): 0.0007, 54.45/s/gpu Batch (t): 0.5877 LR: 0.000100 Step: 37800 Total Loss: 3.5427 Recon Loss: 3.5285 
[01/06 01:42:15 TiTok]: Data (t): 0.0006, 53.63/s/gpu Batch (t): 0.5967 LR: 0.000100 Step: 37850 Total Loss: 3.5050 Recon Loss: 3.4908 
[01/06 01:42:44 TiTok]: Data (t): 0.0011, 55.04/s/gpu Batch (t): 0.5814 LR: 0.000100 Step: 37900 Total Loss: 3.5068 Recon Loss: 3.4926 
[01/06 01:43:13 TiTok]: Data (t): 0.0009, 54.62/s/gpu Batch (t): 0.5858 LR: 0.000100 Step: 37950 Total Loss: 3.5016 Recon Loss: 3.4874 
[01/06 01:43:43 TiTok]: Data (t): 0.0011, 48.26/s/gpu Batch (t): 0.6631 LR: 0.000100 Step: 38000 Total Loss: 3.4946 Recon Loss: 3.4805 
[01/06 01:44:12 TiTok]: Data (t): 0.0011, 54.63/s/gpu Batch (t): 0.5857 LR: 0.000100 Step: 38050 Total Loss: 3.5836 Recon Loss: 3.5694 
[01/06 01:44:42 TiTok]: Data (t): 0.0011, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000100 Step: 38100 Total Loss: 3.4947 Recon Loss: 3.4805 
[01/06 01:45:11 TiTok]: Data (t): 0.0012, 54.74/s/gpu Batch (t): 0.5846 LR: 0.000100 Step: 38150 Total Loss: 3.4875 Recon Loss: 3.4733 
[01/06 01:45:40 TiTok]: Data (t): 0.0011, 55.02/s/gpu Batch (t): 0.5816 LR: 0.000100 Step: 38200 Total Loss: 3.5214 Recon Loss: 3.5072 
[01/06 01:46:10 TiTok]: Data (t): 0.0007, 54.40/s/gpu Batch (t): 0.5883 LR: 0.000100 Step: 38250 Total Loss: 3.5602 Recon Loss: 3.5461 
[01/06 01:46:39 TiTok]: Data (t): 0.0008, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 38300 Total Loss: 3.4893 Recon Loss: 3.4751 
[01/06 01:47:08 TiTok]: Data (t): 0.0011, 54.37/s/gpu Batch (t): 0.5886 LR: 0.000100 Step: 38350 Total Loss: 3.4813 Recon Loss: 3.4671 
[01/06 01:47:38 TiTok]: Data (t): 0.0011, 54.71/s/gpu Batch (t): 0.5849 LR: 0.000100 Step: 38400 Total Loss: 3.5005 Recon Loss: 3.4864 
[01/06 01:48:07 TiTok]: Data (t): 0.0011, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000100 Step: 38450 Total Loss: 3.5021 Recon Loss: 3.4880 
[01/06 01:48:36 TiTok]: Data (t): 0.0012, 54.47/s/gpu Batch (t): 0.5874 LR: 0.000100 Step: 38500 Total Loss: 3.4545 Recon Loss: 3.4403 
[01/06 01:49:06 TiTok]: Data (t): 0.0011, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000100 Step: 38550 Total Loss: 3.5395 Recon Loss: 3.5253 
[01/06 01:49:35 TiTok]: Data (t): 0.0011, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 38600 Total Loss: 3.4637 Recon Loss: 3.4496 
[01/06 01:50:04 TiTok]: Data (t): 0.0007, 53.55/s/gpu Batch (t): 0.5976 LR: 0.000100 Step: 38650 Total Loss: 3.5275 Recon Loss: 3.5133 
[01/06 01:50:34 TiTok]: Data (t): 0.0012, 54.88/s/gpu Batch (t): 0.5830 LR: 0.000100 Step: 38700 Total Loss: 3.5235 Recon Loss: 3.5093 
[01/06 01:51:03 TiTok]: Data (t): 0.0012, 54.37/s/gpu Batch (t): 0.5886 LR: 0.000100 Step: 38750 Total Loss: 3.5169 Recon Loss: 3.5027 
[01/06 01:51:32 TiTok]: Data (t): 0.0011, 54.55/s/gpu Batch (t): 0.5866 LR: 0.000100 Step: 38800 Total Loss: 3.5566 Recon Loss: 3.5424 
[01/06 01:52:02 TiTok]: Data (t): 0.0007, 54.12/s/gpu Batch (t): 0.5913 LR: 0.000100 Step: 38850 Total Loss: 3.4946 Recon Loss: 3.4804 
[01/06 01:52:31 TiTok]: Data (t): 0.0012, 54.55/s/gpu Batch (t): 0.5866 LR: 0.000100 Step: 38900 Total Loss: 3.5089 Recon Loss: 3.4948 
[01/06 01:53:00 TiTok]: Data (t): 0.0012, 54.52/s/gpu Batch (t): 0.5870 LR: 0.000100 Step: 38950 Total Loss: 3.4764 Recon Loss: 3.4622 
[01/06 01:53:30 TiTok]: Data (t): 0.0011, 44.55/s/gpu Batch (t): 0.7183 LR: 0.000100 Step: 39000 Total Loss: 3.4997 Recon Loss: 3.4855 
[01/06 01:53:59 TiTok]: Data (t): 0.0006, 54.05/s/gpu Batch (t): 0.5920 LR: 0.000100 Step: 39050 Total Loss: 3.4894 Recon Loss: 3.4752 
[01/06 01:54:29 TiTok]: Data (t): 0.0011, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 39100 Total Loss: 3.4754 Recon Loss: 3.4612 
[01/06 01:54:58 TiTok]: Data (t): 0.0011, 54.60/s/gpu Batch (t): 0.5861 LR: 0.000100 Step: 39150 Total Loss: 3.4922 Recon Loss: 3.4780 
[01/06 01:55:27 TiTok]: Data (t): 0.0011, 54.89/s/gpu Batch (t): 0.5830 LR: 0.000100 Step: 39200 Total Loss: 3.5179 Recon Loss: 3.5037 
[01/06 01:55:57 TiTok]: Data (t): 0.0011, 54.64/s/gpu Batch (t): 0.5856 LR: 0.000100 Step: 39250 Total Loss: 3.5221 Recon Loss: 3.5079 
[01/06 01:56:26 TiTok]: Data (t): 0.0012, 54.52/s/gpu Batch (t): 0.5870 LR: 0.000100 Step: 39300 Total Loss: 3.4973 Recon Loss: 3.4831 
[01/06 01:56:55 TiTok]: Data (t): 0.0011, 54.96/s/gpu Batch (t): 0.5822 LR: 0.000100 Step: 39350 Total Loss: 3.5320 Recon Loss: 3.5177 
[01/06 01:57:25 TiTok]: Data (t): 0.0007, 54.50/s/gpu Batch (t): 0.5871 LR: 0.000100 Step: 39400 Total Loss: 3.5092 Recon Loss: 3.4950 
[01/06 01:57:54 TiTok]: Data (t): 0.0012, 54.59/s/gpu Batch (t): 0.5861 LR: 0.000100 Step: 39450 Total Loss: 3.5025 Recon Loss: 3.4883 
[01/06 01:58:23 TiTok]: Data (t): 0.0012, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000100 Step: 39500 Total Loss: 3.5037 Recon Loss: 3.4896 
[01/06 01:58:53 TiTok]: Data (t): 0.0011, 54.44/s/gpu Batch (t): 0.5878 LR: 0.000100 Step: 39550 Total Loss: 3.5062 Recon Loss: 3.4920 
[01/06 01:59:22 TiTok]: Data (t): 0.0013, 54.53/s/gpu Batch (t): 0.5868 LR: 0.000100 Step: 39600 Total Loss: 3.5061 Recon Loss: 3.4919 
[01/06 01:59:51 TiTok]: Data (t): 0.0011, 54.66/s/gpu Batch (t): 0.5855 LR: 0.000100 Step: 39650 Total Loss: 3.5120 Recon Loss: 3.4979 
[01/06 02:00:21 TiTok]: Data (t): 0.0011, 54.67/s/gpu Batch (t): 0.5854 LR: 0.000100 Step: 39700 Total Loss: 3.4553 Recon Loss: 3.4412 
[01/06 02:00:50 TiTok]: Data (t): 0.0011, 55.01/s/gpu Batch (t): 0.5817 LR: 0.000100 Step: 39750 Total Loss: 3.4749 Recon Loss: 3.4608 
[01/06 02:01:19 TiTok]: Data (t): 0.0007, 54.52/s/gpu Batch (t): 0.5870 LR: 0.000100 Step: 39800 Total Loss: 3.5067 Recon Loss: 3.4925 
[01/06 02:01:49 TiTok]: Data (t): 0.0011, 55.04/s/gpu Batch (t): 0.5814 LR: 0.000100 Step: 39850 Total Loss: 3.5182 Recon Loss: 3.5040 
[01/06 02:02:18 TiTok]: Data (t): 0.0011, 54.47/s/gpu Batch (t): 0.5875 LR: 0.000100 Step: 39900 Total Loss: 3.4738 Recon Loss: 3.4596 
[01/06 02:02:47 TiTok]: Data (t): 0.0011, 55.08/s/gpu Batch (t): 0.5810 LR: 0.000100 Step: 39950 Total Loss: 3.4870 Recon Loss: 3.4728 
[01/06 02:03:17 TiTok]: Data (t): 0.0011, 44.92/s/gpu Batch (t): 0.7123 LR: 0.000100 Step: 40000 Total Loss: 3.4702 Recon Loss: 3.4561 
[01/06 02:03:18 TiTok]: Reconstructing images...
[01/06 02:03:52 TiTok]: Data (t): 0.0011, 54.86/s/gpu Batch (t): 0.5833 LR: 0.000100 Step: 40050 Total Loss: 3.4986 Recon Loss: 3.4844 
[01/06 02:04:22 TiTok]: Data (t): 0.0007, 54.55/s/gpu Batch (t): 0.5866 LR: 0.000100 Step: 40100 Total Loss: 3.4905 Recon Loss: 3.4764 
Epoch 8/199 started.
[01/06 02:04:53 TiTok]: Data (t): 0.0011, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 40150 Total Loss: 3.4920 Recon Loss: 3.4778 
[01/06 02:05:22 TiTok]: Data (t): 0.0011, 54.48/s/gpu Batch (t): 0.5873 LR: 0.000100 Step: 40200 Total Loss: 3.4739 Recon Loss: 3.4597 
[01/06 02:05:51 TiTok]: Data (t): 0.0012, 54.43/s/gpu Batch (t): 0.5880 LR: 0.000100 Step: 40250 Total Loss: 3.4794 Recon Loss: 3.4652 
[01/06 02:06:21 TiTok]: Data (t): 0.0011, 54.58/s/gpu Batch (t): 0.5863 LR: 0.000100 Step: 40300 Total Loss: 3.4975 Recon Loss: 3.4833 
[01/06 02:06:50 TiTok]: Data (t): 0.0011, 54.64/s/gpu Batch (t): 0.5856 LR: 0.000100 Step: 40350 Total Loss: 3.4490 Recon Loss: 3.4348 
[01/06 02:07:19 TiTok]: Data (t): 0.0016, 54.65/s/gpu Batch (t): 0.5856 LR: 0.000100 Step: 40400 Total Loss: 3.4597 Recon Loss: 3.4455 
[01/06 02:07:49 TiTok]: Data (t): 0.0011, 54.15/s/gpu Batch (t): 0.5910 LR: 0.000100 Step: 40450 Total Loss: 3.4908 Recon Loss: 3.4766 
[01/06 02:08:18 TiTok]: Data (t): 0.0011, 54.64/s/gpu Batch (t): 0.5857 LR: 0.000100 Step: 40500 Total Loss: 3.5123 Recon Loss: 3.4981 
[01/06 02:08:47 TiTok]: Data (t): 0.0012, 54.47/s/gpu Batch (t): 0.5875 LR: 0.000100 Step: 40550 Total Loss: 3.4830 Recon Loss: 3.4688 
[01/06 02:09:17 TiTok]: Data (t): 0.0011, 54.67/s/gpu Batch (t): 0.5853 LR: 0.000100 Step: 40600 Total Loss: 3.4584 Recon Loss: 3.4443 
[01/06 02:09:46 TiTok]: Data (t): 0.0006, 54.68/s/gpu Batch (t): 0.5852 LR: 0.000100 Step: 40650 Total Loss: 3.4617 Recon Loss: 3.4476 
[01/06 02:10:16 TiTok]: Data (t): 0.0011, 54.61/s/gpu Batch (t): 0.5860 LR: 0.000100 Step: 40700 Total Loss: 3.4408 Recon Loss: 3.4266 
[01/06 02:10:45 TiTok]: Data (t): 0.0014, 54.46/s/gpu Batch (t): 0.5876 LR: 0.000100 Step: 40750 Total Loss: 3.4451 Recon Loss: 3.4309 
[01/06 02:11:14 TiTok]: Data (t): 0.0011, 54.58/s/gpu Batch (t): 0.5863 LR: 0.000100 Step: 40800 Total Loss: 3.4871 Recon Loss: 3.4729 
[01/06 02:11:44 TiTok]: Data (t): 0.0007, 54.18/s/gpu Batch (t): 0.5906 LR: 0.000100 Step: 40850 Total Loss: 3.5222 Recon Loss: 3.5079 
[01/06 02:12:13 TiTok]: Data (t): 0.0006, 54.63/s/gpu Batch (t): 0.5858 LR: 0.000100 Step: 40900 Total Loss: 3.4888 Recon Loss: 3.4746 
[01/06 02:12:42 TiTok]: Data (t): 0.0014, 54.48/s/gpu Batch (t): 0.5874 LR: 0.000100 Step: 40950 Total Loss: 3.4784 Recon Loss: 3.4642 
[01/06 02:13:12 TiTok]: Data (t): 0.0011, 45.19/s/gpu Batch (t): 0.7081 LR: 0.000100 Step: 41000 Total Loss: 3.4527 Recon Loss: 3.4385 
[01/06 02:13:41 TiTok]: Data (t): 0.0011, 54.67/s/gpu Batch (t): 0.5853 LR: 0.000100 Step: 41050 Total Loss: 3.4597 Recon Loss: 3.4455 
[01/06 02:14:10 TiTok]: Data (t): 0.0011, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 41100 Total Loss: 3.4537 Recon Loss: 3.4395 
[01/06 02:14:40 TiTok]: Data (t): 0.0013, 54.48/s/gpu Batch (t): 0.5874 LR: 0.000100 Step: 41150 Total Loss: 3.4519 Recon Loss: 3.4378 
[01/06 02:15:09 TiTok]: Data (t): 0.0006, 54.22/s/gpu Batch (t): 0.5902 LR: 0.000100 Step: 41200 Total Loss: 3.4869 Recon Loss: 3.4727 
[01/06 02:15:38 TiTok]: Data (t): 0.0007, 54.10/s/gpu Batch (t): 0.5915 LR: 0.000100 Step: 41250 Total Loss: 3.4384 Recon Loss: 3.4242 
[01/06 02:16:08 TiTok]: Data (t): 0.0006, 54.00/s/gpu Batch (t): 0.5926 LR: 0.000100 Step: 41300 Total Loss: 3.4705 Recon Loss: 3.4563 
[01/06 02:16:37 TiTok]: Data (t): 0.0011, 54.54/s/gpu Batch (t): 0.5867 LR: 0.000100 Step: 41350 Total Loss: 3.4850 Recon Loss: 3.4708 
[01/06 02:17:06 TiTok]: Data (t): 0.0012, 55.09/s/gpu Batch (t): 0.5809 LR: 0.000100 Step: 41400 Total Loss: 3.4510 Recon Loss: 3.4368 
[01/06 02:17:36 TiTok]: Data (t): 0.0011, 54.53/s/gpu Batch (t): 0.5869 LR: 0.000100 Step: 41450 Total Loss: 3.4493 Recon Loss: 3.4352 
[01/06 02:18:05 TiTok]: Data (t): 0.0011, 55.27/s/gpu Batch (t): 0.5790 LR: 0.000100 Step: 41500 Total Loss: 3.4702 Recon Loss: 3.4561 
[01/06 02:18:34 TiTok]: Data (t): 0.0006, 54.15/s/gpu Batch (t): 0.5909 LR: 0.000100 Step: 41550 Total Loss: 3.5068 Recon Loss: 3.4927 
[01/06 02:19:04 TiTok]: Data (t): 0.0006, 54.14/s/gpu Batch (t): 0.5911 LR: 0.000100 Step: 41600 Total Loss: 3.4649 Recon Loss: 3.4507 
[01/06 02:19:33 TiTok]: Data (t): 0.0006, 54.50/s/gpu Batch (t): 0.5871 LR: 0.000100 Step: 41650 Total Loss: 3.4592 Recon Loss: 3.4450 
[01/06 02:20:02 TiTok]: Data (t): 0.0011, 54.50/s/gpu Batch (t): 0.5871 LR: 0.000100 Step: 41700 Total Loss: 3.4927 Recon Loss: 3.4786 
[01/06 02:20:32 TiTok]: Data (t): 0.0011, 54.08/s/gpu Batch (t): 0.5917 LR: 0.000100 Step: 41750 Total Loss: 3.4665 Recon Loss: 3.4524 
[01/06 02:21:01 TiTok]: Data (t): 0.0011, 54.63/s/gpu Batch (t): 0.5858 LR: 0.000100 Step: 41800 Total Loss: 3.4707 Recon Loss: 3.4566 
[01/06 02:21:30 TiTok]: Data (t): 0.0008, 54.53/s/gpu Batch (t): 0.5868 LR: 0.000100 Step: 41850 Total Loss: 3.4626 Recon Loss: 3.4484 
[01/06 02:22:00 TiTok]: Data (t): 0.0011, 54.64/s/gpu Batch (t): 0.5856 LR: 0.000100 Step: 41900 Total Loss: 3.4594 Recon Loss: 3.4452 
[01/06 02:22:29 TiTok]: Data (t): 0.0011, 54.54/s/gpu Batch (t): 0.5867 LR: 0.000100 Step: 41950 Total Loss: 3.4212 Recon Loss: 3.4071 
[01/06 02:22:58 TiTok]: Data (t): 0.0011, 47.96/s/gpu Batch (t): 0.6673 LR: 0.000100 Step: 42000 Total Loss: 3.4786 Recon Loss: 3.4645 
[01/06 02:23:28 TiTok]: Data (t): 0.0011, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 42050 Total Loss: 3.4880 Recon Loss: 3.4739 
[01/06 02:23:57 TiTok]: Data (t): 0.0011, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 42100 Total Loss: 3.4924 Recon Loss: 3.4783 
[01/06 02:24:26 TiTok]: Data (t): 0.0012, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 42150 Total Loss: 3.4440 Recon Loss: 3.4298 
[01/06 02:24:56 TiTok]: Data (t): 0.0006, 54.13/s/gpu Batch (t): 0.5911 LR: 0.000100 Step: 42200 Total Loss: 3.5025 Recon Loss: 3.4883 
[01/06 02:25:25 TiTok]: Data (t): 0.0011, 54.49/s/gpu Batch (t): 0.5872 LR: 0.000100 Step: 42250 Total Loss: 3.4995 Recon Loss: 3.4853 
[01/06 02:25:54 TiTok]: Data (t): 0.0011, 55.05/s/gpu Batch (t): 0.5813 LR: 0.000100 Step: 42300 Total Loss: 3.5096 Recon Loss: 3.4954 
[01/06 02:26:24 TiTok]: Data (t): 0.0011, 54.99/s/gpu Batch (t): 0.5819 LR: 0.000100 Step: 42350 Total Loss: 3.4813 Recon Loss: 3.4671 
[01/06 02:26:53 TiTok]: Data (t): 0.0006, 54.72/s/gpu Batch (t): 0.5847 LR: 0.000100 Step: 42400 Total Loss: 3.4770 Recon Loss: 3.4629 
[01/06 02:27:22 TiTok]: Data (t): 0.0011, 54.62/s/gpu Batch (t): 0.5858 LR: 0.000100 Step: 42450 Total Loss: 3.4460 Recon Loss: 3.4318 
[01/06 02:27:52 TiTok]: Data (t): 0.0007, 54.24/s/gpu Batch (t): 0.5900 LR: 0.000100 Step: 42500 Total Loss: 3.4917 Recon Loss: 3.4774 
[01/06 02:28:21 TiTok]: Data (t): 0.0011, 54.55/s/gpu Batch (t): 0.5866 LR: 0.000100 Step: 42550 Total Loss: 3.4847 Recon Loss: 3.4705 
[01/06 02:28:50 TiTok]: Data (t): 0.0006, 54.03/s/gpu Batch (t): 0.5922 LR: 0.000100 Step: 42600 Total Loss: 3.4963 Recon Loss: 3.4821 
[01/06 02:29:20 TiTok]: Data (t): 0.0011, 54.60/s/gpu Batch (t): 0.5861 LR: 0.000100 Step: 42650 Total Loss: 3.4816 Recon Loss: 3.4674 
[01/06 02:29:49 TiTok]: Data (t): 0.0011, 54.46/s/gpu Batch (t): 0.5876 LR: 0.000100 Step: 42700 Total Loss: 3.4299 Recon Loss: 3.4157 
[01/06 02:30:19 TiTok]: Data (t): 0.0011, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 42750 Total Loss: 3.4313 Recon Loss: 3.4171 
[01/06 02:30:48 TiTok]: Data (t): 0.0011, 54.51/s/gpu Batch (t): 0.5871 LR: 0.000100 Step: 42800 Total Loss: 3.4486 Recon Loss: 3.4345 
[01/06 02:31:17 TiTok]: Data (t): 0.0006, 54.20/s/gpu Batch (t): 0.5904 LR: 0.000100 Step: 42850 Total Loss: 3.4513 Recon Loss: 3.4371 
[01/06 02:31:47 TiTok]: Data (t): 0.0011, 55.55/s/gpu Batch (t): 0.5761 LR: 0.000100 Step: 42900 Total Loss: 3.4478 Recon Loss: 3.4336 
[01/06 02:32:16 TiTok]: Data (t): 0.0011, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000100 Step: 42950 Total Loss: 3.4034 Recon Loss: 3.3893 
[01/06 02:32:45 TiTok]: Data (t): 0.0013, 46.58/s/gpu Batch (t): 0.6870 LR: 0.000100 Step: 43000 Total Loss: 3.4598 Recon Loss: 3.4456 
[01/06 02:33:15 TiTok]: Data (t): 0.0011, 54.62/s/gpu Batch (t): 0.5859 LR: 0.000100 Step: 43050 Total Loss: 3.4485 Recon Loss: 3.4344 
[01/06 02:33:44 TiTok]: Data (t): 0.0006, 54.12/s/gpu Batch (t): 0.5912 LR: 0.000100 Step: 43100 Total Loss: 3.4406 Recon Loss: 3.4265 
[01/06 02:34:13 TiTok]: Data (t): 0.0008, 54.53/s/gpu Batch (t): 0.5868 LR: 0.000100 Step: 43150 Total Loss: 3.4916 Recon Loss: 3.4774 
[01/06 02:34:43 TiTok]: Data (t): 0.0011, 54.37/s/gpu Batch (t): 0.5886 LR: 0.000100 Step: 43200 Total Loss: 3.4751 Recon Loss: 3.4609 
[01/06 02:35:12 TiTok]: Data (t): 0.0011, 54.54/s/gpu Batch (t): 0.5867 LR: 0.000100 Step: 43250 Total Loss: 3.4232 Recon Loss: 3.4090 
[01/06 02:35:41 TiTok]: Data (t): 0.0013, 54.62/s/gpu Batch (t): 0.5858 LR: 0.000100 Step: 43300 Total Loss: 3.4647 Recon Loss: 3.4505 
[01/06 02:36:11 TiTok]: Data (t): 0.0011, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 43350 Total Loss: 3.4672 Recon Loss: 3.4530 
[01/06 02:36:40 TiTok]: Data (t): 0.0008, 54.20/s/gpu Batch (t): 0.5904 LR: 0.000100 Step: 43400 Total Loss: 3.4401 Recon Loss: 3.4259 
[01/06 02:37:09 TiTok]: Data (t): 0.0006, 54.24/s/gpu Batch (t): 0.5899 LR: 0.000100 Step: 43450 Total Loss: 3.4433 Recon Loss: 3.4292 
[01/06 02:37:39 TiTok]: Data (t): 0.0011, 54.58/s/gpu Batch (t): 0.5863 LR: 0.000100 Step: 43500 Total Loss: 3.4573 Recon Loss: 3.4431 
[01/06 02:38:08 TiTok]: Data (t): 0.0012, 54.53/s/gpu Batch (t): 0.5869 LR: 0.000100 Step: 43550 Total Loss: 3.4220 Recon Loss: 3.4079 
[01/06 02:38:37 TiTok]: Data (t): 0.0011, 54.63/s/gpu Batch (t): 0.5858 LR: 0.000100 Step: 43600 Total Loss: 3.4537 Recon Loss: 3.4395 
[01/06 02:39:07 TiTok]: Data (t): 0.0011, 54.61/s/gpu Batch (t): 0.5860 LR: 0.000100 Step: 43650 Total Loss: 3.4180 Recon Loss: 3.4038 
[01/06 02:39:36 TiTok]: Data (t): 0.0011, 54.59/s/gpu Batch (t): 0.5861 LR: 0.000100 Step: 43700 Total Loss: 3.4278 Recon Loss: 3.4137 
[01/06 02:40:05 TiTok]: Data (t): 0.0011, 54.26/s/gpu Batch (t): 0.5898 LR: 0.000100 Step: 43750 Total Loss: 3.4289 Recon Loss: 3.4148 
[01/06 02:40:35 TiTok]: Data (t): 0.0011, 54.53/s/gpu Batch (t): 0.5868 LR: 0.000100 Step: 43800 Total Loss: 3.4434 Recon Loss: 3.4292 
[01/06 02:41:04 TiTok]: Data (t): 0.0006, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000100 Step: 43850 Total Loss: 3.4305 Recon Loss: 3.4163 
[01/06 02:41:34 TiTok]: Data (t): 0.0011, 54.95/s/gpu Batch (t): 0.5823 LR: 0.000100 Step: 43900 Total Loss: 3.4526 Recon Loss: 3.4385 
[01/06 02:42:03 TiTok]: Data (t): 0.0011, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 43950 Total Loss: 3.4443 Recon Loss: 3.4302 
[01/06 02:42:32 TiTok]: Data (t): 0.0014, 46.69/s/gpu Batch (t): 0.6854 LR: 0.000100 Step: 44000 Total Loss: 3.4830 Recon Loss: 3.4688 
[01/06 02:43:02 TiTok]: Data (t): 0.0006, 54.50/s/gpu Batch (t): 0.5871 LR: 0.000100 Step: 44050 Total Loss: 3.3784 Recon Loss: 3.3643 
[01/06 02:43:31 TiTok]: Data (t): 0.0011, 54.52/s/gpu Batch (t): 0.5869 LR: 0.000100 Step: 44100 Total Loss: 3.5191 Recon Loss: 3.5049 
[01/06 02:44:00 TiTok]: Data (t): 0.0011, 54.70/s/gpu Batch (t): 0.5850 LR: 0.000100 Step: 44150 Total Loss: 3.4557 Recon Loss: 3.4415 
[01/06 02:44:30 TiTok]: Data (t): 0.0011, 54.66/s/gpu Batch (t): 0.5855 LR: 0.000100 Step: 44200 Total Loss: 3.4506 Recon Loss: 3.4365 
[01/06 02:44:59 TiTok]: Data (t): 0.0006, 54.61/s/gpu Batch (t): 0.5860 LR: 0.000100 Step: 44250 Total Loss: 3.4463 Recon Loss: 3.4322 
[01/06 02:45:28 TiTok]: Data (t): 0.0011, 54.59/s/gpu Batch (t): 0.5861 LR: 0.000100 Step: 44300 Total Loss: 3.4600 Recon Loss: 3.4458 
[01/06 02:45:58 TiTok]: Data (t): 0.0011, 54.45/s/gpu Batch (t): 0.5877 LR: 0.000100 Step: 44350 Total Loss: 3.4309 Recon Loss: 3.4168 
[01/06 02:46:27 TiTok]: Data (t): 0.0006, 54.10/s/gpu Batch (t): 0.5915 LR: 0.000100 Step: 44400 Total Loss: 3.4217 Recon Loss: 3.4075 
[01/06 02:46:56 TiTok]: Data (t): 0.0011, 54.43/s/gpu Batch (t): 0.5879 LR: 0.000100 Step: 44450 Total Loss: 3.4309 Recon Loss: 3.4167 
[01/06 02:47:26 TiTok]: Data (t): 0.0012, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 44500 Total Loss: 3.4463 Recon Loss: 3.4321 
[01/06 02:47:55 TiTok]: Data (t): 0.0011, 54.64/s/gpu Batch (t): 0.5856 LR: 0.000100 Step: 44550 Total Loss: 3.4368 Recon Loss: 3.4226 
[01/06 02:48:24 TiTok]: Data (t): 0.0008, 54.10/s/gpu Batch (t): 0.5915 LR: 0.000100 Step: 44600 Total Loss: 3.4812 Recon Loss: 3.4670 
[01/06 02:48:54 TiTok]: Data (t): 0.0011, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000100 Step: 44650 Total Loss: 3.4632 Recon Loss: 3.4491 
[01/06 02:49:23 TiTok]: Data (t): 0.0011, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 44700 Total Loss: 3.4132 Recon Loss: 3.3990 
[01/06 02:49:52 TiTok]: Data (t): 0.0011, 54.56/s/gpu Batch (t): 0.5866 LR: 0.000100 Step: 44750 Total Loss: 3.4307 Recon Loss: 3.4165 
[01/06 02:50:22 TiTok]: Data (t): 0.0011, 54.14/s/gpu Batch (t): 0.5911 LR: 0.000100 Step: 44800 Total Loss: 3.4636 Recon Loss: 3.4494 
[01/06 02:50:51 TiTok]: Data (t): 0.0011, 54.49/s/gpu Batch (t): 0.5872 LR: 0.000100 Step: 44850 Total Loss: 3.3912 Recon Loss: 3.3771 
[01/06 02:51:20 TiTok]: Data (t): 0.0008, 54.11/s/gpu Batch (t): 0.5914 LR: 0.000100 Step: 44900 Total Loss: 3.4026 Recon Loss: 3.3885 
[01/06 02:51:50 TiTok]: Data (t): 0.0011, 54.58/s/gpu Batch (t): 0.5863 LR: 0.000100 Step: 44950 Total Loss: 3.4192 Recon Loss: 3.4050 
[01/06 02:52:19 TiTok]: Data (t): 0.0011, 47.84/s/gpu Batch (t): 0.6689 LR: 0.000100 Step: 45000 Total Loss: 3.4235 Recon Loss: 3.4093 
[01/06 02:52:21 TiTok]: Reconstructing images...
[01/06 02:52:56 TiTok]: Data (t): 0.0011, 54.62/s/gpu Batch (t): 0.5859 LR: 0.000100 Step: 45050 Total Loss: 3.4346 Recon Loss: 3.4204 
[01/06 02:53:25 TiTok]: Data (t): 0.0011, 55.12/s/gpu Batch (t): 0.5806 LR: 0.000100 Step: 45100 Total Loss: 3.4109 Recon Loss: 3.3967 
Epoch 9/199 started.
[01/06 02:53:56 TiTok]: Data (t): 0.0006, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 45150 Total Loss: 3.4316 Recon Loss: 3.4174 
[01/06 02:54:26 TiTok]: Data (t): 0.0011, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000100 Step: 45200 Total Loss: 3.4264 Recon Loss: 3.4123 
[01/06 02:54:55 TiTok]: Data (t): 0.0011, 54.54/s/gpu Batch (t): 0.5867 LR: 0.000100 Step: 45250 Total Loss: 3.3983 Recon Loss: 3.3842 
[01/06 02:55:24 TiTok]: Data (t): 0.0006, 54.42/s/gpu Batch (t): 0.5880 LR: 0.000100 Step: 45300 Total Loss: 3.4112 Recon Loss: 3.3970 
[01/06 02:55:54 TiTok]: Data (t): 0.0011, 54.60/s/gpu Batch (t): 0.5860 LR: 0.000100 Step: 45350 Total Loss: 3.4439 Recon Loss: 3.4298 
[01/06 02:56:23 TiTok]: Data (t): 0.0012, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000100 Step: 45400 Total Loss: 3.4235 Recon Loss: 3.4093 
[01/06 02:56:52 TiTok]: Data (t): 0.0006, 54.52/s/gpu Batch (t): 0.5869 LR: 0.000100 Step: 45450 Total Loss: 3.4733 Recon Loss: 3.4592 
[01/06 02:57:22 TiTok]: Data (t): 0.0011, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 45500 Total Loss: 3.4066 Recon Loss: 3.3925 
[01/06 02:57:51 TiTok]: Data (t): 0.0011, 54.97/s/gpu Batch (t): 0.5822 LR: 0.000100 Step: 45550 Total Loss: 3.4102 Recon Loss: 3.3960 
[01/06 02:58:20 TiTok]: Data (t): 0.0009, 54.14/s/gpu Batch (t): 0.5911 LR: 0.000100 Step: 45600 Total Loss: 3.4659 Recon Loss: 3.4517 
[01/06 02:58:50 TiTok]: Data (t): 0.0011, 54.60/s/gpu Batch (t): 0.5861 LR: 0.000100 Step: 45650 Total Loss: 3.3939 Recon Loss: 3.3798 
[01/06 02:59:19 TiTok]: Data (t): 0.0006, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 45700 Total Loss: 3.4160 Recon Loss: 3.4019 
[01/06 02:59:49 TiTok]: Data (t): 0.0011, 54.62/s/gpu Batch (t): 0.5859 LR: 0.000100 Step: 45750 Total Loss: 3.4315 Recon Loss: 3.4174 
[01/06 03:00:18 TiTok]: Data (t): 0.0006, 54.18/s/gpu Batch (t): 0.5906 LR: 0.000100 Step: 45800 Total Loss: 3.4326 Recon Loss: 3.4184 
[01/06 03:00:47 TiTok]: Data (t): 0.0011, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 45850 Total Loss: 3.3660 Recon Loss: 3.3518 
[01/06 03:01:17 TiTok]: Data (t): 0.0014, 54.55/s/gpu Batch (t): 0.5867 LR: 0.000100 Step: 45900 Total Loss: 3.4202 Recon Loss: 3.4061 
[01/06 03:01:46 TiTok]: Data (t): 0.0011, 54.35/s/gpu Batch (t): 0.5887 LR: 0.000100 Step: 45950 Total Loss: 3.4278 Recon Loss: 3.4136 
[01/06 03:02:16 TiTok]: Data (t): 0.0009, 43.83/s/gpu Batch (t): 0.7301 LR: 0.000100 Step: 46000 Total Loss: 3.4330 Recon Loss: 3.4189 
[01/06 03:02:45 TiTok]: Data (t): 0.0011, 54.50/s/gpu Batch (t): 0.5871 LR: 0.000100 Step: 46050 Total Loss: 3.4130 Recon Loss: 3.3988 
[01/06 03:03:14 TiTok]: Data (t): 0.0012, 54.58/s/gpu Batch (t): 0.5863 LR: 0.000100 Step: 46100 Total Loss: 3.3844 Recon Loss: 3.3703 
[01/06 03:03:44 TiTok]: Data (t): 0.0011, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000100 Step: 46150 Total Loss: 3.3944 Recon Loss: 3.3802 
[01/06 03:04:13 TiTok]: Data (t): 0.0011, 54.58/s/gpu Batch (t): 0.5863 LR: 0.000100 Step: 46200 Total Loss: 3.4234 Recon Loss: 3.4092 
[01/06 03:04:42 TiTok]: Data (t): 0.0011, 55.03/s/gpu Batch (t): 0.5815 LR: 0.000100 Step: 46250 Total Loss: 3.4449 Recon Loss: 3.4308 
[01/06 03:05:12 TiTok]: Data (t): 0.0014, 54.54/s/gpu Batch (t): 0.5867 LR: 0.000100 Step: 46300 Total Loss: 3.3989 Recon Loss: 3.3848 
[01/06 03:05:41 TiTok]: Data (t): 0.0011, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 46350 Total Loss: 3.4185 Recon Loss: 3.4044 
[01/06 03:06:10 TiTok]: Data (t): 0.0011, 54.55/s/gpu Batch (t): 0.5866 LR: 0.000100 Step: 46400 Total Loss: 3.3813 Recon Loss: 3.3672 
[01/06 03:06:40 TiTok]: Data (t): 0.0011, 54.49/s/gpu Batch (t): 0.5873 LR: 0.000100 Step: 46450 Total Loss: 3.4156 Recon Loss: 3.4015 
[01/06 03:07:09 TiTok]: Data (t): 0.0007, 53.97/s/gpu Batch (t): 0.5929 LR: 0.000100 Step: 46500 Total Loss: 3.4225 Recon Loss: 3.4083 
[01/06 03:07:39 TiTok]: Data (t): 0.0011, 54.63/s/gpu Batch (t): 0.5857 LR: 0.000100 Step: 46550 Total Loss: 3.3916 Recon Loss: 3.3775 
[01/06 03:08:08 TiTok]: Data (t): 0.0011, 54.66/s/gpu Batch (t): 0.5854 LR: 0.000100 Step: 46600 Total Loss: 3.4688 Recon Loss: 3.4546 
[01/06 03:08:37 TiTok]: Data (t): 0.0011, 54.53/s/gpu Batch (t): 0.5868 LR: 0.000100 Step: 46650 Total Loss: 3.4701 Recon Loss: 3.4559 
[01/06 03:09:07 TiTok]: Data (t): 0.0011, 54.53/s/gpu Batch (t): 0.5869 LR: 0.000100 Step: 46700 Total Loss: 3.3892 Recon Loss: 3.3751 
[01/06 03:09:36 TiTok]: Data (t): 0.0011, 54.43/s/gpu Batch (t): 0.5879 LR: 0.000100 Step: 46750 Total Loss: 3.4291 Recon Loss: 3.4149 
[01/06 03:10:05 TiTok]: Data (t): 0.0011, 54.44/s/gpu Batch (t): 0.5878 LR: 0.000100 Step: 46800 Total Loss: 3.3837 Recon Loss: 3.3696 
[01/06 03:10:35 TiTok]: Data (t): 0.0011, 54.58/s/gpu Batch (t): 0.5863 LR: 0.000100 Step: 46850 Total Loss: 3.3986 Recon Loss: 3.3845 
[01/06 03:11:04 TiTok]: Data (t): 0.0011, 54.51/s/gpu Batch (t): 0.5870 LR: 0.000100 Step: 46900 Total Loss: 3.3953 Recon Loss: 3.3811 
[01/06 03:11:33 TiTok]: Data (t): 0.0011, 54.32/s/gpu Batch (t): 0.5891 LR: 0.000100 Step: 46950 Total Loss: 3.3935 Recon Loss: 3.3793 
[01/06 03:12:03 TiTok]: Data (t): 0.0009, 46.38/s/gpu Batch (t): 0.6899 LR: 0.000100 Step: 47000 Total Loss: 3.4588 Recon Loss: 3.4446 
[01/06 03:12:32 TiTok]: Data (t): 0.0011, 54.94/s/gpu Batch (t): 0.5824 LR: 0.000100 Step: 47050 Total Loss: 3.4248 Recon Loss: 3.4106 
[01/06 03:13:01 TiTok]: Data (t): 0.0006, 54.10/s/gpu Batch (t): 0.5915 LR: 0.000100 Step: 47100 Total Loss: 3.3955 Recon Loss: 3.3813 
[01/06 03:13:31 TiTok]: Data (t): 0.0006, 54.55/s/gpu Batch (t): 0.5866 LR: 0.000100 Step: 47150 Total Loss: 3.4284 Recon Loss: 3.4143 
[01/06 03:14:00 TiTok]: Data (t): 0.0007, 53.93/s/gpu Batch (t): 0.5934 LR: 0.000100 Step: 47200 Total Loss: 3.4040 Recon Loss: 3.3898 
[01/06 03:14:30 TiTok]: Data (t): 0.0011, 54.46/s/gpu Batch (t): 0.5875 LR: 0.000100 Step: 47250 Total Loss: 3.3945 Recon Loss: 3.3804 
[01/06 03:14:59 TiTok]: Data (t): 0.0011, 54.51/s/gpu Batch (t): 0.5871 LR: 0.000100 Step: 47300 Total Loss: 3.4169 Recon Loss: 3.4028 
[01/06 03:15:28 TiTok]: Data (t): 0.0013, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 47350 Total Loss: 3.3862 Recon Loss: 3.3720 
[01/06 03:15:58 TiTok]: Data (t): 0.0014, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 47400 Total Loss: 3.4095 Recon Loss: 3.3954 
[01/06 03:16:27 TiTok]: Data (t): 0.0011, 54.60/s/gpu Batch (t): 0.5861 LR: 0.000100 Step: 47450 Total Loss: 3.3878 Recon Loss: 3.3737 
[01/06 03:16:56 TiTok]: Data (t): 0.0008, 54.06/s/gpu Batch (t): 0.5919 LR: 0.000100 Step: 47500 Total Loss: 3.4042 Recon Loss: 3.3901 
[01/06 03:17:26 TiTok]: Data (t): 0.0006, 54.49/s/gpu Batch (t): 0.5872 LR: 0.000100 Step: 47550 Total Loss: 3.3965 Recon Loss: 3.3823 
[01/06 03:17:55 TiTok]: Data (t): 0.0011, 54.60/s/gpu Batch (t): 0.5861 LR: 0.000100 Step: 47600 Total Loss: 3.4008 Recon Loss: 3.3866 
[01/06 03:18:24 TiTok]: Data (t): 0.0007, 54.42/s/gpu Batch (t): 0.5880 LR: 0.000100 Step: 47650 Total Loss: 3.4544 Recon Loss: 3.4403 
[01/06 03:18:54 TiTok]: Data (t): 0.0011, 54.67/s/gpu Batch (t): 0.5854 LR: 0.000100 Step: 47700 Total Loss: 3.3960 Recon Loss: 3.3819 
[01/06 03:19:23 TiTok]: Data (t): 0.0011, 54.50/s/gpu Batch (t): 0.5871 LR: 0.000100 Step: 47750 Total Loss: 3.3867 Recon Loss: 3.3726 
[01/06 03:19:52 TiTok]: Data (t): 0.0006, 53.92/s/gpu Batch (t): 0.5934 LR: 0.000100 Step: 47800 Total Loss: 3.3874 Recon Loss: 3.3733 
[01/06 03:20:22 TiTok]: Data (t): 0.0011, 54.59/s/gpu Batch (t): 0.5862 LR: 0.000100 Step: 47850 Total Loss: 3.3947 Recon Loss: 3.3805 
[01/06 03:20:51 TiTok]: Data (t): 0.0011, 54.63/s/gpu Batch (t): 0.5857 LR: 0.000100 Step: 47900 Total Loss: 3.4192 Recon Loss: 3.4050 
[01/06 03:21:20 TiTok]: Data (t): 0.0006, 54.09/s/gpu Batch (t): 0.5916 LR: 0.000100 Step: 47950 Total Loss: 3.3518 Recon Loss: 3.3377 
[01/06 03:21:50 TiTok]: Data (t): 0.0011, 44.30/s/gpu Batch (t): 0.7223 LR: 0.000100 Step: 48000 Total Loss: 3.4312 Recon Loss: 3.4171 
[01/06 03:22:19 TiTok]: Data (t): 0.0011, 54.99/s/gpu Batch (t): 0.5819 LR: 0.000100 Step: 48050 Total Loss: 3.4214 Recon Loss: 3.4072 
[01/06 03:22:48 TiTok]: Data (t): 0.0011, 54.48/s/gpu Batch (t): 0.5874 LR: 0.000100 Step: 48100 Total Loss: 3.4037 Recon Loss: 3.3896 
[01/06 03:23:18 TiTok]: Data (t): 0.0011, 55.05/s/gpu Batch (t): 0.5813 LR: 0.000100 Step: 48150 Total Loss: 3.3573 Recon Loss: 3.3432 
[01/06 03:23:47 TiTok]: Data (t): 0.0011, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 48200 Total Loss: 3.3606 Recon Loss: 3.3465 
[01/06 03:24:17 TiTok]: Data (t): 0.0014, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 48250 Total Loss: 3.3932 Recon Loss: 3.3791 
[01/06 03:24:46 TiTok]: Data (t): 0.0012, 54.71/s/gpu Batch (t): 0.5848 LR: 0.000100 Step: 48300 Total Loss: 3.3851 Recon Loss: 3.3710 
[01/06 03:25:15 TiTok]: Data (t): 0.0012, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 48350 Total Loss: 3.4060 Recon Loss: 3.3919 
[01/06 03:25:45 TiTok]: Data (t): 0.0011, 54.53/s/gpu Batch (t): 0.5868 LR: 0.000100 Step: 48400 Total Loss: 3.4121 Recon Loss: 3.3979 
[01/06 03:26:14 TiTok]: Data (t): 0.0006, 54.13/s/gpu Batch (t): 0.5912 LR: 0.000100 Step: 48450 Total Loss: 3.3493 Recon Loss: 3.3351 
[01/06 03:26:43 TiTok]: Data (t): 0.0014, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 48500 Total Loss: 3.4095 Recon Loss: 3.3953 
[01/06 03:27:13 TiTok]: Data (t): 0.0007, 54.12/s/gpu Batch (t): 0.5913 LR: 0.000100 Step: 48550 Total Loss: 3.3868 Recon Loss: 3.3726 
[01/06 03:27:42 TiTok]: Data (t): 0.0012, 54.63/s/gpu Batch (t): 0.5858 LR: 0.000100 Step: 48600 Total Loss: 3.3580 Recon Loss: 3.3438 
[01/06 03:28:11 TiTok]: Data (t): 0.0009, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 48650 Total Loss: 3.4313 Recon Loss: 3.4171 
[01/06 03:28:41 TiTok]: Data (t): 0.0011, 54.66/s/gpu Batch (t): 0.5855 LR: 0.000100 Step: 48700 Total Loss: 3.4177 Recon Loss: 3.4035 
[01/06 03:29:10 TiTok]: Data (t): 0.0006, 54.63/s/gpu Batch (t): 0.5858 LR: 0.000100 Step: 48750 Total Loss: 3.4215 Recon Loss: 3.4074 
[01/06 03:29:39 TiTok]: Data (t): 0.0011, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 48800 Total Loss: 3.4123 Recon Loss: 3.3981 
[01/06 03:30:09 TiTok]: Data (t): 0.0014, 55.05/s/gpu Batch (t): 0.5813 LR: 0.000100 Step: 48850 Total Loss: 3.3743 Recon Loss: 3.3601 
[01/06 03:30:38 TiTok]: Data (t): 0.0007, 54.05/s/gpu Batch (t): 0.5920 LR: 0.000100 Step: 48900 Total Loss: 3.3674 Recon Loss: 3.3532 
[01/06 03:31:07 TiTok]: Data (t): 0.0007, 54.14/s/gpu Batch (t): 0.5911 LR: 0.000100 Step: 48950 Total Loss: 3.4283 Recon Loss: 3.4141 
[01/06 03:31:37 TiTok]: Data (t): 0.0013, 46.33/s/gpu Batch (t): 0.6906 LR: 0.000100 Step: 49000 Total Loss: 3.3952 Recon Loss: 3.3811 
[01/06 03:32:06 TiTok]: Data (t): 0.0007, 54.70/s/gpu Batch (t): 0.5850 LR: 0.000100 Step: 49050 Total Loss: 3.3805 Recon Loss: 3.3664 
[01/06 03:32:35 TiTok]: Data (t): 0.0007, 54.15/s/gpu Batch (t): 0.5910 LR: 0.000100 Step: 49100 Total Loss: 3.3331 Recon Loss: 3.3190 
[01/06 03:33:05 TiTok]: Data (t): 0.0011, 54.49/s/gpu Batch (t): 0.5872 LR: 0.000100 Step: 49150 Total Loss: 3.4061 Recon Loss: 3.3919 
[01/06 03:33:34 TiTok]: Data (t): 0.0006, 54.60/s/gpu Batch (t): 0.5861 LR: 0.000100 Step: 49200 Total Loss: 3.4097 Recon Loss: 3.3956 
[01/06 03:34:03 TiTok]: Data (t): 0.0007, 54.42/s/gpu Batch (t): 0.5880 LR: 0.000100 Step: 49250 Total Loss: 3.3984 Recon Loss: 3.3842 
[01/06 03:34:33 TiTok]: Data (t): 0.0011, 54.54/s/gpu Batch (t): 0.5867 LR: 0.000100 Step: 49300 Total Loss: 3.3589 Recon Loss: 3.3447 
[01/06 03:35:02 TiTok]: Data (t): 0.0011, 55.02/s/gpu Batch (t): 0.5816 LR: 0.000100 Step: 49350 Total Loss: 3.3505 Recon Loss: 3.3363 
[01/06 03:35:31 TiTok]: Data (t): 0.0012, 54.60/s/gpu Batch (t): 0.5861 LR: 0.000100 Step: 49400 Total Loss: 3.3643 Recon Loss: 3.3502 
[01/06 03:36:01 TiTok]: Data (t): 0.0011, 54.55/s/gpu Batch (t): 0.5866 LR: 0.000100 Step: 49450 Total Loss: 3.3561 Recon Loss: 3.3420 
[01/06 03:36:30 TiTok]: Data (t): 0.0007, 54.50/s/gpu Batch (t): 0.5872 LR: 0.000100 Step: 49500 Total Loss: 3.3697 Recon Loss: 3.3555 
[01/06 03:36:59 TiTok]: Data (t): 0.0011, 54.50/s/gpu Batch (t): 0.5872 LR: 0.000100 Step: 49550 Total Loss: 3.4028 Recon Loss: 3.3885 
[01/06 03:37:29 TiTok]: Data (t): 0.0007, 54.07/s/gpu Batch (t): 0.5918 LR: 0.000100 Step: 49600 Total Loss: 3.3459 Recon Loss: 3.3317 
[01/06 03:37:58 TiTok]: Data (t): 0.0009, 54.09/s/gpu Batch (t): 0.5916 LR: 0.000100 Step: 49650 Total Loss: 3.3899 Recon Loss: 3.3757 
[01/06 03:38:27 TiTok]: Data (t): 0.0011, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 49700 Total Loss: 3.3849 Recon Loss: 3.3707 
[01/06 03:38:57 TiTok]: Data (t): 0.0011, 54.17/s/gpu Batch (t): 0.5907 LR: 0.000100 Step: 49750 Total Loss: 3.4107 Recon Loss: 3.3965 
[01/06 03:39:26 TiTok]: Data (t): 0.0011, 54.63/s/gpu Batch (t): 0.5857 LR: 0.000100 Step: 49800 Total Loss: 3.3753 Recon Loss: 3.3612 
[01/06 03:39:56 TiTok]: Data (t): 0.0011, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000100 Step: 49850 Total Loss: 3.3787 Recon Loss: 3.3645 
[01/06 03:40:25 TiTok]: Data (t): 0.0009, 54.11/s/gpu Batch (t): 0.5914 LR: 0.000100 Step: 49900 Total Loss: 3.3840 Recon Loss: 3.3699 
[01/06 03:40:54 TiTok]: Data (t): 0.0011, 54.54/s/gpu Batch (t): 0.5868 LR: 0.000100 Step: 49950 Total Loss: 3.3766 Recon Loss: 3.3624 
[01/06 03:41:24 TiTok]: Data (t): 0.0006, 50.58/s/gpu Batch (t): 0.6327 LR: 0.000100 Step: 50000 Total Loss: 3.4100 Recon Loss: 3.3958 
Model weights saved in titok_s128_matryoshka_annealing_stage1_run1/checkpoint-50000/unwrapped_model/pytorch_model.bin
[01/06 03:41:26 TiTok]: Saved state to titok_s128_matryoshka_annealing_stage1_run1/checkpoint-50000
Model weights saved in titok_s128_matryoshka_annealing_stage1_run1/checkpoint-50000/ema_model/pytorch_model.bin
[01/06 03:41:51 TiTok]: Reconstructing images...
[01/06 03:41:56 TiTok]: Computing metrics on the validation set.
[01/06 04:00:46 TiTok]: EMA EVALUATION with 100.0% tokensStep: 50000 
[01/06 04:00:46 TiTok]: {'CodebookEntropy': tensor(11.9947, device='cuda:0', dtype=torch.float64),
 'CodebookUsage': 1.0,
 'InceptionScore': 119.67561880188012,
 'rFID': 9.073323782654484}
[01/06 04:19:11 TiTok]: EMA EVALUATION with 75.0% tokensStep: 50000 
[01/06 04:19:11 TiTok]: {'CodebookEntropy': tensor(11.9947, device='cuda:0', dtype=torch.float64),
 'CodebookUsage': 1.0,
 'InceptionScore': 80.34819183687225,
 'rFID': 16.436408029798258}
[01/06 04:37:20 TiTok]: EMA EVALUATION with 50.0% tokensStep: 50000 
[01/06 04:37:20 TiTok]: {'CodebookEntropy': tensor(11.9947, device='cuda:0', dtype=torch.float64),
 'CodebookUsage': 1.0,
 'InceptionScore': 25.49246139399914,
 'rFID': 53.79909427443215}
[01/06 04:55:24 TiTok]: EMA EVALUATION with 25.0% tokensStep: 50000 
[01/06 04:55:24 TiTok]: {'CodebookEntropy': tensor(11.9947, device='cuda:0', dtype=torch.float64),
 'CodebookUsage': 1.0,
 'InceptionScore': 5.51416251770664,
 'rFID': 160.67259993534407}
[01/06 04:56:38 TiTok]: Data (t): 0.0007, 53.98/s/gpu Batch (t): 0.5929 LR: 0.000100 Step: 50050 Total Loss: 3.3999 Recon Loss: 3.3857 
[01/06 04:57:08 TiTok]: Data (t): 0.0006, 53.76/s/gpu Batch (t): 0.5953 LR: 0.000100 Step: 50100 Total Loss: 3.3679 Recon Loss: 3.3538 
[01/06 04:57:37 TiTok]: Data (t): 0.0006, 54.17/s/gpu Batch (t): 0.5908 LR: 0.000100 Step: 50150 Total Loss: 3.3732 Recon Loss: 3.3590 
Epoch 10/199 started.
[01/06 04:58:09 TiTok]: Data (t): 0.0012, 54.56/s/gpu Batch (t): 0.5865 LR: 0.000100 Step: 50200 Total Loss: 3.3639 Recon Loss: 3.3498 
[01/06 04:58:38 TiTok]: Data (t): 0.0007, 54.00/s/gpu Batch (t): 0.5926 LR: 0.000100 Step: 50250 Total Loss: 3.3711 Recon Loss: 3.3570 
[01/06 04:59:08 TiTok]: Data (t): 0.0012, 53.99/s/gpu Batch (t): 0.5927 LR: 0.000100 Step: 50300 Total Loss: 3.3453 Recon Loss: 3.3311 
[01/06 04:59:38 TiTok]: Data (t): 0.0007, 53.20/s/gpu Batch (t): 0.6015 LR: 0.000100 Step: 50350 Total Loss: 3.4191 Recon Loss: 3.4050 
[01/06 05:00:07 TiTok]: Data (t): 0.0007, 53.97/s/gpu Batch (t): 0.5929 LR: 0.000100 Step: 50400 Total Loss: 3.3732 Recon Loss: 3.3591 
[01/06 05:00:37 TiTok]: Data (t): 0.0011, 54.36/s/gpu Batch (t): 0.5887 LR: 0.000100 Step: 50450 Total Loss: 3.3543 Recon Loss: 3.3402 
[01/06 05:01:07 TiTok]: Data (t): 0.0011, 54.01/s/gpu Batch (t): 0.5924 LR: 0.000100 Step: 50500 Total Loss: 3.4352 Recon Loss: 3.4210 
[01/06 05:01:36 TiTok]: Data (t): 0.0013, 54.24/s/gpu Batch (t): 0.5900 LR: 0.000100 Step: 50550 Total Loss: 3.3338 Recon Loss: 3.3196 
[01/06 05:02:06 TiTok]: Data (t): 0.0007, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000100 Step: 50600 Total Loss: 3.3810 Recon Loss: 3.3669 
[01/06 05:02:36 TiTok]: Data (t): 0.0006, 53.48/s/gpu Batch (t): 0.5983 LR: 0.000100 Step: 50650 Total Loss: 3.3721 Recon Loss: 3.3580 
[01/06 05:03:05 TiTok]: Data (t): 0.0011, 53.65/s/gpu Batch (t): 0.5965 LR: 0.000100 Step: 50700 Total Loss: 3.3826 Recon Loss: 3.3684 
[01/06 05:03:35 TiTok]: Data (t): 0.0013, 53.86/s/gpu Batch (t): 0.5941 LR: 0.000100 Step: 50750 Total Loss: 3.3471 Recon Loss: 3.3329 
[01/06 05:04:05 TiTok]: Data (t): 0.0016, 53.35/s/gpu Batch (t): 0.5998 LR: 0.000100 Step: 50800 Total Loss: 3.3840 Recon Loss: 3.3699 
[01/06 05:04:35 TiTok]: Data (t): 0.0007, 53.53/s/gpu Batch (t): 0.5977 LR: 0.000100 Step: 50850 Total Loss: 3.3816 Recon Loss: 3.3674 
[01/06 05:05:04 TiTok]: Data (t): 0.0011, 54.37/s/gpu Batch (t): 0.5886 LR: 0.000100 Step: 50900 Total Loss: 3.3992 Recon Loss: 3.3851 
[01/06 05:05:34 TiTok]: Data (t): 0.0011, 53.94/s/gpu Batch (t): 0.5932 LR: 0.000100 Step: 50950 Total Loss: 3.4040 Recon Loss: 3.3899 
[01/06 05:06:04 TiTok]: Data (t): 0.0014, 44.09/s/gpu Batch (t): 0.7259 LR: 0.000100 Step: 51000 Total Loss: 3.3765 Recon Loss: 3.3624 
[01/06 05:06:33 TiTok]: Data (t): 0.0011, 53.73/s/gpu Batch (t): 0.5956 LR: 0.000100 Step: 51050 Total Loss: 3.3976 Recon Loss: 3.3834 
[01/06 05:07:03 TiTok]: Data (t): 0.0011, 54.36/s/gpu Batch (t): 0.5887 LR: 0.000100 Step: 51100 Total Loss: 3.3616 Recon Loss: 3.3474 
[01/06 05:07:33 TiTok]: Data (t): 0.0007, 53.86/s/gpu Batch (t): 0.5942 LR: 0.000100 Step: 51150 Total Loss: 3.3652 Recon Loss: 3.3510 
[01/06 05:08:03 TiTok]: Data (t): 0.0007, 53.48/s/gpu Batch (t): 0.5983 LR: 0.000100 Step: 51200 Total Loss: 3.3414 Recon Loss: 3.3272 
[01/06 05:08:32 TiTok]: Data (t): 0.0012, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000100 Step: 51250 Total Loss: 3.3521 Recon Loss: 3.3379 
[01/06 05:09:02 TiTok]: Data (t): 0.0007, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000100 Step: 51300 Total Loss: 3.3727 Recon Loss: 3.3586 
[01/06 05:09:32 TiTok]: Data (t): 0.0012, 53.56/s/gpu Batch (t): 0.5974 LR: 0.000100 Step: 51350 Total Loss: 3.3857 Recon Loss: 3.3715 
[01/06 05:10:01 TiTok]: Data (t): 0.0011, 53.67/s/gpu Batch (t): 0.5962 LR: 0.000100 Step: 51400 Total Loss: 3.3623 Recon Loss: 3.3481 
[01/06 05:10:31 TiTok]: Data (t): 0.0007, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000100 Step: 51450 Total Loss: 3.3621 Recon Loss: 3.3480 
[01/06 05:11:01 TiTok]: Data (t): 0.0012, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000100 Step: 51500 Total Loss: 3.3610 Recon Loss: 3.3469 
[01/06 05:11:30 TiTok]: Data (t): 0.0011, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000100 Step: 51550 Total Loss: 3.3434 Recon Loss: 3.3293 
[01/06 05:12:00 TiTok]: Data (t): 0.0014, 54.22/s/gpu Batch (t): 0.5901 LR: 0.000100 Step: 51600 Total Loss: 3.3709 Recon Loss: 3.3568 
[01/06 05:12:30 TiTok]: Data (t): 0.0011, 54.27/s/gpu Batch (t): 0.5897 LR: 0.000100 Step: 51650 Total Loss: 3.3940 Recon Loss: 3.3798 
[01/06 05:13:00 TiTok]: Data (t): 0.0011, 53.93/s/gpu Batch (t): 0.5933 LR: 0.000100 Step: 51700 Total Loss: 3.3397 Recon Loss: 3.3256 
[01/06 05:13:29 TiTok]: Data (t): 0.0011, 54.32/s/gpu Batch (t): 0.5891 LR: 0.000100 Step: 51750 Total Loss: 3.3614 Recon Loss: 3.3473 
[01/06 05:13:59 TiTok]: Data (t): 0.0007, 53.44/s/gpu Batch (t): 0.5988 LR: 0.000100 Step: 51800 Total Loss: 3.3460 Recon Loss: 3.3318 
[01/06 05:14:29 TiTok]: Data (t): 0.0007, 53.71/s/gpu Batch (t): 0.5958 LR: 0.000100 Step: 51850 Total Loss: 3.3579 Recon Loss: 3.3437 
[01/06 05:14:58 TiTok]: Data (t): 0.0007, 53.88/s/gpu Batch (t): 0.5940 LR: 0.000100 Step: 51900 Total Loss: 3.3451 Recon Loss: 3.3309 
[01/06 05:15:28 TiTok]: Data (t): 0.0012, 54.08/s/gpu Batch (t): 0.5917 LR: 0.000100 Step: 51950 Total Loss: 3.4017 Recon Loss: 3.3876 
[01/06 05:15:58 TiTok]: Data (t): 0.0009, 45.16/s/gpu Batch (t): 0.7085 LR: 0.000100 Step: 52000 Total Loss: 3.3507 Recon Loss: 3.3365 
[01/06 05:16:28 TiTok]: Data (t): 0.0011, 54.44/s/gpu Batch (t): 0.5878 LR: 0.000100 Step: 52050 Total Loss: 3.3292 Recon Loss: 3.3151 
[01/06 05:16:57 TiTok]: Data (t): 0.0011, 54.33/s/gpu Batch (t): 0.5890 LR: 0.000100 Step: 52100 Total Loss: 3.3992 Recon Loss: 3.3851 
[01/06 05:17:27 TiTok]: Data (t): 0.0013, 53.97/s/gpu Batch (t): 0.5929 LR: 0.000100 Step: 52150 Total Loss: 3.3828 Recon Loss: 3.3687 
[01/06 05:17:57 TiTok]: Data (t): 0.0007, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000100 Step: 52200 Total Loss: 3.3572 Recon Loss: 3.3430 
[01/06 05:18:26 TiTok]: Data (t): 0.0007, 53.59/s/gpu Batch (t): 0.5971 LR: 0.000100 Step: 52250 Total Loss: 3.3305 Recon Loss: 3.3164 
[01/06 05:18:56 TiTok]: Data (t): 0.0007, 53.41/s/gpu Batch (t): 0.5991 LR: 0.000100 Step: 52300 Total Loss: 3.3515 Recon Loss: 3.3374 
[01/06 05:19:26 TiTok]: Data (t): 0.0012, 54.08/s/gpu Batch (t): 0.5918 LR: 0.000100 Step: 52350 Total Loss: 3.3949 Recon Loss: 3.3808 
[01/06 05:19:56 TiTok]: Data (t): 0.0007, 53.56/s/gpu Batch (t): 0.5974 LR: 0.000100 Step: 52400 Total Loss: 3.3457 Recon Loss: 3.3316 
[01/06 05:20:25 TiTok]: Data (t): 0.0012, 53.93/s/gpu Batch (t): 0.5934 LR: 0.000100 Step: 52450 Total Loss: 3.3642 Recon Loss: 3.3500 
[01/06 05:20:55 TiTok]: Data (t): 0.0013, 53.84/s/gpu Batch (t): 0.5944 LR: 0.000100 Step: 52500 Total Loss: 3.3257 Recon Loss: 3.3116 
[01/06 05:21:25 TiTok]: Data (t): 0.0011, 53.96/s/gpu Batch (t): 0.5931 LR: 0.000100 Step: 52550 Total Loss: 3.3829 Recon Loss: 3.3688 
[01/06 05:21:54 TiTok]: Data (t): 0.0008, 53.52/s/gpu Batch (t): 0.5979 LR: 0.000100 Step: 52600 Total Loss: 3.3870 Recon Loss: 3.3728 
[01/06 05:22:24 TiTok]: Data (t): 0.0006, 53.48/s/gpu Batch (t): 0.5984 LR: 0.000100 Step: 52650 Total Loss: 3.3437 Recon Loss: 3.3295 
[01/06 05:22:54 TiTok]: Data (t): 0.0011, 54.29/s/gpu Batch (t): 0.5894 LR: 0.000100 Step: 52700 Total Loss: 3.3439 Recon Loss: 3.3298 
[01/06 05:23:27 TiTok]: Data (t): 0.0007, 54.02/s/gpu Batch (t): 0.5923 LR: 0.000100 Step: 52750 Total Loss: 3.3753 Recon Loss: 3.3612 
[01/06 05:23:56 TiTok]: Data (t): 0.0012, 54.01/s/gpu Batch (t): 0.5924 LR: 0.000100 Step: 52800 Total Loss: 3.3411 Recon Loss: 3.3269 
[01/06 05:24:26 TiTok]: Data (t): 0.0111, 55.25/s/gpu Batch (t): 0.5792 LR: 0.000100 Step: 52850 Total Loss: 3.3408 Recon Loss: 3.3267 
[01/06 05:24:56 TiTok]: Data (t): 0.0007, 53.51/s/gpu Batch (t): 0.5981 LR: 0.000100 Step: 52900 Total Loss: 3.3273 Recon Loss: 3.3131 
[01/06 05:25:26 TiTok]: Data (t): 0.0011, 53.93/s/gpu Batch (t): 0.5934 LR: 0.000100 Step: 52950 Total Loss: 3.3914 Recon Loss: 3.3772 
[01/06 05:25:55 TiTok]: Data (t): 0.0007, 45.41/s/gpu Batch (t): 0.7047 LR: 0.000100 Step: 53000 Total Loss: 3.3624 Recon Loss: 3.3483 
[01/06 05:26:25 TiTok]: Data (t): 0.0011, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000100 Step: 53050 Total Loss: 3.3381 Recon Loss: 3.3239 
[01/06 05:26:55 TiTok]: Data (t): 0.0007, 53.45/s/gpu Batch (t): 0.5987 LR: 0.000100 Step: 53100 Total Loss: 3.4000 Recon Loss: 3.3858 
[01/06 05:27:25 TiTok]: Data (t): 0.0014, 54.22/s/gpu Batch (t): 0.5902 LR: 0.000100 Step: 53150 Total Loss: 3.3392 Recon Loss: 3.3250 
[01/06 05:27:54 TiTok]: Data (t): 0.0012, 53.77/s/gpu Batch (t): 0.5951 LR: 0.000100 Step: 53200 Total Loss: 3.3369 Recon Loss: 3.3228 
[01/06 05:28:24 TiTok]: Data (t): 0.0012, 53.96/s/gpu Batch (t): 0.5930 LR: 0.000100 Step: 53250 Total Loss: 3.3451 Recon Loss: 3.3309 
[01/06 05:28:54 TiTok]: Data (t): 0.0012, 53.75/s/gpu Batch (t): 0.5954 LR: 0.000100 Step: 53300 Total Loss: 3.3459 Recon Loss: 3.3318 
[01/06 05:29:23 TiTok]: Data (t): 0.0013, 54.05/s/gpu Batch (t): 0.5921 LR: 0.000100 Step: 53350 Total Loss: 3.3647 Recon Loss: 3.3505 
[01/06 05:29:53 TiTok]: Data (t): 0.0011, 53.97/s/gpu Batch (t): 0.5930 LR: 0.000100 Step: 53400 Total Loss: 3.3024 Recon Loss: 3.2882 
[01/06 05:30:23 TiTok]: Data (t): 0.0011, 54.33/s/gpu Batch (t): 0.5890 LR: 0.000100 Step: 53450 Total Loss: 3.3495 Recon Loss: 3.3354 
[01/06 05:30:53 TiTok]: Data (t): 0.0008, 53.97/s/gpu Batch (t): 0.5930 LR: 0.000100 Step: 53500 Total Loss: 3.3667 Recon Loss: 3.3526 
[01/06 05:31:22 TiTok]: Data (t): 0.0008, 53.86/s/gpu Batch (t): 0.5941 LR: 0.000100 Step: 53550 Total Loss: 3.3560 Recon Loss: 3.3419 
[01/06 05:31:52 TiTok]: Data (t): 0.0008, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000100 Step: 53600 Total Loss: 3.3254 Recon Loss: 3.3113 
[01/06 05:32:22 TiTok]: Data (t): 0.0012, 53.97/s/gpu Batch (t): 0.5930 LR: 0.000100 Step: 53650 Total Loss: 3.3699 Recon Loss: 3.3558 
[01/06 05:32:51 TiTok]: Data (t): 0.0011, 53.81/s/gpu Batch (t): 0.5947 LR: 0.000100 Step: 53700 Total Loss: 3.3122 Recon Loss: 3.2981 
[01/06 05:33:21 TiTok]: Data (t): 0.0006, 53.92/s/gpu Batch (t): 0.5935 LR: 0.000100 Step: 53750 Total Loss: 3.3732 Recon Loss: 3.3591 
[01/06 05:33:51 TiTok]: Data (t): 0.0012, 54.32/s/gpu Batch (t): 0.5891 LR: 0.000100 Step: 53800 Total Loss: 3.3134 Recon Loss: 3.2993 
[01/06 05:34:20 TiTok]: Data (t): 0.0012, 53.94/s/gpu Batch (t): 0.5933 LR: 0.000100 Step: 53850 Total Loss: 3.3764 Recon Loss: 3.3623 
[01/06 05:34:50 TiTok]: Data (t): 0.0014, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000100 Step: 53900 Total Loss: 3.3002 Recon Loss: 3.2860 
[01/06 05:35:20 TiTok]: Data (t): 0.0011, 54.26/s/gpu Batch (t): 0.5898 LR: 0.000100 Step: 53950 Total Loss: 3.3619 Recon Loss: 3.3477 
[01/06 05:35:50 TiTok]: Data (t): 0.0007, 43.91/s/gpu Batch (t): 0.7288 LR: 0.000100 Step: 54000 Total Loss: 3.3719 Recon Loss: 3.3577 
[01/06 05:36:19 TiTok]: Data (t): 0.0007, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000100 Step: 54050 Total Loss: 3.3433 Recon Loss: 3.3292 
[01/06 05:36:49 TiTok]: Data (t): 0.0030, 53.64/s/gpu Batch (t): 0.5965 LR: 0.000100 Step: 54100 Total Loss: 3.3306 Recon Loss: 3.3165 
[01/06 05:37:19 TiTok]: Data (t): 0.0007, 53.93/s/gpu Batch (t): 0.5934 LR: 0.000100 Step: 54150 Total Loss: 3.3211 Recon Loss: 3.3069 
[01/06 05:37:49 TiTok]: Data (t): 0.0007, 53.23/s/gpu Batch (t): 0.6012 LR: 0.000100 Step: 54200 Total Loss: 3.3284 Recon Loss: 3.3143 
[01/06 05:38:18 TiTok]: Data (t): 0.0006, 53.50/s/gpu Batch (t): 0.5982 LR: 0.000100 Step: 54250 Total Loss: 3.3119 Recon Loss: 3.2977 
[01/06 05:38:48 TiTok]: Data (t): 0.0007, 53.72/s/gpu Batch (t): 0.5956 LR: 0.000100 Step: 54300 Total Loss: 3.3576 Recon Loss: 3.3434 
[01/06 05:39:18 TiTok]: Data (t): 0.0012, 53.71/s/gpu Batch (t): 0.5958 LR: 0.000100 Step: 54350 Total Loss: 3.3575 Recon Loss: 3.3433 
[01/06 05:39:48 TiTok]: Data (t): 0.0009, 53.88/s/gpu Batch (t): 0.5940 LR: 0.000100 Step: 54400 Total Loss: 3.3588 Recon Loss: 3.3447 
[01/06 05:40:17 TiTok]: Data (t): 0.0013, 55.23/s/gpu Batch (t): 0.5794 LR: 0.000100 Step: 54450 Total Loss: 3.3970 Recon Loss: 3.3829 
[01/06 05:40:47 TiTok]: Data (t): 0.0026, 53.93/s/gpu Batch (t): 0.5934 LR: 0.000100 Step: 54500 Total Loss: 3.3701 Recon Loss: 3.3560 
[01/06 05:41:17 TiTok]: Data (t): 0.0012, 54.36/s/gpu Batch (t): 0.5887 LR: 0.000100 Step: 54550 Total Loss: 3.3573 Recon Loss: 3.3431 
[01/06 05:41:47 TiTok]: Data (t): 0.0011, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000100 Step: 54600 Total Loss: 3.3243 Recon Loss: 3.3102 
[01/06 05:42:16 TiTok]: Data (t): 0.0006, 53.43/s/gpu Batch (t): 0.5989 LR: 0.000100 Step: 54650 Total Loss: 3.3355 Recon Loss: 3.3214 
[01/06 05:42:46 TiTok]: Data (t): 0.0007, 53.29/s/gpu Batch (t): 0.6004 LR: 0.000100 Step: 54700 Total Loss: 3.3423 Recon Loss: 3.3281 
[01/06 05:43:16 TiTok]: Data (t): 0.0011, 53.80/s/gpu Batch (t): 0.5948 LR: 0.000100 Step: 54750 Total Loss: 3.3924 Recon Loss: 3.3783 
[01/06 05:43:46 TiTok]: Data (t): 0.0012, 52.32/s/gpu Batch (t): 0.6116 LR: 0.000100 Step: 54800 Total Loss: 3.3206 Recon Loss: 3.3065 
[01/06 05:44:15 TiTok]: Data (t): 0.0007, 53.11/s/gpu Batch (t): 0.6025 LR: 0.000100 Step: 54850 Total Loss: 3.3301 Recon Loss: 3.3159 
[01/06 05:44:45 TiTok]: Data (t): 0.0011, 54.29/s/gpu Batch (t): 0.5894 LR: 0.000100 Step: 54900 Total Loss: 3.3428 Recon Loss: 3.3287 
[01/06 05:45:15 TiTok]: Data (t): 0.0013, 53.52/s/gpu Batch (t): 0.5979 LR: 0.000100 Step: 54950 Total Loss: 3.3561 Recon Loss: 3.3419 
[01/06 05:45:45 TiTok]: Data (t): 0.0007, 43.72/s/gpu Batch (t): 0.7319 LR: 0.000100 Step: 55000 Total Loss: 3.2714 Recon Loss: 3.2573 
[01/06 05:45:46 TiTok]: Reconstructing images...
[01/06 05:46:22 TiTok]: Data (t): 0.0006, 53.29/s/gpu Batch (t): 0.6005 LR: 0.000100 Step: 55050 Total Loss: 3.3161 Recon Loss: 3.3019 
[01/06 05:46:51 TiTok]: Data (t): 0.0008, 53.50/s/gpu Batch (t): 0.5982 LR: 0.000100 Step: 55100 Total Loss: 3.3201 Recon Loss: 3.3059 
[01/06 05:47:26 TiTok]: Data (t): 0.0012, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000100 Step: 55150 Total Loss: 3.3576 Recon Loss: 3.3435 
Epoch 11/199 started.
[01/06 05:47:57 TiTok]: Data (t): 0.0007, 54.27/s/gpu Batch (t): 0.5896 LR: 0.000100 Step: 55200 Total Loss: 3.3182 Recon Loss: 3.3040 
[01/06 05:48:27 TiTok]: Data (t): 0.0009, 54.04/s/gpu Batch (t): 0.5921 LR: 0.000100 Step: 55250 Total Loss: 3.3323 Recon Loss: 3.3181 
[01/06 05:48:57 TiTok]: Data (t): 0.0007, 53.80/s/gpu Batch (t): 0.5948 LR: 0.000100 Step: 55300 Total Loss: 3.2967 Recon Loss: 3.2826 
[01/06 05:49:26 TiTok]: Data (t): 0.0012, 53.99/s/gpu Batch (t): 0.5927 LR: 0.000100 Step: 55350 Total Loss: 3.3079 Recon Loss: 3.2937 
[01/06 05:49:56 TiTok]: Data (t): 0.0012, 54.00/s/gpu Batch (t): 0.5926 LR: 0.000100 Step: 55400 Total Loss: 3.3633 Recon Loss: 3.3491 
[01/06 05:50:26 TiTok]: Data (t): 0.0007, 53.94/s/gpu Batch (t): 0.5932 LR: 0.000100 Step: 55450 Total Loss: 3.3176 Recon Loss: 3.3035 
[01/06 05:50:55 TiTok]: Data (t): 0.0011, 53.92/s/gpu Batch (t): 0.5935 LR: 0.000100 Step: 55500 Total Loss: 3.2954 Recon Loss: 3.2813 
[01/06 05:51:25 TiTok]: Data (t): 0.0012, 54.22/s/gpu Batch (t): 0.5902 LR: 0.000100 Step: 55550 Total Loss: 3.3234 Recon Loss: 3.3092 
[01/06 05:51:55 TiTok]: Data (t): 0.0007, 53.54/s/gpu Batch (t): 0.5976 LR: 0.000100 Step: 55600 Total Loss: 3.3338 Recon Loss: 3.3197 
[01/06 05:52:25 TiTok]: Data (t): 0.0012, 53.64/s/gpu Batch (t): 0.5966 LR: 0.000100 Step: 55650 Total Loss: 3.3322 Recon Loss: 3.3181 
[01/06 05:52:54 TiTok]: Data (t): 0.0007, 53.52/s/gpu Batch (t): 0.5979 LR: 0.000100 Step: 55700 Total Loss: 3.3222 Recon Loss: 3.3081 
[01/06 05:53:24 TiTok]: Data (t): 0.0007, 53.99/s/gpu Batch (t): 0.5928 LR: 0.000100 Step: 55750 Total Loss: 3.3332 Recon Loss: 3.3191 
[01/06 05:53:54 TiTok]: Data (t): 0.0014, 53.78/s/gpu Batch (t): 0.5950 LR: 0.000100 Step: 55800 Total Loss: 3.3392 Recon Loss: 3.3251 
[01/06 05:54:23 TiTok]: Data (t): 0.0011, 54.39/s/gpu Batch (t): 0.5883 LR: 0.000100 Step: 55850 Total Loss: 3.3207 Recon Loss: 3.3066 
[01/06 05:54:53 TiTok]: Data (t): 0.0006, 53.93/s/gpu Batch (t): 0.5934 LR: 0.000100 Step: 55900 Total Loss: 3.3364 Recon Loss: 3.3223 
[01/06 05:55:23 TiTok]: Data (t): 0.0011, 54.15/s/gpu Batch (t): 0.5910 LR: 0.000100 Step: 55950 Total Loss: 3.3187 Recon Loss: 3.3046 
[01/06 05:55:53 TiTok]: Data (t): 0.0022, 45.54/s/gpu Batch (t): 0.7027 LR: 0.000100 Step: 56000 Total Loss: 3.3395 Recon Loss: 3.3253 
[01/06 05:56:22 TiTok]: Data (t): 0.0011, 54.34/s/gpu Batch (t): 0.5889 LR: 0.000100 Step: 56050 Total Loss: 3.3362 Recon Loss: 3.3221 
[01/06 05:56:52 TiTok]: Data (t): 0.0011, 54.39/s/gpu Batch (t): 0.5883 LR: 0.000100 Step: 56100 Total Loss: 3.3322 Recon Loss: 3.3180 
[01/06 05:57:22 TiTok]: Data (t): 0.0012, 54.35/s/gpu Batch (t): 0.5888 LR: 0.000100 Step: 56150 Total Loss: 3.3352 Recon Loss: 3.3210 
[01/06 05:57:51 TiTok]: Data (t): 0.0012, 53.92/s/gpu Batch (t): 0.5934 LR: 0.000100 Step: 56200 Total Loss: 3.3030 Recon Loss: 3.2888 
[01/06 05:58:21 TiTok]: Data (t): 0.0011, 53.94/s/gpu Batch (t): 0.5933 LR: 0.000100 Step: 56250 Total Loss: 3.3409 Recon Loss: 3.3268 
[01/06 05:58:51 TiTok]: Data (t): 0.0012, 53.96/s/gpu Batch (t): 0.5930 LR: 0.000100 Step: 56300 Total Loss: 3.2929 Recon Loss: 3.2788 
[01/06 05:59:21 TiTok]: Data (t): 0.0011, 54.31/s/gpu Batch (t): 0.5892 LR: 0.000100 Step: 56350 Total Loss: 3.3318 Recon Loss: 3.3176 
[01/06 05:59:50 TiTok]: Data (t): 0.0007, 53.48/s/gpu Batch (t): 0.5983 LR: 0.000100 Step: 56400 Total Loss: 3.2917 Recon Loss: 3.2775 
[01/06 06:00:20 TiTok]: Data (t): 0.0007, 53.50/s/gpu Batch (t): 0.5981 LR: 0.000100 Step: 56450 Total Loss: 3.3066 Recon Loss: 3.2924 
[01/06 06:00:50 TiTok]: Data (t): 0.0009, 53.66/s/gpu Batch (t): 0.5963 LR: 0.000100 Step: 56500 Total Loss: 3.2990 Recon Loss: 3.2848 
[01/06 06:01:19 TiTok]: Data (t): 0.0007, 53.43/s/gpu Batch (t): 0.5989 LR: 0.000100 Step: 56550 Total Loss: 3.3443 Recon Loss: 3.3302 
[01/06 06:01:49 TiTok]: Data (t): 0.0011, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000100 Step: 56600 Total Loss: 3.3352 Recon Loss: 3.3211 
[01/06 06:02:19 TiTok]: Data (t): 0.0012, 53.35/s/gpu Batch (t): 0.5998 LR: 0.000100 Step: 56650 Total Loss: 3.3099 Recon Loss: 3.2958 
[01/06 06:02:49 TiTok]: Data (t): 0.0007, 54.01/s/gpu Batch (t): 0.5924 LR: 0.000100 Step: 56700 Total Loss: 3.3212 Recon Loss: 3.3070 
[01/06 06:03:18 TiTok]: Data (t): 0.0012, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000100 Step: 56750 Total Loss: 3.2935 Recon Loss: 3.2792 
[01/06 06:03:48 TiTok]: Data (t): 0.0011, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000100 Step: 56800 Total Loss: 3.3456 Recon Loss: 3.3314 
[01/06 06:04:18 TiTok]: Data (t): 0.0011, 54.02/s/gpu Batch (t): 0.5924 LR: 0.000100 Step: 56850 Total Loss: 3.3449 Recon Loss: 3.3307 
[01/06 06:04:47 TiTok]: Data (t): 0.0011, 53.99/s/gpu Batch (t): 0.5927 LR: 0.000100 Step: 56900 Total Loss: 3.3080 Recon Loss: 3.2939 
[01/06 06:05:17 TiTok]: Data (t): 0.0012, 53.99/s/gpu Batch (t): 0.5927 LR: 0.000100 Step: 56950 Total Loss: 3.3428 Recon Loss: 3.3286 
[01/06 06:05:47 TiTok]: Data (t): 0.0012, 42.29/s/gpu Batch (t): 0.7566 LR: 0.000100 Step: 57000 Total Loss: 3.3102 Recon Loss: 3.2960 
[01/06 06:06:17 TiTok]: Data (t): 0.0011, 53.51/s/gpu Batch (t): 0.5980 LR: 0.000099 Step: 57050 Total Loss: 3.3430 Recon Loss: 3.3288 
[01/06 06:06:46 TiTok]: Data (t): 0.0011, 53.94/s/gpu Batch (t): 0.5933 LR: 0.000099 Step: 57100 Total Loss: 3.3057 Recon Loss: 3.2915 
[01/06 06:07:16 TiTok]: Data (t): 0.0006, 53.78/s/gpu Batch (t): 0.5950 LR: 0.000099 Step: 57150 Total Loss: 3.3219 Recon Loss: 3.3078 
[01/06 06:07:46 TiTok]: Data (t): 0.0012, 54.37/s/gpu Batch (t): 0.5885 LR: 0.000099 Step: 57200 Total Loss: 3.3568 Recon Loss: 3.3427 
[01/06 06:08:16 TiTok]: Data (t): 0.0007, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000099 Step: 57250 Total Loss: 3.2979 Recon Loss: 3.2837 
[01/06 06:08:45 TiTok]: Data (t): 0.0012, 54.40/s/gpu Batch (t): 0.5882 LR: 0.000099 Step: 57300 Total Loss: 3.3218 Recon Loss: 3.3076 
[01/06 06:09:15 TiTok]: Data (t): 0.0007, 53.80/s/gpu Batch (t): 0.5948 LR: 0.000099 Step: 57350 Total Loss: 3.2826 Recon Loss: 3.2685 
[01/06 06:09:45 TiTok]: Data (t): 0.0011, 53.81/s/gpu Batch (t): 0.5947 LR: 0.000099 Step: 57400 Total Loss: 3.3146 Recon Loss: 3.3004 
[01/06 06:10:14 TiTok]: Data (t): 0.0012, 54.24/s/gpu Batch (t): 0.5900 LR: 0.000099 Step: 57450 Total Loss: 3.3149 Recon Loss: 3.3008 
[01/06 06:10:44 TiTok]: Data (t): 0.0007, 53.57/s/gpu Batch (t): 0.5973 LR: 0.000099 Step: 57500 Total Loss: 3.3455 Recon Loss: 3.3313 
[01/06 06:11:14 TiTok]: Data (t): 0.0011, 53.90/s/gpu Batch (t): 0.5936 LR: 0.000099 Step: 57550 Total Loss: 3.3175 Recon Loss: 3.3033 
[01/06 06:11:43 TiTok]: Data (t): 0.0011, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000099 Step: 57600 Total Loss: 3.3454 Recon Loss: 3.3312 
[01/06 06:12:13 TiTok]: Data (t): 0.0007, 53.94/s/gpu Batch (t): 0.5932 LR: 0.000099 Step: 57650 Total Loss: 3.3001 Recon Loss: 3.2860 
[01/06 06:12:43 TiTok]: Data (t): 0.0008, 53.75/s/gpu Batch (t): 0.5954 LR: 0.000099 Step: 57700 Total Loss: 3.2947 Recon Loss: 3.2805 
[01/06 06:13:13 TiTok]: Data (t): 0.0011, 54.37/s/gpu Batch (t): 0.5885 LR: 0.000099 Step: 57750 Total Loss: 3.3260 Recon Loss: 3.3118 
[01/06 06:13:42 TiTok]: Data (t): 0.0007, 53.48/s/gpu Batch (t): 0.5983 LR: 0.000099 Step: 57800 Total Loss: 3.3755 Recon Loss: 3.3613 
[01/06 06:14:12 TiTok]: Data (t): 0.0006, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000099 Step: 57850 Total Loss: 3.3281 Recon Loss: 3.3139 
[01/06 06:14:42 TiTok]: Data (t): 0.0012, 53.97/s/gpu Batch (t): 0.5930 LR: 0.000099 Step: 57900 Total Loss: 3.3269 Recon Loss: 3.3127 
[01/06 06:15:12 TiTok]: Data (t): 0.0012, 54.15/s/gpu Batch (t): 0.5909 LR: 0.000099 Step: 57950 Total Loss: 3.3503 Recon Loss: 3.3361 
[01/06 06:15:41 TiTok]: Data (t): 0.0007, 47.18/s/gpu Batch (t): 0.6783 LR: 0.000099 Step: 58000 Total Loss: 3.3112 Recon Loss: 3.2970 
[01/06 06:16:11 TiTok]: Data (t): 0.0011, 54.39/s/gpu Batch (t): 0.5884 LR: 0.000099 Step: 58050 Total Loss: 3.3044 Recon Loss: 3.2903 
[01/06 06:16:41 TiTok]: Data (t): 0.0007, 53.38/s/gpu Batch (t): 0.5995 LR: 0.000099 Step: 58100 Total Loss: 3.2772 Recon Loss: 3.2630 
[01/06 06:17:10 TiTok]: Data (t): 0.0006, 53.85/s/gpu Batch (t): 0.5942 LR: 0.000099 Step: 58150 Total Loss: 3.3189 Recon Loss: 3.3047 
[01/06 06:17:40 TiTok]: Data (t): 0.0008, 53.92/s/gpu Batch (t): 0.5935 LR: 0.000099 Step: 58200 Total Loss: 3.3278 Recon Loss: 3.3137 
[01/06 06:18:10 TiTok]: Data (t): 0.0007, 53.99/s/gpu Batch (t): 0.5927 LR: 0.000099 Step: 58250 Total Loss: 3.2904 Recon Loss: 3.2762 
[01/06 06:18:39 TiTok]: Data (t): 0.0011, 53.84/s/gpu Batch (t): 0.5944 LR: 0.000099 Step: 58300 Total Loss: 3.3353 Recon Loss: 3.3212 
[01/06 06:19:09 TiTok]: Data (t): 0.0011, 53.94/s/gpu Batch (t): 0.5933 LR: 0.000099 Step: 58350 Total Loss: 3.3026 Recon Loss: 3.2885 
[01/06 06:19:39 TiTok]: Data (t): 0.0011, 54.34/s/gpu Batch (t): 0.5889 LR: 0.000099 Step: 58400 Total Loss: 3.2893 Recon Loss: 3.2751 
[01/06 06:20:09 TiTok]: Data (t): 0.0007, 53.84/s/gpu Batch (t): 0.5944 LR: 0.000099 Step: 58450 Total Loss: 3.2895 Recon Loss: 3.2754 
[01/06 06:20:38 TiTok]: Data (t): 0.0009, 53.42/s/gpu Batch (t): 0.5990 LR: 0.000099 Step: 58500 Total Loss: 3.3182 Recon Loss: 3.3040 
[01/06 06:21:08 TiTok]: Data (t): 0.0007, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000099 Step: 58550 Total Loss: 3.3136 Recon Loss: 3.2995 
[01/06 06:21:38 TiTok]: Data (t): 0.0007, 53.86/s/gpu Batch (t): 0.5941 LR: 0.000099 Step: 58600 Total Loss: 3.3144 Recon Loss: 3.3002 
[01/06 06:22:08 TiTok]: Data (t): 0.0007, 53.40/s/gpu Batch (t): 0.5993 LR: 0.000099 Step: 58650 Total Loss: 3.3348 Recon Loss: 3.3207 
[01/06 06:22:37 TiTok]: Data (t): 0.0026, 54.69/s/gpu Batch (t): 0.5852 LR: 0.000099 Step: 58700 Total Loss: 3.3239 Recon Loss: 3.3098 
[01/06 06:23:07 TiTok]: Data (t): 0.0007, 53.96/s/gpu Batch (t): 0.5930 LR: 0.000099 Step: 58750 Total Loss: 3.3029 Recon Loss: 3.2888 
[01/06 06:23:37 TiTok]: Data (t): 0.0011, 54.53/s/gpu Batch (t): 0.5868 LR: 0.000099 Step: 58800 Total Loss: 3.3193 Recon Loss: 3.3052 
[01/06 06:24:06 TiTok]: Data (t): 0.0007, 53.49/s/gpu Batch (t): 0.5983 LR: 0.000099 Step: 58850 Total Loss: 3.3154 Recon Loss: 3.3012 
[01/06 06:24:36 TiTok]: Data (t): 0.0009, 53.52/s/gpu Batch (t): 0.5979 LR: 0.000099 Step: 58900 Total Loss: 3.3109 Recon Loss: 3.2968 
[01/06 06:25:06 TiTok]: Data (t): 0.0011, 53.76/s/gpu Batch (t): 0.5952 LR: 0.000099 Step: 58950 Total Loss: 3.3135 Recon Loss: 3.2993 
[01/06 06:25:36 TiTok]: Data (t): 0.0009, 44.21/s/gpu Batch (t): 0.7239 LR: 0.000099 Step: 59000 Total Loss: 3.2964 Recon Loss: 3.2822 
[01/06 06:26:05 TiTok]: Data (t): 0.0012, 52.03/s/gpu Batch (t): 0.6150 LR: 0.000099 Step: 59050 Total Loss: 3.2928 Recon Loss: 3.2787 
[01/06 06:26:35 TiTok]: Data (t): 0.0007, 53.70/s/gpu Batch (t): 0.5959 LR: 0.000099 Step: 59100 Total Loss: 3.3082 Recon Loss: 3.2940 
[01/06 06:27:05 TiTok]: Data (t): 0.0009, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000099 Step: 59150 Total Loss: 3.2892 Recon Loss: 3.2751 
[01/06 06:27:34 TiTok]: Data (t): 0.0011, 54.27/s/gpu Batch (t): 0.5897 LR: 0.000099 Step: 59200 Total Loss: 3.3325 Recon Loss: 3.3184 
[01/06 06:28:04 TiTok]: Data (t): 0.0011, 54.33/s/gpu Batch (t): 0.5890 LR: 0.000099 Step: 59250 Total Loss: 3.2964 Recon Loss: 3.2822 
[01/06 06:28:34 TiTok]: Data (t): 0.0006, 53.51/s/gpu Batch (t): 0.5981 LR: 0.000099 Step: 59300 Total Loss: 3.3034 Recon Loss: 3.2892 
[01/06 06:29:04 TiTok]: Data (t): 0.0007, 53.44/s/gpu Batch (t): 0.5988 LR: 0.000099 Step: 59350 Total Loss: 3.2935 Recon Loss: 3.2793 
[01/06 06:29:33 TiTok]: Data (t): 0.0011, 53.81/s/gpu Batch (t): 0.5947 LR: 0.000099 Step: 59400 Total Loss: 3.3238 Recon Loss: 3.3096 
[01/06 06:30:03 TiTok]: Data (t): 0.0007, 54.09/s/gpu Batch (t): 0.5916 LR: 0.000099 Step: 59450 Total Loss: 3.2794 Recon Loss: 3.2653 
[01/06 06:30:33 TiTok]: Data (t): 0.0012, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000099 Step: 59500 Total Loss: 3.2897 Recon Loss: 3.2755 
[01/06 06:31:02 TiTok]: Data (t): 0.0006, 53.51/s/gpu Batch (t): 0.5980 LR: 0.000099 Step: 59550 Total Loss: 3.3153 Recon Loss: 3.3011 
[01/06 06:31:32 TiTok]: Data (t): 0.0011, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000099 Step: 59600 Total Loss: 3.3646 Recon Loss: 3.3505 
[01/06 06:32:02 TiTok]: Data (t): 0.0007, 53.50/s/gpu Batch (t): 0.5981 LR: 0.000099 Step: 59650 Total Loss: 3.3364 Recon Loss: 3.3222 
[01/06 06:32:32 TiTok]: Data (t): 0.0011, 53.85/s/gpu Batch (t): 0.5942 LR: 0.000099 Step: 59700 Total Loss: 3.2822 Recon Loss: 3.2680 
[01/06 06:33:01 TiTok]: Data (t): 0.0012, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000099 Step: 59750 Total Loss: 3.3044 Recon Loss: 3.2903 
[01/06 06:33:31 TiTok]: Data (t): 0.0006, 52.66/s/gpu Batch (t): 0.6077 LR: 0.000099 Step: 59800 Total Loss: 3.2969 Recon Loss: 3.2827 
[01/06 06:34:01 TiTok]: Data (t): 0.0012, 53.96/s/gpu Batch (t): 0.5930 LR: 0.000099 Step: 59850 Total Loss: 3.2500 Recon Loss: 3.2358 
[01/06 06:34:30 TiTok]: Data (t): 0.0006, 53.86/s/gpu Batch (t): 0.5941 LR: 0.000099 Step: 59900 Total Loss: 3.3456 Recon Loss: 3.3315 
[01/06 06:35:00 TiTok]: Data (t): 0.0007, 53.36/s/gpu Batch (t): 0.5996 LR: 0.000099 Step: 59950 Total Loss: 3.3148 Recon Loss: 3.3006 
[01/06 06:35:30 TiTok]: Data (t): 0.0012, 45.02/s/gpu Batch (t): 0.7109 LR: 0.000099 Step: 60000 Total Loss: 3.2331 Recon Loss: 3.2189 
[01/06 06:35:31 TiTok]: Reconstructing images...
[01/06 06:36:07 TiTok]: Data (t): 0.0007, 53.56/s/gpu Batch (t): 0.5974 LR: 0.000099 Step: 60050 Total Loss: 3.3197 Recon Loss: 3.3055 
[01/06 06:36:36 TiTok]: Data (t): 0.0008, 53.88/s/gpu Batch (t): 0.5940 LR: 0.000099 Step: 60100 Total Loss: 3.3387 Recon Loss: 3.3245 
[01/06 06:37:06 TiTok]: Data (t): 0.0012, 54.35/s/gpu Batch (t): 0.5888 LR: 0.000099 Step: 60150 Total Loss: 3.3026 Recon Loss: 3.2885 
Epoch 12/199 started.
[01/06 06:37:38 TiTok]: Data (t): 0.0006, 53.56/s/gpu Batch (t): 0.5975 LR: 0.000099 Step: 60200 Total Loss: 3.3118 Recon Loss: 3.2976 
[01/06 06:38:07 TiTok]: Data (t): 0.0007, 53.59/s/gpu Batch (t): 0.5971 LR: 0.000099 Step: 60250 Total Loss: 3.2952 Recon Loss: 3.2811 
[01/06 06:38:37 TiTok]: Data (t): 0.0007, 53.95/s/gpu Batch (t): 0.5932 LR: 0.000099 Step: 60300 Total Loss: 3.3048 Recon Loss: 3.2906 
[01/06 06:39:07 TiTok]: Data (t): 0.0011, 53.93/s/gpu Batch (t): 0.5934 LR: 0.000099 Step: 60350 Total Loss: 3.2791 Recon Loss: 3.2649 
[01/06 06:39:36 TiTok]: Data (t): 0.0011, 54.41/s/gpu Batch (t): 0.5882 LR: 0.000099 Step: 60400 Total Loss: 3.3074 Recon Loss: 3.2932 
[01/06 06:40:06 TiTok]: Data (t): 0.0006, 53.94/s/gpu Batch (t): 0.5932 LR: 0.000099 Step: 60450 Total Loss: 3.3129 Recon Loss: 3.2987 
[01/06 06:40:36 TiTok]: Data (t): 0.0011, 52.60/s/gpu Batch (t): 0.6084 LR: 0.000099 Step: 60500 Total Loss: 3.3463 Recon Loss: 3.3321 
[01/06 06:41:05 TiTok]: Data (t): 0.0011, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000099 Step: 60550 Total Loss: 3.3296 Recon Loss: 3.3154 
[01/06 06:41:35 TiTok]: Data (t): 0.0011, 53.73/s/gpu Batch (t): 0.5956 LR: 0.000099 Step: 60600 Total Loss: 3.3020 Recon Loss: 3.2879 
[01/06 06:42:05 TiTok]: Data (t): 0.0011, 53.84/s/gpu Batch (t): 0.5944 LR: 0.000099 Step: 60650 Total Loss: 3.2595 Recon Loss: 3.2454 
[01/06 06:42:35 TiTok]: Data (t): 0.0012, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000099 Step: 60700 Total Loss: 3.2941 Recon Loss: 3.2800 
[01/06 06:43:04 TiTok]: Data (t): 0.0012, 53.92/s/gpu Batch (t): 0.5934 LR: 0.000099 Step: 60750 Total Loss: 3.2892 Recon Loss: 3.2750 
[01/06 06:43:34 TiTok]: Data (t): 0.0013, 53.92/s/gpu Batch (t): 0.5935 LR: 0.000099 Step: 60800 Total Loss: 3.3027 Recon Loss: 3.2885 
[01/06 06:44:04 TiTok]: Data (t): 0.0007, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000099 Step: 60850 Total Loss: 3.2647 Recon Loss: 3.2505 
[01/06 06:44:33 TiTok]: Data (t): 0.0011, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000099 Step: 60900 Total Loss: 3.2916 Recon Loss: 3.2774 
[01/06 06:45:03 TiTok]: Data (t): 0.0007, 53.86/s/gpu Batch (t): 0.5941 LR: 0.000099 Step: 60950 Total Loss: 3.3406 Recon Loss: 3.3265 
[01/06 06:45:33 TiTok]: Data (t): 0.0007, 44.52/s/gpu Batch (t): 0.7187 LR: 0.000099 Step: 61000 Total Loss: 3.2730 Recon Loss: 3.2589 
[01/06 06:46:03 TiTok]: Data (t): 0.0007, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000099 Step: 61050 Total Loss: 3.2967 Recon Loss: 3.2825 
[01/06 06:46:32 TiTok]: Data (t): 0.0011, 54.43/s/gpu Batch (t): 0.5879 LR: 0.000099 Step: 61100 Total Loss: 3.2858 Recon Loss: 3.2717 
[01/06 06:47:02 TiTok]: Data (t): 0.0007, 53.20/s/gpu Batch (t): 0.6015 LR: 0.000099 Step: 61150 Total Loss: 3.2646 Recon Loss: 3.2505 
[01/06 06:47:32 TiTok]: Data (t): 0.0009, 53.78/s/gpu Batch (t): 0.5950 LR: 0.000099 Step: 61200 Total Loss: 3.2993 Recon Loss: 3.2852 
[01/06 06:48:02 TiTok]: Data (t): 0.0007, 53.31/s/gpu Batch (t): 0.6002 LR: 0.000099 Step: 61250 Total Loss: 3.2841 Recon Loss: 3.2699 
[01/06 06:48:31 TiTok]: Data (t): 0.0011, 53.82/s/gpu Batch (t): 0.5945 LR: 0.000099 Step: 61300 Total Loss: 3.3085 Recon Loss: 3.2943 
[01/06 06:49:01 TiTok]: Data (t): 0.0011, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000099 Step: 61350 Total Loss: 3.2833 Recon Loss: 3.2691 
[01/06 06:49:31 TiTok]: Data (t): 0.0009, 53.58/s/gpu Batch (t): 0.5972 LR: 0.000099 Step: 61400 Total Loss: 3.3039 Recon Loss: 3.2898 
[01/06 06:50:00 TiTok]: Data (t): 0.0015, 54.27/s/gpu Batch (t): 0.5896 LR: 0.000099 Step: 61450 Total Loss: 3.2958 Recon Loss: 3.2817 
[01/06 06:50:30 TiTok]: Data (t): 0.0006, 53.53/s/gpu Batch (t): 0.5978 LR: 0.000099 Step: 61500 Total Loss: 3.2956 Recon Loss: 3.2815 
[01/06 06:51:00 TiTok]: Data (t): 0.0012, 54.01/s/gpu Batch (t): 0.5925 LR: 0.000099 Step: 61550 Total Loss: 3.3038 Recon Loss: 3.2897 
[01/06 06:51:30 TiTok]: Data (t): 0.0007, 53.94/s/gpu Batch (t): 0.5933 LR: 0.000099 Step: 61600 Total Loss: 3.3308 Recon Loss: 3.3166 
[01/06 06:51:59 TiTok]: Data (t): 0.0007, 53.39/s/gpu Batch (t): 0.5994 LR: 0.000099 Step: 61650 Total Loss: 3.2774 Recon Loss: 3.2633 
[01/06 06:52:29 TiTok]: Data (t): 0.0011, 54.39/s/gpu Batch (t): 0.5883 LR: 0.000099 Step: 61700 Total Loss: 3.2738 Recon Loss: 3.2596 
[01/06 06:52:59 TiTok]: Data (t): 0.0007, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000099 Step: 61750 Total Loss: 3.3169 Recon Loss: 3.3027 
[01/06 06:53:28 TiTok]: Data (t): 0.0011, 53.94/s/gpu Batch (t): 0.5932 LR: 0.000099 Step: 61800 Total Loss: 3.2593 Recon Loss: 3.2451 
[01/06 06:53:58 TiTok]: Data (t): 0.0007, 53.36/s/gpu Batch (t): 0.5997 LR: 0.000099 Step: 61850 Total Loss: 3.2689 Recon Loss: 3.2547 
[01/06 06:54:28 TiTok]: Data (t): 0.0007, 53.52/s/gpu Batch (t): 0.5979 LR: 0.000099 Step: 61900 Total Loss: 3.3040 Recon Loss: 3.2899 
[01/06 06:54:58 TiTok]: Data (t): 0.0007, 53.38/s/gpu Batch (t): 0.5995 LR: 0.000099 Step: 61950 Total Loss: 3.2725 Recon Loss: 3.2583 
[01/06 06:55:27 TiTok]: Data (t): 0.0012, 47.83/s/gpu Batch (t): 0.6690 LR: 0.000099 Step: 62000 Total Loss: 3.2989 Recon Loss: 3.2847 
[01/06 06:55:57 TiTok]: Data (t): 0.0011, 53.99/s/gpu Batch (t): 0.5927 LR: 0.000099 Step: 62050 Total Loss: 3.2639 Recon Loss: 3.2497 
[01/06 06:56:27 TiTok]: Data (t): 0.0012, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000099 Step: 62100 Total Loss: 3.2773 Recon Loss: 3.2631 
[01/06 06:56:56 TiTok]: Data (t): 0.0011, 54.43/s/gpu Batch (t): 0.5879 LR: 0.000099 Step: 62150 Total Loss: 3.2815 Recon Loss: 3.2674 
[01/06 06:57:26 TiTok]: Data (t): 0.0015, 55.33/s/gpu Batch (t): 0.5783 LR: 0.000099 Step: 62200 Total Loss: 3.2857 Recon Loss: 3.2716 
[01/06 06:57:56 TiTok]: Data (t): 0.0007, 53.51/s/gpu Batch (t): 0.5980 LR: 0.000099 Step: 62250 Total Loss: 3.2914 Recon Loss: 3.2773 
[01/06 06:58:26 TiTok]: Data (t): 0.0008, 53.36/s/gpu Batch (t): 0.5997 LR: 0.000099 Step: 62300 Total Loss: 3.2797 Recon Loss: 3.2656 
[01/06 06:58:55 TiTok]: Data (t): 0.0012, 53.94/s/gpu Batch (t): 0.5932 LR: 0.000099 Step: 62350 Total Loss: 3.2213 Recon Loss: 3.2071 
[01/06 06:59:25 TiTok]: Data (t): 0.0007, 54.00/s/gpu Batch (t): 0.5925 LR: 0.000099 Step: 62400 Total Loss: 3.2893 Recon Loss: 3.2752 
[01/06 06:59:55 TiTok]: Data (t): 0.0012, 53.77/s/gpu Batch (t): 0.5951 LR: 0.000099 Step: 62450 Total Loss: 3.2953 Recon Loss: 3.2811 
[01/06 07:00:24 TiTok]: Data (t): 0.0007, 53.92/s/gpu Batch (t): 0.5934 LR: 0.000099 Step: 62500 Total Loss: 3.2846 Recon Loss: 3.2704 
[01/06 07:00:54 TiTok]: Data (t): 0.0007, 53.25/s/gpu Batch (t): 0.6010 LR: 0.000099 Step: 62550 Total Loss: 3.2536 Recon Loss: 3.2394 
[01/06 07:01:24 TiTok]: Data (t): 0.0012, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000099 Step: 62600 Total Loss: 3.2797 Recon Loss: 3.2655 
[01/06 07:01:54 TiTok]: Data (t): 0.0012, 54.36/s/gpu Batch (t): 0.5887 LR: 0.000099 Step: 62650 Total Loss: 3.3108 Recon Loss: 3.2966 
[01/06 07:02:23 TiTok]: Data (t): 0.0012, 53.97/s/gpu Batch (t): 0.5930 LR: 0.000099 Step: 62700 Total Loss: 3.2968 Recon Loss: 3.2826 
[01/06 07:02:53 TiTok]: Data (t): 0.0008, 53.93/s/gpu Batch (t): 0.5934 LR: 0.000099 Step: 62750 Total Loss: 3.2681 Recon Loss: 3.2539 
[01/06 07:03:23 TiTok]: Data (t): 0.0007, 53.32/s/gpu Batch (t): 0.6001 LR: 0.000099 Step: 62800 Total Loss: 3.2877 Recon Loss: 3.2736 
[01/06 07:03:53 TiTok]: Data (t): 0.0012, 53.73/s/gpu Batch (t): 0.5956 LR: 0.000099 Step: 62850 Total Loss: 3.2556 Recon Loss: 3.2414 
[01/06 07:04:22 TiTok]: Data (t): 0.0007, 53.81/s/gpu Batch (t): 0.5947 LR: 0.000099 Step: 62900 Total Loss: 3.2733 Recon Loss: 3.2591 
[01/06 07:04:52 TiTok]: Data (t): 0.0014, 54.36/s/gpu Batch (t): 0.5886 LR: 0.000099 Step: 62950 Total Loss: 3.3208 Recon Loss: 3.3067 
[01/06 07:05:22 TiTok]: Data (t): 0.0007, 43.59/s/gpu Batch (t): 0.7340 LR: 0.000099 Step: 63000 Total Loss: 3.3271 Recon Loss: 3.3130 
[01/06 07:05:52 TiTok]: Data (t): 0.0011, 53.94/s/gpu Batch (t): 0.5932 LR: 0.000099 Step: 63050 Total Loss: 3.2923 Recon Loss: 3.2781 
[01/06 07:06:21 TiTok]: Data (t): 0.0007, 53.51/s/gpu Batch (t): 0.5980 LR: 0.000099 Step: 63100 Total Loss: 3.2950 Recon Loss: 3.2808 
[01/06 07:06:51 TiTok]: Data (t): 0.0014, 54.15/s/gpu Batch (t): 0.5910 LR: 0.000099 Step: 63150 Total Loss: 3.2725 Recon Loss: 3.2584 
[01/06 07:07:21 TiTok]: Data (t): 0.0012, 53.96/s/gpu Batch (t): 0.5930 LR: 0.000099 Step: 63200 Total Loss: 3.3030 Recon Loss: 3.2888 
[01/06 07:07:51 TiTok]: Data (t): 0.0012, 53.71/s/gpu Batch (t): 0.5958 LR: 0.000099 Step: 63250 Total Loss: 3.3283 Recon Loss: 3.3141 
[01/06 07:08:20 TiTok]: Data (t): 0.0014, 53.85/s/gpu Batch (t): 0.5942 LR: 0.000099 Step: 63300 Total Loss: 3.3008 Recon Loss: 3.2867 
[01/06 07:08:50 TiTok]: Data (t): 0.0012, 53.66/s/gpu Batch (t): 0.5964 LR: 0.000099 Step: 63350 Total Loss: 3.3448 Recon Loss: 3.3306 
[01/06 07:09:20 TiTok]: Data (t): 0.0007, 53.33/s/gpu Batch (t): 0.6001 LR: 0.000099 Step: 63400 Total Loss: 3.2784 Recon Loss: 3.2643 
[01/06 07:09:49 TiTok]: Data (t): 0.0011, 54.30/s/gpu Batch (t): 0.5894 LR: 0.000099 Step: 63450 Total Loss: 3.2992 Recon Loss: 3.2850 
[01/06 07:10:19 TiTok]: Data (t): 0.0011, 54.29/s/gpu Batch (t): 0.5894 LR: 0.000099 Step: 63500 Total Loss: 3.2970 Recon Loss: 3.2829 
[01/06 07:10:49 TiTok]: Data (t): 0.0008, 53.44/s/gpu Batch (t): 0.5988 LR: 0.000099 Step: 63550 Total Loss: 3.2567 Recon Loss: 3.2426 
[01/06 07:11:19 TiTok]: Data (t): 0.0006, 53.43/s/gpu Batch (t): 0.5989 LR: 0.000099 Step: 63600 Total Loss: 3.2738 Recon Loss: 3.2596 
[01/06 07:11:48 TiTok]: Data (t): 0.0011, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000099 Step: 63650 Total Loss: 3.2692 Recon Loss: 3.2551 
[01/06 07:12:18 TiTok]: Data (t): 0.0011, 54.38/s/gpu Batch (t): 0.5885 LR: 0.000099 Step: 63700 Total Loss: 3.2287 Recon Loss: 3.2146 
[01/06 07:12:48 TiTok]: Data (t): 0.0012, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000099 Step: 63750 Total Loss: 3.2691 Recon Loss: 3.2549 
[01/06 07:13:18 TiTok]: Data (t): 0.0007, 53.52/s/gpu Batch (t): 0.5980 LR: 0.000099 Step: 63800 Total Loss: 3.2152 Recon Loss: 3.2010 
[01/06 07:13:48 TiTok]: Data (t): 0.0012, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000099 Step: 63850 Total Loss: 3.3273 Recon Loss: 3.3131 
[01/06 07:14:17 TiTok]: Data (t): 0.0006, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000099 Step: 63900 Total Loss: 3.3052 Recon Loss: 3.2910 
[01/06 07:14:47 TiTok]: Data (t): 0.0012, 53.99/s/gpu Batch (t): 0.5927 LR: 0.000099 Step: 63950 Total Loss: 3.2571 Recon Loss: 3.2429 
[01/06 07:15:17 TiTok]: Data (t): 0.0037, 43.83/s/gpu Batch (t): 0.7301 LR: 0.000099 Step: 64000 Total Loss: 3.2706 Recon Loss: 3.2564 
[01/06 07:15:47 TiTok]: Data (t): 0.0007, 53.51/s/gpu Batch (t): 0.5980 LR: 0.000099 Step: 64050 Total Loss: 3.2533 Recon Loss: 3.2392 
[01/06 07:16:16 TiTok]: Data (t): 0.0011, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000099 Step: 64100 Total Loss: 3.3026 Recon Loss: 3.2884 
[01/06 07:16:46 TiTok]: Data (t): 0.0014, 53.85/s/gpu Batch (t): 0.5943 LR: 0.000099 Step: 64150 Total Loss: 3.2297 Recon Loss: 3.2156 
[01/06 07:17:16 TiTok]: Data (t): 0.0012, 53.95/s/gpu Batch (t): 0.5931 LR: 0.000099 Step: 64200 Total Loss: 3.2879 Recon Loss: 3.2737 
[01/06 07:17:45 TiTok]: Data (t): 0.0012, 54.39/s/gpu Batch (t): 0.5884 LR: 0.000099 Step: 64250 Total Loss: 3.2773 Recon Loss: 3.2631 
[01/06 07:18:15 TiTok]: Data (t): 0.0012, 53.97/s/gpu Batch (t): 0.5929 LR: 0.000099 Step: 64300 Total Loss: 3.3172 Recon Loss: 3.3030 
[01/06 07:18:45 TiTok]: Data (t): 0.0012, 54.35/s/gpu Batch (t): 0.5888 LR: 0.000099 Step: 64350 Total Loss: 3.2693 Recon Loss: 3.2552 
[01/06 07:19:15 TiTok]: Data (t): 0.0011, 53.80/s/gpu Batch (t): 0.5948 LR: 0.000099 Step: 64400 Total Loss: 3.2904 Recon Loss: 3.2762 
[01/06 07:19:44 TiTok]: Data (t): 0.0006, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000099 Step: 64450 Total Loss: 3.2653 Recon Loss: 3.2512 
[01/06 07:20:14 TiTok]: Data (t): 0.0011, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000099 Step: 64500 Total Loss: 3.2908 Recon Loss: 3.2766 
[01/06 07:20:44 TiTok]: Data (t): 0.0012, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000099 Step: 64550 Total Loss: 3.2713 Recon Loss: 3.2571 
[01/06 07:21:14 TiTok]: Data (t): 0.0017, 54.53/s/gpu Batch (t): 0.5869 LR: 0.000099 Step: 64600 Total Loss: 3.2624 Recon Loss: 3.2482 
[01/06 07:21:43 TiTok]: Data (t): 0.0007, 53.44/s/gpu Batch (t): 0.5988 LR: 0.000099 Step: 64650 Total Loss: 3.2555 Recon Loss: 3.2413 
[01/06 07:22:13 TiTok]: Data (t): 0.0012, 53.78/s/gpu Batch (t): 0.5950 LR: 0.000099 Step: 64700 Total Loss: 3.2545 Recon Loss: 3.2403 
[01/06 07:22:43 TiTok]: Data (t): 0.0012, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000099 Step: 64750 Total Loss: 3.2489 Recon Loss: 3.2348 
[01/06 07:23:13 TiTok]: Data (t): 0.0012, 53.68/s/gpu Batch (t): 0.5962 LR: 0.000099 Step: 64800 Total Loss: 3.3103 Recon Loss: 3.2962 
[01/06 07:23:42 TiTok]: Data (t): 0.0007, 53.84/s/gpu Batch (t): 0.5943 LR: 0.000099 Step: 64850 Total Loss: 3.2743 Recon Loss: 3.2601 
[01/06 07:24:12 TiTok]: Data (t): 0.0011, 53.81/s/gpu Batch (t): 0.5946 LR: 0.000099 Step: 64900 Total Loss: 3.2580 Recon Loss: 3.2439 
[01/06 07:24:42 TiTok]: Data (t): 0.0007, 52.25/s/gpu Batch (t): 0.6124 LR: 0.000099 Step: 64950 Total Loss: 3.2551 Recon Loss: 3.2410 
[01/06 07:25:12 TiTok]: Data (t): 0.0011, 45.64/s/gpu Batch (t): 0.7011 LR: 0.000099 Step: 65000 Total Loss: 3.2491 Recon Loss: 3.2349 
[01/06 07:25:13 TiTok]: Reconstructing images...
[01/06 07:25:49 TiTok]: Data (t): 0.0011, 53.86/s/gpu Batch (t): 0.5941 LR: 0.000099 Step: 65050 Total Loss: 3.2666 Recon Loss: 3.2524 
[01/06 07:26:18 TiTok]: Data (t): 0.0007, 53.54/s/gpu Batch (t): 0.5977 LR: 0.000099 Step: 65100 Total Loss: 3.2381 Recon Loss: 3.2240 
[01/06 07:26:48 TiTok]: Data (t): 0.0012, 53.84/s/gpu Batch (t): 0.5944 LR: 0.000099 Step: 65150 Total Loss: 3.2172 Recon Loss: 3.2031 
[01/06 07:27:18 TiTok]: Data (t): 0.0011, 54.42/s/gpu Batch (t): 0.5880 LR: 0.000099 Step: 65200 Total Loss: 3.2311 Recon Loss: 3.2170 
Epoch 13/199 started.
[01/06 07:27:49 TiTok]: Data (t): 0.0013, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000099 Step: 65250 Total Loss: 3.2533 Recon Loss: 3.2391 
[01/06 07:28:19 TiTok]: Data (t): 0.0007, 53.42/s/gpu Batch (t): 0.5991 LR: 0.000099 Step: 65300 Total Loss: 3.2539 Recon Loss: 3.2398 
[01/06 07:28:49 TiTok]: Data (t): 0.0012, 54.25/s/gpu Batch (t): 0.5898 LR: 0.000099 Step: 65350 Total Loss: 3.2695 Recon Loss: 3.2554 
[01/06 07:29:18 TiTok]: Data (t): 0.0012, 53.79/s/gpu Batch (t): 0.5949 LR: 0.000099 Step: 65400 Total Loss: 3.2403 Recon Loss: 3.2262 
[01/06 07:29:48 TiTok]: Data (t): 0.0007, 53.89/s/gpu Batch (t): 0.5939 LR: 0.000099 Step: 65450 Total Loss: 3.2923 Recon Loss: 3.2781 
[01/06 07:30:18 TiTok]: Data (t): 0.0007, 53.39/s/gpu Batch (t): 0.5993 LR: 0.000099 Step: 65500 Total Loss: 3.2717 Recon Loss: 3.2576 
[01/06 07:30:48 TiTok]: Data (t): 0.0007, 53.48/s/gpu Batch (t): 0.5984 LR: 0.000099 Step: 65550 Total Loss: 3.2540 Recon Loss: 3.2399 
[01/06 07:31:17 TiTok]: Data (t): 0.0007, 53.30/s/gpu Batch (t): 0.6004 LR: 0.000099 Step: 65600 Total Loss: 3.2776 Recon Loss: 3.2634 
[01/06 07:31:47 TiTok]: Data (t): 0.0012, 53.78/s/gpu Batch (t): 0.5951 LR: 0.000099 Step: 65650 Total Loss: 3.2698 Recon Loss: 3.2556 
[01/06 07:32:17 TiTok]: Data (t): 0.0007, 53.50/s/gpu Batch (t): 0.5981 LR: 0.000099 Step: 65700 Total Loss: 3.2219 Recon Loss: 3.2078 
[01/06 07:32:47 TiTok]: Data (t): 0.0007, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000099 Step: 65750 Total Loss: 3.2370 Recon Loss: 3.2229 
[01/06 07:33:16 TiTok]: Data (t): 0.0012, 53.75/s/gpu Batch (t): 0.5953 LR: 0.000099 Step: 65800 Total Loss: 3.2680 Recon Loss: 3.2539 
[01/06 07:33:46 TiTok]: Data (t): 0.0012, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000099 Step: 65850 Total Loss: 3.2789 Recon Loss: 3.2647 
[01/06 07:34:16 TiTok]: Data (t): 0.0007, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000099 Step: 65900 Total Loss: 3.2358 Recon Loss: 3.2217 
[01/06 07:34:45 TiTok]: Data (t): 0.0012, 54.27/s/gpu Batch (t): 0.5896 LR: 0.000099 Step: 65950 Total Loss: 3.2432 Recon Loss: 3.2290 
[01/06 07:35:15 TiTok]: Data (t): 0.0007, 45.57/s/gpu Batch (t): 0.7022 LR: 0.000099 Step: 66000 Total Loss: 3.2766 Recon Loss: 3.2625 
[01/06 07:35:45 TiTok]: Data (t): 0.0011, 53.77/s/gpu Batch (t): 0.5951 LR: 0.000099 Step: 66050 Total Loss: 3.2879 Recon Loss: 3.2738 
[01/06 07:36:15 TiTok]: Data (t): 0.0012, 53.92/s/gpu Batch (t): 0.5935 LR: 0.000099 Step: 66100 Total Loss: 3.2553 Recon Loss: 3.2411 
[01/06 07:36:44 TiTok]: Data (t): 0.0012, 53.97/s/gpu Batch (t): 0.5930 LR: 0.000099 Step: 66150 Total Loss: 3.2861 Recon Loss: 3.2720 
[01/06 07:37:14 TiTok]: Data (t): 0.0012, 53.85/s/gpu Batch (t): 0.5943 LR: 0.000099 Step: 66200 Total Loss: 3.2831 Recon Loss: 3.2689 
[01/06 07:37:44 TiTok]: Data (t): 0.0011, 53.94/s/gpu Batch (t): 0.5933 LR: 0.000099 Step: 66250 Total Loss: 3.2638 Recon Loss: 3.2496 
[01/06 07:38:14 TiTok]: Data (t): 0.0012, 54.39/s/gpu Batch (t): 0.5883 LR: 0.000099 Step: 66300 Total Loss: 3.2800 Recon Loss: 3.2658 
[01/06 07:38:43 TiTok]: Data (t): 0.0007, 53.99/s/gpu Batch (t): 0.5927 LR: 0.000099 Step: 66350 Total Loss: 3.2912 Recon Loss: 3.2770 
[01/06 07:39:13 TiTok]: Data (t): 0.0007, 53.40/s/gpu Batch (t): 0.5993 LR: 0.000099 Step: 66400 Total Loss: 3.2584 Recon Loss: 3.2443 
[01/06 07:39:43 TiTok]: Data (t): 0.0012, 53.87/s/gpu Batch (t): 0.5941 LR: 0.000099 Step: 66450 Total Loss: 3.2556 Recon Loss: 3.2414 
[01/06 07:40:12 TiTok]: Data (t): 0.0015, 53.85/s/gpu Batch (t): 0.5942 LR: 0.000099 Step: 66500 Total Loss: 3.2729 Recon Loss: 3.2588 
[01/06 07:40:42 TiTok]: Data (t): 0.0011, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000099 Step: 66550 Total Loss: 3.2774 Recon Loss: 3.2633 
[01/06 07:41:12 TiTok]: Data (t): 0.0007, 53.45/s/gpu Batch (t): 0.5986 LR: 0.000099 Step: 66600 Total Loss: 3.2444 Recon Loss: 3.2302 
[01/06 07:41:42 TiTok]: Data (t): 0.0007, 53.47/s/gpu Batch (t): 0.5985 LR: 0.000099 Step: 66650 Total Loss: 3.2880 Recon Loss: 3.2738 
[01/06 07:42:11 TiTok]: Data (t): 0.0008, 53.86/s/gpu Batch (t): 0.5941 LR: 0.000099 Step: 66700 Total Loss: 3.2356 Recon Loss: 3.2215 
[01/06 07:42:41 TiTok]: Data (t): 0.0012, 53.85/s/gpu Batch (t): 0.5942 LR: 0.000099 Step: 66750 Total Loss: 3.3027 Recon Loss: 3.2885 
[01/06 07:43:11 TiTok]: Data (t): 0.0007, 53.88/s/gpu Batch (t): 0.5940 LR: 0.000099 Step: 66800 Total Loss: 3.2567 Recon Loss: 3.2425 
[01/06 07:43:40 TiTok]: Data (t): 0.0007, 53.94/s/gpu Batch (t): 0.5933 LR: 0.000099 Step: 66850 Total Loss: 3.2562 Recon Loss: 3.2420 
[01/06 07:44:10 TiTok]: Data (t): 0.0007, 53.72/s/gpu Batch (t): 0.5957 LR: 0.000099 Step: 66900 Total Loss: 3.2619 Recon Loss: 3.2478 
[01/06 07:44:40 TiTok]: Data (t): 0.0011, 53.58/s/gpu Batch (t): 0.5973 LR: 0.000099 Step: 66950 Total Loss: 3.2498 Recon Loss: 3.2356 
[01/06 07:45:10 TiTok]: Data (t): 0.0011, 43.64/s/gpu Batch (t): 0.7333 LR: 0.000099 Step: 67000 Total Loss: 3.2683 Recon Loss: 3.2542 
[01/06 07:45:39 TiTok]: Data (t): 0.0012, 54.12/s/gpu Batch (t): 0.5913 LR: 0.000099 Step: 67050 Total Loss: 3.2807 Recon Loss: 3.2665 
[01/06 07:46:09 TiTok]: Data (t): 0.0011, 54.02/s/gpu Batch (t): 0.5923 LR: 0.000099 Step: 67100 Total Loss: 3.2786 Recon Loss: 3.2645 
[01/06 07:46:39 TiTok]: Data (t): 0.0014, 54.31/s/gpu Batch (t): 0.5892 LR: 0.000099 Step: 67150 Total Loss: 3.2399 Recon Loss: 3.2257 
[01/06 07:47:09 TiTok]: Data (t): 0.0015, 54.35/s/gpu Batch (t): 0.5888 LR: 0.000099 Step: 67200 Total Loss: 3.2953 Recon Loss: 3.2811 
[01/06 07:47:38 TiTok]: Data (t): 0.0011, 54.29/s/gpu Batch (t): 0.5894 LR: 0.000099 Step: 67250 Total Loss: 3.2607 Recon Loss: 3.2466 
[01/06 07:48:08 TiTok]: Data (t): 0.0007, 53.88/s/gpu Batch (t): 0.5940 LR: 0.000099 Step: 67300 Total Loss: 3.2625 Recon Loss: 3.2484 
[01/06 07:48:38 TiTok]: Data (t): 0.0010, 53.48/s/gpu Batch (t): 0.5983 LR: 0.000099 Step: 67350 Total Loss: 3.2466 Recon Loss: 3.2324 
[01/06 07:49:08 TiTok]: Data (t): 0.0011, 53.96/s/gpu Batch (t): 0.5931 LR: 0.000099 Step: 67400 Total Loss: 3.2519 Recon Loss: 3.2378 
[01/06 07:49:37 TiTok]: Data (t): 0.0011, 53.75/s/gpu Batch (t): 0.5954 LR: 0.000099 Step: 67450 Total Loss: 3.2541 Recon Loss: 3.2399 
[01/06 07:50:07 TiTok]: Data (t): 0.0014, 53.84/s/gpu Batch (t): 0.5944 LR: 0.000099 Step: 67500 Total Loss: 3.2988 Recon Loss: 3.2847 
[01/06 07:50:37 TiTok]: Data (t): 0.0011, 53.93/s/gpu Batch (t): 0.5933 LR: 0.000099 Step: 67550 Total Loss: 3.2394 Recon Loss: 3.2253 
[01/06 07:51:07 TiTok]: Data (t): 0.0011, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000099 Step: 67600 Total Loss: 3.2298 Recon Loss: 3.2156 
[01/06 07:51:36 TiTok]: Data (t): 0.0015, 54.42/s/gpu Batch (t): 0.5880 LR: 0.000099 Step: 67650 Total Loss: 3.2520 Recon Loss: 3.2379 
[01/06 07:52:06 TiTok]: Data (t): 0.0006, 53.85/s/gpu Batch (t): 0.5943 LR: 0.000099 Step: 67700 Total Loss: 3.2875 Recon Loss: 3.2734 
[01/06 07:52:36 TiTok]: Data (t): 0.0009, 53.44/s/gpu Batch (t): 0.5988 LR: 0.000099 Step: 67750 Total Loss: 3.2797 Recon Loss: 3.2655 
[01/06 07:53:06 TiTok]: Data (t): 0.0011, 54.29/s/gpu Batch (t): 0.5894 LR: 0.000099 Step: 67800 Total Loss: 3.2641 Recon Loss: 3.2499 
[01/06 07:53:35 TiTok]: Data (t): 0.0011, 54.28/s/gpu Batch (t): 0.5896 LR: 0.000099 Step: 67850 Total Loss: 3.2500 Recon Loss: 3.2358 
[01/06 07:54:05 TiTok]: Data (t): 0.0011, 53.74/s/gpu Batch (t): 0.5955 LR: 0.000099 Step: 67900 Total Loss: 3.2898 Recon Loss: 3.2756 
[01/06 07:54:35 TiTok]: Data (t): 0.0011, 53.84/s/gpu Batch (t): 0.5944 LR: 0.000099 Step: 67950 Total Loss: 3.2678 Recon Loss: 3.2536 
[01/06 07:55:05 TiTok]: Data (t): 0.0012, 44.54/s/gpu Batch (t): 0.7184 LR: 0.000099 Step: 68000 Total Loss: 3.2412 Recon Loss: 3.2270 
[01/06 07:55:34 TiTok]: Data (t): 0.0011, 54.26/s/gpu Batch (t): 0.5898 LR: 0.000099 Step: 68050 Total Loss: 3.2414 Recon Loss: 3.2273 
[01/06 07:56:04 TiTok]: Data (t): 0.0007, 53.31/s/gpu Batch (t): 0.6002 LR: 0.000099 Step: 68100 Total Loss: 3.2727 Recon Loss: 3.2586 
[01/06 07:56:34 TiTok]: Data (t): 0.0007, 53.87/s/gpu Batch (t): 0.5941 LR: 0.000099 Step: 68150 Total Loss: 3.2588 Recon Loss: 3.2446 
[01/06 07:57:04 TiTok]: Data (t): 0.0011, 53.74/s/gpu Batch (t): 0.5955 LR: 0.000099 Step: 68200 Total Loss: 3.2165 Recon Loss: 3.2024 
[01/06 07:57:33 TiTok]: Data (t): 0.0011, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000099 Step: 68250 Total Loss: 3.2337 Recon Loss: 3.2196 
[01/06 07:58:03 TiTok]: Data (t): 0.0011, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000099 Step: 68300 Total Loss: 3.2309 Recon Loss: 3.2167 
[01/06 07:58:33 TiTok]: Data (t): 0.0007, 53.43/s/gpu Batch (t): 0.5989 LR: 0.000099 Step: 68350 Total Loss: 3.2774 Recon Loss: 3.2632 
[01/06 07:59:03 TiTok]: Data (t): 0.0014, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000099 Step: 68400 Total Loss: 3.1799 Recon Loss: 3.1657 
[01/06 07:59:32 TiTok]: Data (t): 0.0011, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000099 Step: 68450 Total Loss: 3.2577 Recon Loss: 3.2436 
[01/06 08:00:02 TiTok]: Data (t): 0.0011, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000099 Step: 68500 Total Loss: 3.2532 Recon Loss: 3.2391 
[01/06 08:00:32 TiTok]: Data (t): 0.0013, 54.36/s/gpu Batch (t): 0.5886 LR: 0.000099 Step: 68550 Total Loss: 3.2090 Recon Loss: 3.1948 
[01/06 08:01:01 TiTok]: Data (t): 0.0011, 53.66/s/gpu Batch (t): 0.5964 LR: 0.000099 Step: 68600 Total Loss: 3.2515 Recon Loss: 3.2374 
[01/06 08:01:31 TiTok]: Data (t): 0.0007, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000099 Step: 68650 Total Loss: 3.2443 Recon Loss: 3.2301 
[01/06 08:02:01 TiTok]: Data (t): 0.0011, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000099 Step: 68700 Total Loss: 3.2266 Recon Loss: 3.2124 
[01/06 08:02:31 TiTok]: Data (t): 0.0011, 53.96/s/gpu Batch (t): 0.5931 LR: 0.000099 Step: 68750 Total Loss: 3.1990 Recon Loss: 3.1848 
[01/06 08:03:00 TiTok]: Data (t): 0.0008, 53.44/s/gpu Batch (t): 0.5988 LR: 0.000099 Step: 68800 Total Loss: 3.2424 Recon Loss: 3.2282 
[01/06 08:03:30 TiTok]: Data (t): 0.0012, 53.88/s/gpu Batch (t): 0.5940 LR: 0.000099 Step: 68850 Total Loss: 3.2256 Recon Loss: 3.2115 
[01/06 08:04:00 TiTok]: Data (t): 0.0007, 53.81/s/gpu Batch (t): 0.5947 LR: 0.000099 Step: 68900 Total Loss: 3.2420 Recon Loss: 3.2278 
[01/06 08:04:30 TiTok]: Data (t): 0.0012, 53.81/s/gpu Batch (t): 0.5947 LR: 0.000099 Step: 68950 Total Loss: 3.2500 Recon Loss: 3.2359 
[01/06 08:04:59 TiTok]: Data (t): 0.0011, 47.99/s/gpu Batch (t): 0.6668 LR: 0.000099 Step: 69000 Total Loss: 3.2796 Recon Loss: 3.2654 
[01/06 08:05:29 TiTok]: Data (t): 0.0011, 53.92/s/gpu Batch (t): 0.5935 LR: 0.000099 Step: 69050 Total Loss: 3.2680 Recon Loss: 3.2538 
[01/06 08:05:59 TiTok]: Data (t): 0.0012, 51.86/s/gpu Batch (t): 0.6171 LR: 0.000099 Step: 69100 Total Loss: 3.2702 Recon Loss: 3.2561 
[01/06 08:06:29 TiTok]: Data (t): 0.0007, 53.50/s/gpu Batch (t): 0.5982 LR: 0.000099 Step: 69150 Total Loss: 3.2236 Recon Loss: 3.2094 
[01/06 08:06:58 TiTok]: Data (t): 0.0011, 53.97/s/gpu Batch (t): 0.5929 LR: 0.000099 Step: 69200 Total Loss: 3.2355 Recon Loss: 3.2213 
[01/06 08:07:28 TiTok]: Data (t): 0.0012, 53.65/s/gpu Batch (t): 0.5964 LR: 0.000099 Step: 69250 Total Loss: 3.2171 Recon Loss: 3.2029 
[01/06 08:07:58 TiTok]: Data (t): 0.0011, 53.92/s/gpu Batch (t): 0.5935 LR: 0.000099 Step: 69300 Total Loss: 3.2453 Recon Loss: 3.2311 
[01/06 08:08:28 TiTok]: Data (t): 0.0011, 53.72/s/gpu Batch (t): 0.5956 LR: 0.000099 Step: 69350 Total Loss: 3.2298 Recon Loss: 3.2156 
[01/06 08:08:57 TiTok]: Data (t): 0.0011, 53.98/s/gpu Batch (t): 0.5929 LR: 0.000099 Step: 69400 Total Loss: 3.2418 Recon Loss: 3.2277 
[01/06 08:09:27 TiTok]: Data (t): 0.0018, 53.34/s/gpu Batch (t): 0.5999 LR: 0.000099 Step: 69450 Total Loss: 3.2681 Recon Loss: 3.2539 
[01/06 08:09:57 TiTok]: Data (t): 0.0011, 53.78/s/gpu Batch (t): 0.5951 LR: 0.000099 Step: 69500 Total Loss: 3.2800 Recon Loss: 3.2658 
[01/06 08:10:26 TiTok]: Data (t): 0.0007, 53.52/s/gpu Batch (t): 0.5979 LR: 0.000099 Step: 69550 Total Loss: 3.2670 Recon Loss: 3.2528 
[01/06 08:10:56 TiTok]: Data (t): 0.0011, 53.71/s/gpu Batch (t): 0.5957 LR: 0.000099 Step: 69600 Total Loss: 3.2055 Recon Loss: 3.1913 
[01/06 08:11:26 TiTok]: Data (t): 0.0011, 53.86/s/gpu Batch (t): 0.5942 LR: 0.000099 Step: 69650 Total Loss: 3.2420 Recon Loss: 3.2278 
[01/06 08:11:56 TiTok]: Data (t): 0.0011, 53.62/s/gpu Batch (t): 0.5968 LR: 0.000099 Step: 69700 Total Loss: 3.2724 Recon Loss: 3.2583 
[01/06 08:12:25 TiTok]: Data (t): 0.0011, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000099 Step: 69750 Total Loss: 3.2426 Recon Loss: 3.2285 
[01/06 08:12:55 TiTok]: Data (t): 0.0011, 53.75/s/gpu Batch (t): 0.5953 LR: 0.000099 Step: 69800 Total Loss: 3.2116 Recon Loss: 3.1974 
[01/06 08:13:25 TiTok]: Data (t): 0.0006, 53.34/s/gpu Batch (t): 0.5999 LR: 0.000099 Step: 69850 Total Loss: 3.2009 Recon Loss: 3.1868 
[01/06 08:13:55 TiTok]: Data (t): 0.0011, 53.79/s/gpu Batch (t): 0.5950 LR: 0.000099 Step: 69900 Total Loss: 3.2306 Recon Loss: 3.2164 
[01/06 08:14:24 TiTok]: Data (t): 0.0011, 52.69/s/gpu Batch (t): 0.6074 LR: 0.000099 Step: 69950 Total Loss: 3.2128 Recon Loss: 3.1987 
[01/06 08:14:54 TiTok]: Data (t): 0.0008, 45.73/s/gpu Batch (t): 0.6998 LR: 0.000099 Step: 70000 Total Loss: 3.2359 Recon Loss: 3.2217 
[01/06 08:14:56 TiTok]: Reconstructing images...
[01/06 08:15:29 TiTok]: Data (t): 0.0011, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000099 Step: 70050 Total Loss: 3.2299 Recon Loss: 3.2158 
[01/06 08:15:59 TiTok]: Data (t): 0.0011, 53.85/s/gpu Batch (t): 0.5943 LR: 0.000099 Step: 70100 Total Loss: 3.2520 Recon Loss: 3.2378 
[01/06 08:16:28 TiTok]: Data (t): 0.0008, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000099 Step: 70150 Total Loss: 3.2444 Recon Loss: 3.2303 
[01/06 08:16:58 TiTok]: Data (t): 0.0012, 54.04/s/gpu Batch (t): 0.5921 LR: 0.000099 Step: 70200 Total Loss: 3.2432 Recon Loss: 3.2290 
Epoch 14/199 started.
[01/06 08:17:30 TiTok]: Data (t): 0.0011, 54.18/s/gpu Batch (t): 0.5907 LR: 0.000099 Step: 70250 Total Loss: 3.2059 Recon Loss: 3.1917 
[01/06 08:17:59 TiTok]: Data (t): 0.0011, 53.96/s/gpu Batch (t): 0.5930 LR: 0.000099 Step: 70300 Total Loss: 3.2138 Recon Loss: 3.1997 
[01/06 08:18:29 TiTok]: Data (t): 0.0007, 53.97/s/gpu Batch (t): 0.5930 LR: 0.000099 Step: 70350 Total Loss: 3.2640 Recon Loss: 3.2498 
[01/06 08:18:59 TiTok]: Data (t): 0.0014, 53.97/s/gpu Batch (t): 0.5929 LR: 0.000099 Step: 70400 Total Loss: 3.2516 Recon Loss: 3.2374 
[01/06 08:19:29 TiTok]: Data (t): 0.0012, 53.69/s/gpu Batch (t): 0.5960 LR: 0.000099 Step: 70450 Total Loss: 3.2074 Recon Loss: 3.1933 
[01/06 08:19:58 TiTok]: Data (t): 0.0012, 53.93/s/gpu Batch (t): 0.5934 LR: 0.000099 Step: 70500 Total Loss: 3.2968 Recon Loss: 3.2826 
[01/06 08:20:28 TiTok]: Data (t): 0.0011, 52.40/s/gpu Batch (t): 0.6106 LR: 0.000099 Step: 70550 Total Loss: 3.2291 Recon Loss: 3.2149 
[01/06 08:20:58 TiTok]: Data (t): 0.0008, 53.47/s/gpu Batch (t): 0.5984 LR: 0.000099 Step: 70600 Total Loss: 3.2549 Recon Loss: 3.2408 
[01/06 08:21:27 TiTok]: Data (t): 0.0011, 53.95/s/gpu Batch (t): 0.5932 LR: 0.000099 Step: 70650 Total Loss: 3.2567 Recon Loss: 3.2425 
[01/06 08:21:57 TiTok]: Data (t): 0.0011, 53.94/s/gpu Batch (t): 0.5933 LR: 0.000099 Step: 70700 Total Loss: 3.2315 Recon Loss: 3.2173 
[01/06 08:22:27 TiTok]: Data (t): 0.0007, 52.93/s/gpu Batch (t): 0.6046 LR: 0.000099 Step: 70750 Total Loss: 3.2206 Recon Loss: 3.2065 
[01/06 08:22:57 TiTok]: Data (t): 0.0011, 53.80/s/gpu Batch (t): 0.5948 LR: 0.000099 Step: 70800 Total Loss: 3.2496 Recon Loss: 3.2355 
[01/06 08:23:26 TiTok]: Data (t): 0.0012, 54.24/s/gpu Batch (t): 0.5900 LR: 0.000099 Step: 70850 Total Loss: 3.2722 Recon Loss: 3.2580 
[01/06 08:23:56 TiTok]: Data (t): 0.0011, 53.79/s/gpu Batch (t): 0.5949 LR: 0.000099 Step: 70900 Total Loss: 3.2067 Recon Loss: 3.1926 
[01/06 08:24:26 TiTok]: Data (t): 0.0011, 53.80/s/gpu Batch (t): 0.5948 LR: 0.000099 Step: 70950 Total Loss: 3.2492 Recon Loss: 3.2350 
[01/06 08:24:56 TiTok]: Data (t): 0.0007, 44.00/s/gpu Batch (t): 0.7273 LR: 0.000099 Step: 71000 Total Loss: 3.3091 Recon Loss: 3.2950 
[01/06 08:25:25 TiTok]: Data (t): 0.0011, 53.96/s/gpu Batch (t): 0.5930 LR: 0.000099 Step: 71050 Total Loss: 3.2267 Recon Loss: 3.2125 
[01/06 08:25:55 TiTok]: Data (t): 0.0007, 53.95/s/gpu Batch (t): 0.5932 LR: 0.000099 Step: 71100 Total Loss: 3.2434 Recon Loss: 3.2292 
[01/06 08:26:25 TiTok]: Data (t): 0.0011, 54.28/s/gpu Batch (t): 0.5895 LR: 0.000099 Step: 71150 Total Loss: 3.2804 Recon Loss: 3.2662 
[01/06 08:26:54 TiTok]: Data (t): 0.0011, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000099 Step: 71200 Total Loss: 3.2312 Recon Loss: 3.2170 
[01/06 08:27:24 TiTok]: Data (t): 0.0012, 54.06/s/gpu Batch (t): 0.5920 LR: 0.000099 Step: 71250 Total Loss: 3.2226 Recon Loss: 3.2084 
[01/06 08:27:54 TiTok]: Data (t): 0.0012, 54.53/s/gpu Batch (t): 0.5868 LR: 0.000099 Step: 71300 Total Loss: 3.2683 Recon Loss: 3.2541 
[01/06 08:28:24 TiTok]: Data (t): 0.0007, 53.81/s/gpu Batch (t): 0.5947 LR: 0.000099 Step: 71350 Total Loss: 3.2485 Recon Loss: 3.2343 
[01/06 08:28:53 TiTok]: Data (t): 0.0012, 54.36/s/gpu Batch (t): 0.5887 LR: 0.000099 Step: 71400 Total Loss: 3.2196 Recon Loss: 3.2055 
[01/06 08:29:23 TiTok]: Data (t): 0.0007, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000099 Step: 71450 Total Loss: 3.2558 Recon Loss: 3.2417 
[01/06 08:29:53 TiTok]: Data (t): 0.0011, 53.77/s/gpu Batch (t): 0.5951 LR: 0.000099 Step: 71500 Total Loss: 3.2460 Recon Loss: 3.2319 
[01/06 08:30:22 TiTok]: Data (t): 0.0011, 53.94/s/gpu Batch (t): 0.5932 LR: 0.000099 Step: 71550 Total Loss: 3.2253 Recon Loss: 3.2112 
[01/06 08:30:52 TiTok]: Data (t): 0.0011, 54.39/s/gpu Batch (t): 0.5883 LR: 0.000099 Step: 71600 Total Loss: 3.2321 Recon Loss: 3.2180 
[01/06 08:31:22 TiTok]: Data (t): 0.0007, 53.90/s/gpu Batch (t): 0.5936 LR: 0.000099 Step: 71650 Total Loss: 3.2309 Recon Loss: 3.2167 
[01/06 08:31:51 TiTok]: Data (t): 0.0007, 53.35/s/gpu Batch (t): 0.5998 LR: 0.000099 Step: 71700 Total Loss: 3.2474 Recon Loss: 3.2332 
[01/06 08:32:21 TiTok]: Data (t): 0.0007, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000099 Step: 71750 Total Loss: 3.2681 Recon Loss: 3.2539 
[01/06 08:32:51 TiTok]: Data (t): 0.0007, 53.81/s/gpu Batch (t): 0.5947 LR: 0.000099 Step: 71800 Total Loss: 3.2148 Recon Loss: 3.2007 
[01/06 08:33:21 TiTok]: Data (t): 0.0011, 53.99/s/gpu Batch (t): 0.5927 LR: 0.000099 Step: 71850 Total Loss: 3.2149 Recon Loss: 3.2008 
[01/06 08:33:50 TiTok]: Data (t): 0.0011, 52.56/s/gpu Batch (t): 0.6088 LR: 0.000099 Step: 71900 Total Loss: 3.2798 Recon Loss: 3.2656 
[01/06 08:34:20 TiTok]: Data (t): 0.0011, 53.78/s/gpu Batch (t): 0.5950 LR: 0.000099 Step: 71950 Total Loss: 3.2340 Recon Loss: 3.2198 
[01/06 08:34:50 TiTok]: Data (t): 0.0012, 44.56/s/gpu Batch (t): 0.7181 LR: 0.000099 Step: 72000 Total Loss: 3.2396 Recon Loss: 3.2254 
[01/06 08:35:20 TiTok]: Data (t): 0.0007, 53.52/s/gpu Batch (t): 0.5979 LR: 0.000099 Step: 72050 Total Loss: 3.2034 Recon Loss: 3.1892 
[01/06 08:35:49 TiTok]: Data (t): 0.0015, 54.47/s/gpu Batch (t): 0.5874 LR: 0.000099 Step: 72100 Total Loss: 3.2297 Recon Loss: 3.2156 
[01/06 08:36:19 TiTok]: Data (t): 0.0011, 53.73/s/gpu Batch (t): 0.5956 LR: 0.000099 Step: 72150 Total Loss: 3.2548 Recon Loss: 3.2407 
[01/06 08:36:49 TiTok]: Data (t): 0.0006, 53.46/s/gpu Batch (t): 0.5986 LR: 0.000099 Step: 72200 Total Loss: 3.2732 Recon Loss: 3.2591 
[01/06 08:37:19 TiTok]: Data (t): 0.0014, 54.83/s/gpu Batch (t): 0.5836 LR: 0.000099 Step: 72250 Total Loss: 3.2364 Recon Loss: 3.2222 
[01/06 08:37:48 TiTok]: Data (t): 0.0011, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000099 Step: 72300 Total Loss: 3.2486 Recon Loss: 3.2344 
[01/06 08:38:18 TiTok]: Data (t): 0.0011, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000099 Step: 72350 Total Loss: 3.2372 Recon Loss: 3.2231 
[01/06 08:38:48 TiTok]: Data (t): 0.0012, 53.85/s/gpu Batch (t): 0.5943 LR: 0.000099 Step: 72400 Total Loss: 3.2336 Recon Loss: 3.2195 
[01/06 08:39:17 TiTok]: Data (t): 0.0012, 53.85/s/gpu Batch (t): 0.5943 LR: 0.000099 Step: 72450 Total Loss: 3.1979 Recon Loss: 3.1838 
[01/06 08:39:47 TiTok]: Data (t): 0.0008, 53.41/s/gpu Batch (t): 0.5991 LR: 0.000099 Step: 72500 Total Loss: 3.2442 Recon Loss: 3.2300 
[01/06 08:40:17 TiTok]: Data (t): 0.0013, 53.81/s/gpu Batch (t): 0.5947 LR: 0.000099 Step: 72550 Total Loss: 3.2173 Recon Loss: 3.2032 
[01/06 08:40:47 TiTok]: Data (t): 0.0013, 53.15/s/gpu Batch (t): 0.6020 LR: 0.000099 Step: 72600 Total Loss: 3.2156 Recon Loss: 3.2014 
[01/06 08:41:16 TiTok]: Data (t): 0.0007, 53.97/s/gpu Batch (t): 0.5929 LR: 0.000099 Step: 72650 Total Loss: 3.2460 Recon Loss: 3.2318 
[01/06 08:41:46 TiTok]: Data (t): 0.0011, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000099 Step: 72700 Total Loss: 3.2156 Recon Loss: 3.2014 
[01/06 08:42:16 TiTok]: Data (t): 0.0013, 54.19/s/gpu Batch (t): 0.5905 LR: 0.000099 Step: 72750 Total Loss: 3.2155 Recon Loss: 3.2014 
[01/06 08:42:46 TiTok]: Data (t): 0.0012, 53.97/s/gpu Batch (t): 0.5929 LR: 0.000099 Step: 72800 Total Loss: 3.2533 Recon Loss: 3.2392 
[01/06 08:43:15 TiTok]: Data (t): 0.0012, 54.94/s/gpu Batch (t): 0.5825 LR: 0.000099 Step: 72850 Total Loss: 3.2295 Recon Loss: 3.2153 
[01/06 08:43:45 TiTok]: Data (t): 0.0012, 53.72/s/gpu Batch (t): 0.5957 LR: 0.000099 Step: 72900 Total Loss: 3.2404 Recon Loss: 3.2262 
[01/06 08:44:15 TiTok]: Data (t): 0.0015, 54.32/s/gpu Batch (t): 0.5891 LR: 0.000099 Step: 72950 Total Loss: 3.2311 Recon Loss: 3.2169 
[01/06 08:44:45 TiTok]: Data (t): 0.0015, 44.92/s/gpu Batch (t): 0.7123 LR: 0.000099 Step: 73000 Total Loss: 3.2184 Recon Loss: 3.2043 
[01/06 08:45:14 TiTok]: Data (t): 0.0012, 54.00/s/gpu Batch (t): 0.5926 LR: 0.000099 Step: 73050 Total Loss: 3.2029 Recon Loss: 3.1887 
[01/06 08:45:44 TiTok]: Data (t): 0.0012, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000099 Step: 73100 Total Loss: 3.2153 Recon Loss: 3.2011 
[01/06 08:46:14 TiTok]: Data (t): 0.0009, 53.94/s/gpu Batch (t): 0.5933 LR: 0.000099 Step: 73150 Total Loss: 3.2332 Recon Loss: 3.2191 
[01/06 08:46:43 TiTok]: Data (t): 0.0011, 53.97/s/gpu Batch (t): 0.5930 LR: 0.000099 Step: 73200 Total Loss: 3.1717 Recon Loss: 3.1576 
[01/06 08:47:13 TiTok]: Data (t): 0.0011, 53.86/s/gpu Batch (t): 0.5942 LR: 0.000099 Step: 73250 Total Loss: 3.2333 Recon Loss: 3.2191 
[01/06 08:47:43 TiTok]: Data (t): 0.0011, 53.81/s/gpu Batch (t): 0.5946 LR: 0.000099 Step: 73300 Total Loss: 3.2250 Recon Loss: 3.2109 
[01/06 08:48:13 TiTok]: Data (t): 0.0011, 53.95/s/gpu Batch (t): 0.5931 LR: 0.000099 Step: 73350 Total Loss: 3.2351 Recon Loss: 3.2209 
[01/06 08:48:42 TiTok]: Data (t): 0.0011, 53.96/s/gpu Batch (t): 0.5930 LR: 0.000099 Step: 73400 Total Loss: 3.2172 Recon Loss: 3.2031 
[01/06 08:49:12 TiTok]: Data (t): 0.0012, 53.80/s/gpu Batch (t): 0.5948 LR: 0.000099 Step: 73450 Total Loss: 3.2488 Recon Loss: 3.2346 
[01/06 08:49:42 TiTok]: Data (t): 0.0007, 53.83/s/gpu Batch (t): 0.5944 LR: 0.000099 Step: 73500 Total Loss: 3.2279 Recon Loss: 3.2137 
[01/06 08:50:11 TiTok]: Data (t): 0.0011, 53.72/s/gpu Batch (t): 0.5957 LR: 0.000099 Step: 73550 Total Loss: 3.2156 Recon Loss: 3.2015 
[01/06 08:50:41 TiTok]: Data (t): 0.0011, 53.71/s/gpu Batch (t): 0.5958 LR: 0.000099 Step: 73600 Total Loss: 3.2187 Recon Loss: 3.2046 
[01/06 08:51:11 TiTok]: Data (t): 0.0011, 53.79/s/gpu Batch (t): 0.5949 LR: 0.000099 Step: 73650 Total Loss: 3.2306 Recon Loss: 3.2164 
[01/06 08:51:41 TiTok]: Data (t): 0.0007, 53.48/s/gpu Batch (t): 0.5983 LR: 0.000099 Step: 73700 Total Loss: 3.2556 Recon Loss: 3.2414 
[01/06 08:52:11 TiTok]: Data (t): 0.0013, 54.15/s/gpu Batch (t): 0.5909 LR: 0.000099 Step: 73750 Total Loss: 3.2370 Recon Loss: 3.2228 
[01/06 08:52:40 TiTok]: Data (t): 0.0012, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000099 Step: 73800 Total Loss: 3.2069 Recon Loss: 3.1927 
[01/06 08:53:10 TiTok]: Data (t): 0.0011, 54.41/s/gpu Batch (t): 0.5881 LR: 0.000099 Step: 73850 Total Loss: 3.2379 Recon Loss: 3.2238 
[01/06 08:53:40 TiTok]: Data (t): 0.0007, 53.33/s/gpu Batch (t): 0.6000 LR: 0.000099 Step: 73900 Total Loss: 3.2240 Recon Loss: 3.2098 
[01/06 08:54:09 TiTok]: Data (t): 0.0012, 54.00/s/gpu Batch (t): 0.5926 LR: 0.000099 Step: 73950 Total Loss: 3.2299 Recon Loss: 3.2157 
[01/06 08:54:39 TiTok]: Data (t): 0.0014, 44.86/s/gpu Batch (t): 0.7134 LR: 0.000099 Step: 74000 Total Loss: 3.2038 Recon Loss: 3.1897 
[01/06 08:55:09 TiTok]: Data (t): 0.0011, 53.97/s/gpu Batch (t): 0.5929 LR: 0.000099 Step: 74050 Total Loss: 3.2056 Recon Loss: 3.1914 
[01/06 08:55:39 TiTok]: Data (t): 0.0007, 53.96/s/gpu Batch (t): 0.5931 LR: 0.000099 Step: 74100 Total Loss: 3.2398 Recon Loss: 3.2257 
[01/06 08:56:08 TiTok]: Data (t): 0.0011, 53.58/s/gpu Batch (t): 0.5972 LR: 0.000099 Step: 74150 Total Loss: 3.2359 Recon Loss: 3.2217 
[01/06 08:56:38 TiTok]: Data (t): 0.0011, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000099 Step: 74200 Total Loss: 3.2319 Recon Loss: 3.2178 
[01/06 08:57:08 TiTok]: Data (t): 0.0009, 53.77/s/gpu Batch (t): 0.5951 LR: 0.000099 Step: 74250 Total Loss: 3.2017 Recon Loss: 3.1876 
[01/06 08:57:37 TiTok]: Data (t): 0.0010, 53.71/s/gpu Batch (t): 0.5958 LR: 0.000099 Step: 74300 Total Loss: 3.2445 Recon Loss: 3.2304 
[01/06 08:58:07 TiTok]: Data (t): 0.0011, 53.95/s/gpu Batch (t): 0.5931 LR: 0.000099 Step: 74350 Total Loss: 3.2087 Recon Loss: 3.1946 
[01/06 08:58:37 TiTok]: Data (t): 0.0011, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000099 Step: 74400 Total Loss: 3.2031 Recon Loss: 3.1889 
[01/06 08:59:07 TiTok]: Data (t): 0.0012, 53.93/s/gpu Batch (t): 0.5934 LR: 0.000099 Step: 74450 Total Loss: 3.1984 Recon Loss: 3.1843 
[01/06 08:59:36 TiTok]: Data (t): 0.0008, 53.79/s/gpu Batch (t): 0.5949 LR: 0.000099 Step: 74500 Total Loss: 3.2010 Recon Loss: 3.1868 
[01/06 09:00:06 TiTok]: Data (t): 0.0011, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000099 Step: 74550 Total Loss: 3.2308 Recon Loss: 3.2166 
[01/06 09:00:36 TiTok]: Data (t): 0.0011, 54.10/s/gpu Batch (t): 0.5915 LR: 0.000099 Step: 74600 Total Loss: 3.1968 Recon Loss: 3.1827 
[01/06 09:01:05 TiTok]: Data (t): 0.0007, 52.92/s/gpu Batch (t): 0.6047 LR: 0.000099 Step: 74650 Total Loss: 3.2084 Recon Loss: 3.1942 
[01/06 09:01:35 TiTok]: Data (t): 0.0012, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000099 Step: 74700 Total Loss: 3.2406 Recon Loss: 3.2264 
[01/06 09:02:05 TiTok]: Data (t): 0.0012, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000099 Step: 74750 Total Loss: 3.2155 Recon Loss: 3.2014 
[01/06 09:02:35 TiTok]: Data (t): 0.0012, 53.90/s/gpu Batch (t): 0.5936 LR: 0.000099 Step: 74800 Total Loss: 3.2003 Recon Loss: 3.1862 
[01/06 09:03:04 TiTok]: Data (t): 0.0011, 53.95/s/gpu Batch (t): 0.5931 LR: 0.000099 Step: 74850 Total Loss: 3.2305 Recon Loss: 3.2164 
[01/06 09:03:34 TiTok]: Data (t): 0.0012, 54.29/s/gpu Batch (t): 0.5894 LR: 0.000099 Step: 74900 Total Loss: 3.2419 Recon Loss: 3.2278 
[01/06 09:04:04 TiTok]: Data (t): 0.0011, 53.79/s/gpu Batch (t): 0.5949 LR: 0.000099 Step: 74950 Total Loss: 3.2287 Recon Loss: 3.2145 
[01/06 09:04:33 TiTok]: Data (t): 0.0012, 44.89/s/gpu Batch (t): 0.7128 LR: 0.000099 Step: 75000 Total Loss: 3.1932 Recon Loss: 3.1791 
[01/06 09:04:35 TiTok]: Reconstructing images...
[01/06 09:05:08 TiTok]: Data (t): 0.0007, 53.02/s/gpu Batch (t): 0.6036 LR: 0.000099 Step: 75050 Total Loss: 3.2172 Recon Loss: 3.2031 
[01/06 09:05:38 TiTok]: Data (t): 0.0014, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000099 Step: 75100 Total Loss: 3.2447 Recon Loss: 3.2306 
[01/06 09:06:07 TiTok]: Data (t): 0.0007, 53.88/s/gpu Batch (t): 0.5940 LR: 0.000099 Step: 75150 Total Loss: 3.2128 Recon Loss: 3.1986 
[01/06 09:06:37 TiTok]: Data (t): 0.0007, 53.36/s/gpu Batch (t): 0.5997 LR: 0.000099 Step: 75200 Total Loss: 3.2196 Recon Loss: 3.2055 
Epoch 15/199 started.
[01/06 09:07:09 TiTok]: Data (t): 0.0011, 54.17/s/gpu Batch (t): 0.5908 LR: 0.000099 Step: 75250 Total Loss: 3.2369 Recon Loss: 3.2228 
[01/06 09:07:38 TiTok]: Data (t): 0.0011, 53.98/s/gpu Batch (t): 0.5928 LR: 0.000099 Step: 75300 Total Loss: 3.2155 Recon Loss: 3.2013 
[01/06 09:08:08 TiTok]: Data (t): 0.0012, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000099 Step: 75350 Total Loss: 3.1687 Recon Loss: 3.1546 
[01/06 09:08:38 TiTok]: Data (t): 0.0007, 53.99/s/gpu Batch (t): 0.5927 LR: 0.000099 Step: 75400 Total Loss: 3.1541 Recon Loss: 3.1400 
[01/06 09:09:07 TiTok]: Data (t): 0.0011, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000099 Step: 75450 Total Loss: 3.2034 Recon Loss: 3.1893 
[01/06 09:09:37 TiTok]: Data (t): 0.0007, 53.95/s/gpu Batch (t): 0.5932 LR: 0.000099 Step: 75500 Total Loss: 3.2219 Recon Loss: 3.2078 
[01/06 09:10:07 TiTok]: Data (t): 0.0007, 52.82/s/gpu Batch (t): 0.6058 LR: 0.000099 Step: 75550 Total Loss: 3.2128 Recon Loss: 3.1986 
[01/06 09:10:36 TiTok]: Data (t): 0.0011, 53.85/s/gpu Batch (t): 0.5943 LR: 0.000099 Step: 75600 Total Loss: 3.2268 Recon Loss: 3.2127 
[01/06 09:11:06 TiTok]: Data (t): 0.0011, 54.24/s/gpu Batch (t): 0.5900 LR: 0.000099 Step: 75650 Total Loss: 3.2475 Recon Loss: 3.2333 
[01/06 09:11:36 TiTok]: Data (t): 0.0007, 53.95/s/gpu Batch (t): 0.5931 LR: 0.000099 Step: 75700 Total Loss: 3.2367 Recon Loss: 3.2225 
[01/06 09:12:05 TiTok]: Data (t): 0.0011, 53.85/s/gpu Batch (t): 0.5943 LR: 0.000099 Step: 75750 Total Loss: 3.1805 Recon Loss: 3.1663 
[01/06 09:12:35 TiTok]: Data (t): 0.0006, 53.47/s/gpu Batch (t): 0.5984 LR: 0.000099 Step: 75800 Total Loss: 3.2076 Recon Loss: 3.1934 
[01/06 09:13:05 TiTok]: Data (t): 0.0011, 53.73/s/gpu Batch (t): 0.5956 LR: 0.000099 Step: 75850 Total Loss: 3.1466 Recon Loss: 3.1325 
[01/06 09:13:35 TiTok]: Data (t): 0.0011, 53.76/s/gpu Batch (t): 0.5953 LR: 0.000099 Step: 75900 Total Loss: 3.2109 Recon Loss: 3.1967 
[01/06 09:14:04 TiTok]: Data (t): 0.0007, 53.26/s/gpu Batch (t): 0.6008 LR: 0.000099 Step: 75950 Total Loss: 3.2157 Recon Loss: 3.2016 
[01/06 09:14:34 TiTok]: Data (t): 0.0011, 43.57/s/gpu Batch (t): 0.7345 LR: 0.000099 Step: 76000 Total Loss: 3.2148 Recon Loss: 3.2007 
[01/06 09:15:04 TiTok]: Data (t): 0.0013, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000099 Step: 76050 Total Loss: 3.2143 Recon Loss: 3.2001 
[01/06 09:15:34 TiTok]: Data (t): 0.0007, 53.49/s/gpu Batch (t): 0.5983 LR: 0.000099 Step: 76100 Total Loss: 3.1969 Recon Loss: 3.1828 
[01/06 09:16:03 TiTok]: Data (t): 0.0007, 53.44/s/gpu Batch (t): 0.5988 LR: 0.000099 Step: 76150 Total Loss: 3.2029 Recon Loss: 3.1887 
[01/06 09:16:33 TiTok]: Data (t): 0.0011, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000099 Step: 76200 Total Loss: 3.2170 Recon Loss: 3.2029 
[01/06 09:17:03 TiTok]: Data (t): 0.0007, 53.58/s/gpu Batch (t): 0.5973 LR: 0.000099 Step: 76250 Total Loss: 3.2312 Recon Loss: 3.2171 
[01/06 09:17:32 TiTok]: Data (t): 0.0012, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000099 Step: 76300 Total Loss: 3.2246 Recon Loss: 3.2104 
[01/06 09:18:02 TiTok]: Data (t): 0.0007, 53.60/s/gpu Batch (t): 0.5970 LR: 0.000099 Step: 76350 Total Loss: 3.2061 Recon Loss: 3.1920 
[01/06 09:18:32 TiTok]: Data (t): 0.0007, 49.39/s/gpu Batch (t): 0.6479 LR: 0.000099 Step: 76400 Total Loss: 3.2033 Recon Loss: 3.1891 
[01/06 09:19:01 TiTok]: Data (t): 0.0011, 53.93/s/gpu Batch (t): 0.5934 LR: 0.000099 Step: 76450 Total Loss: 3.1892 Recon Loss: 3.1751 
[01/06 09:19:31 TiTok]: Data (t): 0.0007, 53.34/s/gpu Batch (t): 0.5999 LR: 0.000099 Step: 76500 Total Loss: 3.1981 Recon Loss: 3.1839 
[01/06 09:20:01 TiTok]: Data (t): 0.0007, 53.63/s/gpu Batch (t): 0.5966 LR: 0.000099 Step: 76550 Total Loss: 3.1917 Recon Loss: 3.1776 
[01/06 09:20:31 TiTok]: Data (t): 0.0011, 53.98/s/gpu Batch (t): 0.5928 LR: 0.000099 Step: 76600 Total Loss: 3.2092 Recon Loss: 3.1951 
[01/06 09:21:00 TiTok]: Data (t): 0.0011, 54.30/s/gpu Batch (t): 0.5893 LR: 0.000099 Step: 76650 Total Loss: 3.2067 Recon Loss: 3.1925 
[01/06 09:21:30 TiTok]: Data (t): 0.0011, 53.99/s/gpu Batch (t): 0.5927 LR: 0.000099 Step: 76700 Total Loss: 3.1762 Recon Loss: 3.1621 
[01/06 09:22:00 TiTok]: Data (t): 0.0013, 53.95/s/gpu Batch (t): 0.5931 LR: 0.000099 Step: 76750 Total Loss: 3.2008 Recon Loss: 3.1866 
[01/06 09:22:29 TiTok]: Data (t): 0.0007, 53.55/s/gpu Batch (t): 0.5976 LR: 0.000099 Step: 76800 Total Loss: 3.2385 Recon Loss: 3.2243 
[01/06 09:22:59 TiTok]: Data (t): 0.0011, 53.76/s/gpu Batch (t): 0.5952 LR: 0.000099 Step: 76850 Total Loss: 3.2161 Recon Loss: 3.2020 
[01/06 09:23:29 TiTok]: Data (t): 0.0012, 53.63/s/gpu Batch (t): 0.5966 LR: 0.000099 Step: 76900 Total Loss: 3.2117 Recon Loss: 3.1975 
[01/06 09:23:58 TiTok]: Data (t): 0.0011, 54.38/s/gpu Batch (t): 0.5884 LR: 0.000099 Step: 76950 Total Loss: 3.1984 Recon Loss: 3.1842 
[01/06 09:24:28 TiTok]: Data (t): 0.0011, 44.93/s/gpu Batch (t): 0.7122 LR: 0.000099 Step: 77000 Total Loss: 3.1852 Recon Loss: 3.1710 
[01/06 09:24:58 TiTok]: Data (t): 0.0011, 54.25/s/gpu Batch (t): 0.5899 LR: 0.000099 Step: 77050 Total Loss: 3.2374 Recon Loss: 3.2232 
[01/06 09:25:28 TiTok]: Data (t): 0.0011, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000099 Step: 77100 Total Loss: 3.1754 Recon Loss: 3.1612 
[01/06 09:25:57 TiTok]: Data (t): 0.0014, 54.08/s/gpu Batch (t): 0.5917 LR: 0.000099 Step: 77150 Total Loss: 3.2031 Recon Loss: 3.1890 
[01/06 09:26:27 TiTok]: Data (t): 0.0012, 54.37/s/gpu Batch (t): 0.5885 LR: 0.000099 Step: 77200 Total Loss: 3.1931 Recon Loss: 3.1790 
[01/06 09:26:57 TiTok]: Data (t): 0.0007, 53.41/s/gpu Batch (t): 0.5992 LR: 0.000099 Step: 77250 Total Loss: 3.1911 Recon Loss: 3.1769 
[01/06 09:27:26 TiTok]: Data (t): 0.0007, 54.10/s/gpu Batch (t): 0.5915 LR: 0.000099 Step: 77300 Total Loss: 3.2253 Recon Loss: 3.2112 
[01/06 09:27:56 TiTok]: Data (t): 0.0011, 53.94/s/gpu Batch (t): 0.5933 LR: 0.000099 Step: 77350 Total Loss: 3.2501 Recon Loss: 3.2359 
[01/06 09:28:26 TiTok]: Data (t): 0.0012, 53.97/s/gpu Batch (t): 0.5929 LR: 0.000099 Step: 77400 Total Loss: 3.2121 Recon Loss: 3.1979 
[01/06 09:28:56 TiTok]: Data (t): 0.0007, 53.84/s/gpu Batch (t): 0.5943 LR: 0.000099 Step: 77450 Total Loss: 3.2271 Recon Loss: 3.2129 
[01/06 09:29:25 TiTok]: Data (t): 0.0011, 53.86/s/gpu Batch (t): 0.5942 LR: 0.000099 Step: 77500 Total Loss: 3.2043 Recon Loss: 3.1902 
[01/06 09:29:55 TiTok]: Data (t): 0.0012, 54.36/s/gpu Batch (t): 0.5887 LR: 0.000099 Step: 77550 Total Loss: 3.1877 Recon Loss: 3.1736 
[01/06 09:30:25 TiTok]: Data (t): 0.0011, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000099 Step: 77600 Total Loss: 3.2063 Recon Loss: 3.1921 
[01/06 09:30:54 TiTok]: Data (t): 0.0011, 53.84/s/gpu Batch (t): 0.5944 LR: 0.000099 Step: 77650 Total Loss: 3.2115 Recon Loss: 3.1974 
[01/06 09:31:24 TiTok]: Data (t): 0.0011, 53.84/s/gpu Batch (t): 0.5943 LR: 0.000099 Step: 77700 Total Loss: 3.1927 Recon Loss: 3.1786 
[01/06 09:31:54 TiTok]: Data (t): 0.0863, 49.83/s/gpu Batch (t): 0.6422 LR: 0.000099 Step: 77750 Total Loss: 3.2025 Recon Loss: 3.1884 
[01/06 09:32:24 TiTok]: Data (t): 0.0011, 53.72/s/gpu Batch (t): 0.5957 LR: 0.000099 Step: 77800 Total Loss: 3.1904 Recon Loss: 3.1762 
[01/06 09:32:53 TiTok]: Data (t): 0.0007, 53.57/s/gpu Batch (t): 0.5974 LR: 0.000099 Step: 77850 Total Loss: 3.2123 Recon Loss: 3.1982 
[01/06 09:33:23 TiTok]: Data (t): 0.0011, 53.93/s/gpu Batch (t): 0.5933 LR: 0.000099 Step: 77900 Total Loss: 3.2243 Recon Loss: 3.2101 
[01/06 09:33:53 TiTok]: Data (t): 0.0015, 54.00/s/gpu Batch (t): 0.5926 LR: 0.000099 Step: 77950 Total Loss: 3.2293 Recon Loss: 3.2151 
[01/06 09:34:22 TiTok]: Data (t): 0.0011, 45.42/s/gpu Batch (t): 0.7045 LR: 0.000099 Step: 78000 Total Loss: 3.2631 Recon Loss: 3.2489 
[01/06 09:34:52 TiTok]: Data (t): 0.0011, 54.38/s/gpu Batch (t): 0.5885 LR: 0.000099 Step: 78050 Total Loss: 3.2029 Recon Loss: 3.1888 
[01/06 09:35:22 TiTok]: Data (t): 0.0036, 54.34/s/gpu Batch (t): 0.5889 LR: 0.000099 Step: 78100 Total Loss: 3.1809 Recon Loss: 3.1667 
[01/06 09:35:51 TiTok]: Data (t): 0.0011, 54.38/s/gpu Batch (t): 0.5885 LR: 0.000099 Step: 78150 Total Loss: 3.1836 Recon Loss: 3.1695 
[01/06 09:36:21 TiTok]: Data (t): 0.0012, 53.98/s/gpu Batch (t): 0.5928 LR: 0.000099 Step: 78200 Total Loss: 3.2278 Recon Loss: 3.2136 
[01/06 09:36:51 TiTok]: Data (t): 0.0011, 54.08/s/gpu Batch (t): 0.5917 LR: 0.000099 Step: 78250 Total Loss: 3.1996 Recon Loss: 3.1855 
[01/06 09:37:21 TiTok]: Data (t): 0.0011, 53.83/s/gpu Batch (t): 0.5944 LR: 0.000099 Step: 78300 Total Loss: 3.2372 Recon Loss: 3.2231 
[01/06 09:37:50 TiTok]: Data (t): 0.0011, 53.92/s/gpu Batch (t): 0.5934 LR: 0.000099 Step: 78350 Total Loss: 3.2221 Recon Loss: 3.2079 
[01/06 09:38:20 TiTok]: Data (t): 0.0011, 54.39/s/gpu Batch (t): 0.5883 LR: 0.000099 Step: 78400 Total Loss: 3.2365 Recon Loss: 3.2223 
[01/06 09:38:50 TiTok]: Data (t): 0.0007, 53.94/s/gpu Batch (t): 0.5932 LR: 0.000099 Step: 78450 Total Loss: 3.1596 Recon Loss: 3.1455 
[01/06 09:39:20 TiTok]: Data (t): 0.0012, 53.86/s/gpu Batch (t): 0.5941 LR: 0.000099 Step: 78500 Total Loss: 3.2088 Recon Loss: 3.1947 
[01/06 09:39:49 TiTok]: Data (t): 0.0006, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000099 Step: 78550 Total Loss: 3.1860 Recon Loss: 3.1719 
[01/06 09:40:19 TiTok]: Data (t): 0.0011, 53.93/s/gpu Batch (t): 0.5934 LR: 0.000099 Step: 78600 Total Loss: 3.1623 Recon Loss: 3.1481 
[01/06 09:40:49 TiTok]: Data (t): 0.0011, 53.86/s/gpu Batch (t): 0.5941 LR: 0.000099 Step: 78650 Total Loss: 3.1636 Recon Loss: 3.1495 
[01/06 09:41:18 TiTok]: Data (t): 0.0033, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000099 Step: 78700 Total Loss: 3.2345 Recon Loss: 3.2203 
[01/06 09:41:48 TiTok]: Data (t): 0.0007, 53.53/s/gpu Batch (t): 0.5978 LR: 0.000099 Step: 78750 Total Loss: 3.2016 Recon Loss: 3.1875 
[01/06 09:42:18 TiTok]: Data (t): 0.0012, 53.95/s/gpu Batch (t): 0.5931 LR: 0.000099 Step: 78800 Total Loss: 3.2218 Recon Loss: 3.2076 
[01/06 09:42:47 TiTok]: Data (t): 0.0007, 53.30/s/gpu Batch (t): 0.6004 LR: 0.000099 Step: 78850 Total Loss: 3.1915 Recon Loss: 3.1774 
[01/06 09:43:17 TiTok]: Data (t): 0.0011, 53.94/s/gpu Batch (t): 0.5933 LR: 0.000099 Step: 78900 Total Loss: 3.2457 Recon Loss: 3.2316 
[01/06 09:43:47 TiTok]: Data (t): 0.0011, 53.98/s/gpu Batch (t): 0.5928 LR: 0.000099 Step: 78950 Total Loss: 3.1936 Recon Loss: 3.1794 
[01/06 09:44:17 TiTok]: Data (t): 0.0008, 44.96/s/gpu Batch (t): 0.7117 LR: 0.000099 Step: 79000 Total Loss: 3.2156 Recon Loss: 3.2015 
[01/06 09:44:46 TiTok]: Data (t): 0.0012, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000099 Step: 79050 Total Loss: 3.1992 Recon Loss: 3.1850 
[01/06 09:45:16 TiTok]: Data (t): 0.0007, 53.45/s/gpu Batch (t): 0.5987 LR: 0.000099 Step: 79100 Total Loss: 3.2205 Recon Loss: 3.2063 
[01/06 09:45:46 TiTok]: Data (t): 0.0007, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000099 Step: 79150 Total Loss: 3.1966 Recon Loss: 3.1825 
[01/06 09:46:16 TiTok]: Data (t): 0.0013, 53.76/s/gpu Batch (t): 0.5953 LR: 0.000099 Step: 79200 Total Loss: 3.2030 Recon Loss: 3.1888 
[01/06 09:46:45 TiTok]: Data (t): 0.0012, 53.57/s/gpu Batch (t): 0.5974 LR: 0.000099 Step: 79250 Total Loss: 3.1953 Recon Loss: 3.1811 
[01/06 09:47:15 TiTok]: Data (t): 0.0011, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000099 Step: 79300 Total Loss: 3.2007 Recon Loss: 3.1865 
[01/06 09:47:45 TiTok]: Data (t): 0.0007, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000099 Step: 79350 Total Loss: 3.2283 Recon Loss: 3.2141 
[01/06 09:48:14 TiTok]: Data (t): 0.0015, 53.94/s/gpu Batch (t): 0.5933 LR: 0.000099 Step: 79400 Total Loss: 3.2007 Recon Loss: 3.1865 
[01/06 09:48:44 TiTok]: Data (t): 0.0011, 54.12/s/gpu Batch (t): 0.5913 LR: 0.000099 Step: 79450 Total Loss: 3.1873 Recon Loss: 3.1731 
[01/06 09:49:14 TiTok]: Data (t): 0.0011, 53.85/s/gpu Batch (t): 0.5942 LR: 0.000099 Step: 79500 Total Loss: 3.2052 Recon Loss: 3.1910 
[01/06 09:49:44 TiTok]: Data (t): 0.0011, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000099 Step: 79550 Total Loss: 3.2260 Recon Loss: 3.2118 
[01/06 09:50:13 TiTok]: Data (t): 0.0013, 53.93/s/gpu Batch (t): 0.5934 LR: 0.000099 Step: 79600 Total Loss: 3.2102 Recon Loss: 3.1961 
[01/06 09:50:43 TiTok]: Data (t): 0.0011, 54.27/s/gpu Batch (t): 0.5896 LR: 0.000099 Step: 79650 Total Loss: 3.1797 Recon Loss: 3.1656 
[01/06 09:51:13 TiTok]: Data (t): 0.0011, 53.79/s/gpu Batch (t): 0.5949 LR: 0.000099 Step: 79700 Total Loss: 3.2285 Recon Loss: 3.2144 
[01/06 09:51:42 TiTok]: Data (t): 0.0009, 53.38/s/gpu Batch (t): 0.5994 LR: 0.000099 Step: 79750 Total Loss: 3.2124 Recon Loss: 3.1983 
[01/06 09:52:12 TiTok]: Data (t): 0.0989, 48.29/s/gpu Batch (t): 0.6627 LR: 0.000099 Step: 79800 Total Loss: 3.1771 Recon Loss: 3.1629 
[01/06 09:52:42 TiTok]: Data (t): 0.0007, 53.83/s/gpu Batch (t): 0.5944 LR: 0.000099 Step: 79850 Total Loss: 3.2061 Recon Loss: 3.1919 
[01/06 09:53:12 TiTok]: Data (t): 0.0011, 53.86/s/gpu Batch (t): 0.5942 LR: 0.000099 Step: 79900 Total Loss: 3.2152 Recon Loss: 3.2010 
[01/06 09:53:41 TiTok]: Data (t): 0.0012, 53.88/s/gpu Batch (t): 0.5940 LR: 0.000099 Step: 79950 Total Loss: 3.1665 Recon Loss: 3.1523 
[01/06 09:54:11 TiTok]: Data (t): 0.0014, 45.08/s/gpu Batch (t): 0.7098 LR: 0.000099 Step: 80000 Total Loss: 3.2197 Recon Loss: 3.2055 
[01/06 09:54:12 TiTok]: Reconstructing images...
[01/06 09:54:47 TiTok]: Data (t): 0.0011, 54.15/s/gpu Batch (t): 0.5910 LR: 0.000099 Step: 80050 Total Loss: 3.1707 Recon Loss: 3.1565 
[01/06 09:55:17 TiTok]: Data (t): 0.0011, 53.94/s/gpu Batch (t): 0.5933 LR: 0.000099 Step: 80100 Total Loss: 3.2134 Recon Loss: 3.1992 
[01/06 09:55:47 TiTok]: Data (t): 0.0012, 53.98/s/gpu Batch (t): 0.5928 LR: 0.000099 Step: 80150 Total Loss: 3.1662 Recon Loss: 3.1520 
[01/06 09:56:16 TiTok]: Data (t): 0.0008, 53.47/s/gpu Batch (t): 0.5985 LR: 0.000099 Step: 80200 Total Loss: 3.1847 Recon Loss: 3.1705 
[01/06 09:56:46 TiTok]: Data (t): 0.0011, 54.00/s/gpu Batch (t): 0.5926 LR: 0.000099 Step: 80250 Total Loss: 3.1677 Recon Loss: 3.1536 
Epoch 16/199 started.
[01/06 09:57:18 TiTok]: Data (t): 0.0011, 53.95/s/gpu Batch (t): 0.5931 LR: 0.000099 Step: 80300 Total Loss: 3.1704 Recon Loss: 3.1563 
[01/06 09:57:47 TiTok]: Data (t): 0.0012, 53.97/s/gpu Batch (t): 0.5929 LR: 0.000099 Step: 80350 Total Loss: 3.1860 Recon Loss: 3.1718 
[01/06 09:58:17 TiTok]: Data (t): 0.0007, 53.48/s/gpu Batch (t): 0.5984 LR: 0.000099 Step: 80400 Total Loss: 3.2011 Recon Loss: 3.1869 
[01/06 09:58:47 TiTok]: Data (t): 0.0011, 54.46/s/gpu Batch (t): 0.5876 LR: 0.000099 Step: 80450 Total Loss: 3.1661 Recon Loss: 3.1519 
[01/06 09:59:17 TiTok]: Data (t): 0.0011, 53.93/s/gpu Batch (t): 0.5933 LR: 0.000099 Step: 80500 Total Loss: 3.1903 Recon Loss: 3.1762 
[01/06 09:59:46 TiTok]: Data (t): 0.0007, 53.95/s/gpu Batch (t): 0.5932 LR: 0.000099 Step: 80550 Total Loss: 3.1974 Recon Loss: 3.1833 
[01/06 10:00:16 TiTok]: Data (t): 0.0012, 54.44/s/gpu Batch (t): 0.5878 LR: 0.000099 Step: 80600 Total Loss: 3.1752 Recon Loss: 3.1610 
[01/06 10:00:46 TiTok]: Data (t): 0.0011, 53.84/s/gpu Batch (t): 0.5944 LR: 0.000099 Step: 80650 Total Loss: 3.2102 Recon Loss: 3.1961 
[01/06 10:01:15 TiTok]: Data (t): 0.0007, 53.63/s/gpu Batch (t): 0.5967 LR: 0.000099 Step: 80700 Total Loss: 3.1921 Recon Loss: 3.1779 
[01/06 10:01:45 TiTok]: Data (t): 0.0012, 54.25/s/gpu Batch (t): 0.5899 LR: 0.000099 Step: 80750 Total Loss: 3.1933 Recon Loss: 3.1792 
[01/06 10:02:15 TiTok]: Data (t): 0.0007, 53.95/s/gpu Batch (t): 0.5931 LR: 0.000099 Step: 80800 Total Loss: 3.2133 Recon Loss: 3.1991 
[01/06 10:02:45 TiTok]: Data (t): 0.0007, 53.55/s/gpu Batch (t): 0.5976 LR: 0.000099 Step: 80850 Total Loss: 3.2277 Recon Loss: 3.2135 
[01/06 10:03:14 TiTok]: Data (t): 0.0007, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000099 Step: 80900 Total Loss: 3.2013 Recon Loss: 3.1871 
[01/06 10:03:44 TiTok]: Data (t): 0.0012, 53.89/s/gpu Batch (t): 0.5937 LR: 0.000099 Step: 80950 Total Loss: 3.2388 Recon Loss: 3.2247 
[01/06 10:04:14 TiTok]: Data (t): 0.0011, 47.59/s/gpu Batch (t): 0.6724 LR: 0.000099 Step: 81000 Total Loss: 3.1944 Recon Loss: 3.1802 
[01/06 10:04:43 TiTok]: Data (t): 0.0011, 54.03/s/gpu Batch (t): 0.5923 LR: 0.000099 Step: 81050 Total Loss: 3.1765 Recon Loss: 3.1624 
[01/06 10:05:13 TiTok]: Data (t): 0.0012, 53.94/s/gpu Batch (t): 0.5933 LR: 0.000099 Step: 81100 Total Loss: 3.1756 Recon Loss: 3.1615 
[01/06 10:05:43 TiTok]: Data (t): 0.0012, 53.96/s/gpu Batch (t): 0.5930 LR: 0.000099 Step: 81150 Total Loss: 3.2443 Recon Loss: 3.2302 
[01/06 10:06:13 TiTok]: Data (t): 0.0007, 53.51/s/gpu Batch (t): 0.5981 LR: 0.000099 Step: 81200 Total Loss: 3.2354 Recon Loss: 3.2213 
[01/06 10:06:42 TiTok]: Data (t): 0.0012, 53.50/s/gpu Batch (t): 0.5982 LR: 0.000099 Step: 81250 Total Loss: 3.1851 Recon Loss: 3.1710 
[01/06 10:07:12 TiTok]: Data (t): 0.0012, 54.50/s/gpu Batch (t): 0.5872 LR: 0.000099 Step: 81300 Total Loss: 3.1215 Recon Loss: 3.1074 
[01/06 10:07:42 TiTok]: Data (t): 0.0007, 53.97/s/gpu Batch (t): 0.5929 LR: 0.000099 Step: 81350 Total Loss: 3.1793 Recon Loss: 3.1651 
[01/06 10:08:11 TiTok]: Data (t): 0.0007, 53.51/s/gpu Batch (t): 0.5980 LR: 0.000099 Step: 81400 Total Loss: 3.2141 Recon Loss: 3.1999 
[01/06 10:08:41 TiTok]: Data (t): 0.0012, 54.35/s/gpu Batch (t): 0.5888 LR: 0.000099 Step: 81450 Total Loss: 3.1470 Recon Loss: 3.1329 
[01/06 10:09:11 TiTok]: Data (t): 0.0007, 53.96/s/gpu Batch (t): 0.5930 LR: 0.000099 Step: 81500 Total Loss: 3.2131 Recon Loss: 3.1989 
[01/06 10:09:41 TiTok]: Data (t): 0.0011, 54.36/s/gpu Batch (t): 0.5887 LR: 0.000099 Step: 81550 Total Loss: 3.2164 Recon Loss: 3.2023 
[01/06 10:10:10 TiTok]: Data (t): 0.0007, 52.46/s/gpu Batch (t): 0.6100 LR: 0.000099 Step: 81600 Total Loss: 3.2000 Recon Loss: 3.1858 
[01/06 10:10:40 TiTok]: Data (t): 0.0011, 53.31/s/gpu Batch (t): 0.6002 LR: 0.000099 Step: 81650 Total Loss: 3.1964 Recon Loss: 3.1822 
[01/06 10:11:10 TiTok]: Data (t): 0.0011, 53.84/s/gpu Batch (t): 0.5943 LR: 0.000099 Step: 81700 Total Loss: 3.1529 Recon Loss: 3.1387 
[01/06 10:11:39 TiTok]: Data (t): 0.0011, 54.23/s/gpu Batch (t): 0.5901 LR: 0.000099 Step: 81750 Total Loss: 3.2150 Recon Loss: 3.2008 
[01/06 10:12:09 TiTok]: Data (t): 0.0011, 54.45/s/gpu Batch (t): 0.5877 LR: 0.000099 Step: 81800 Total Loss: 3.2164 Recon Loss: 3.2022 
[01/06 10:12:39 TiTok]: Data (t): 0.0011, 52.85/s/gpu Batch (t): 0.6055 LR: 0.000099 Step: 81850 Total Loss: 3.1969 Recon Loss: 3.1827 
[01/06 10:13:09 TiTok]: Data (t): 0.0011, 53.80/s/gpu Batch (t): 0.5948 LR: 0.000099 Step: 81900 Total Loss: 3.1432 Recon Loss: 3.1290 
[01/06 10:13:38 TiTok]: Data (t): 0.0007, 53.97/s/gpu Batch (t): 0.5930 LR: 0.000099 Step: 81950 Total Loss: 3.1729 Recon Loss: 3.1587 
[01/06 10:14:08 TiTok]: Data (t): 0.0011, 42.95/s/gpu Batch (t): 0.7451 LR: 0.000099 Step: 82000 Total Loss: 3.1562 Recon Loss: 3.1420 
[01/06 10:14:38 TiTok]: Data (t): 0.0011, 39.63/s/gpu Batch (t): 0.8074 LR: 0.000099 Step: 82050 Total Loss: 3.2240 Recon Loss: 3.2099 
[01/06 10:15:08 TiTok]: Data (t): 0.0007, 53.69/s/gpu Batch (t): 0.5961 LR: 0.000099 Step: 82100 Total Loss: 3.1880 Recon Loss: 3.1738 
[01/06 10:15:38 TiTok]: Data (t): 0.0011, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000099 Step: 82150 Total Loss: 3.2399 Recon Loss: 3.2257 
[01/06 10:16:07 TiTok]: Data (t): 0.0013, 53.84/s/gpu Batch (t): 0.5944 LR: 0.000099 Step: 82200 Total Loss: 3.1957 Recon Loss: 3.1816 
[01/06 10:16:37 TiTok]: Data (t): 0.0011, 54.34/s/gpu Batch (t): 0.5889 LR: 0.000099 Step: 82250 Total Loss: 3.1489 Recon Loss: 3.1348 
[01/06 10:17:07 TiTok]: Data (t): 0.0011, 54.40/s/gpu Batch (t): 0.5882 LR: 0.000099 Step: 82300 Total Loss: 3.1746 Recon Loss: 3.1604 
[01/06 10:17:36 TiTok]: Data (t): 0.0011, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000099 Step: 82350 Total Loss: 3.1724 Recon Loss: 3.1582 
[01/06 10:18:06 TiTok]: Data (t): 0.0011, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000099 Step: 82400 Total Loss: 3.1275 Recon Loss: 3.1134 
[01/06 10:18:36 TiTok]: Data (t): 0.0007, 53.59/s/gpu Batch (t): 0.5972 LR: 0.000099 Step: 82450 Total Loss: 3.1750 Recon Loss: 3.1608 
[01/06 10:19:06 TiTok]: Data (t): 0.0007, 53.41/s/gpu Batch (t): 0.5991 LR: 0.000099 Step: 82500 Total Loss: 3.1666 Recon Loss: 3.1525 
[01/06 10:19:35 TiTok]: Data (t): 0.0007, 53.44/s/gpu Batch (t): 0.5989 LR: 0.000099 Step: 82550 Total Loss: 3.1804 Recon Loss: 3.1662 
[01/06 10:20:05 TiTok]: Data (t): 0.0012, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000099 Step: 82600 Total Loss: 3.1901 Recon Loss: 3.1759 
[01/06 10:20:35 TiTok]: Data (t): 0.0007, 53.45/s/gpu Batch (t): 0.5987 LR: 0.000099 Step: 82650 Total Loss: 3.1291 Recon Loss: 3.1149 
[01/06 10:21:05 TiTok]: Data (t): 0.0011, 53.97/s/gpu Batch (t): 0.5929 LR: 0.000099 Step: 82700 Total Loss: 3.2029 Recon Loss: 3.1887 
[01/06 10:21:34 TiTok]: Data (t): 0.0011, 53.92/s/gpu Batch (t): 0.5935 LR: 0.000099 Step: 82750 Total Loss: 3.2049 Recon Loss: 3.1908 
[01/06 10:22:04 TiTok]: Data (t): 0.0012, 53.94/s/gpu Batch (t): 0.5932 LR: 0.000099 Step: 82800 Total Loss: 3.1900 Recon Loss: 3.1758 
[01/06 10:22:34 TiTok]: Data (t): 0.0011, 53.43/s/gpu Batch (t): 0.5989 LR: 0.000099 Step: 82850 Total Loss: 3.1758 Recon Loss: 3.1616 
[01/06 10:23:04 TiTok]: Data (t): 0.0007, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000099 Step: 82900 Total Loss: 3.1697 Recon Loss: 3.1556 
[01/06 10:23:33 TiTok]: Data (t): 0.0011, 53.33/s/gpu Batch (t): 0.6000 LR: 0.000099 Step: 82950 Total Loss: 3.1642 Recon Loss: 3.1500 
[01/06 10:24:03 TiTok]: Data (t): 0.0011, 44.96/s/gpu Batch (t): 0.7117 LR: 0.000099 Step: 83000 Total Loss: 3.1587 Recon Loss: 3.1445 
[01/06 10:24:33 TiTok]: Data (t): 0.0012, 53.72/s/gpu Batch (t): 0.5957 LR: 0.000099 Step: 83050 Total Loss: 3.1838 Recon Loss: 3.1696 
[01/06 10:25:03 TiTok]: Data (t): 0.0007, 53.51/s/gpu Batch (t): 0.5980 LR: 0.000099 Step: 83100 Total Loss: 3.1955 Recon Loss: 3.1813 
[01/06 10:25:32 TiTok]: Data (t): 0.0011, 53.84/s/gpu Batch (t): 0.5943 LR: 0.000099 Step: 83150 Total Loss: 3.1827 Recon Loss: 3.1685 
[01/06 10:26:02 TiTok]: Data (t): 0.0011, 53.91/s/gpu Batch (t): 0.5935 LR: 0.000099 Step: 83200 Total Loss: 3.1829 Recon Loss: 3.1688 
[01/06 10:26:32 TiTok]: Data (t): 0.0012, 53.86/s/gpu Batch (t): 0.5942 LR: 0.000099 Step: 83250 Total Loss: 3.1641 Recon Loss: 3.1499 
[01/06 10:27:01 TiTok]: Data (t): 0.0015, 54.45/s/gpu Batch (t): 0.5876 LR: 0.000099 Step: 83300 Total Loss: 3.1702 Recon Loss: 3.1560 
[01/06 10:27:31 TiTok]: Data (t): 0.0011, 53.84/s/gpu Batch (t): 0.5944 LR: 0.000099 Step: 83350 Total Loss: 3.1631 Recon Loss: 3.1490 
[01/06 10:28:01 TiTok]: Data (t): 0.0011, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000099 Step: 83400 Total Loss: 3.1783 Recon Loss: 3.1641 
[01/06 10:28:31 TiTok]: Data (t): 0.0007, 53.33/s/gpu Batch (t): 0.6000 LR: 0.000099 Step: 83450 Total Loss: 3.2020 Recon Loss: 3.1877 
[01/06 10:29:00 TiTok]: Data (t): 0.0011, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000099 Step: 83500 Total Loss: 3.2032 Recon Loss: 3.1891 
[01/06 10:29:30 TiTok]: Data (t): 0.0014, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000099 Step: 83550 Total Loss: 3.2343 Recon Loss: 3.2202 
[01/06 10:30:00 TiTok]: Data (t): 0.0007, 53.40/s/gpu Batch (t): 0.5992 LR: 0.000099 Step: 83600 Total Loss: 3.1957 Recon Loss: 3.1815 
[01/06 10:30:30 TiTok]: Data (t): 0.0014, 53.69/s/gpu Batch (t): 0.5960 LR: 0.000099 Step: 83650 Total Loss: 3.2033 Recon Loss: 3.1892 
[01/06 10:30:59 TiTok]: Data (t): 0.0011, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000099 Step: 83700 Total Loss: 3.1600 Recon Loss: 3.1458 
[01/06 10:31:29 TiTok]: Data (t): 0.0007, 53.72/s/gpu Batch (t): 0.5957 LR: 0.000099 Step: 83750 Total Loss: 3.1856 Recon Loss: 3.1714 
[01/06 10:31:59 TiTok]: Data (t): 0.0011, 53.99/s/gpu Batch (t): 0.5927 LR: 0.000099 Step: 83800 Total Loss: 3.1580 Recon Loss: 3.1438 
[01/06 10:32:29 TiTok]: Data (t): 0.0007, 53.62/s/gpu Batch (t): 0.5968 LR: 0.000099 Step: 83850 Total Loss: 3.2065 Recon Loss: 3.1923 
[01/06 10:32:58 TiTok]: Data (t): 0.0011, 53.86/s/gpu Batch (t): 0.5941 LR: 0.000099 Step: 83900 Total Loss: 3.1600 Recon Loss: 3.1458 
[01/06 10:33:28 TiTok]: Data (t): 0.0014, 53.94/s/gpu Batch (t): 0.5933 LR: 0.000099 Step: 83950 Total Loss: 3.1174 Recon Loss: 3.1032 
[01/06 10:33:58 TiTok]: Data (t): 0.0016, 41.41/s/gpu Batch (t): 0.7727 LR: 0.000099 Step: 84000 Total Loss: 3.1653 Recon Loss: 3.1511 
[01/06 10:34:28 TiTok]: Data (t): 0.0011, 53.92/s/gpu Batch (t): 0.5935 LR: 0.000099 Step: 84050 Total Loss: 3.2263 Recon Loss: 3.2121 
[01/06 10:34:57 TiTok]: Data (t): 0.0011, 53.85/s/gpu Batch (t): 0.5942 LR: 0.000099 Step: 84100 Total Loss: 3.1597 Recon Loss: 3.1455 
[01/06 10:35:27 TiTok]: Data (t): 0.0011, 54.42/s/gpu Batch (t): 0.5880 LR: 0.000099 Step: 84150 Total Loss: 3.1527 Recon Loss: 3.1386 
[01/06 10:35:57 TiTok]: Data (t): 0.0007, 53.55/s/gpu Batch (t): 0.5976 LR: 0.000099 Step: 84200 Total Loss: 3.1837 Recon Loss: 3.1695 
[01/06 10:36:27 TiTok]: Data (t): 0.0007, 53.35/s/gpu Batch (t): 0.5998 LR: 0.000099 Step: 84250 Total Loss: 3.1768 Recon Loss: 3.1626 
[01/06 10:36:56 TiTok]: Data (t): 0.0016, 53.82/s/gpu Batch (t): 0.5945 LR: 0.000099 Step: 84300 Total Loss: 3.2088 Recon Loss: 3.1946 
[01/06 10:37:26 TiTok]: Data (t): 0.0011, 53.95/s/gpu Batch (t): 0.5931 LR: 0.000099 Step: 84350 Total Loss: 3.1811 Recon Loss: 3.1669 
[01/06 10:37:56 TiTok]: Data (t): 0.0011, 53.93/s/gpu Batch (t): 0.5934 LR: 0.000099 Step: 84400 Total Loss: 3.2288 Recon Loss: 3.2146 
[01/06 10:38:25 TiTok]: Data (t): 0.0007, 53.65/s/gpu Batch (t): 0.5964 LR: 0.000099 Step: 84450 Total Loss: 3.2098 Recon Loss: 3.1957 
[01/06 10:38:55 TiTok]: Data (t): 0.0007, 53.46/s/gpu Batch (t): 0.5986 LR: 0.000099 Step: 84500 Total Loss: 3.1795 Recon Loss: 3.1653 
[01/06 10:39:25 TiTok]: Data (t): 0.0007, 53.56/s/gpu Batch (t): 0.5974 LR: 0.000099 Step: 84550 Total Loss: 3.2010 Recon Loss: 3.1868 
[01/06 10:39:55 TiTok]: Data (t): 0.0012, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000099 Step: 84600 Total Loss: 3.1932 Recon Loss: 3.1790 
[01/06 10:40:24 TiTok]: Data (t): 0.0007, 53.51/s/gpu Batch (t): 0.5980 LR: 0.000099 Step: 84650 Total Loss: 3.1943 Recon Loss: 3.1802 
[01/06 10:40:54 TiTok]: Data (t): 0.0012, 54.18/s/gpu Batch (t): 0.5907 LR: 0.000099 Step: 84700 Total Loss: 3.1572 Recon Loss: 3.1431 
[01/06 10:41:24 TiTok]: Data (t): 0.0011, 53.67/s/gpu Batch (t): 0.5962 LR: 0.000099 Step: 84750 Total Loss: 3.2109 Recon Loss: 3.1968 
[01/06 10:41:53 TiTok]: Data (t): 0.0012, 53.94/s/gpu Batch (t): 0.5933 LR: 0.000099 Step: 84800 Total Loss: 3.1876 Recon Loss: 3.1734 
[01/06 10:42:23 TiTok]: Data (t): 0.0009, 53.46/s/gpu Batch (t): 0.5986 LR: 0.000099 Step: 84850 Total Loss: 3.1429 Recon Loss: 3.1287 
[01/06 10:42:53 TiTok]: Data (t): 0.0011, 54.01/s/gpu Batch (t): 0.5924 LR: 0.000099 Step: 84900 Total Loss: 3.1132 Recon Loss: 3.0991 
[01/06 10:43:23 TiTok]: Data (t): 0.0011, 53.85/s/gpu Batch (t): 0.5943 LR: 0.000099 Step: 84950 Total Loss: 3.1799 Recon Loss: 3.1657 
[01/06 10:43:52 TiTok]: Data (t): 0.0011, 46.65/s/gpu Batch (t): 0.6860 LR: 0.000099 Step: 85000 Total Loss: 3.1974 Recon Loss: 3.1832 
[01/06 10:43:54 TiTok]: Reconstructing images...
[01/06 10:44:29 TiTok]: Data (t): 0.0011, 54.03/s/gpu Batch (t): 0.5922 LR: 0.000099 Step: 85050 Total Loss: 3.1594 Recon Loss: 3.1453 
[01/06 10:44:58 TiTok]: Data (t): 0.0011, 54.00/s/gpu Batch (t): 0.5926 LR: 0.000099 Step: 85100 Total Loss: 3.1722 Recon Loss: 3.1580 
[01/06 10:45:28 TiTok]: Data (t): 0.0014, 53.94/s/gpu Batch (t): 0.5932 LR: 0.000099 Step: 85150 Total Loss: 3.1693 Recon Loss: 3.1552 
[01/06 10:45:58 TiTok]: Data (t): 0.0007, 53.48/s/gpu Batch (t): 0.5984 LR: 0.000099 Step: 85200 Total Loss: 3.1583 Recon Loss: 3.1441 
[01/06 10:46:27 TiTok]: Data (t): 0.0007, 53.95/s/gpu Batch (t): 0.5932 LR: 0.000099 Step: 85250 Total Loss: 3.1988 Recon Loss: 3.1846 
Epoch 17/199 started.
[01/06 10:46:59 TiTok]: Data (t): 0.0021, 54.10/s/gpu Batch (t): 0.5915 LR: 0.000099 Step: 85300 Total Loss: 3.1626 Recon Loss: 3.1485 
[01/06 10:47:29 TiTok]: Data (t): 0.0007, 53.48/s/gpu Batch (t): 0.5983 LR: 0.000099 Step: 85350 Total Loss: 3.1727 Recon Loss: 3.1586 
[01/06 10:47:59 TiTok]: Data (t): 0.0012, 53.97/s/gpu Batch (t): 0.5930 LR: 0.000099 Step: 85400 Total Loss: 3.1922 Recon Loss: 3.1780 
[01/06 10:48:29 TiTok]: Data (t): 0.0011, 52.88/s/gpu Batch (t): 0.6052 LR: 0.000099 Step: 85450 Total Loss: 3.2043 Recon Loss: 3.1901 
[01/06 10:48:58 TiTok]: Data (t): 0.0014, 54.39/s/gpu Batch (t): 0.5883 LR: 0.000099 Step: 85500 Total Loss: 3.1545 Recon Loss: 3.1403 
[01/06 10:49:28 TiTok]: Data (t): 0.0011, 54.35/s/gpu Batch (t): 0.5887 LR: 0.000099 Step: 85550 Total Loss: 3.1943 Recon Loss: 3.1801 
[01/06 10:49:58 TiTok]: Data (t): 0.0011, 54.05/s/gpu Batch (t): 0.5920 LR: 0.000099 Step: 85600 Total Loss: 3.1511 Recon Loss: 3.1369 
[01/06 10:50:27 TiTok]: Data (t): 0.0007, 53.50/s/gpu Batch (t): 0.5982 LR: 0.000099 Step: 85650 Total Loss: 3.1921 Recon Loss: 3.1779 
[01/06 10:50:57 TiTok]: Data (t): 0.0011, 53.96/s/gpu Batch (t): 0.5930 LR: 0.000099 Step: 85700 Total Loss: 3.1980 Recon Loss: 3.1838 
[01/06 10:51:27 TiTok]: Data (t): 0.0008, 53.55/s/gpu Batch (t): 0.5976 LR: 0.000099 Step: 85750 Total Loss: 3.1805 Recon Loss: 3.1663 
[01/06 10:51:56 TiTok]: Data (t): 0.0011, 53.92/s/gpu Batch (t): 0.5935 LR: 0.000099 Step: 85800 Total Loss: 3.1795 Recon Loss: 3.1654 
[01/06 10:52:26 TiTok]: Data (t): 0.0012, 54.28/s/gpu Batch (t): 0.5895 LR: 0.000099 Step: 85850 Total Loss: 3.1632 Recon Loss: 3.1491 
[01/06 10:52:56 TiTok]: Data (t): 0.0011, 54.40/s/gpu Batch (t): 0.5883 LR: 0.000099 Step: 85900 Total Loss: 3.2089 Recon Loss: 3.1948 
[01/06 10:53:26 TiTok]: Data (t): 0.0012, 53.93/s/gpu Batch (t): 0.5934 LR: 0.000099 Step: 85950 Total Loss: 3.1511 Recon Loss: 3.1370 
[01/06 10:53:55 TiTok]: Data (t): 0.0011, 45.55/s/gpu Batch (t): 0.7025 LR: 0.000099 Step: 86000 Total Loss: 3.1581 Recon Loss: 3.1440 
[01/06 10:54:25 TiTok]: Data (t): 0.0007, 53.51/s/gpu Batch (t): 0.5980 LR: 0.000099 Step: 86050 Total Loss: 3.1551 Recon Loss: 3.1409 
[01/06 10:54:55 TiTok]: Data (t): 0.0011, 53.86/s/gpu Batch (t): 0.5941 LR: 0.000099 Step: 86100 Total Loss: 3.1981 Recon Loss: 3.1839 
[01/06 10:55:24 TiTok]: Data (t): 0.0011, 53.86/s/gpu Batch (t): 0.5941 LR: 0.000099 Step: 86150 Total Loss: 3.1526 Recon Loss: 3.1384 
[01/06 10:55:54 TiTok]: Data (t): 0.0014, 53.78/s/gpu Batch (t): 0.5950 LR: 0.000099 Step: 86200 Total Loss: 3.1807 Recon Loss: 3.1666 
[01/06 10:56:24 TiTok]: Data (t): 0.0014, 53.93/s/gpu Batch (t): 0.5934 LR: 0.000099 Step: 86250 Total Loss: 3.1841 Recon Loss: 3.1699 
[01/06 10:56:54 TiTok]: Data (t): 0.0011, 54.33/s/gpu Batch (t): 0.5890 LR: 0.000099 Step: 86300 Total Loss: 3.1377 Recon Loss: 3.1235 
[01/06 10:57:23 TiTok]: Data (t): 0.0007, 53.95/s/gpu Batch (t): 0.5932 LR: 0.000099 Step: 86350 Total Loss: 3.1433 Recon Loss: 3.1291 
[01/06 10:57:53 TiTok]: Data (t): 0.0011, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000099 Step: 86400 Total Loss: 3.1671 Recon Loss: 3.1529 
[01/06 10:58:23 TiTok]: Data (t): 0.0007, 53.16/s/gpu Batch (t): 0.6019 LR: 0.000099 Step: 86450 Total Loss: 3.1774 Recon Loss: 3.1632 
[01/06 10:58:52 TiTok]: Data (t): 0.0007, 53.43/s/gpu Batch (t): 0.5990 LR: 0.000099 Step: 86500 Total Loss: 3.1915 Recon Loss: 3.1773 
[01/06 10:59:22 TiTok]: Data (t): 0.0011, 54.23/s/gpu Batch (t): 0.5901 LR: 0.000099 Step: 86550 Total Loss: 3.1992 Recon Loss: 3.1850 
[01/06 10:59:52 TiTok]: Data (t): 0.0012, 53.86/s/gpu Batch (t): 0.5942 LR: 0.000099 Step: 86600 Total Loss: 3.1730 Recon Loss: 3.1588 
[01/06 11:00:22 TiTok]: Data (t): 0.0010, 53.44/s/gpu Batch (t): 0.5988 LR: 0.000099 Step: 86650 Total Loss: 3.1793 Recon Loss: 3.1651 
[01/06 11:00:51 TiTok]: Data (t): 0.0010, 53.42/s/gpu Batch (t): 0.5990 LR: 0.000099 Step: 86700 Total Loss: 3.1132 Recon Loss: 3.0991 
[01/06 11:01:21 TiTok]: Data (t): 0.0014, 53.86/s/gpu Batch (t): 0.5941 LR: 0.000099 Step: 86750 Total Loss: 3.1825 Recon Loss: 3.1683 
[01/06 11:01:51 TiTok]: Data (t): 0.0011, 53.96/s/gpu Batch (t): 0.5931 LR: 0.000099 Step: 86800 Total Loss: 3.1566 Recon Loss: 3.1424 
[01/06 11:02:20 TiTok]: Data (t): 0.0012, 53.90/s/gpu Batch (t): 0.5936 LR: 0.000099 Step: 86850 Total Loss: 3.1710 Recon Loss: 3.1568 
[01/06 11:02:50 TiTok]: Data (t): 0.0007, 52.94/s/gpu Batch (t): 0.6044 LR: 0.000099 Step: 86900 Total Loss: 3.1827 Recon Loss: 3.1686 
[01/06 11:03:20 TiTok]: Data (t): 0.0014, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000099 Step: 86950 Total Loss: 3.1361 Recon Loss: 3.1219 
[01/06 11:03:50 TiTok]: Data (t): 0.0011, 43.58/s/gpu Batch (t): 0.7342 LR: 0.000099 Step: 87000 Total Loss: 3.1463 Recon Loss: 3.1321 
[01/06 11:04:19 TiTok]: Data (t): 0.0012, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000099 Step: 87050 Total Loss: 3.1845 Recon Loss: 3.1703 
[01/06 11:04:49 TiTok]: Data (t): 0.0011, 53.82/s/gpu Batch (t): 0.5945 LR: 0.000099 Step: 87100 Total Loss: 3.1882 Recon Loss: 3.1740 
[01/06 11:05:19 TiTok]: Data (t): 0.0014, 53.31/s/gpu Batch (t): 0.6002 LR: 0.000099 Step: 87150 Total Loss: 3.2052 Recon Loss: 3.1910 
[01/06 11:05:48 TiTok]: Data (t): 0.0011, 53.85/s/gpu Batch (t): 0.5942 LR: 0.000099 Step: 87200 Total Loss: 3.2010 Recon Loss: 3.1868 
[01/06 11:06:18 TiTok]: Data (t): 0.0006, 53.52/s/gpu Batch (t): 0.5979 LR: 0.000099 Step: 87250 Total Loss: 3.1945 Recon Loss: 3.1804 
[01/06 11:06:48 TiTok]: Data (t): 0.0011, 53.95/s/gpu Batch (t): 0.5932 LR: 0.000099 Step: 87300 Total Loss: 3.1220 Recon Loss: 3.1078 
[01/06 11:07:18 TiTok]: Data (t): 0.0007, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000099 Step: 87350 Total Loss: 3.1730 Recon Loss: 3.1589 
[01/06 11:07:47 TiTok]: Data (t): 0.0011, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000099 Step: 87400 Total Loss: 3.1176 Recon Loss: 3.1035 
[01/06 11:08:17 TiTok]: Data (t): 0.0007, 53.86/s/gpu Batch (t): 0.5941 LR: 0.000099 Step: 87450 Total Loss: 3.1608 Recon Loss: 3.1466 
[01/06 11:08:47 TiTok]: Data (t): 0.0008, 52.80/s/gpu Batch (t): 0.6060 LR: 0.000099 Step: 87500 Total Loss: 3.1910 Recon Loss: 3.1769 
[01/06 11:09:16 TiTok]: Data (t): 0.0007, 53.74/s/gpu Batch (t): 0.5955 LR: 0.000099 Step: 87550 Total Loss: 3.1942 Recon Loss: 3.1801 
[01/06 11:09:46 TiTok]: Data (t): 0.0011, 54.01/s/gpu Batch (t): 0.5925 LR: 0.000099 Step: 87600 Total Loss: 3.1511 Recon Loss: 3.1369 
[01/06 11:10:16 TiTok]: Data (t): 0.0011, 53.94/s/gpu Batch (t): 0.5932 LR: 0.000099 Step: 87650 Total Loss: 3.1986 Recon Loss: 3.1844 
[01/06 11:10:45 TiTok]: Data (t): 0.0014, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000099 Step: 87700 Total Loss: 3.1999 Recon Loss: 3.1857 
[01/06 11:11:15 TiTok]: Data (t): 0.0011, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000099 Step: 87750 Total Loss: 3.1620 Recon Loss: 3.1478 
[01/06 11:11:45 TiTok]: Data (t): 0.0012, 54.31/s/gpu Batch (t): 0.5892 LR: 0.000099 Step: 87800 Total Loss: 3.1523 Recon Loss: 3.1381 
[01/06 11:12:15 TiTok]: Data (t): 0.0011, 53.95/s/gpu Batch (t): 0.5932 LR: 0.000099 Step: 87850 Total Loss: 3.1582 Recon Loss: 3.1441 
[01/06 11:12:44 TiTok]: Data (t): 0.0007, 53.47/s/gpu Batch (t): 0.5984 LR: 0.000099 Step: 87900 Total Loss: 3.1778 Recon Loss: 3.1636 
[01/06 11:13:14 TiTok]: Data (t): 0.0011, 54.18/s/gpu Batch (t): 0.5906 LR: 0.000099 Step: 87950 Total Loss: 3.1655 Recon Loss: 3.1514 
[01/06 11:13:44 TiTok]: Data (t): 0.0011, 44.03/s/gpu Batch (t): 0.7268 LR: 0.000099 Step: 88000 Total Loss: 3.1533 Recon Loss: 3.1391 
[01/06 11:14:14 TiTok]: Data (t): 0.0011, 53.79/s/gpu Batch (t): 0.5949 LR: 0.000099 Step: 88050 Total Loss: 3.1670 Recon Loss: 3.1528 
[01/06 11:14:43 TiTok]: Data (t): 0.0012, 54.02/s/gpu Batch (t): 0.5924 LR: 0.000099 Step: 88100 Total Loss: 3.1747 Recon Loss: 3.1605 
[01/06 11:15:13 TiTok]: Data (t): 0.0007, 53.84/s/gpu Batch (t): 0.5944 LR: 0.000099 Step: 88150 Total Loss: 3.2149 Recon Loss: 3.2007 
[01/06 11:15:43 TiTok]: Data (t): 0.0011, 53.95/s/gpu Batch (t): 0.5932 LR: 0.000099 Step: 88200 Total Loss: 3.1601 Recon Loss: 3.1460 
[01/06 11:16:13 TiTok]: Data (t): 0.0012, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000099 Step: 88250 Total Loss: 3.1653 Recon Loss: 3.1511 
[01/06 11:16:42 TiTok]: Data (t): 0.0011, 53.77/s/gpu Batch (t): 0.5951 LR: 0.000099 Step: 88300 Total Loss: 3.1997 Recon Loss: 3.1855 
[01/06 11:17:12 TiTok]: Data (t): 0.0011, 53.70/s/gpu Batch (t): 0.5959 LR: 0.000099 Step: 88350 Total Loss: 3.1546 Recon Loss: 3.1404 
[01/06 11:17:42 TiTok]: Data (t): 0.0011, 53.84/s/gpu Batch (t): 0.5943 LR: 0.000099 Step: 88400 Total Loss: 3.1666 Recon Loss: 3.1524 
[01/06 11:18:11 TiTok]: Data (t): 0.0012, 53.84/s/gpu Batch (t): 0.5944 LR: 0.000099 Step: 88450 Total Loss: 3.1477 Recon Loss: 3.1335 
[01/06 11:18:41 TiTok]: Data (t): 0.0011, 53.63/s/gpu Batch (t): 0.5967 LR: 0.000099 Step: 88500 Total Loss: 3.1594 Recon Loss: 3.1452 
[01/06 11:19:11 TiTok]: Data (t): 0.0007, 53.47/s/gpu Batch (t): 0.5985 LR: 0.000099 Step: 88550 Total Loss: 3.1765 Recon Loss: 3.1623 
[01/06 11:19:41 TiTok]: Data (t): 0.0007, 53.42/s/gpu Batch (t): 0.5990 LR: 0.000099 Step: 88600 Total Loss: 3.1306 Recon Loss: 3.1164 
[01/06 11:20:10 TiTok]: Data (t): 0.0011, 53.94/s/gpu Batch (t): 0.5932 LR: 0.000099 Step: 88650 Total Loss: 3.1539 Recon Loss: 3.1397 
[01/06 11:20:40 TiTok]: Data (t): 0.0011, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000099 Step: 88700 Total Loss: 3.1722 Recon Loss: 3.1581 
[01/06 11:21:10 TiTok]: Data (t): 0.0007, 53.45/s/gpu Batch (t): 0.5986 LR: 0.000099 Step: 88750 Total Loss: 3.1431 Recon Loss: 3.1290 
[01/06 11:21:40 TiTok]: Data (t): 0.0007, 53.93/s/gpu Batch (t): 0.5933 LR: 0.000099 Step: 88800 Total Loss: 3.1492 Recon Loss: 3.1351 
[01/06 11:22:09 TiTok]: Data (t): 0.0011, 54.43/s/gpu Batch (t): 0.5879 LR: 0.000099 Step: 88850 Total Loss: 3.1354 Recon Loss: 3.1212 
[01/06 11:22:39 TiTok]: Data (t): 0.0011, 53.69/s/gpu Batch (t): 0.5960 LR: 0.000099 Step: 88900 Total Loss: 3.1408 Recon Loss: 3.1266 
[01/06 11:23:09 TiTok]: Data (t): 0.0014, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000099 Step: 88950 Total Loss: 3.1709 Recon Loss: 3.1567 
[01/06 11:23:39 TiTok]: Data (t): 0.0012, 44.82/s/gpu Batch (t): 0.7140 LR: 0.000099 Step: 89000 Total Loss: 3.1781 Recon Loss: 3.1639 
[01/06 11:24:08 TiTok]: Data (t): 0.0014, 53.95/s/gpu Batch (t): 0.5932 LR: 0.000099 Step: 89050 Total Loss: 3.1770 Recon Loss: 3.1629 
[01/06 11:24:38 TiTok]: Data (t): 0.0011, 53.86/s/gpu Batch (t): 0.5941 LR: 0.000099 Step: 89100 Total Loss: 3.1601 Recon Loss: 3.1459 
[01/06 11:25:08 TiTok]: Data (t): 0.0012, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000099 Step: 89150 Total Loss: 3.1571 Recon Loss: 3.1429 
[01/06 11:25:37 TiTok]: Data (t): 0.0028, 53.96/s/gpu Batch (t): 0.5931 LR: 0.000099 Step: 89200 Total Loss: 3.1256 Recon Loss: 3.1114 
[01/06 11:26:07 TiTok]: Data (t): 0.0006, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000099 Step: 89250 Total Loss: 3.1356 Recon Loss: 3.1214 
[01/06 11:26:37 TiTok]: Data (t): 0.0011, 54.07/s/gpu Batch (t): 0.5918 LR: 0.000099 Step: 89300 Total Loss: 3.1837 Recon Loss: 3.1695 
[01/06 11:27:07 TiTok]: Data (t): 0.0011, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000099 Step: 89350 Total Loss: 3.1722 Recon Loss: 3.1580 
[01/06 11:27:36 TiTok]: Data (t): 0.0008, 53.86/s/gpu Batch (t): 0.5942 LR: 0.000099 Step: 89400 Total Loss: 3.1230 Recon Loss: 3.1088 
[01/06 11:28:06 TiTok]: Data (t): 0.0012, 53.85/s/gpu Batch (t): 0.5943 LR: 0.000099 Step: 89450 Total Loss: 3.1647 Recon Loss: 3.1505 
[01/06 11:28:36 TiTok]: Data (t): 0.0007, 53.96/s/gpu Batch (t): 0.5930 LR: 0.000099 Step: 89500 Total Loss: 3.1847 Recon Loss: 3.1705 
[01/06 11:29:05 TiTok]: Data (t): 0.0011, 52.75/s/gpu Batch (t): 0.6067 LR: 0.000099 Step: 89550 Total Loss: 3.1940 Recon Loss: 3.1797 
[01/06 11:29:35 TiTok]: Data (t): 0.0011, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000099 Step: 89600 Total Loss: 3.1467 Recon Loss: 3.1325 
[01/06 11:30:05 TiTok]: Data (t): 0.0011, 53.86/s/gpu Batch (t): 0.5942 LR: 0.000099 Step: 89650 Total Loss: 3.1616 Recon Loss: 3.1474 
[01/06 11:30:34 TiTok]: Data (t): 0.0011, 54.36/s/gpu Batch (t): 0.5887 LR: 0.000099 Step: 89700 Total Loss: 3.1512 Recon Loss: 3.1371 
[01/06 11:31:04 TiTok]: Data (t): 0.0012, 53.52/s/gpu Batch (t): 0.5980 LR: 0.000099 Step: 89750 Total Loss: 3.1389 Recon Loss: 3.1248 
[01/06 11:31:34 TiTok]: Data (t): 0.0011, 53.69/s/gpu Batch (t): 0.5960 LR: 0.000099 Step: 89800 Total Loss: 3.1915 Recon Loss: 3.1773 
[01/06 11:32:04 TiTok]: Data (t): 0.0012, 53.81/s/gpu Batch (t): 0.5946 LR: 0.000099 Step: 89850 Total Loss: 3.1353 Recon Loss: 3.1211 
[01/06 11:32:34 TiTok]: Data (t): 0.0014, 53.95/s/gpu Batch (t): 0.5931 LR: 0.000099 Step: 89900 Total Loss: 3.1417 Recon Loss: 3.1275 
[01/06 11:33:03 TiTok]: Data (t): 0.0007, 53.29/s/gpu Batch (t): 0.6004 LR: 0.000099 Step: 89950 Total Loss: 3.2027 Recon Loss: 3.1885 
[01/06 11:33:33 TiTok]: Data (t): 0.0012, 46.97/s/gpu Batch (t): 0.6813 LR: 0.000099 Step: 90000 Total Loss: 3.1984 Recon Loss: 3.1842 
[01/06 11:33:35 TiTok]: Reconstructing images...
[01/06 11:34:10 TiTok]: Data (t): 0.0012, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000099 Step: 90050 Total Loss: 3.1963 Recon Loss: 3.1821 
[01/06 11:34:39 TiTok]: Data (t): 0.0011, 54.13/s/gpu Batch (t): 0.5912 LR: 0.000099 Step: 90100 Total Loss: 3.1825 Recon Loss: 3.1683 
[01/06 11:35:09 TiTok]: Data (t): 0.0007, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000099 Step: 90150 Total Loss: 3.1852 Recon Loss: 3.1710 
[01/06 11:35:39 TiTok]: Data (t): 0.0011, 54.28/s/gpu Batch (t): 0.5895 LR: 0.000099 Step: 90200 Total Loss: 3.1459 Recon Loss: 3.1318 
[01/06 11:36:08 TiTok]: Data (t): 0.0012, 53.79/s/gpu Batch (t): 0.5950 LR: 0.000099 Step: 90250 Total Loss: 3.1672 Recon Loss: 3.1531 
Epoch 18/199 started.
[01/06 11:36:40 TiTok]: Data (t): 0.0012, 53.70/s/gpu Batch (t): 0.5959 LR: 0.000099 Step: 90300 Total Loss: 3.1485 Recon Loss: 3.1343 
[01/06 11:37:09 TiTok]: Data (t): 0.0012, 53.98/s/gpu Batch (t): 0.5929 LR: 0.000099 Step: 90350 Total Loss: 3.1312 Recon Loss: 3.1170 
[01/06 11:37:39 TiTok]: Data (t): 0.0011, 53.92/s/gpu Batch (t): 0.5935 LR: 0.000099 Step: 90400 Total Loss: 3.1788 Recon Loss: 3.1646 
[01/06 11:38:09 TiTok]: Data (t): 0.0007, 53.66/s/gpu Batch (t): 0.5963 LR: 0.000099 Step: 90450 Total Loss: 3.2029 Recon Loss: 3.1887 
[01/06 11:38:39 TiTok]: Data (t): 0.0006, 53.47/s/gpu Batch (t): 0.5985 LR: 0.000099 Step: 90500 Total Loss: 3.1613 Recon Loss: 3.1472 
[01/06 11:39:08 TiTok]: Data (t): 0.0012, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000099 Step: 90550 Total Loss: 3.1754 Recon Loss: 3.1611 
[01/06 11:39:38 TiTok]: Data (t): 0.0011, 54.00/s/gpu Batch (t): 0.5926 LR: 0.000099 Step: 90600 Total Loss: 3.1600 Recon Loss: 3.1458 
[01/06 11:40:08 TiTok]: Data (t): 0.0356, 53.20/s/gpu Batch (t): 0.6015 LR: 0.000099 Step: 90650 Total Loss: 3.1614 Recon Loss: 3.1472 
[01/06 11:40:38 TiTok]: Data (t): 0.0011, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000099 Step: 90700 Total Loss: 3.1624 Recon Loss: 3.1482 
[01/06 11:41:07 TiTok]: Data (t): 0.0014, 53.85/s/gpu Batch (t): 0.5942 LR: 0.000099 Step: 90750 Total Loss: 3.1659 Recon Loss: 3.1517 
[01/06 11:41:37 TiTok]: Data (t): 0.0014, 53.94/s/gpu Batch (t): 0.5933 LR: 0.000099 Step: 90800 Total Loss: 3.1630 Recon Loss: 3.1488 
[01/06 11:42:07 TiTok]: Data (t): 0.0012, 54.24/s/gpu Batch (t): 0.5899 LR: 0.000099 Step: 90850 Total Loss: 3.1653 Recon Loss: 3.1511 
[01/06 11:42:36 TiTok]: Data (t): 0.0013, 53.97/s/gpu Batch (t): 0.5929 LR: 0.000099 Step: 90900 Total Loss: 3.1441 Recon Loss: 3.1299 
[01/06 11:43:06 TiTok]: Data (t): 0.0007, 53.31/s/gpu Batch (t): 0.6003 LR: 0.000099 Step: 90950 Total Loss: 3.1107 Recon Loss: 3.0966 
[01/06 11:43:36 TiTok]: Data (t): 0.0012, 43.61/s/gpu Batch (t): 0.7338 LR: 0.000099 Step: 91000 Total Loss: 3.1746 Recon Loss: 3.1605 
[01/06 11:44:06 TiTok]: Data (t): 0.0010, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000099 Step: 91050 Total Loss: 3.1711 Recon Loss: 3.1569 
[01/06 11:44:35 TiTok]: Data (t): 0.0012, 53.74/s/gpu Batch (t): 0.5955 LR: 0.000099 Step: 91100 Total Loss: 3.1682 Recon Loss: 3.1540 
[01/06 11:45:05 TiTok]: Data (t): 0.0011, 53.97/s/gpu Batch (t): 0.5929 LR: 0.000099 Step: 91150 Total Loss: 3.1955 Recon Loss: 3.1813 
[01/06 11:45:35 TiTok]: Data (t): 0.0012, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000099 Step: 91200 Total Loss: 3.1311 Recon Loss: 3.1169 
[01/06 11:46:05 TiTok]: Data (t): 0.0011, 54.19/s/gpu Batch (t): 0.5905 LR: 0.000099 Step: 91250 Total Loss: 3.1824 Recon Loss: 3.1682 
[01/06 11:46:34 TiTok]: Data (t): 0.0008, 53.37/s/gpu Batch (t): 0.5996 LR: 0.000099 Step: 91300 Total Loss: 3.1595 Recon Loss: 3.1453 
[01/06 11:47:04 TiTok]: Data (t): 0.0012, 53.74/s/gpu Batch (t): 0.5954 LR: 0.000099 Step: 91350 Total Loss: 3.1781 Recon Loss: 3.1639 
[01/06 11:47:34 TiTok]: Data (t): 0.0007, 53.95/s/gpu Batch (t): 0.5932 LR: 0.000099 Step: 91400 Total Loss: 3.1485 Recon Loss: 3.1343 
[01/06 11:48:03 TiTok]: Data (t): 0.0007, 53.45/s/gpu Batch (t): 0.5987 LR: 0.000099 Step: 91450 Total Loss: 3.1437 Recon Loss: 3.1295 
[01/06 11:48:33 TiTok]: Data (t): 0.0011, 53.78/s/gpu Batch (t): 0.5950 LR: 0.000099 Step: 91500 Total Loss: 3.1320 Recon Loss: 3.1177 
[01/06 11:49:03 TiTok]: Data (t): 0.0011, 53.97/s/gpu Batch (t): 0.5930 LR: 0.000099 Step: 91550 Total Loss: 3.1538 Recon Loss: 3.1397 
[01/06 11:49:33 TiTok]: Data (t): 0.0011, 54.38/s/gpu Batch (t): 0.5884 LR: 0.000099 Step: 91600 Total Loss: 3.1265 Recon Loss: 3.1123 
[01/06 11:50:02 TiTok]: Data (t): 0.0014, 54.39/s/gpu Batch (t): 0.5883 LR: 0.000098 Step: 91650 Total Loss: 3.1541 Recon Loss: 3.1399 
[01/06 11:50:32 TiTok]: Data (t): 0.0012, 53.94/s/gpu Batch (t): 0.5932 LR: 0.000098 Step: 91700 Total Loss: 3.1658 Recon Loss: 3.1516 
[01/06 11:51:02 TiTok]: Data (t): 0.0012, 54.31/s/gpu Batch (t): 0.5892 LR: 0.000098 Step: 91750 Total Loss: 3.1792 Recon Loss: 3.1650 
[01/06 11:51:31 TiTok]: Data (t): 0.0008, 53.81/s/gpu Batch (t): 0.5946 LR: 0.000098 Step: 91800 Total Loss: 3.1411 Recon Loss: 3.1269 
[01/06 11:52:01 TiTok]: Data (t): 0.0011, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000098 Step: 91850 Total Loss: 3.1433 Recon Loss: 3.1291 
[01/06 11:52:31 TiTok]: Data (t): 0.0011, 53.85/s/gpu Batch (t): 0.5943 LR: 0.000098 Step: 91900 Total Loss: 3.1290 Recon Loss: 3.1148 
[01/06 11:53:01 TiTok]: Data (t): 0.0012, 54.38/s/gpu Batch (t): 0.5884 LR: 0.000098 Step: 91950 Total Loss: 3.1721 Recon Loss: 3.1579 
[01/06 11:53:31 TiTok]: Data (t): 0.0012, 45.18/s/gpu Batch (t): 0.7082 LR: 0.000098 Step: 92000 Total Loss: 3.1280 Recon Loss: 3.1138 
[01/06 11:54:00 TiTok]: Data (t): 0.0011, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000098 Step: 92050 Total Loss: 3.1531 Recon Loss: 3.1390 
[01/06 11:54:30 TiTok]: Data (t): 0.0012, 53.73/s/gpu Batch (t): 0.5956 LR: 0.000098 Step: 92100 Total Loss: 3.2135 Recon Loss: 3.1993 
[01/06 11:55:00 TiTok]: Data (t): 0.0009, 53.80/s/gpu Batch (t): 0.5948 LR: 0.000098 Step: 92150 Total Loss: 3.1812 Recon Loss: 3.1670 
[01/06 11:55:30 TiTok]: Data (t): 0.0011, 53.74/s/gpu Batch (t): 0.5954 LR: 0.000098 Step: 92200 Total Loss: 3.2131 Recon Loss: 3.1989 
[01/06 11:55:59 TiTok]: Data (t): 0.0011, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000098 Step: 92250 Total Loss: 3.1180 Recon Loss: 3.1038 
[01/06 11:56:29 TiTok]: Data (t): 0.0012, 53.81/s/gpu Batch (t): 0.5947 LR: 0.000098 Step: 92300 Total Loss: 3.1324 Recon Loss: 3.1182 
[01/06 11:56:59 TiTok]: Data (t): 0.0012, 53.86/s/gpu Batch (t): 0.5941 LR: 0.000098 Step: 92350 Total Loss: 3.1296 Recon Loss: 3.1154 
[01/06 11:57:29 TiTok]: Data (t): 0.0007, 53.45/s/gpu Batch (t): 0.5987 LR: 0.000098 Step: 92400 Total Loss: 3.1569 Recon Loss: 3.1426 
[01/06 11:57:58 TiTok]: Data (t): 0.0011, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000098 Step: 92450 Total Loss: 3.1572 Recon Loss: 3.1429 
[01/06 11:58:28 TiTok]: Data (t): 0.0008, 53.47/s/gpu Batch (t): 0.5984 LR: 0.000098 Step: 92500 Total Loss: 3.1244 Recon Loss: 3.1101 
[01/06 11:58:58 TiTok]: Data (t): 0.0012, 54.25/s/gpu Batch (t): 0.5898 LR: 0.000098 Step: 92550 Total Loss: 3.1333 Recon Loss: 3.1191 
[01/06 11:59:27 TiTok]: Data (t): 0.0012, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000098 Step: 92600 Total Loss: 3.1523 Recon Loss: 3.1380 
[01/06 11:59:57 TiTok]: Data (t): 0.0007, 53.63/s/gpu Batch (t): 0.5966 LR: 0.000098 Step: 92650 Total Loss: 3.1687 Recon Loss: 3.1545 
[01/06 12:00:27 TiTok]: Data (t): 0.0012, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000098 Step: 92700 Total Loss: 3.1262 Recon Loss: 3.1120 
[01/06 12:00:57 TiTok]: Data (t): 0.0012, 53.77/s/gpu Batch (t): 0.5952 LR: 0.000098 Step: 92750 Total Loss: 3.1448 Recon Loss: 3.1305 
[01/06 12:01:26 TiTok]: Data (t): 0.0007, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000098 Step: 92800 Total Loss: 3.1730 Recon Loss: 3.1588 
[01/06 12:01:56 TiTok]: Data (t): 0.0007, 53.81/s/gpu Batch (t): 0.5947 LR: 0.000098 Step: 92850 Total Loss: 3.1712 Recon Loss: 3.1570 
[01/06 12:02:26 TiTok]: Data (t): 0.0012, 53.75/s/gpu Batch (t): 0.5954 LR: 0.000098 Step: 92900 Total Loss: 3.1473 Recon Loss: 3.1331 
[01/06 12:02:55 TiTok]: Data (t): 0.0012, 53.86/s/gpu Batch (t): 0.5942 LR: 0.000098 Step: 92950 Total Loss: 3.1556 Recon Loss: 3.1414 
[01/06 12:03:25 TiTok]: Data (t): 0.0007, 43.80/s/gpu Batch (t): 0.7305 LR: 0.000098 Step: 93000 Total Loss: 3.1568 Recon Loss: 3.1426 
[01/06 12:03:55 TiTok]: Data (t): 0.0030, 54.68/s/gpu Batch (t): 0.5853 LR: 0.000098 Step: 93050 Total Loss: 3.1974 Recon Loss: 3.1832 
[01/06 12:04:25 TiTok]: Data (t): 0.0012, 53.80/s/gpu Batch (t): 0.5948 LR: 0.000098 Step: 93100 Total Loss: 3.1322 Recon Loss: 3.1180 
[01/06 12:04:54 TiTok]: Data (t): 0.0011, 54.32/s/gpu Batch (t): 0.5891 LR: 0.000098 Step: 93150 Total Loss: 3.1568 Recon Loss: 3.1425 
[01/06 12:05:24 TiTok]: Data (t): 0.0012, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000098 Step: 93200 Total Loss: 3.1734 Recon Loss: 3.1593 
[01/06 12:05:54 TiTok]: Data (t): 0.0011, 53.79/s/gpu Batch (t): 0.5949 LR: 0.000098 Step: 93250 Total Loss: 3.1450 Recon Loss: 3.1308 
[01/06 12:06:24 TiTok]: Data (t): 0.0012, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000098 Step: 93300 Total Loss: 3.1732 Recon Loss: 3.1589 
[01/06 12:06:53 TiTok]: Data (t): 0.0011, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000098 Step: 93350 Total Loss: 3.1428 Recon Loss: 3.1285 
[01/06 12:07:23 TiTok]: Data (t): 0.0007, 52.15/s/gpu Batch (t): 0.6137 LR: 0.000098 Step: 93400 Total Loss: 3.1273 Recon Loss: 3.1131 
[01/06 12:07:53 TiTok]: Data (t): 0.0007, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000098 Step: 93450 Total Loss: 3.1055 Recon Loss: 3.0912 
[01/06 12:08:23 TiTok]: Data (t): 0.0007, 52.97/s/gpu Batch (t): 0.6041 LR: 0.000098 Step: 93500 Total Loss: 3.1579 Recon Loss: 3.1437 
[01/06 12:08:52 TiTok]: Data (t): 0.0007, 53.95/s/gpu Batch (t): 0.5932 LR: 0.000098 Step: 93550 Total Loss: 3.1765 Recon Loss: 3.1622 
[01/06 12:09:22 TiTok]: Data (t): 0.0011, 53.96/s/gpu Batch (t): 0.5930 LR: 0.000098 Step: 93600 Total Loss: 3.1618 Recon Loss: 3.1475 
[01/06 12:09:52 TiTok]: Data (t): 0.0011, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000098 Step: 93650 Total Loss: 3.1482 Recon Loss: 3.1340 
[01/06 12:10:21 TiTok]: Data (t): 0.0012, 53.92/s/gpu Batch (t): 0.5934 LR: 0.000098 Step: 93700 Total Loss: 3.1406 Recon Loss: 3.1264 
[01/06 12:10:51 TiTok]: Data (t): 0.0011, 53.81/s/gpu Batch (t): 0.5947 LR: 0.000098 Step: 93750 Total Loss: 3.1453 Recon Loss: 3.1311 
[01/06 12:11:21 TiTok]: Data (t): 0.0007, 53.38/s/gpu Batch (t): 0.5994 LR: 0.000098 Step: 93800 Total Loss: 3.1408 Recon Loss: 3.1266 
[01/06 12:11:51 TiTok]: Data (t): 0.0011, 53.82/s/gpu Batch (t): 0.5945 LR: 0.000098 Step: 93850 Total Loss: 3.1257 Recon Loss: 3.1115 
[01/06 12:12:20 TiTok]: Data (t): 0.0012, 53.84/s/gpu Batch (t): 0.5943 LR: 0.000098 Step: 93900 Total Loss: 3.1697 Recon Loss: 3.1555 
[01/06 12:12:50 TiTok]: Data (t): 0.0011, 53.92/s/gpu Batch (t): 0.5934 LR: 0.000098 Step: 93950 Total Loss: 3.1369 Recon Loss: 3.1227 
[01/06 12:13:20 TiTok]: Data (t): 0.0007, 42.61/s/gpu Batch (t): 0.7511 LR: 0.000098 Step: 94000 Total Loss: 3.1564 Recon Loss: 3.1421 
[01/06 12:13:50 TiTok]: Data (t): 0.0012, 53.84/s/gpu Batch (t): 0.5943 LR: 0.000098 Step: 94050 Total Loss: 3.1490 Recon Loss: 3.1348 
[01/06 12:14:19 TiTok]: Data (t): 0.0011, 53.73/s/gpu Batch (t): 0.5956 LR: 0.000098 Step: 94100 Total Loss: 3.1423 Recon Loss: 3.1280 
[01/06 12:14:49 TiTok]: Data (t): 0.0011, 53.96/s/gpu Batch (t): 0.5931 LR: 0.000098 Step: 94150 Total Loss: 3.1536 Recon Loss: 3.1394 
[01/06 12:15:19 TiTok]: Data (t): 0.0007, 53.57/s/gpu Batch (t): 0.5973 LR: 0.000098 Step: 94200 Total Loss: 3.1407 Recon Loss: 3.1265 
[01/06 12:15:49 TiTok]: Data (t): 0.0011, 53.91/s/gpu Batch (t): 0.5935 LR: 0.000098 Step: 94250 Total Loss: 3.1415 Recon Loss: 3.1273 
[01/06 12:16:18 TiTok]: Data (t): 0.0007, 53.65/s/gpu Batch (t): 0.5965 LR: 0.000098 Step: 94300 Total Loss: 3.1228 Recon Loss: 3.1085 
[01/06 12:16:48 TiTok]: Data (t): 0.0011, 54.29/s/gpu Batch (t): 0.5894 LR: 0.000098 Step: 94350 Total Loss: 3.1430 Recon Loss: 3.1288 
[01/06 12:17:18 TiTok]: Data (t): 0.0011, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000098 Step: 94400 Total Loss: 3.1851 Recon Loss: 3.1709 
[01/06 12:17:48 TiTok]: Data (t): 0.0012, 53.73/s/gpu Batch (t): 0.5955 LR: 0.000098 Step: 94450 Total Loss: 3.1197 Recon Loss: 3.1055 
[01/06 12:18:17 TiTok]: Data (t): 0.0007, 53.39/s/gpu Batch (t): 0.5994 LR: 0.000098 Step: 94500 Total Loss: 3.1177 Recon Loss: 3.1034 
[01/06 12:18:47 TiTok]: Data (t): 0.0012, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000098 Step: 94550 Total Loss: 3.1556 Recon Loss: 3.1414 
[01/06 12:19:17 TiTok]: Data (t): 0.0011, 53.92/s/gpu Batch (t): 0.5934 LR: 0.000098 Step: 94600 Total Loss: 3.1390 Recon Loss: 3.1247 
[01/06 12:19:46 TiTok]: Data (t): 0.0012, 54.03/s/gpu Batch (t): 0.5923 LR: 0.000098 Step: 94650 Total Loss: 3.1210 Recon Loss: 3.1068 
[01/06 12:20:16 TiTok]: Data (t): 0.0011, 52.95/s/gpu Batch (t): 0.6044 LR: 0.000098 Step: 94700 Total Loss: 3.1399 Recon Loss: 3.1257 
[01/06 12:20:46 TiTok]: Data (t): 0.0012, 53.92/s/gpu Batch (t): 0.5935 LR: 0.000098 Step: 94750 Total Loss: 3.1614 Recon Loss: 3.1472 
[01/06 12:21:16 TiTok]: Data (t): 0.0012, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000098 Step: 94800 Total Loss: 3.1522 Recon Loss: 3.1380 
[01/06 12:21:45 TiTok]: Data (t): 0.0012, 53.94/s/gpu Batch (t): 0.5933 LR: 0.000098 Step: 94850 Total Loss: 3.1359 Recon Loss: 3.1216 
[01/06 12:22:15 TiTok]: Data (t): 0.0014, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000098 Step: 94900 Total Loss: 3.1354 Recon Loss: 3.1211 
[01/06 12:22:45 TiTok]: Data (t): 0.0011, 53.23/s/gpu Batch (t): 0.6012 LR: 0.000098 Step: 94950 Total Loss: 3.1624 Recon Loss: 3.1481 
[01/06 12:23:14 TiTok]: Data (t): 0.0014, 47.61/s/gpu Batch (t): 0.6721 LR: 0.000098 Step: 95000 Total Loss: 3.1287 Recon Loss: 3.1145 
[01/06 12:23:16 TiTok]: Reconstructing images...
[01/06 12:23:51 TiTok]: Data (t): 0.0006, 53.98/s/gpu Batch (t): 0.5928 LR: 0.000098 Step: 95050 Total Loss: 3.1860 Recon Loss: 3.1718 
[01/06 12:24:21 TiTok]: Data (t): 0.0011, 53.72/s/gpu Batch (t): 0.5957 LR: 0.000098 Step: 95100 Total Loss: 3.1693 Recon Loss: 3.1550 
[01/06 12:24:51 TiTok]: Data (t): 0.0007, 53.53/s/gpu Batch (t): 0.5977 LR: 0.000098 Step: 95150 Total Loss: 3.1690 Recon Loss: 3.1547 
[01/06 12:25:20 TiTok]: Data (t): 0.0011, 53.76/s/gpu Batch (t): 0.5953 LR: 0.000098 Step: 95200 Total Loss: 3.1205 Recon Loss: 3.1063 
[01/06 12:25:50 TiTok]: Data (t): 0.0007, 53.31/s/gpu Batch (t): 0.6003 LR: 0.000098 Step: 95250 Total Loss: 3.1127 Recon Loss: 3.0985 
[01/06 12:26:20 TiTok]: Data (t): 0.0006, 53.18/s/gpu Batch (t): 0.6017 LR: 0.000098 Step: 95300 Total Loss: 3.1797 Recon Loss: 3.1654 
Epoch 19/199 started.
[01/06 12:26:52 TiTok]: Data (t): 0.0011, 54.04/s/gpu Batch (t): 0.5921 LR: 0.000098 Step: 95350 Total Loss: 3.1261 Recon Loss: 3.1118 
[01/06 12:27:21 TiTok]: Data (t): 0.0007, 53.97/s/gpu Batch (t): 0.5929 LR: 0.000098 Step: 95400 Total Loss: 3.1360 Recon Loss: 3.1217 
[01/06 12:27:51 TiTok]: Data (t): 0.0013, 53.84/s/gpu Batch (t): 0.5944 LR: 0.000098 Step: 95450 Total Loss: 3.1488 Recon Loss: 3.1346 
[01/06 12:28:21 TiTok]: Data (t): 0.0011, 53.86/s/gpu Batch (t): 0.5942 LR: 0.000098 Step: 95500 Total Loss: 3.1285 Recon Loss: 3.1143 
[01/06 12:28:50 TiTok]: Data (t): 0.0012, 53.96/s/gpu Batch (t): 0.5930 LR: 0.000098 Step: 95550 Total Loss: 3.1180 Recon Loss: 3.1038 
[01/06 12:29:20 TiTok]: Data (t): 0.0011, 53.98/s/gpu Batch (t): 0.5928 LR: 0.000098 Step: 95600 Total Loss: 3.1560 Recon Loss: 3.1418 
[01/06 12:29:50 TiTok]: Data (t): 0.0013, 53.85/s/gpu Batch (t): 0.5943 LR: 0.000098 Step: 95650 Total Loss: 3.1664 Recon Loss: 3.1522 
[01/06 12:30:20 TiTok]: Data (t): 0.0012, 53.92/s/gpu Batch (t): 0.5935 LR: 0.000098 Step: 95700 Total Loss: 3.1565 Recon Loss: 3.1423 
[01/06 12:30:49 TiTok]: Data (t): 0.0012, 53.81/s/gpu Batch (t): 0.5946 LR: 0.000098 Step: 95750 Total Loss: 3.1149 Recon Loss: 3.1007 
[01/06 12:31:19 TiTok]: Data (t): 0.0012, 53.94/s/gpu Batch (t): 0.5932 LR: 0.000098 Step: 95800 Total Loss: 3.1440 Recon Loss: 3.1297 
[01/06 12:31:49 TiTok]: Data (t): 0.0011, 54.00/s/gpu Batch (t): 0.5926 LR: 0.000098 Step: 95850 Total Loss: 3.1080 Recon Loss: 3.0938 
[01/06 12:32:18 TiTok]: Data (t): 0.0012, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000098 Step: 95900 Total Loss: 3.1583 Recon Loss: 3.1440 
[01/06 12:32:48 TiTok]: Data (t): 0.0012, 54.06/s/gpu Batch (t): 0.5919 LR: 0.000098 Step: 95950 Total Loss: 3.1530 Recon Loss: 3.1387 
[01/06 12:33:18 TiTok]: Data (t): 0.0017, 42.37/s/gpu Batch (t): 0.7552 LR: 0.000098 Step: 96000 Total Loss: 3.1450 Recon Loss: 3.1308 
[01/06 12:33:48 TiTok]: Data (t): 0.0011, 54.36/s/gpu Batch (t): 0.5887 LR: 0.000098 Step: 96050 Total Loss: 3.1649 Recon Loss: 3.1507 
[01/06 12:34:17 TiTok]: Data (t): 0.0012, 54.25/s/gpu Batch (t): 0.5898 LR: 0.000098 Step: 96100 Total Loss: 3.1604 Recon Loss: 3.1461 
[01/06 12:34:47 TiTok]: Data (t): 0.0007, 53.96/s/gpu Batch (t): 0.5930 LR: 0.000098 Step: 96150 Total Loss: 3.1110 Recon Loss: 3.0968 
[01/06 12:35:17 TiTok]: Data (t): 0.0012, 53.94/s/gpu Batch (t): 0.5933 LR: 0.000098 Step: 96200 Total Loss: 3.1563 Recon Loss: 3.1421 
[01/06 12:35:46 TiTok]: Data (t): 0.0016, 53.96/s/gpu Batch (t): 0.5930 LR: 0.000098 Step: 96250 Total Loss: 3.1528 Recon Loss: 3.1386 
[01/06 12:36:16 TiTok]: Data (t): 0.0007, 53.54/s/gpu Batch (t): 0.5977 LR: 0.000098 Step: 96300 Total Loss: 3.1880 Recon Loss: 3.1738 
[01/06 12:36:46 TiTok]: Data (t): 0.0012, 53.94/s/gpu Batch (t): 0.5932 LR: 0.000098 Step: 96350 Total Loss: 3.1171 Recon Loss: 3.1029 
[01/06 12:37:16 TiTok]: Data (t): 0.0012, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000098 Step: 96400 Total Loss: 3.1554 Recon Loss: 3.1412 
[01/06 12:37:45 TiTok]: Data (t): 0.0012, 54.01/s/gpu Batch (t): 0.5924 LR: 0.000098 Step: 96450 Total Loss: 3.1283 Recon Loss: 3.1140 
[01/06 12:38:15 TiTok]: Data (t): 0.0014, 53.67/s/gpu Batch (t): 0.5962 LR: 0.000098 Step: 96500 Total Loss: 3.1319 Recon Loss: 3.1177 
[01/06 12:38:45 TiTok]: Data (t): 0.0012, 53.86/s/gpu Batch (t): 0.5942 LR: 0.000098 Step: 96550 Total Loss: 3.1126 Recon Loss: 3.0984 
[01/06 12:39:14 TiTok]: Data (t): 0.0007, 53.56/s/gpu Batch (t): 0.5974 LR: 0.000098 Step: 96600 Total Loss: 3.1023 Recon Loss: 3.0881 
[01/06 12:39:44 TiTok]: Data (t): 0.0014, 53.99/s/gpu Batch (t): 0.5927 LR: 0.000098 Step: 96650 Total Loss: 3.0939 Recon Loss: 3.0796 
[01/06 12:40:14 TiTok]: Data (t): 0.0012, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000098 Step: 96700 Total Loss: 3.1539 Recon Loss: 3.1396 
[01/06 12:40:43 TiTok]: Data (t): 0.0014, 53.92/s/gpu Batch (t): 0.5935 LR: 0.000098 Step: 96750 Total Loss: 3.1570 Recon Loss: 3.1427 
[01/06 12:41:13 TiTok]: Data (t): 0.0011, 53.89/s/gpu Batch (t): 0.5939 LR: 0.000098 Step: 96800 Total Loss: 3.1735 Recon Loss: 3.1593 
[01/06 12:41:43 TiTok]: Data (t): 0.0012, 53.95/s/gpu Batch (t): 0.5932 LR: 0.000098 Step: 96850 Total Loss: 3.1188 Recon Loss: 3.1046 
[01/06 12:42:13 TiTok]: Data (t): 0.0032, 54.88/s/gpu Batch (t): 0.5831 LR: 0.000098 Step: 96900 Total Loss: 3.1411 Recon Loss: 3.1268 
[01/06 12:42:42 TiTok]: Data (t): 0.0007, 53.63/s/gpu Batch (t): 0.5967 LR: 0.000098 Step: 96950 Total Loss: 3.1493 Recon Loss: 3.1351 
[01/06 12:43:12 TiTok]: Data (t): 0.0011, 47.23/s/gpu Batch (t): 0.6775 LR: 0.000098 Step: 97000 Total Loss: 3.1448 Recon Loss: 3.1305 
[01/06 12:43:42 TiTok]: Data (t): 0.0008, 53.39/s/gpu Batch (t): 0.5993 LR: 0.000098 Step: 97050 Total Loss: 3.1069 Recon Loss: 3.0926 
[01/06 12:44:11 TiTok]: Data (t): 0.0012, 54.25/s/gpu Batch (t): 0.5898 LR: 0.000098 Step: 97100 Total Loss: 3.1518 Recon Loss: 3.1376 
[01/06 12:44:41 TiTok]: Data (t): 0.0011, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000098 Step: 97150 Total Loss: 3.1345 Recon Loss: 3.1202 
[01/06 12:45:11 TiTok]: Data (t): 0.0007, 53.94/s/gpu Batch (t): 0.5933 LR: 0.000098 Step: 97200 Total Loss: 3.1197 Recon Loss: 3.1055 
[01/06 12:45:41 TiTok]: Data (t): 0.0008, 53.96/s/gpu Batch (t): 0.5930 LR: 0.000098 Step: 97250 Total Loss: 3.1640 Recon Loss: 3.1498 
[01/06 12:46:10 TiTok]: Data (t): 0.0011, 53.92/s/gpu Batch (t): 0.5935 LR: 0.000098 Step: 97300 Total Loss: 3.1514 Recon Loss: 3.1372 
[01/06 12:46:40 TiTok]: Data (t): 0.0011, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000098 Step: 97350 Total Loss: 3.0555 Recon Loss: 3.0412 
[01/06 12:47:10 TiTok]: Data (t): 0.0011, 53.93/s/gpu Batch (t): 0.5933 LR: 0.000098 Step: 97400 Total Loss: 3.1839 Recon Loss: 3.1697 
[01/06 12:47:39 TiTok]: Data (t): 0.0011, 53.72/s/gpu Batch (t): 0.5957 LR: 0.000098 Step: 97450 Total Loss: 3.1016 Recon Loss: 3.0874 
[01/06 12:48:09 TiTok]: Data (t): 0.0007, 53.29/s/gpu Batch (t): 0.6005 LR: 0.000098 Step: 97500 Total Loss: 3.1282 Recon Loss: 3.1139 
[01/06 12:48:39 TiTok]: Data (t): 0.0007, 53.91/s/gpu Batch (t): 0.5935 LR: 0.000098 Step: 97550 Total Loss: 3.1133 Recon Loss: 3.0990 
[01/06 12:49:09 TiTok]: Data (t): 0.0014, 53.93/s/gpu Batch (t): 0.5934 LR: 0.000098 Step: 97600 Total Loss: 3.1165 Recon Loss: 3.1023 
[01/06 12:49:38 TiTok]: Data (t): 0.0007, 53.49/s/gpu Batch (t): 0.5982 LR: 0.000098 Step: 97650 Total Loss: 3.1110 Recon Loss: 3.0968 
[01/06 12:50:08 TiTok]: Data (t): 0.0011, 53.82/s/gpu Batch (t): 0.5945 LR: 0.000098 Step: 97700 Total Loss: 3.1883 Recon Loss: 3.1740 
[01/06 12:50:38 TiTok]: Data (t): 0.0011, 54.36/s/gpu Batch (t): 0.5887 LR: 0.000098 Step: 97750 Total Loss: 3.1506 Recon Loss: 3.1364 
[01/06 12:51:07 TiTok]: Data (t): 0.0008, 53.45/s/gpu Batch (t): 0.5986 LR: 0.000098 Step: 97800 Total Loss: 3.1138 Recon Loss: 3.0996 
[01/06 12:51:37 TiTok]: Data (t): 0.0011, 54.19/s/gpu Batch (t): 0.5905 LR: 0.000098 Step: 97850 Total Loss: 3.0996 Recon Loss: 3.0854 
[01/06 12:52:07 TiTok]: Data (t): 0.0011, 53.92/s/gpu Batch (t): 0.5935 LR: 0.000098 Step: 97900 Total Loss: 3.1123 Recon Loss: 3.0981 
[01/06 12:52:37 TiTok]: Data (t): 0.0011, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000098 Step: 97950 Total Loss: 3.1358 Recon Loss: 3.1215 
[01/06 12:53:06 TiTok]: Data (t): 0.0011, 45.56/s/gpu Batch (t): 0.7023 LR: 0.000098 Step: 98000 Total Loss: 3.1344 Recon Loss: 3.1201 
[01/06 12:53:36 TiTok]: Data (t): 0.0007, 53.85/s/gpu Batch (t): 0.5943 LR: 0.000098 Step: 98050 Total Loss: 3.1578 Recon Loss: 3.1435 
[01/06 12:54:06 TiTok]: Data (t): 0.0014, 53.94/s/gpu Batch (t): 0.5933 LR: 0.000098 Step: 98100 Total Loss: 3.1206 Recon Loss: 3.1064 
[01/06 12:54:35 TiTok]: Data (t): 0.0011, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000098 Step: 98150 Total Loss: 3.1463 Recon Loss: 3.1321 
[01/06 12:55:05 TiTok]: Data (t): 0.0011, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000098 Step: 98200 Total Loss: 3.1494 Recon Loss: 3.1351 
[01/06 12:55:35 TiTok]: Data (t): 0.0011, 54.38/s/gpu Batch (t): 0.5884 LR: 0.000098 Step: 98250 Total Loss: 3.0913 Recon Loss: 3.0771 
[01/06 12:56:05 TiTok]: Data (t): 0.0011, 54.00/s/gpu Batch (t): 0.5926 LR: 0.000098 Step: 98300 Total Loss: 3.1128 Recon Loss: 3.0986 
[01/06 12:56:34 TiTok]: Data (t): 0.0012, 53.58/s/gpu Batch (t): 0.5972 LR: 0.000098 Step: 98350 Total Loss: 3.1492 Recon Loss: 3.1349 
[01/06 12:57:04 TiTok]: Data (t): 0.0006, 53.88/s/gpu Batch (t): 0.5940 LR: 0.000098 Step: 98400 Total Loss: 3.1400 Recon Loss: 3.1257 
[01/06 12:57:34 TiTok]: Data (t): 0.0006, 53.86/s/gpu Batch (t): 0.5941 LR: 0.000098 Step: 98450 Total Loss: 3.1479 Recon Loss: 3.1336 
[01/06 12:58:03 TiTok]: Data (t): 0.0012, 53.78/s/gpu Batch (t): 0.5951 LR: 0.000098 Step: 98500 Total Loss: 3.1325 Recon Loss: 3.1182 
[01/06 12:58:33 TiTok]: Data (t): 0.0011, 54.22/s/gpu Batch (t): 0.5902 LR: 0.000098 Step: 98550 Total Loss: 3.1555 Recon Loss: 3.1412 
[01/06 12:59:03 TiTok]: Data (t): 0.0007, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000098 Step: 98600 Total Loss: 3.0957 Recon Loss: 3.0814 
[01/06 12:59:33 TiTok]: Data (t): 0.0011, 54.23/s/gpu Batch (t): 0.5901 LR: 0.000098 Step: 98650 Total Loss: 3.1190 Recon Loss: 3.1047 
[01/06 13:00:03 TiTok]: Data (t): 0.0013, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000098 Step: 98700 Total Loss: 3.1554 Recon Loss: 3.1411 
[01/06 13:00:32 TiTok]: Data (t): 0.0011, 54.12/s/gpu Batch (t): 0.5912 LR: 0.000098 Step: 98750 Total Loss: 3.1219 Recon Loss: 3.1077 
[01/06 13:01:02 TiTok]: Data (t): 0.0012, 54.26/s/gpu Batch (t): 0.5897 LR: 0.000098 Step: 98800 Total Loss: 3.1279 Recon Loss: 3.1137 
[01/06 13:01:32 TiTok]: Data (t): 0.0007, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000098 Step: 98850 Total Loss: 3.1475 Recon Loss: 3.1332 
[01/06 13:02:02 TiTok]: Data (t): 0.0011, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000098 Step: 98900 Total Loss: 3.1298 Recon Loss: 3.1155 
[01/06 13:02:31 TiTok]: Data (t): 0.0011, 53.79/s/gpu Batch (t): 0.5949 LR: 0.000098 Step: 98950 Total Loss: 3.1297 Recon Loss: 3.1155 
[01/06 13:03:01 TiTok]: Data (t): 0.0012, 45.25/s/gpu Batch (t): 0.7073 LR: 0.000098 Step: 99000 Total Loss: 3.1091 Recon Loss: 3.0949 
[01/06 13:03:31 TiTok]: Data (t): 0.0011, 53.87/s/gpu Batch (t): 0.5941 LR: 0.000098 Step: 99050 Total Loss: 3.1336 Recon Loss: 3.1194 
[01/06 13:04:00 TiTok]: Data (t): 0.0011, 53.97/s/gpu Batch (t): 0.5930 LR: 0.000098 Step: 99100 Total Loss: 3.1140 Recon Loss: 3.0998 
[01/06 13:04:30 TiTok]: Data (t): 0.0007, 53.79/s/gpu Batch (t): 0.5949 LR: 0.000098 Step: 99150 Total Loss: 3.0815 Recon Loss: 3.0672 
[01/06 13:05:00 TiTok]: Data (t): 0.0011, 54.27/s/gpu Batch (t): 0.5896 LR: 0.000098 Step: 99200 Total Loss: 3.1026 Recon Loss: 3.0883 
[01/06 13:05:30 TiTok]: Data (t): 0.0008, 39.76/s/gpu Batch (t): 0.8049 LR: 0.000098 Step: 99250 Total Loss: 3.1002 Recon Loss: 3.0860 
[01/06 13:06:00 TiTok]: Data (t): 0.0007, 51.91/s/gpu Batch (t): 0.6164 LR: 0.000098 Step: 99300 Total Loss: 3.1292 Recon Loss: 3.1149 
[01/06 13:06:29 TiTok]: Data (t): 0.0013, 53.95/s/gpu Batch (t): 0.5932 LR: 0.000098 Step: 99350 Total Loss: 3.1310 Recon Loss: 3.1167 
[01/06 13:06:59 TiTok]: Data (t): 0.0011, 54.22/s/gpu Batch (t): 0.5901 LR: 0.000098 Step: 99400 Total Loss: 3.1004 Recon Loss: 3.0862 
[01/06 13:07:29 TiTok]: Data (t): 0.0012, 54.34/s/gpu Batch (t): 0.5889 LR: 0.000098 Step: 99450 Total Loss: 3.1470 Recon Loss: 3.1327 
[01/06 13:07:58 TiTok]: Data (t): 0.0011, 54.34/s/gpu Batch (t): 0.5889 LR: 0.000098 Step: 99500 Total Loss: 3.1387 Recon Loss: 3.1244 
[01/06 13:08:28 TiTok]: Data (t): 0.0012, 53.86/s/gpu Batch (t): 0.5941 LR: 0.000098 Step: 99550 Total Loss: 3.1164 Recon Loss: 3.1021 
[01/06 13:08:58 TiTok]: Data (t): 0.0012, 53.85/s/gpu Batch (t): 0.5942 LR: 0.000098 Step: 99600 Total Loss: 3.1395 Recon Loss: 3.1252 
[01/06 13:09:28 TiTok]: Data (t): 0.0013, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000098 Step: 99650 Total Loss: 3.1518 Recon Loss: 3.1375 
[01/06 13:09:57 TiTok]: Data (t): 0.0011, 53.95/s/gpu Batch (t): 0.5931 LR: 0.000098 Step: 99700 Total Loss: 3.0868 Recon Loss: 3.0726 
[01/06 13:10:27 TiTok]: Data (t): 0.0011, 53.81/s/gpu Batch (t): 0.5946 LR: 0.000098 Step: 99750 Total Loss: 3.0504 Recon Loss: 3.0361 
[01/06 13:10:57 TiTok]: Data (t): 0.0011, 53.96/s/gpu Batch (t): 0.5930 LR: 0.000098 Step: 99800 Total Loss: 3.1612 Recon Loss: 3.1470 
[01/06 13:11:27 TiTok]: Data (t): 0.0012, 53.76/s/gpu Batch (t): 0.5952 LR: 0.000098 Step: 99850 Total Loss: 3.1202 Recon Loss: 3.1060 
[01/06 13:11:56 TiTok]: Data (t): 0.0011, 54.34/s/gpu Batch (t): 0.5888 LR: 0.000098 Step: 99900 Total Loss: 3.1085 Recon Loss: 3.0943 
[01/06 13:12:26 TiTok]: Data (t): 0.0011, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000098 Step: 99950 Total Loss: 3.1226 Recon Loss: 3.1083 
[01/06 13:12:56 TiTok]: Data (t): 0.0007, 50.00/s/gpu Batch (t): 0.6400 LR: 0.000098 Step: 100000 Total Loss: 3.1089 Recon Loss: 3.0946 
Model weights saved in titok_s128_matryoshka_annealing_stage1_run1/checkpoint-100000/unwrapped_model/pytorch_model.bin
[01/06 13:12:58 TiTok]: Saved state to titok_s128_matryoshka_annealing_stage1_run1/checkpoint-100000
Model weights saved in titok_s128_matryoshka_annealing_stage1_run1/checkpoint-100000/ema_model/pytorch_model.bin
[01/06 13:13:21 TiTok]: Reconstructing images...
[01/06 13:13:26 TiTok]: Computing metrics on the validation set.
[01/06 13:32:33 TiTok]: EMA EVALUATION with 100.0% tokensStep: 100000 
[01/06 13:32:33 TiTok]: {'CodebookEntropy': tensor(11.9957, device='cuda:0', dtype=torch.float64),
 'CodebookUsage': 1.0,
 'InceptionScore': 130.66545081648184,
 'rFID': 7.461109815197801}
[01/06 13:50:19 TiTok]: EMA EVALUATION with 75.0% tokensStep: 100000 
[01/06 13:50:19 TiTok]: {'CodebookEntropy': tensor(11.9957, device='cuda:0', dtype=torch.float64),
 'CodebookUsage': 1.0,
 'InceptionScore': 83.14479374858924,
 'rFID': 15.22676855474765}
[01/06 14:06:03 TiTok]: EMA EVALUATION with 50.0% tokensStep: 100000 
[01/06 14:06:03 TiTok]: {'CodebookEntropy': tensor(11.9957, device='cuda:0', dtype=torch.float64),
 'CodebookUsage': 1.0,
 'InceptionScore': 18.383279765906703,
 'rFID': 70.0451260210092}
