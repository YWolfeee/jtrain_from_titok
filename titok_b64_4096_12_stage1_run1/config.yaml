experiment:
  project: titok_b64_4096_12_stage1
  name: titok_b64_4096_12_stage1_run1
  output_dir: titok_b64_4096_12_stage1_run1
  max_train_examples: 1281167
  save_every: 5000
  eval_every: 5000
  eval_loss_every: 500
  generate_every: 5000
  log_every: 50
  log_grad_norm_every: 1000
  resume: true
  init_weight: ''
  logging_dir: titok_b64_4096_12_stage1_run1/logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: base
    vit_dec_model_size: base
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 64
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
  use_reconstruction_regularization: true
  reconstruction_regularization:
    name: matryoshka
    mask_ratio_method: hierarchical
    max_mask_rate: 0.95
    use_annealing: true
    annealing:
      time_start: 0.0
      time_end: 0.1
      is_increasing: true
losses:
  quantizer_weight: 1.0
  use_self_distilliation: true
dataset:
  params:
    train_shards_path_or_url: /mnt/rdata8/imagenet_wds/imagenet-train-{000000..000252}.tar
    eval_shards_path_or_url: /mnt/rdata8/imagenet_wds/imagenet-val-{000000..000009}.tar
    num_workers_per_gpu: 12
  preprocessing:
    resize_shorter_edge: 256
    crop_size: 256
    random_crop: true
    random_flip: true
optimizer:
  name: adamw
  params:
    learning_rate: 0.0004
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
    epsilon: 1.0e-08
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: true
  use_ema: true
  seed: 42
  max_train_steps: 250000
  num_generated_images: 8
  max_grad_norm: 1.0
  per_gpu_batch_size: 32
config: configs/training/stage1/titok_b64_4096_12.yaml
