Mon Jan  6 16:32:29 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.28.03              Driver Version: 560.28.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A40                     Off |   00000000:2D:00.0 Off |                    0 |
|  0%   27C    P8             21W /  300W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA A40                     Off |   00000000:3A:00.0 Off |                    0 |
|  0%   26C    P8             22W /  300W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA A40                     Off |   00000000:3B:00.0 Off |                    0 |
|  0%   27C    P8             21W /  300W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA A40                     Off |   00000000:3C:00.0 Off |                    0 |
|  0%   27C    P8             22W /  300W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA A40                     Off |   00000000:AD:00.0 Off |                    0 |
|  0%   28C    P8             21W /  300W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA A40                     Off |   00000000:AE:00.0 Off |                    0 |
|  0%   28C    P8             20W /  300W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA A40                     Off |   00000000:BD:00.0 Off |                    0 |
|  0%   27C    P8             21W /  300W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA A40                     Off |   00000000:BE:00.0 Off |                    0 |
|  0%   28C    P8             27W /  300W |       1MiB /  46068MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
attention mode is flashattention mode is flashattention mode is flashattention mode is flash
attention mode is flashattention mode is flash




attention mode is flash
attention mode is flash
[01/06 16:33:11 TiTok]: Saving config to titok_s128_matryoshka_annealing_stage1_run1/config.yaml
[01/06 16:33:11 TiTok]: Config:
experiment:
  project: titok_s128_matryoshka_annealing_stage1
  name: titok_s128_matryoshka_annealing_stage1_run1
  output_dir: titok_s128_matryoshka_annealing_stage1_run1
  max_train_examples: 1281167
  save_every: 50000
  eval_every: 50000
  generate_every: 5000
  log_every: 50
  log_grad_norm_every: 1000
  resume: true
  init_weight: ''
  logging_dir: titok_s128_matryoshka_annealing_stage1_run1/logs
model:
  vq_model:
    codebook_size: 4096
    token_size: 12
    use_l2_norm: true
    commitment_cost: 0.25
    vit_enc_model_size: base
    vit_dec_model_size: base
    vit_enc_patch_size: 16
    vit_dec_patch_size: 16
    num_latent_tokens: 128
    finetune_decoder: false
    pretrained_tokenizer_weight: maskgit-vqgan-imagenet-f16-256.bin
  reconstruction_regularization:
    name: matryoshka
    mask_ratio_method: uniform
    max_mask_rate: 0.999
    annealing:
      time_start: 0.25
      time_end: 0.75
losses:
  quantizer_weight: 1.0
dataset:
  params:
    train_shards_path_or_url: /mnt/rdata8/imagenet_wds/imagenet-train-{000000..000252}.tar
    eval_shards_path_or_url: /mnt/rdata8/imagenet_wds/imagenet-val-{000000..000009}.tar
    num_workers_per_gpu: 12
  preprocessing:
    resize_shorter_edge: 256
    crop_size: 256
    random_crop: true
    random_flip: true
optimizer:
  name: adamw
  params:
    learning_rate: 0.0001
    beta1: 0.9
    beta2: 0.99
    weight_decay: 0.0001
lr_scheduler:
  scheduler: cosine
  params:
    learning_rate: ${optimizer.params.learning_rate}
    warmup_steps: 10000
    end_lr: 1.0e-05
training:
  gradient_accumulation_steps: 1
  per_gpu_batch_size: 32
  mixed_precision: fp16
  enable_tf32: true
  enable_wandb: true
  use_ema: true
  seed: 42
  max_train_steps: 1000000
  num_generated_images: 2
  max_grad_norm: 1.0
config: configs/training/stage1/titok_s128_matryoshka_annealing.yaml

[01/06 16:33:20 TiTok]: Creating model and loss module.
self.train_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    RandomCrop(size=(256, 256), padding=None)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.eval_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(256, 256))
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.train_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    RandomCrop(size=(256, 256), padding=None)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.eval_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(256, 256))
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.train_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    RandomCrop(size=(256, 256), padding=None)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.eval_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(256, 256))
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.train_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    RandomCrop(size=(256, 256), padding=None)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.eval_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(256, 256))
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.train_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    RandomCrop(size=(256, 256), padding=None)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.eval_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(256, 256))
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.train_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    RandomCrop(size=(256, 256), padding=None)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.eval_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(256, 256))
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.train_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    RandomCrop(size=(256, 256), padding=None)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.eval_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(256, 256))
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
===================================================================================================================================================================================================
Layer (type:depth-idx)                        Input Shape               Output Shape              Param #                   Param %                   Kernel Shape              Mult-Adds
===================================================================================================================================================================================================
TiTok                                         [1, 3, 256, 256]          [1, 1024, 16, 16]         98,304                      0.06%                   --                        --
├─TiTokEncoder: 1-1                           --                        [1, 12, 1, 128]           296,448                     0.17%                   --                        --
│    └─Conv2d: 2-1                            [1, 3, 256, 256]          [1, 768, 16, 16]          590,592                     0.34%                   [16, 16]                  151,191,552
│    └─LayerNorm: 2-2                         [1, 385, 768]             [1, 385, 768]             1,536                       0.00%                   --                        1,536
│    └─ModuleList: 2-3                        --                        --                        --                             --                   --                        --
│    │    └─ResidualAttentionBlock: 3-1       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-1               [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-2      [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-3               [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-4              [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-1             [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-2               [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-3             [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-2       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-5               [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-6      [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-7               [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-8              [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-4             [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-5               [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-6             [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-3       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-9               [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-10     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-11              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-12             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-7             [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-8               [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-9             [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-4       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-13              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-14     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-15              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-16             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-10            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-11              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-12            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-5       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-17              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-18     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-19              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-20             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-13            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-14              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-15            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-6       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-21              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-22     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-23              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-24             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-16            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-17              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-18            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-7       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-25              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-26     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-27              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-28             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-19            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-20              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-21            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-8       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-29              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-30     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-31              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-32             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-22            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-23              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-24            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-9       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-33              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-34     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-35              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-36             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-25            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-26              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-27            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-10      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-37              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-38     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-39              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-40             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-28            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-29              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-30            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-11      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-41              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-42     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-43              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-44             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-31            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-32              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-33            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-12      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-45              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-46     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-47              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-48             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-34            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-35              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-36            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    └─LayerNorm: 2-4                         [1, 128, 768]             [1, 128, 768]             1,536                       0.00%                   --                        1,536
│    └─Conv2d: 2-5                            [1, 768, 128, 1]          [1, 12, 128, 1]           9,228                       0.01%                   [1, 1]                    1,181,184
├─VectorQuantizer: 1-2                        [1, 12, 1, 128]           [1, 12, 1, 128]           --                             --                   --                        --
│    └─Embedding: 2-6                         [128]                     [128, 12]                 49,152                      0.03%                   --                        6,291,456
├─TiTokDecoder: 1-3                           [1, 12, 1, 128]           [1, 1024, 16, 16]         297,216                     0.17%                   --                        --
│    └─Linear: 2-7                            [1, 128, 12]              [1, 128, 768]             9,984                       0.01%                   --                        9,984
│    └─LayerNorm: 2-8                         [1, 385, 768]             [1, 385, 768]             1,536                       0.00%                   --                        1,536
│    └─ModuleList: 2-9                        --                        --                        --                             --                   --                        --
│    │    └─ResidualAttentionBlock: 3-13      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-49              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-50     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-51              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-52             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-37            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-38              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-39            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-14      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-53              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-54     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-55              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-56             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-40            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-41              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-42            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-15      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-57              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-58     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-59              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-60             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-43            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-44              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-45            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-16      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-61              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-62     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-63              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-64             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-46            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-47              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-48            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-17      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-65              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-66     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-67              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-68             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-49            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-50              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-51            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-18      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-69              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-70     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-71              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-72             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-52            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-53              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-54            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-19      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-73              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-74     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-75              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-76             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-55            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-56              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-57            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-20      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-77              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-78     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-79              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-80             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-58            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-59              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-60            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-21      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-81              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-82     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-83              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-84             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-61            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-62              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-63            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-22      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-85              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-86     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-87              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-88             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-64            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-65              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-66            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-23      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-89              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-90     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-91              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-92             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-67            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-68              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-69            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-24      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-93              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-94     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-95              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-96             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-70            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-71              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-72            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    └─LayerNorm: 2-10                        [1, 256, 768]             [1, 256, 768]             1,536                       0.00%                   --                        1,536
│    └─Sequential: 2-11                       [1, 768, 16, 16]          [1, 1024, 16, 16]         --                             --                   --                        --
│    │    └─Conv2d: 3-25                      [1, 768, 16, 16]          [1, 1536, 16, 16]         1,181,184                   0.68%                   [1, 1]                    302,383,104
│    │    └─Tanh: 3-26                        [1, 1536, 16, 16]         [1, 1536, 16, 16]         --                             --                   --                        --
│    │    └─Conv2d: 3-27                      [1, 1536, 16, 16]         [1, 1024, 16, 16]         1,573,888                   0.90%                   [1, 1]                    402,915,328
│    └─Identity: 2-12                         [1, 1024, 16, 16]         [1, 1024, 16, 16]         --                             --                   --                        --
===================================================================================================================================================================================================
Total params: 174,221,068
Trainable params: 174,221,068
Non-trainable params: 0
Total mult-adds (G): 44.53
===================================================================================================================================================================================================
Input size (MB): 0.79
Forward/backward pass size (MB): 412.11
Params size (MB): 467.33
Estimated Total Size (MB): 880.23
===================================================================================================================================================================================================
[01/06 16:34:00 TiTok]: ===================================================================================================================================================================================================
Layer (type:depth-idx)                        Input Shape               Output Shape              Param #                   Param %                   Kernel Shape              Mult-Adds
===================================================================================================================================================================================================
TiTok                                         [1, 3, 256, 256]          [1, 1024, 16, 16]         98,304                      0.06%                   --                        --
├─TiTokEncoder: 1-1                           --                        [1, 12, 1, 128]           296,448                     0.17%                   --                        --
│    └─Conv2d: 2-1                            [1, 3, 256, 256]          [1, 768, 16, 16]          590,592                     0.34%                   [16, 16]                  151,191,552
│    └─LayerNorm: 2-2                         [1, 385, 768]             [1, 385, 768]             1,536                       0.00%                   --                        1,536
│    └─ModuleList: 2-3                        --                        --                        --                             --                   --                        --
│    │    └─ResidualAttentionBlock: 3-1       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-1               [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-2      [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-3               [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-4              [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-1             [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-2               [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-3             [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-2       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-5               [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-6      [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-7               [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-8              [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-4             [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-5               [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-6             [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-3       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-9               [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-10     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-11              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-12             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-7             [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-8               [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-9             [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-4       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-13              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-14     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-15              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-16             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-10            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-11              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-12            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-5       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-17              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-18     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-19              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-20             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-13            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-14              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-15            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-6       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-21              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-22     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-23              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-24             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-16            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-17              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-18            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-7       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-25              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-26     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-27              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-28             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-19            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-20              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-21            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-8       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-29              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-30     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-31              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-32             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-22            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-23              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-24            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-9       [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-33              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-34     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-35              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-36             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-25            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-26              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-27            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-10      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-37              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-38     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-39              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-40             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-28            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-29              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-30            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-11      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-41              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-42     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-43              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-44             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-31            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-32              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-33            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-12      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-45              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-46     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-47              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-48             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-34            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-35              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-36            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    └─LayerNorm: 2-4                         [1, 128, 768]             [1, 128, 768]             1,536                       0.00%                   --                        1,536
│    └─Conv2d: 2-5                            [1, 768, 128, 1]          [1, 12, 128, 1]           9,228                       0.01%                   [1, 1]                    1,181,184
├─VectorQuantizer: 1-2                        [1, 12, 1, 128]           [1, 12, 1, 128]           --                             --                   --                        --
│    └─Embedding: 2-6                         [128]                     [128, 12]                 49,152                      0.03%                   --                        6,291,456
├─TiTokDecoder: 1-3                           [1, 12, 1, 128]           [1, 1024, 16, 16]         297,216                     0.17%                   --                        --
│    └─Linear: 2-7                            [1, 128, 12]              [1, 128, 768]             9,984                       0.01%                   --                        9,984
│    └─LayerNorm: 2-8                         [1, 385, 768]             [1, 385, 768]             1,536                       0.00%                   --                        1,536
│    └─ModuleList: 2-9                        --                        --                        --                             --                   --                        --
│    │    └─ResidualAttentionBlock: 3-13      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-49              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-50     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-51              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-52             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-37            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-38              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-39            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-14      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-53              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-54     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-55              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-56             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-40            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-41              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-42            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-15      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-57              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-58     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-59              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-60             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-43            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-44              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-45            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-16      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-61              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-62     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-63              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-64             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-46            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-47              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-48            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-17      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-65              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-66     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-67              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-68             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-49            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-50              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-51            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-18      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-69              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-70     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-71              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-72             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-52            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-53              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-54            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-19      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-73              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-74     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-75              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-76             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-55            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-56              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-57            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-20      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-77              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-78     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-79              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-80             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-58            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-59              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-60            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-21      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-81              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-82     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-83              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-84             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-61            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-62              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-63            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-22      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-85              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-86     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-87              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-88             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-64            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-65              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-66            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-23      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-89              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-90     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-91              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-92             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-67            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-68              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-69            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    │    └─ResidualAttentionBlock: 3-24      [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    └─LayerNorm: 4-93              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─MultiheadAttention: 4-94     [385, 1, 768]             [385, 1, 768]             2,362,368                   1.36%                   --                        --
│    │    │    └─LayerNorm: 4-95              [385, 1, 768]             [385, 1, 768]             1,536                       0.00%                   --                        591,360
│    │    │    └─Sequential: 4-96             [385, 1, 768]             [385, 1, 768]             --                             --                   --                        --
│    │    │    │    └─Linear: 5-70            [385, 1, 768]             [385, 1, 3072]            2,362,368                   1.36%                   --                        909,511,680
│    │    │    │    └─GELU: 5-71              [385, 1, 3072]            [385, 1, 3072]            --                             --                   --                        --
│    │    │    │    └─Linear: 5-72            [385, 1, 3072]            [385, 1, 768]             2,360,064                   1.35%                   --                        908,624,640
│    └─LayerNorm: 2-10                        [1, 256, 768]             [1, 256, 768]             1,536                       0.00%                   --                        1,536
│    └─Sequential: 2-11                       [1, 768, 16, 16]          [1, 1024, 16, 16]         --                             --                   --                        --
│    │    └─Conv2d: 3-25                      [1, 768, 16, 16]          [1, 1536, 16, 16]         1,181,184                   0.68%                   [1, 1]                    302,383,104
│    │    └─Tanh: 3-26                        [1, 1536, 16, 16]         [1, 1536, 16, 16]         --                             --                   --                        --
│    │    └─Conv2d: 3-27                      [1, 1536, 16, 16]         [1, 1024, 16, 16]         1,573,888                   0.90%                   [1, 1]                    402,915,328
│    └─Identity: 2-12                         [1, 1024, 16, 16]         [1, 1024, 16, 16]         --                             --                   --                        --
===================================================================================================================================================================================================
Total params: 174,221,068
Trainable params: 174,221,068
Non-trainable params: 0
Total mult-adds (G): 44.53
===================================================================================================================================================================================================
Input size (MB): 0.79
Forward/backward pass size (MB): 412.11
Params size (MB): 467.33
Estimated Total Size (MB): 880.23
===================================================================================================================================================================================================
[01/06 16:34:00 TiTok]: Creating optimizers.
[01/06 16:34:00 TiTok]: Creating lr_schedulers.
[01/06 16:34:00 TiTok]: Creating dataloaders.
self.train_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    RandomCrop(size=(256, 256), padding=None)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
self.eval_transform: Compose(
    Resize(size=256, interpolation=bicubic, max_size=None, antialias=True)
    CenterCrop(size=(256, 256))
    ToTensor()
    Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])
)
[01/06 16:34:00 TiTok]: Creating evaluator.
[01/06 16:34:10 TiTok]: Preparing model, optimizer and dataloaders
[01/06 16:34:10 TiTok]: ***** Running training *****
[01/06 16:34:10 TiTok]:   Num training steps = 1000000
[01/06 16:34:10 TiTok]:   Gradient Accumulation steps = 1
[01/06 16:34:10 TiTok]:   Instantaneous batch size per gpu = 32
[01/06 16:34:10 TiTok]:   Total train batch size (w. parallel, distributed & accumulation) = 256
[01/06 16:34:11 TiTok]: All globbed checkpoints are: ['titok_s128_matryoshka_annealing_stage1_run1/checkpoint-100000', 'titok_s128_matryoshka_annealing_stage1_run1/checkpoint-50000']
[01/06 16:34:11 TiTok]: Load checkpoint from titok_s128_matryoshka_annealing_stage1_run1/checkpoint-100000
loading weight from titok_s128_matryoshka_annealing_stage1_run1/checkpoint-100000/ema_model/pytorch_model.bin, msg: <All keys matched successfully>
loading weight from titok_s128_matryoshka_annealing_stage1_run1/checkpoint-100000/ema_model/pytorch_model.bin, msg: <All keys matched successfully>
loading weight from titok_s128_matryoshka_annealing_stage1_run1/checkpoint-100000/ema_model/pytorch_model.bin, msg: <All keys matched successfully>
loading weight from titok_s128_matryoshka_annealing_stage1_run1/checkpoint-100000/ema_model/pytorch_model.bin, msg: <All keys matched successfully>
loading weight from titok_s128_matryoshka_annealing_stage1_run1/checkpoint-100000/ema_model/pytorch_model.bin, msg: <All keys matched successfully>
loading weight from titok_s128_matryoshka_annealing_stage1_run1/checkpoint-100000/ema_model/pytorch_model.bin, msg: <All keys matched successfully>
loading weight from titok_s128_matryoshka_annealing_stage1_run1/checkpoint-100000/ema_model/pytorch_model.bin, msg: <All keys matched successfully>
loading weight from titok_s128_matryoshka_annealing_stage1_run1/checkpoint-100000/ema_model/pytorch_model.bin, msg: <All keys matched successfully>
[01/06 16:35:03 TiTok]: Resuming at global_step 100000
Epoch 19/199 started.
[01/06 16:35:36 TiTok]: Data (t): 0.0006, 54.20/s/gpu Batch (t): 0.5904 LR: 0.000098 Step: 100050 Total Loss: 3.1154 Recon Loss: 3.1012 
[01/06 16:36:06 TiTok]: Data (t): 0.0011, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000098 Step: 100100 Total Loss: 3.1296 Recon Loss: 3.1153 
[01/06 16:36:36 TiTok]: Data (t): 0.0006, 54.27/s/gpu Batch (t): 0.5897 LR: 0.000098 Step: 100150 Total Loss: 3.1014 Recon Loss: 3.0871 
[01/06 16:37:06 TiTok]: Data (t): 0.0006, 52.05/s/gpu Batch (t): 0.6148 LR: 0.000098 Step: 100200 Total Loss: 3.1206 Recon Loss: 3.1063 
[01/06 16:37:36 TiTok]: Data (t): 0.0011, 52.23/s/gpu Batch (t): 0.6127 LR: 0.000098 Step: 100250 Total Loss: 3.1350 Recon Loss: 3.1207 
[01/06 16:38:07 TiTok]: Data (t): 0.0011, 54.43/s/gpu Batch (t): 0.5879 LR: 0.000098 Step: 100300 Total Loss: 3.1493 Recon Loss: 3.1350 
[01/06 16:38:37 TiTok]: Data (t): 0.0006, 51.94/s/gpu Batch (t): 0.6161 LR: 0.000098 Step: 100350 Total Loss: 3.1014 Recon Loss: 3.0871 
[01/06 16:39:07 TiTok]: Data (t): 0.0006, 51.85/s/gpu Batch (t): 0.6172 LR: 0.000098 Step: 100400 Total Loss: 3.1087 Recon Loss: 3.0944 
[01/06 16:39:37 TiTok]: Data (t): 0.0007, 51.10/s/gpu Batch (t): 0.6263 LR: 0.000098 Step: 100450 Total Loss: 3.1422 Recon Loss: 3.1279 
[01/06 16:40:08 TiTok]: Data (t): 0.0011, 52.01/s/gpu Batch (t): 0.6153 LR: 0.000098 Step: 100500 Total Loss: 3.1029 Recon Loss: 3.0887 
[01/06 16:40:38 TiTok]: Data (t): 0.0006, 52.24/s/gpu Batch (t): 0.6126 LR: 0.000098 Step: 100550 Total Loss: 3.1303 Recon Loss: 3.1160 
[01/06 16:41:08 TiTok]: Data (t): 0.0011, 52.21/s/gpu Batch (t): 0.6130 LR: 0.000098 Step: 100600 Total Loss: 3.1432 Recon Loss: 3.1289 
[01/06 16:41:38 TiTok]: Data (t): 0.0007, 54.03/s/gpu Batch (t): 0.5922 LR: 0.000098 Step: 100650 Total Loss: 3.0939 Recon Loss: 3.0797 
[01/06 16:42:08 TiTok]: Data (t): 0.0007, 52.58/s/gpu Batch (t): 0.6086 LR: 0.000098 Step: 100700 Total Loss: 3.1103 Recon Loss: 3.0960 
[01/06 16:42:38 TiTok]: Data (t): 0.0011, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000098 Step: 100750 Total Loss: 3.1523 Recon Loss: 3.1380 
[01/06 16:43:08 TiTok]: Data (t): 0.0006, 53.40/s/gpu Batch (t): 0.5992 LR: 0.000098 Step: 100800 Total Loss: 3.1584 Recon Loss: 3.1442 
[01/06 16:43:39 TiTok]: Data (t): 0.0012, 52.28/s/gpu Batch (t): 0.6121 LR: 0.000098 Step: 100850 Total Loss: 3.1042 Recon Loss: 3.0900 
[01/06 16:44:09 TiTok]: Data (t): 0.0011, 54.60/s/gpu Batch (t): 0.5861 LR: 0.000098 Step: 100900 Total Loss: 3.1091 Recon Loss: 3.0948 
[01/06 16:44:39 TiTok]: Data (t): 0.0012, 54.45/s/gpu Batch (t): 0.5877 LR: 0.000098 Step: 100950 Total Loss: 3.0929 Recon Loss: 3.0786 
[01/06 16:45:09 TiTok]: Data (t): 0.0006, 45.08/s/gpu Batch (t): 0.7099 LR: 0.000098 Step: 101000 Total Loss: 3.1244 Recon Loss: 3.1102 
[01/06 16:45:40 TiTok]: Data (t): 0.0013, 52.23/s/gpu Batch (t): 0.6126 LR: 0.000098 Step: 101050 Total Loss: 3.1375 Recon Loss: 3.1233 
[01/06 16:46:10 TiTok]: Data (t): 0.0011, 53.43/s/gpu Batch (t): 0.5989 LR: 0.000098 Step: 101100 Total Loss: 3.1334 Recon Loss: 3.1191 
[01/06 16:46:40 TiTok]: Data (t): 0.0011, 53.93/s/gpu Batch (t): 0.5933 LR: 0.000098 Step: 101150 Total Loss: 3.1194 Recon Loss: 3.1052 
[01/06 16:47:11 TiTok]: Data (t): 0.0007, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000098 Step: 101200 Total Loss: 3.1346 Recon Loss: 3.1203 
[01/06 16:47:41 TiTok]: Data (t): 0.0011, 52.87/s/gpu Batch (t): 0.6053 LR: 0.000098 Step: 101250 Total Loss: 3.0967 Recon Loss: 3.0824 
[01/06 16:48:11 TiTok]: Data (t): 0.0012, 50.00/s/gpu Batch (t): 0.6400 LR: 0.000098 Step: 101300 Total Loss: 3.1199 Recon Loss: 3.1056 
[01/06 16:48:41 TiTok]: Data (t): 0.0006, 53.21/s/gpu Batch (t): 0.6014 LR: 0.000098 Step: 101350 Total Loss: 3.1485 Recon Loss: 3.1343 
[01/06 16:49:12 TiTok]: Data (t): 0.0008, 52.15/s/gpu Batch (t): 0.6136 LR: 0.000098 Step: 101400 Total Loss: 3.1242 Recon Loss: 3.1099 
[01/06 16:49:42 TiTok]: Data (t): 0.0011, 53.11/s/gpu Batch (t): 0.6026 LR: 0.000098 Step: 101450 Total Loss: 3.1609 Recon Loss: 3.1466 
[01/06 16:50:12 TiTok]: Data (t): 0.0007, 52.40/s/gpu Batch (t): 0.6107 LR: 0.000098 Step: 101500 Total Loss: 3.1524 Recon Loss: 3.1382 
[01/06 16:50:42 TiTok]: Data (t): 0.0011, 52.52/s/gpu Batch (t): 0.6093 LR: 0.000098 Step: 101550 Total Loss: 3.1584 Recon Loss: 3.1441 
[01/06 16:51:13 TiTok]: Data (t): 0.0011, 53.58/s/gpu Batch (t): 0.5972 LR: 0.000098 Step: 101600 Total Loss: 3.0874 Recon Loss: 3.0731 
[01/06 16:51:43 TiTok]: Data (t): 0.0011, 54.23/s/gpu Batch (t): 0.5901 LR: 0.000098 Step: 101650 Total Loss: 3.1370 Recon Loss: 3.1227 
[01/06 16:52:13 TiTok]: Data (t): 0.0006, 54.00/s/gpu Batch (t): 0.5926 LR: 0.000098 Step: 101700 Total Loss: 3.1439 Recon Loss: 3.1295 
[01/06 16:52:43 TiTok]: Data (t): 0.0006, 53.95/s/gpu Batch (t): 0.5932 LR: 0.000098 Step: 101750 Total Loss: 3.1095 Recon Loss: 3.0952 
[01/06 16:53:13 TiTok]: Data (t): 0.0012, 53.30/s/gpu Batch (t): 0.6004 LR: 0.000098 Step: 101800 Total Loss: 3.1664 Recon Loss: 3.1521 
[01/06 16:53:44 TiTok]: Data (t): 0.0006, 51.99/s/gpu Batch (t): 0.6155 LR: 0.000098 Step: 101850 Total Loss: 3.1200 Recon Loss: 3.1057 
[01/06 16:54:14 TiTok]: Data (t): 0.0011, 52.13/s/gpu Batch (t): 0.6138 LR: 0.000098 Step: 101900 Total Loss: 3.0977 Recon Loss: 3.0835 
[01/06 16:54:44 TiTok]: Data (t): 0.0011, 54.53/s/gpu Batch (t): 0.5868 LR: 0.000098 Step: 101950 Total Loss: 3.1173 Recon Loss: 3.1031 
[01/06 16:55:14 TiTok]: Data (t): 0.0006, 48.07/s/gpu Batch (t): 0.6657 LR: 0.000098 Step: 102000 Total Loss: 3.1255 Recon Loss: 3.1112 
[01/06 16:55:44 TiTok]: Data (t): 0.0011, 52.37/s/gpu Batch (t): 0.6110 LR: 0.000098 Step: 102050 Total Loss: 3.1263 Recon Loss: 3.1120 
[01/06 16:56:15 TiTok]: Data (t): 0.0007, 52.77/s/gpu Batch (t): 0.6064 LR: 0.000098 Step: 102100 Total Loss: 3.1400 Recon Loss: 3.1257 
[01/06 16:56:45 TiTok]: Data (t): 0.0011, 53.87/s/gpu Batch (t): 0.5941 LR: 0.000098 Step: 102150 Total Loss: 3.1565 Recon Loss: 3.1422 
[01/06 16:57:15 TiTok]: Data (t): 0.0011, 51.80/s/gpu Batch (t): 0.6177 LR: 0.000098 Step: 102200 Total Loss: 3.1281 Recon Loss: 3.1138 
[01/06 16:57:45 TiTok]: Data (t): 0.0011, 54.36/s/gpu Batch (t): 0.5886 LR: 0.000098 Step: 102250 Total Loss: 3.1245 Recon Loss: 3.1102 
[01/06 16:58:15 TiTok]: Data (t): 0.0007, 53.31/s/gpu Batch (t): 0.6002 LR: 0.000098 Step: 102300 Total Loss: 3.1281 Recon Loss: 3.1138 
[01/06 16:58:45 TiTok]: Data (t): 0.0007, 53.20/s/gpu Batch (t): 0.6015 LR: 0.000098 Step: 102350 Total Loss: 3.0833 Recon Loss: 3.0690 
[01/06 16:59:16 TiTok]: Data (t): 0.0009, 51.97/s/gpu Batch (t): 0.6158 LR: 0.000098 Step: 102400 Total Loss: 3.1144 Recon Loss: 3.1001 
[01/06 16:59:46 TiTok]: Data (t): 0.0008, 51.03/s/gpu Batch (t): 0.6270 LR: 0.000098 Step: 102450 Total Loss: 3.1028 Recon Loss: 3.0886 
[01/06 17:00:16 TiTok]: Data (t): 0.0006, 53.94/s/gpu Batch (t): 0.5933 LR: 0.000098 Step: 102500 Total Loss: 3.1337 Recon Loss: 3.1194 
[01/06 17:00:46 TiTok]: Data (t): 0.0006, 53.36/s/gpu Batch (t): 0.5997 LR: 0.000098 Step: 102550 Total Loss: 3.1055 Recon Loss: 3.0912 
[01/06 17:01:17 TiTok]: Data (t): 0.0007, 52.76/s/gpu Batch (t): 0.6065 LR: 0.000098 Step: 102600 Total Loss: 3.1703 Recon Loss: 3.1560 
[01/06 17:01:47 TiTok]: Data (t): 0.0007, 53.34/s/gpu Batch (t): 0.5999 LR: 0.000098 Step: 102650 Total Loss: 3.0985 Recon Loss: 3.0842 
[01/06 17:02:17 TiTok]: Data (t): 0.0011, 52.15/s/gpu Batch (t): 0.6137 LR: 0.000098 Step: 102700 Total Loss: 3.1153 Recon Loss: 3.1010 
[01/06 17:02:47 TiTok]: Data (t): 0.0014, 53.95/s/gpu Batch (t): 0.5931 LR: 0.000098 Step: 102750 Total Loss: 3.1176 Recon Loss: 3.1033 
[01/06 17:03:18 TiTok]: Data (t): 0.0006, 53.61/s/gpu Batch (t): 0.5969 LR: 0.000098 Step: 102800 Total Loss: 3.1280 Recon Loss: 3.1137 
[01/06 17:03:48 TiTok]: Data (t): 0.0006, 53.25/s/gpu Batch (t): 0.6009 LR: 0.000098 Step: 102850 Total Loss: 3.1315 Recon Loss: 3.1171 
[01/06 17:04:18 TiTok]: Data (t): 0.0011, 51.84/s/gpu Batch (t): 0.6173 LR: 0.000098 Step: 102900 Total Loss: 3.1333 Recon Loss: 3.1190 
[01/06 17:04:48 TiTok]: Data (t): 0.0011, 54.34/s/gpu Batch (t): 0.5889 LR: 0.000098 Step: 102950 Total Loss: 3.1177 Recon Loss: 3.1034 
[01/06 17:05:19 TiTok]: Data (t): 0.0011, 45.00/s/gpu Batch (t): 0.7111 LR: 0.000098 Step: 103000 Total Loss: 3.1132 Recon Loss: 3.0989 
[01/06 17:05:49 TiTok]: Data (t): 0.0011, 54.00/s/gpu Batch (t): 0.5926 LR: 0.000098 Step: 103050 Total Loss: 3.0869 Recon Loss: 3.0727 
[01/06 17:06:19 TiTok]: Data (t): 0.0011, 52.95/s/gpu Batch (t): 0.6043 LR: 0.000098 Step: 103100 Total Loss: 3.1050 Recon Loss: 3.0907 
[01/06 17:06:49 TiTok]: Data (t): 0.0420, 51.74/s/gpu Batch (t): 0.6185 LR: 0.000098 Step: 103150 Total Loss: 3.1255 Recon Loss: 3.1113 
[01/06 17:07:20 TiTok]: Data (t): 0.0011, 54.04/s/gpu Batch (t): 0.5922 LR: 0.000098 Step: 103200 Total Loss: 3.1036 Recon Loss: 3.0893 
[01/06 17:07:50 TiTok]: Data (t): 0.0011, 51.82/s/gpu Batch (t): 0.6175 LR: 0.000098 Step: 103250 Total Loss: 3.1514 Recon Loss: 3.1371 
[01/06 17:08:20 TiTok]: Data (t): 0.0011, 51.13/s/gpu Batch (t): 0.6258 LR: 0.000098 Step: 103300 Total Loss: 3.1395 Recon Loss: 3.1252 
[01/06 17:08:50 TiTok]: Data (t): 0.0346, 52.44/s/gpu Batch (t): 0.6103 LR: 0.000098 Step: 103350 Total Loss: 3.0522 Recon Loss: 3.0380 
[01/06 17:09:20 TiTok]: Data (t): 0.0006, 51.76/s/gpu Batch (t): 0.6183 LR: 0.000098 Step: 103400 Total Loss: 3.1314 Recon Loss: 3.1171 
[01/06 17:09:51 TiTok]: Data (t): 0.0006, 53.93/s/gpu Batch (t): 0.5934 LR: 0.000098 Step: 103450 Total Loss: 3.1135 Recon Loss: 3.0992 
[01/06 17:10:21 TiTok]: Data (t): 0.0006, 53.36/s/gpu Batch (t): 0.5997 LR: 0.000098 Step: 103500 Total Loss: 3.1540 Recon Loss: 3.1397 
[01/06 17:10:51 TiTok]: Data (t): 0.0014, 54.43/s/gpu Batch (t): 0.5879 LR: 0.000098 Step: 103550 Total Loss: 3.1198 Recon Loss: 3.1056 
[01/06 17:11:21 TiTok]: Data (t): 0.0011, 53.34/s/gpu Batch (t): 0.5999 LR: 0.000098 Step: 103600 Total Loss: 3.1061 Recon Loss: 3.0917 
[01/06 17:11:51 TiTok]: Data (t): 0.0011, 53.68/s/gpu Batch (t): 0.5961 LR: 0.000098 Step: 103650 Total Loss: 3.1160 Recon Loss: 3.1017 
[01/06 17:12:22 TiTok]: Data (t): 0.0007, 54.25/s/gpu Batch (t): 0.5899 LR: 0.000098 Step: 103700 Total Loss: 3.1398 Recon Loss: 3.1254 
[01/06 17:12:52 TiTok]: Data (t): 0.0006, 52.39/s/gpu Batch (t): 0.6108 LR: 0.000098 Step: 103750 Total Loss: 3.1100 Recon Loss: 3.0957 
[01/06 17:13:23 TiTok]: Data (t): 0.0006, 52.92/s/gpu Batch (t): 0.6047 LR: 0.000098 Step: 103800 Total Loss: 3.1325 Recon Loss: 3.1182 
[01/06 17:13:53 TiTok]: Data (t): 0.0007, 51.03/s/gpu Batch (t): 0.6270 LR: 0.000098 Step: 103850 Total Loss: 3.1452 Recon Loss: 3.1309 
[01/06 17:14:23 TiTok]: Data (t): 0.0011, 53.84/s/gpu Batch (t): 0.5943 LR: 0.000098 Step: 103900 Total Loss: 3.1336 Recon Loss: 3.1193 
[01/06 17:14:53 TiTok]: Data (t): 0.0011, 52.55/s/gpu Batch (t): 0.6089 LR: 0.000098 Step: 103950 Total Loss: 3.1103 Recon Loss: 3.0960 
[01/06 17:15:24 TiTok]: Data (t): 0.0011, 46.78/s/gpu Batch (t): 0.6841 LR: 0.000098 Step: 104000 Total Loss: 3.1222 Recon Loss: 3.1079 
[01/06 17:15:54 TiTok]: Data (t): 0.0009, 52.14/s/gpu Batch (t): 0.6137 LR: 0.000098 Step: 104050 Total Loss: 3.1286 Recon Loss: 3.1142 
[01/06 17:16:24 TiTok]: Data (t): 0.0011, 51.02/s/gpu Batch (t): 0.6272 LR: 0.000098 Step: 104100 Total Loss: 3.1060 Recon Loss: 3.0917 
[01/06 17:16:54 TiTok]: Data (t): 0.0011, 54.34/s/gpu Batch (t): 0.5889 LR: 0.000098 Step: 104150 Total Loss: 3.1223 Recon Loss: 3.1080 
[01/06 17:17:24 TiTok]: Data (t): 0.0006, 52.97/s/gpu Batch (t): 0.6041 LR: 0.000098 Step: 104200 Total Loss: 3.1248 Recon Loss: 3.1105 
[01/06 17:17:55 TiTok]: Data (t): 0.0006, 50.70/s/gpu Batch (t): 0.6311 LR: 0.000098 Step: 104250 Total Loss: 3.1354 Recon Loss: 3.1211 
[01/06 17:18:25 TiTok]: Data (t): 0.0011, 53.94/s/gpu Batch (t): 0.5932 LR: 0.000098 Step: 104300 Total Loss: 3.0975 Recon Loss: 3.0832 
[01/06 17:18:55 TiTok]: Data (t): 0.0006, 54.00/s/gpu Batch (t): 0.5926 LR: 0.000098 Step: 104350 Total Loss: 3.1120 Recon Loss: 3.0977 
[01/06 17:19:25 TiTok]: Data (t): 0.0007, 53.19/s/gpu Batch (t): 0.6017 LR: 0.000098 Step: 104400 Total Loss: 3.1187 Recon Loss: 3.1044 
[01/06 17:19:56 TiTok]: Data (t): 0.0011, 51.05/s/gpu Batch (t): 0.6269 LR: 0.000098 Step: 104450 Total Loss: 3.0962 Recon Loss: 3.0819 
[01/06 17:20:26 TiTok]: Data (t): 0.0011, 53.71/s/gpu Batch (t): 0.5958 LR: 0.000098 Step: 104500 Total Loss: 3.1172 Recon Loss: 3.1028 
[01/06 17:20:56 TiTok]: Data (t): 0.0007, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000098 Step: 104550 Total Loss: 3.1277 Recon Loss: 3.1134 
[01/06 17:21:26 TiTok]: Data (t): 0.0006, 52.82/s/gpu Batch (t): 0.6059 LR: 0.000098 Step: 104600 Total Loss: 3.0893 Recon Loss: 3.0750 
[01/06 17:21:56 TiTok]: Data (t): 0.0006, 53.92/s/gpu Batch (t): 0.5934 LR: 0.000098 Step: 104650 Total Loss: 3.1416 Recon Loss: 3.1273 
[01/06 17:22:26 TiTok]: Data (t): 0.0006, 51.25/s/gpu Batch (t): 0.6244 LR: 0.000098 Step: 104700 Total Loss: 3.0968 Recon Loss: 3.0825 
[01/06 17:22:56 TiTok]: Data (t): 0.0011, 52.53/s/gpu Batch (t): 0.6092 LR: 0.000098 Step: 104750 Total Loss: 3.1271 Recon Loss: 3.1127 
[01/06 17:23:27 TiTok]: Data (t): 0.0011, 54.00/s/gpu Batch (t): 0.5926 LR: 0.000098 Step: 104800 Total Loss: 3.1781 Recon Loss: 3.1638 
[01/06 17:23:57 TiTok]: Data (t): 0.0006, 53.43/s/gpu Batch (t): 0.5989 LR: 0.000098 Step: 104850 Total Loss: 3.0882 Recon Loss: 3.0740 
[01/06 17:24:27 TiTok]: Data (t): 0.0007, 53.98/s/gpu Batch (t): 0.5928 LR: 0.000098 Step: 104900 Total Loss: 3.1089 Recon Loss: 3.0945 
[01/06 17:24:57 TiTok]: Data (t): 0.0006, 53.96/s/gpu Batch (t): 0.5931 LR: 0.000098 Step: 104950 Total Loss: 3.1017 Recon Loss: 3.0874 
[01/06 17:25:27 TiTok]: Data (t): 0.0011, 45.93/s/gpu Batch (t): 0.6967 LR: 0.000098 Step: 105000 Total Loss: 3.1006 Recon Loss: 3.0863 
[01/06 17:25:29 TiTok]: Reconstructing images...
Epoch 20/199 started.
[01/06 17:26:09 TiTok]: Data (t): 0.0007, 53.97/s/gpu Batch (t): 0.5930 LR: 0.000098 Step: 105050 Total Loss: 3.1256 Recon Loss: 3.1113 
[01/06 17:26:39 TiTok]: Data (t): 0.0007, 54.03/s/gpu Batch (t): 0.5923 LR: 0.000098 Step: 105100 Total Loss: 3.1018 Recon Loss: 3.0875 
[01/06 17:27:09 TiTok]: Data (t): 0.0011, 52.26/s/gpu Batch (t): 0.6124 LR: 0.000098 Step: 105150 Total Loss: 3.1300 Recon Loss: 3.1157 
[01/06 17:27:40 TiTok]: Data (t): 0.0007, 52.60/s/gpu Batch (t): 0.6083 LR: 0.000098 Step: 105200 Total Loss: 3.0859 Recon Loss: 3.0716 
[01/06 17:28:10 TiTok]: Data (t): 0.0012, 50.91/s/gpu Batch (t): 0.6285 LR: 0.000098 Step: 105250 Total Loss: 3.1278 Recon Loss: 3.1134 
[01/06 17:28:40 TiTok]: Data (t): 0.0014, 53.81/s/gpu Batch (t): 0.5946 LR: 0.000098 Step: 105300 Total Loss: 3.1296 Recon Loss: 3.1153 
[01/06 17:29:11 TiTok]: Data (t): 0.0011, 54.39/s/gpu Batch (t): 0.5883 LR: 0.000098 Step: 105350 Total Loss: 3.1203 Recon Loss: 3.1059 
[01/06 17:29:41 TiTok]: Data (t): 0.0006, 52.60/s/gpu Batch (t): 0.6083 LR: 0.000098 Step: 105400 Total Loss: 3.1037 Recon Loss: 3.0894 
[01/06 17:30:11 TiTok]: Data (t): 0.0011, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000098 Step: 105450 Total Loss: 3.1161 Recon Loss: 3.1018 
[01/06 17:30:41 TiTok]: Data (t): 0.0011, 54.42/s/gpu Batch (t): 0.5880 LR: 0.000098 Step: 105500 Total Loss: 3.1264 Recon Loss: 3.1121 
[01/06 17:31:11 TiTok]: Data (t): 0.0011, 53.81/s/gpu Batch (t): 0.5947 LR: 0.000098 Step: 105550 Total Loss: 3.1675 Recon Loss: 3.1532 
[01/06 17:31:41 TiTok]: Data (t): 0.0006, 53.93/s/gpu Batch (t): 0.5934 LR: 0.000098 Step: 105600 Total Loss: 3.0932 Recon Loss: 3.0789 
[01/06 17:32:11 TiTok]: Data (t): 0.0011, 53.97/s/gpu Batch (t): 0.5929 LR: 0.000098 Step: 105650 Total Loss: 3.1034 Recon Loss: 3.0891 
[01/06 17:32:41 TiTok]: Data (t): 0.0011, 53.92/s/gpu Batch (t): 0.5934 LR: 0.000098 Step: 105700 Total Loss: 3.0798 Recon Loss: 3.0654 
[01/06 17:33:11 TiTok]: Data (t): 0.0011, 53.25/s/gpu Batch (t): 0.6010 LR: 0.000098 Step: 105750 Total Loss: 3.1425 Recon Loss: 3.1281 
[01/06 17:33:41 TiTok]: Data (t): 0.0011, 53.16/s/gpu Batch (t): 0.6020 LR: 0.000098 Step: 105800 Total Loss: 3.0877 Recon Loss: 3.0734 
[01/06 17:34:11 TiTok]: Data (t): 0.0013, 54.56/s/gpu Batch (t): 0.5866 LR: 0.000098 Step: 105850 Total Loss: 3.1290 Recon Loss: 3.1147 
[01/06 17:34:42 TiTok]: Data (t): 0.0011, 54.45/s/gpu Batch (t): 0.5877 LR: 0.000098 Step: 105900 Total Loss: 3.0314 Recon Loss: 3.0171 
[01/06 17:35:12 TiTok]: Data (t): 0.0006, 53.41/s/gpu Batch (t): 0.5992 LR: 0.000098 Step: 105950 Total Loss: 3.1638 Recon Loss: 3.1495 
[01/06 17:35:42 TiTok]: Data (t): 0.0011, 40.04/s/gpu Batch (t): 0.7993 LR: 0.000098 Step: 106000 Total Loss: 3.1110 Recon Loss: 3.0967 
[01/06 17:36:12 TiTok]: Data (t): 0.0011, 52.73/s/gpu Batch (t): 0.6069 LR: 0.000098 Step: 106050 Total Loss: 3.1080 Recon Loss: 3.0938 
[01/06 17:36:42 TiTok]: Data (t): 0.0006, 52.30/s/gpu Batch (t): 0.6119 LR: 0.000098 Step: 106100 Total Loss: 3.1170 Recon Loss: 3.1027 
[01/06 17:37:13 TiTok]: Data (t): 0.0011, 53.69/s/gpu Batch (t): 0.5960 LR: 0.000098 Step: 106150 Total Loss: 3.0948 Recon Loss: 3.0805 
[01/06 17:37:43 TiTok]: Data (t): 0.0006, 53.75/s/gpu Batch (t): 0.5954 LR: 0.000098 Step: 106200 Total Loss: 3.1175 Recon Loss: 3.1032 
[01/06 17:38:13 TiTok]: Data (t): 0.0009, 53.39/s/gpu Batch (t): 0.5993 LR: 0.000098 Step: 106250 Total Loss: 3.1115 Recon Loss: 3.0972 
[01/06 17:38:43 TiTok]: Data (t): 0.0007, 53.40/s/gpu Batch (t): 0.5993 LR: 0.000098 Step: 106300 Total Loss: 3.1409 Recon Loss: 3.1266 
[01/06 17:39:13 TiTok]: Data (t): 0.0007, 53.34/s/gpu Batch (t): 0.5999 LR: 0.000098 Step: 106350 Total Loss: 3.1078 Recon Loss: 3.0935 
[01/06 17:39:43 TiTok]: Data (t): 0.0011, 52.73/s/gpu Batch (t): 0.6069 LR: 0.000098 Step: 106400 Total Loss: 3.1090 Recon Loss: 3.0947 
[01/06 17:40:13 TiTok]: Data (t): 0.0006, 53.38/s/gpu Batch (t): 0.5995 LR: 0.000098 Step: 106450 Total Loss: 3.1311 Recon Loss: 3.1168 
[01/06 17:40:43 TiTok]: Data (t): 0.0011, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000098 Step: 106500 Total Loss: 3.1183 Recon Loss: 3.1039 
[01/06 17:41:13 TiTok]: Data (t): 0.0006, 53.71/s/gpu Batch (t): 0.5958 LR: 0.000098 Step: 106550 Total Loss: 3.1115 Recon Loss: 3.0972 
[01/06 17:41:43 TiTok]: Data (t): 0.0007, 53.94/s/gpu Batch (t): 0.5932 LR: 0.000098 Step: 106600 Total Loss: 3.1046 Recon Loss: 3.0903 
[01/06 17:42:14 TiTok]: Data (t): 0.0011, 52.12/s/gpu Batch (t): 0.6140 LR: 0.000098 Step: 106650 Total Loss: 3.1187 Recon Loss: 3.1043 
[01/06 17:42:44 TiTok]: Data (t): 0.0012, 54.41/s/gpu Batch (t): 0.5881 LR: 0.000098 Step: 106700 Total Loss: 3.1642 Recon Loss: 3.1498 
[01/06 17:43:14 TiTok]: Data (t): 0.0007, 53.45/s/gpu Batch (t): 0.5987 LR: 0.000098 Step: 106750 Total Loss: 3.0594 Recon Loss: 3.0451 
[01/06 17:43:44 TiTok]: Data (t): 0.0011, 53.74/s/gpu Batch (t): 0.5954 LR: 0.000098 Step: 106800 Total Loss: 3.1043 Recon Loss: 3.0900 
[01/06 17:44:14 TiTok]: Data (t): 0.0006, 53.74/s/gpu Batch (t): 0.5955 LR: 0.000098 Step: 106850 Total Loss: 3.1131 Recon Loss: 3.0987 
[01/06 17:44:45 TiTok]: Data (t): 0.0011, 53.95/s/gpu Batch (t): 0.5931 LR: 0.000098 Step: 106900 Total Loss: 3.1208 Recon Loss: 3.1064 
[01/06 17:45:15 TiTok]: Data (t): 0.0007, 53.26/s/gpu Batch (t): 0.6008 LR: 0.000098 Step: 106950 Total Loss: 3.1338 Recon Loss: 3.1194 
[01/06 17:45:45 TiTok]: Data (t): 0.0006, 44.34/s/gpu Batch (t): 0.7218 LR: 0.000098 Step: 107000 Total Loss: 3.1158 Recon Loss: 3.1014 
[01/06 17:46:15 TiTok]: Data (t): 0.0011, 52.67/s/gpu Batch (t): 0.6076 LR: 0.000098 Step: 107050 Total Loss: 3.0965 Recon Loss: 3.0821 
[01/06 17:46:45 TiTok]: Data (t): 0.0011, 50.40/s/gpu Batch (t): 0.6350 LR: 0.000098 Step: 107100 Total Loss: 3.1536 Recon Loss: 3.1392 
[01/06 17:47:15 TiTok]: Data (t): 0.0011, 53.68/s/gpu Batch (t): 0.5961 LR: 0.000098 Step: 107150 Total Loss: 3.1061 Recon Loss: 3.0917 
[01/06 17:47:45 TiTok]: Data (t): 0.0011, 53.48/s/gpu Batch (t): 0.5984 LR: 0.000098 Step: 107200 Total Loss: 3.0888 Recon Loss: 3.0744 
[01/06 17:48:15 TiTok]: Data (t): 0.0011, 51.80/s/gpu Batch (t): 0.6177 LR: 0.000098 Step: 107250 Total Loss: 3.0824 Recon Loss: 3.0681 
[01/06 17:48:46 TiTok]: Data (t): 0.0006, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000098 Step: 107300 Total Loss: 3.1256 Recon Loss: 3.1113 
[01/06 17:49:16 TiTok]: Data (t): 0.0011, 54.42/s/gpu Batch (t): 0.5880 LR: 0.000098 Step: 107350 Total Loss: 3.1157 Recon Loss: 3.1013 
[01/06 17:49:46 TiTok]: Data (t): 0.0011, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000098 Step: 107400 Total Loss: 3.1102 Recon Loss: 3.0958 
[01/06 17:50:16 TiTok]: Data (t): 0.0006, 52.61/s/gpu Batch (t): 0.6082 LR: 0.000098 Step: 107450 Total Loss: 3.0951 Recon Loss: 3.0808 
[01/06 17:50:46 TiTok]: Data (t): 0.0011, 53.85/s/gpu Batch (t): 0.5943 LR: 0.000098 Step: 107500 Total Loss: 3.1151 Recon Loss: 3.1008 
[01/06 17:51:16 TiTok]: Data (t): 0.0011, 53.76/s/gpu Batch (t): 0.5952 LR: 0.000098 Step: 107550 Total Loss: 3.0859 Recon Loss: 3.0715 
[01/06 17:51:46 TiTok]: Data (t): 0.0006, 53.49/s/gpu Batch (t): 0.5982 LR: 0.000098 Step: 107600 Total Loss: 3.1158 Recon Loss: 3.1015 
[01/06 17:52:16 TiTok]: Data (t): 0.0006, 52.88/s/gpu Batch (t): 0.6051 LR: 0.000098 Step: 107650 Total Loss: 3.0901 Recon Loss: 3.0757 
[01/06 17:52:46 TiTok]: Data (t): 0.0015, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000098 Step: 107700 Total Loss: 3.1144 Recon Loss: 3.1001 
[01/06 17:53:16 TiTok]: Data (t): 0.0014, 52.10/s/gpu Batch (t): 0.6142 LR: 0.000098 Step: 107750 Total Loss: 3.1347 Recon Loss: 3.1203 
[01/06 17:53:47 TiTok]: Data (t): 0.0011, 53.31/s/gpu Batch (t): 0.6003 LR: 0.000098 Step: 107800 Total Loss: 3.0849 Recon Loss: 3.0706 
[01/06 17:54:17 TiTok]: Data (t): 0.0007, 52.23/s/gpu Batch (t): 0.6126 LR: 0.000098 Step: 107850 Total Loss: 3.0784 Recon Loss: 3.0641 
[01/06 17:54:47 TiTok]: Data (t): 0.0006, 53.81/s/gpu Batch (t): 0.5947 LR: 0.000098 Step: 107900 Total Loss: 3.0387 Recon Loss: 3.0244 
[01/06 17:55:17 TiTok]: Data (t): 0.0006, 50.61/s/gpu Batch (t): 0.6323 LR: 0.000098 Step: 107950 Total Loss: 3.1040 Recon Loss: 3.0896 
[01/06 17:55:47 TiTok]: Data (t): 0.0012, 43.76/s/gpu Batch (t): 0.7312 LR: 0.000098 Step: 108000 Total Loss: 3.1359 Recon Loss: 3.1216 
[01/06 17:56:18 TiTok]: Data (t): 0.0011, 54.22/s/gpu Batch (t): 0.5902 LR: 0.000098 Step: 108050 Total Loss: 3.1104 Recon Loss: 3.0961 
[01/06 17:56:48 TiTok]: Data (t): 0.0011, 50.66/s/gpu Batch (t): 0.6316 LR: 0.000098 Step: 108100 Total Loss: 3.1510 Recon Loss: 3.1366 
[01/06 17:57:18 TiTok]: Data (t): 0.0007, 54.01/s/gpu Batch (t): 0.5925 LR: 0.000098 Step: 108150 Total Loss: 3.1188 Recon Loss: 3.1045 
[01/06 17:57:48 TiTok]: Data (t): 0.0011, 53.93/s/gpu Batch (t): 0.5934 LR: 0.000098 Step: 108200 Total Loss: 3.0977 Recon Loss: 3.0834 
[01/06 17:58:18 TiTok]: Data (t): 0.0011, 53.77/s/gpu Batch (t): 0.5951 LR: 0.000098 Step: 108250 Total Loss: 3.0938 Recon Loss: 3.0794 
[01/06 17:58:48 TiTok]: Data (t): 0.0006, 53.99/s/gpu Batch (t): 0.5927 LR: 0.000098 Step: 108300 Total Loss: 3.1327 Recon Loss: 3.1183 
[01/06 17:59:18 TiTok]: Data (t): 0.0011, 52.63/s/gpu Batch (t): 0.6081 LR: 0.000098 Step: 108350 Total Loss: 3.0915 Recon Loss: 3.0771 
[01/06 17:59:48 TiTok]: Data (t): 0.0011, 53.76/s/gpu Batch (t): 0.5953 LR: 0.000098 Step: 108400 Total Loss: 3.1253 Recon Loss: 3.1109 
[01/06 18:00:19 TiTok]: Data (t): 0.0006, 53.12/s/gpu Batch (t): 0.6025 LR: 0.000098 Step: 108450 Total Loss: 3.1009 Recon Loss: 3.0865 
[01/06 18:00:49 TiTok]: Data (t): 0.0009, 50.18/s/gpu Batch (t): 0.6377 LR: 0.000098 Step: 108500 Total Loss: 3.0931 Recon Loss: 3.0787 
[01/06 18:01:19 TiTok]: Data (t): 0.0011, 53.34/s/gpu Batch (t): 0.6000 LR: 0.000098 Step: 108550 Total Loss: 3.1337 Recon Loss: 3.1192 
[01/06 18:01:50 TiTok]: Data (t): 0.0007, 50.36/s/gpu Batch (t): 0.6354 LR: 0.000098 Step: 108600 Total Loss: 3.0839 Recon Loss: 3.0695 
[01/06 18:02:20 TiTok]: Data (t): 0.0011, 52.62/s/gpu Batch (t): 0.6081 LR: 0.000098 Step: 108650 Total Loss: 3.1221 Recon Loss: 3.1077 
[01/06 18:02:51 TiTok]: Data (t): 0.0011, 53.56/s/gpu Batch (t): 0.5975 LR: 0.000098 Step: 108700 Total Loss: 3.0986 Recon Loss: 3.0842 
[01/06 18:03:21 TiTok]: Data (t): 0.0011, 51.38/s/gpu Batch (t): 0.6228 LR: 0.000098 Step: 108750 Total Loss: 3.0985 Recon Loss: 3.0842 
[01/06 18:03:51 TiTok]: Data (t): 0.0007, 52.19/s/gpu Batch (t): 0.6131 LR: 0.000098 Step: 108800 Total Loss: 3.1315 Recon Loss: 3.1171 
[01/06 18:04:21 TiTok]: Data (t): 0.0011, 51.90/s/gpu Batch (t): 0.6166 LR: 0.000098 Step: 108850 Total Loss: 3.1171 Recon Loss: 3.1027 
[01/06 18:04:51 TiTok]: Data (t): 0.0011, 51.74/s/gpu Batch (t): 0.6185 LR: 0.000098 Step: 108900 Total Loss: 3.0838 Recon Loss: 3.0694 
[01/06 18:05:21 TiTok]: Data (t): 0.0011, 53.77/s/gpu Batch (t): 0.5951 LR: 0.000098 Step: 108950 Total Loss: 3.1232 Recon Loss: 3.1088 
[01/06 18:05:51 TiTok]: Data (t): 0.0011, 48.53/s/gpu Batch (t): 0.6593 LR: 0.000098 Step: 109000 Total Loss: 3.1291 Recon Loss: 3.1148 
[01/06 18:06:21 TiTok]: Data (t): 0.0011, 54.03/s/gpu Batch (t): 0.5923 LR: 0.000098 Step: 109050 Total Loss: 3.1150 Recon Loss: 3.1007 
[01/06 18:06:52 TiTok]: Data (t): 0.0011, 54.50/s/gpu Batch (t): 0.5872 LR: 0.000098 Step: 109100 Total Loss: 3.1009 Recon Loss: 3.0865 
[01/06 18:07:22 TiTok]: Data (t): 0.0011, 54.33/s/gpu Batch (t): 0.5890 LR: 0.000098 Step: 109150 Total Loss: 3.0688 Recon Loss: 3.0544 
[01/06 18:07:52 TiTok]: Data (t): 0.0011, 54.48/s/gpu Batch (t): 0.5873 LR: 0.000098 Step: 109200 Total Loss: 3.1171 Recon Loss: 3.1027 
[01/06 18:08:22 TiTok]: Data (t): 0.0006, 53.02/s/gpu Batch (t): 0.6035 LR: 0.000098 Step: 109250 Total Loss: 3.1226 Recon Loss: 3.1082 
[01/06 18:08:52 TiTok]: Data (t): 0.0011, 51.92/s/gpu Batch (t): 0.6164 LR: 0.000098 Step: 109300 Total Loss: 3.1249 Recon Loss: 3.1105 
[01/06 18:09:23 TiTok]: Data (t): 0.0012, 53.96/s/gpu Batch (t): 0.5931 LR: 0.000098 Step: 109350 Total Loss: 3.0995 Recon Loss: 3.0852 
[01/06 18:09:53 TiTok]: Data (t): 0.0007, 53.85/s/gpu Batch (t): 0.5943 LR: 0.000098 Step: 109400 Total Loss: 3.0769 Recon Loss: 3.0626 
[01/06 18:10:23 TiTok]: Data (t): 0.0011, 51.10/s/gpu Batch (t): 0.6262 LR: 0.000098 Step: 109450 Total Loss: 3.1578 Recon Loss: 3.1434 
[01/06 18:10:53 TiTok]: Data (t): 0.0007, 53.94/s/gpu Batch (t): 0.5933 LR: 0.000098 Step: 109500 Total Loss: 3.0896 Recon Loss: 3.0753 
[01/06 18:11:23 TiTok]: Data (t): 0.0011, 52.92/s/gpu Batch (t): 0.6047 LR: 0.000098 Step: 109550 Total Loss: 3.1043 Recon Loss: 3.0900 
[01/06 18:11:53 TiTok]: Data (t): 0.0007, 53.42/s/gpu Batch (t): 0.5990 LR: 0.000098 Step: 109600 Total Loss: 3.1000 Recon Loss: 3.0857 
[01/06 18:12:23 TiTok]: Data (t): 0.0006, 53.99/s/gpu Batch (t): 0.5927 LR: 0.000098 Step: 109650 Total Loss: 3.1007 Recon Loss: 3.0863 
[01/06 18:12:53 TiTok]: Data (t): 0.0011, 54.57/s/gpu Batch (t): 0.5864 LR: 0.000098 Step: 109700 Total Loss: 3.0680 Recon Loss: 3.0536 
[01/06 18:13:23 TiTok]: Data (t): 0.0011, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000098 Step: 109750 Total Loss: 3.1039 Recon Loss: 3.0895 
[01/06 18:13:53 TiTok]: Data (t): 0.0007, 52.93/s/gpu Batch (t): 0.6045 LR: 0.000098 Step: 109800 Total Loss: 3.1413 Recon Loss: 3.1269 
[01/06 18:14:23 TiTok]: Data (t): 0.0011, 52.95/s/gpu Batch (t): 0.6043 LR: 0.000098 Step: 109850 Total Loss: 3.0830 Recon Loss: 3.0686 
[01/06 18:14:54 TiTok]: Data (t): 0.0012, 51.99/s/gpu Batch (t): 0.6155 LR: 0.000098 Step: 109900 Total Loss: 3.1260 Recon Loss: 3.1116 
[01/06 18:15:24 TiTok]: Data (t): 0.0012, 52.94/s/gpu Batch (t): 0.6044 LR: 0.000098 Step: 109950 Total Loss: 3.1317 Recon Loss: 3.1173 
[01/06 18:15:54 TiTok]: Data (t): 0.0012, 46.56/s/gpu Batch (t): 0.6873 LR: 0.000098 Step: 110000 Total Loss: 3.0991 Recon Loss: 3.0848 
[01/06 18:15:56 TiTok]: Reconstructing images...
Epoch 21/199 started.
[01/06 18:16:31 TiTok]: Data (t): 0.0011, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000098 Step: 110050 Total Loss: 3.1026 Recon Loss: 3.0883 
[01/06 18:17:01 TiTok]: Data (t): 0.0012, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000098 Step: 110100 Total Loss: 3.1289 Recon Loss: 3.1145 
[01/06 18:17:31 TiTok]: Data (t): 0.0009, 52.22/s/gpu Batch (t): 0.6128 LR: 0.000098 Step: 110150 Total Loss: 3.1100 Recon Loss: 3.0956 
[01/06 18:18:02 TiTok]: Data (t): 0.0011, 53.86/s/gpu Batch (t): 0.5941 LR: 0.000098 Step: 110200 Total Loss: 3.0924 Recon Loss: 3.0780 
[01/06 18:18:32 TiTok]: Data (t): 0.0011, 54.13/s/gpu Batch (t): 0.5912 LR: 0.000098 Step: 110250 Total Loss: 3.1121 Recon Loss: 3.0977 
[01/06 18:19:02 TiTok]: Data (t): 0.0012, 51.74/s/gpu Batch (t): 0.6184 LR: 0.000098 Step: 110300 Total Loss: 3.1123 Recon Loss: 3.0980 
[01/06 18:19:32 TiTok]: Data (t): 0.0011, 54.41/s/gpu Batch (t): 0.5881 LR: 0.000098 Step: 110350 Total Loss: 3.1142 Recon Loss: 3.0999 
[01/06 18:20:02 TiTok]: Data (t): 0.0009, 53.36/s/gpu Batch (t): 0.5997 LR: 0.000098 Step: 110400 Total Loss: 3.1206 Recon Loss: 3.1062 
[01/06 18:20:32 TiTok]: Data (t): 0.0012, 54.41/s/gpu Batch (t): 0.5881 LR: 0.000098 Step: 110450 Total Loss: 3.0901 Recon Loss: 3.0757 
[01/06 18:21:02 TiTok]: Data (t): 0.0012, 54.30/s/gpu Batch (t): 0.5893 LR: 0.000098 Step: 110500 Total Loss: 3.0830 Recon Loss: 3.0686 
[01/06 18:21:32 TiTok]: Data (t): 0.0007, 54.00/s/gpu Batch (t): 0.5926 LR: 0.000098 Step: 110550 Total Loss: 3.0966 Recon Loss: 3.0822 
[01/06 18:22:03 TiTok]: Data (t): 0.0012, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000098 Step: 110600 Total Loss: 3.0853 Recon Loss: 3.0709 
[01/06 18:22:33 TiTok]: Data (t): 0.0007, 53.42/s/gpu Batch (t): 0.5990 LR: 0.000098 Step: 110650 Total Loss: 3.0997 Recon Loss: 3.0853 
[01/06 18:23:03 TiTok]: Data (t): 0.0011, 53.10/s/gpu Batch (t): 0.6026 LR: 0.000098 Step: 110700 Total Loss: 3.1107 Recon Loss: 3.0963 
[01/06 18:23:33 TiTok]: Data (t): 0.0007, 50.24/s/gpu Batch (t): 0.6369 LR: 0.000098 Step: 110750 Total Loss: 3.1214 Recon Loss: 3.1070 
[01/06 18:24:03 TiTok]: Data (t): 0.0012, 53.64/s/gpu Batch (t): 0.5965 LR: 0.000098 Step: 110800 Total Loss: 3.1300 Recon Loss: 3.1156 
[01/06 18:24:33 TiTok]: Data (t): 0.0011, 54.44/s/gpu Batch (t): 0.5878 LR: 0.000098 Step: 110850 Total Loss: 3.1023 Recon Loss: 3.0879 
[01/06 18:25:03 TiTok]: Data (t): 0.0014, 48.81/s/gpu Batch (t): 0.6555 LR: 0.000098 Step: 110900 Total Loss: 3.1077 Recon Loss: 3.0934 
[01/06 18:25:33 TiTok]: Data (t): 0.0012, 53.90/s/gpu Batch (t): 0.5936 LR: 0.000098 Step: 110950 Total Loss: 3.1022 Recon Loss: 3.0878 
[01/06 18:26:04 TiTok]: Data (t): 0.0012, 44.85/s/gpu Batch (t): 0.7135 LR: 0.000098 Step: 111000 Total Loss: 3.0900 Recon Loss: 3.0757 
[01/06 18:26:34 TiTok]: Data (t): 0.0286, 53.10/s/gpu Batch (t): 0.6027 LR: 0.000098 Step: 111050 Total Loss: 3.1049 Recon Loss: 3.0905 
[01/06 18:27:04 TiTok]: Data (t): 0.0012, 53.84/s/gpu Batch (t): 0.5944 LR: 0.000098 Step: 111100 Total Loss: 3.1094 Recon Loss: 3.0950 
[01/06 18:27:34 TiTok]: Data (t): 0.0012, 53.99/s/gpu Batch (t): 0.5927 LR: 0.000098 Step: 111150 Total Loss: 3.1142 Recon Loss: 3.0998 
[01/06 18:28:04 TiTok]: Data (t): 0.0007, 53.31/s/gpu Batch (t): 0.6002 LR: 0.000098 Step: 111200 Total Loss: 3.1185 Recon Loss: 3.1041 
[01/06 18:28:35 TiTok]: Data (t): 0.0014, 50.31/s/gpu Batch (t): 0.6360 LR: 0.000098 Step: 111250 Total Loss: 3.0979 Recon Loss: 3.0835 
[01/06 18:29:05 TiTok]: Data (t): 0.0011, 54.06/s/gpu Batch (t): 0.5919 LR: 0.000098 Step: 111300 Total Loss: 3.0921 Recon Loss: 3.0777 
[01/06 18:29:35 TiTok]: Data (t): 0.0007, 53.33/s/gpu Batch (t): 0.6000 LR: 0.000098 Step: 111350 Total Loss: 3.0945 Recon Loss: 3.0800 
[01/06 18:30:05 TiTok]: Data (t): 0.0011, 50.70/s/gpu Batch (t): 0.6312 LR: 0.000098 Step: 111400 Total Loss: 3.0875 Recon Loss: 3.0731 
[01/06 18:30:35 TiTok]: Data (t): 0.0007, 52.07/s/gpu Batch (t): 0.6146 LR: 0.000098 Step: 111450 Total Loss: 3.0602 Recon Loss: 3.0458 
[01/06 18:31:05 TiTok]: Data (t): 0.0011, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000098 Step: 111500 Total Loss: 3.0770 Recon Loss: 3.0626 
[01/06 18:31:35 TiTok]: Data (t): 0.0011, 54.21/s/gpu Batch (t): 0.5903 LR: 0.000098 Step: 111550 Total Loss: 3.1112 Recon Loss: 3.0968 
[01/06 18:32:06 TiTok]: Data (t): 0.0011, 53.96/s/gpu Batch (t): 0.5931 LR: 0.000098 Step: 111600 Total Loss: 3.1087 Recon Loss: 3.0943 
[01/06 18:32:36 TiTok]: Data (t): 0.0011, 52.56/s/gpu Batch (t): 0.6088 LR: 0.000098 Step: 111650 Total Loss: 3.0792 Recon Loss: 3.0647 
[01/06 18:33:06 TiTok]: Data (t): 0.0007, 48.37/s/gpu Batch (t): 0.6615 LR: 0.000098 Step: 111700 Total Loss: 3.0497 Recon Loss: 3.0353 
[01/06 18:33:37 TiTok]: Data (t): 0.0012, 53.59/s/gpu Batch (t): 0.5972 LR: 0.000098 Step: 111750 Total Loss: 3.1020 Recon Loss: 3.0875 
[01/06 18:34:07 TiTok]: Data (t): 0.0013, 53.84/s/gpu Batch (t): 0.5943 LR: 0.000098 Step: 111800 Total Loss: 3.0995 Recon Loss: 3.0851 
[01/06 18:34:37 TiTok]: Data (t): 0.0012, 53.86/s/gpu Batch (t): 0.5941 LR: 0.000098 Step: 111850 Total Loss: 3.1115 Recon Loss: 3.0971 
[01/06 18:35:07 TiTok]: Data (t): 0.0007, 53.28/s/gpu Batch (t): 0.6006 LR: 0.000098 Step: 111900 Total Loss: 3.1121 Recon Loss: 3.0977 
[01/06 18:35:37 TiTok]: Data (t): 0.0012, 53.36/s/gpu Batch (t): 0.5997 LR: 0.000098 Step: 111950 Total Loss: 3.1138 Recon Loss: 3.0994 
[01/06 18:36:07 TiTok]: Data (t): 0.0007, 45.13/s/gpu Batch (t): 0.7090 LR: 0.000098 Step: 112000 Total Loss: 3.0756 Recon Loss: 3.0612 
[01/06 18:36:37 TiTok]: Data (t): 0.0011, 54.05/s/gpu Batch (t): 0.5921 LR: 0.000098 Step: 112050 Total Loss: 3.1095 Recon Loss: 3.0951 
[01/06 18:37:08 TiTok]: Data (t): 0.0009, 54.00/s/gpu Batch (t): 0.5926 LR: 0.000098 Step: 112100 Total Loss: 3.0907 Recon Loss: 3.0763 
[01/06 18:37:38 TiTok]: Data (t): 0.0012, 54.32/s/gpu Batch (t): 0.5891 LR: 0.000098 Step: 112150 Total Loss: 3.0781 Recon Loss: 3.0637 
[01/06 18:38:08 TiTok]: Data (t): 0.0007, 53.31/s/gpu Batch (t): 0.6003 LR: 0.000098 Step: 112200 Total Loss: 3.1225 Recon Loss: 3.1081 
[01/06 18:38:38 TiTok]: Data (t): 0.0012, 53.79/s/gpu Batch (t): 0.5950 LR: 0.000098 Step: 112250 Total Loss: 3.0756 Recon Loss: 3.0612 
[01/06 18:39:09 TiTok]: Data (t): 0.0012, 53.74/s/gpu Batch (t): 0.5955 LR: 0.000098 Step: 112300 Total Loss: 3.0648 Recon Loss: 3.0504 
[01/06 18:39:39 TiTok]: Data (t): 0.0011, 53.79/s/gpu Batch (t): 0.5949 LR: 0.000098 Step: 112350 Total Loss: 3.1253 Recon Loss: 3.1110 
[01/06 18:40:09 TiTok]: Data (t): 0.0012, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000098 Step: 112400 Total Loss: 3.1032 Recon Loss: 3.0888 
[01/06 18:40:39 TiTok]: Data (t): 0.0009, 53.79/s/gpu Batch (t): 0.5949 LR: 0.000098 Step: 112450 Total Loss: 3.1234 Recon Loss: 3.1090 
[01/06 18:41:09 TiTok]: Data (t): 0.0011, 54.36/s/gpu Batch (t): 0.5887 LR: 0.000098 Step: 112500 Total Loss: 3.1094 Recon Loss: 3.0950 
[01/06 18:41:39 TiTok]: Data (t): 0.0015, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000098 Step: 112550 Total Loss: 3.1294 Recon Loss: 3.1150 
[01/06 18:42:09 TiTok]: Data (t): 0.0011, 53.64/s/gpu Batch (t): 0.5965 LR: 0.000098 Step: 112600 Total Loss: 3.1144 Recon Loss: 3.1000 
[01/06 18:42:39 TiTok]: Data (t): 0.0006, 52.79/s/gpu Batch (t): 0.6062 LR: 0.000098 Step: 112650 Total Loss: 3.0517 Recon Loss: 3.0373 
[01/06 18:43:09 TiTok]: Data (t): 0.0227, 53.07/s/gpu Batch (t): 0.6030 LR: 0.000098 Step: 112700 Total Loss: 3.0751 Recon Loss: 3.0607 
[01/06 18:43:39 TiTok]: Data (t): 0.0012, 54.31/s/gpu Batch (t): 0.5892 LR: 0.000098 Step: 112750 Total Loss: 3.0616 Recon Loss: 3.0471 
[01/06 18:44:09 TiTok]: Data (t): 0.0012, 54.36/s/gpu Batch (t): 0.5886 LR: 0.000098 Step: 112800 Total Loss: 3.0931 Recon Loss: 3.0786 
[01/06 18:44:40 TiTok]: Data (t): 0.0013, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000098 Step: 112850 Total Loss: 3.0709 Recon Loss: 3.0565 
[01/06 18:45:10 TiTok]: Data (t): 0.0012, 51.60/s/gpu Batch (t): 0.6202 LR: 0.000098 Step: 112900 Total Loss: 3.0631 Recon Loss: 3.0487 
[01/06 18:45:40 TiTok]: Data (t): 0.0011, 53.94/s/gpu Batch (t): 0.5932 LR: 0.000098 Step: 112950 Total Loss: 3.0849 Recon Loss: 3.0704 
[01/06 18:46:11 TiTok]: Data (t): 0.0014, 46.34/s/gpu Batch (t): 0.6905 LR: 0.000098 Step: 113000 Total Loss: 3.1075 Recon Loss: 3.0931 
[01/06 18:46:41 TiTok]: Data (t): 0.0011, 53.14/s/gpu Batch (t): 0.6022 LR: 0.000098 Step: 113050 Total Loss: 3.1128 Recon Loss: 3.0984 
[01/06 18:47:11 TiTok]: Data (t): 0.0012, 54.21/s/gpu Batch (t): 0.5903 LR: 0.000098 Step: 113100 Total Loss: 3.0900 Recon Loss: 3.0756 
[01/06 18:47:41 TiTok]: Data (t): 0.0012, 53.78/s/gpu Batch (t): 0.5951 LR: 0.000098 Step: 113150 Total Loss: 3.1109 Recon Loss: 3.0965 
[01/06 18:48:11 TiTok]: Data (t): 0.0012, 53.60/s/gpu Batch (t): 0.5970 LR: 0.000098 Step: 113200 Total Loss: 3.0887 Recon Loss: 3.0742 
[01/06 18:48:41 TiTok]: Data (t): 0.0011, 53.19/s/gpu Batch (t): 0.6016 LR: 0.000098 Step: 113250 Total Loss: 3.1067 Recon Loss: 3.0923 
[01/06 18:49:12 TiTok]: Data (t): 0.0007, 51.31/s/gpu Batch (t): 0.6237 LR: 0.000098 Step: 113300 Total Loss: 3.0869 Recon Loss: 3.0725 
[01/06 18:49:42 TiTok]: Data (t): 0.0007, 53.23/s/gpu Batch (t): 0.6012 LR: 0.000098 Step: 113350 Total Loss: 3.1495 Recon Loss: 3.1350 
[01/06 18:50:12 TiTok]: Data (t): 0.0011, 53.90/s/gpu Batch (t): 0.5936 LR: 0.000098 Step: 113400 Total Loss: 3.0895 Recon Loss: 3.0751 
[01/06 18:50:42 TiTok]: Data (t): 0.0011, 53.02/s/gpu Batch (t): 0.6035 LR: 0.000098 Step: 113450 Total Loss: 3.0896 Recon Loss: 3.0751 
[01/06 18:51:12 TiTok]: Data (t): 0.0006, 52.80/s/gpu Batch (t): 0.6061 LR: 0.000098 Step: 113500 Total Loss: 3.0954 Recon Loss: 3.0810 
[01/06 18:51:42 TiTok]: Data (t): 0.0011, 53.24/s/gpu Batch (t): 0.6011 LR: 0.000098 Step: 113550 Total Loss: 3.1001 Recon Loss: 3.0857 
[01/06 18:52:13 TiTok]: Data (t): 0.0012, 54.32/s/gpu Batch (t): 0.5891 LR: 0.000098 Step: 113600 Total Loss: 3.0535 Recon Loss: 3.0391 
[01/06 18:52:43 TiTok]: Data (t): 0.0006, 51.64/s/gpu Batch (t): 0.6197 LR: 0.000098 Step: 113650 Total Loss: 3.0956 Recon Loss: 3.0811 
[01/06 18:53:13 TiTok]: Data (t): 0.0012, 53.82/s/gpu Batch (t): 0.5945 LR: 0.000098 Step: 113700 Total Loss: 3.1552 Recon Loss: 3.1408 
[01/06 18:53:43 TiTok]: Data (t): 0.0012, 53.14/s/gpu Batch (t): 0.6021 LR: 0.000098 Step: 113750 Total Loss: 3.0689 Recon Loss: 3.0544 
[01/06 18:54:14 TiTok]: Data (t): 0.0011, 53.75/s/gpu Batch (t): 0.5954 LR: 0.000098 Step: 113800 Total Loss: 3.0678 Recon Loss: 3.0533 
[01/06 18:54:44 TiTok]: Data (t): 0.0006, 53.46/s/gpu Batch (t): 0.5985 LR: 0.000098 Step: 113850 Total Loss: 3.0700 Recon Loss: 3.0555 
[01/06 18:55:14 TiTok]: Data (t): 0.0011, 52.68/s/gpu Batch (t): 0.6075 LR: 0.000098 Step: 113900 Total Loss: 3.0766 Recon Loss: 3.0621 
[01/06 18:55:44 TiTok]: Data (t): 0.0011, 53.99/s/gpu Batch (t): 0.5927 LR: 0.000098 Step: 113950 Total Loss: 3.1079 Recon Loss: 3.0934 
[01/06 18:56:15 TiTok]: Data (t): 0.0011, 45.29/s/gpu Batch (t): 0.7066 LR: 0.000098 Step: 114000 Total Loss: 3.1352 Recon Loss: 3.1208 
[01/06 18:56:45 TiTok]: Data (t): 0.0011, 53.54/s/gpu Batch (t): 0.5977 LR: 0.000098 Step: 114050 Total Loss: 3.1026 Recon Loss: 3.0882 
[01/06 18:57:15 TiTok]: Data (t): 0.0006, 52.83/s/gpu Batch (t): 0.6057 LR: 0.000098 Step: 114100 Total Loss: 3.0678 Recon Loss: 3.0534 
[01/06 18:57:45 TiTok]: Data (t): 0.0007, 52.42/s/gpu Batch (t): 0.6105 LR: 0.000098 Step: 114150 Total Loss: 3.0588 Recon Loss: 3.0443 
[01/06 18:58:15 TiTok]: Data (t): 0.0014, 54.48/s/gpu Batch (t): 0.5874 LR: 0.000098 Step: 114200 Total Loss: 3.0888 Recon Loss: 3.0744 
[01/06 18:58:45 TiTok]: Data (t): 0.0007, 53.26/s/gpu Batch (t): 0.6008 LR: 0.000098 Step: 114250 Total Loss: 3.1153 Recon Loss: 3.1008 
[01/06 18:59:15 TiTok]: Data (t): 0.0014, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000098 Step: 114300 Total Loss: 3.0591 Recon Loss: 3.0446 
[01/06 18:59:46 TiTok]: Data (t): 0.0006, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000098 Step: 114350 Total Loss: 3.0722 Recon Loss: 3.0577 
[01/06 19:00:16 TiTok]: Data (t): 0.0011, 54.27/s/gpu Batch (t): 0.5897 LR: 0.000098 Step: 114400 Total Loss: 3.0645 Recon Loss: 3.0500 
[01/06 19:00:46 TiTok]: Data (t): 0.0009, 53.92/s/gpu Batch (t): 0.5934 LR: 0.000098 Step: 114450 Total Loss: 3.1401 Recon Loss: 3.1256 
[01/06 19:01:16 TiTok]: Data (t): 0.0011, 52.66/s/gpu Batch (t): 0.6077 LR: 0.000098 Step: 114500 Total Loss: 3.1023 Recon Loss: 3.0879 
[01/06 19:01:46 TiTok]: Data (t): 0.0011, 53.90/s/gpu Batch (t): 0.5936 LR: 0.000098 Step: 114550 Total Loss: 3.1008 Recon Loss: 3.0864 
[01/06 19:02:16 TiTok]: Data (t): 0.0014, 54.08/s/gpu Batch (t): 0.5917 LR: 0.000098 Step: 114600 Total Loss: 3.0563 Recon Loss: 3.0419 
[01/06 19:02:47 TiTok]: Data (t): 0.0011, 52.00/s/gpu Batch (t): 0.6154 LR: 0.000098 Step: 114650 Total Loss: 3.0970 Recon Loss: 3.0826 
[01/06 19:03:17 TiTok]: Data (t): 0.0007, 52.66/s/gpu Batch (t): 0.6077 LR: 0.000098 Step: 114700 Total Loss: 3.0605 Recon Loss: 3.0460 
[01/06 19:03:47 TiTok]: Data (t): 0.0012, 52.19/s/gpu Batch (t): 0.6132 LR: 0.000098 Step: 114750 Total Loss: 3.1161 Recon Loss: 3.1017 
[01/06 19:04:17 TiTok]: Data (t): 0.0011, 53.88/s/gpu Batch (t): 0.5940 LR: 0.000098 Step: 114800 Total Loss: 3.0975 Recon Loss: 3.0830 
[01/06 19:04:47 TiTok]: Data (t): 0.0011, 54.20/s/gpu Batch (t): 0.5904 LR: 0.000098 Step: 114850 Total Loss: 3.1259 Recon Loss: 3.1115 
[01/06 19:05:17 TiTok]: Data (t): 0.0011, 54.25/s/gpu Batch (t): 0.5898 LR: 0.000098 Step: 114900 Total Loss: 3.0798 Recon Loss: 3.0654 
[01/06 19:05:47 TiTok]: Data (t): 0.0011, 53.71/s/gpu Batch (t): 0.5958 LR: 0.000098 Step: 114950 Total Loss: 3.0859 Recon Loss: 3.0714 
[01/06 19:06:18 TiTok]: Data (t): 0.0011, 45.24/s/gpu Batch (t): 0.7073 LR: 0.000098 Step: 115000 Total Loss: 3.0961 Recon Loss: 3.0816 
[01/06 19:06:19 TiTok]: Reconstructing images...
Epoch 22/199 started.
[01/06 19:06:55 TiTok]: Data (t): 0.0012, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000098 Step: 115050 Total Loss: 3.1124 Recon Loss: 3.0980 
[01/06 19:07:25 TiTok]: Data (t): 0.0006, 50.10/s/gpu Batch (t): 0.6387 LR: 0.000098 Step: 115100 Total Loss: 3.0698 Recon Loss: 3.0554 
[01/06 19:07:55 TiTok]: Data (t): 0.0012, 54.42/s/gpu Batch (t): 0.5880 LR: 0.000098 Step: 115150 Total Loss: 3.1159 Recon Loss: 3.1014 
[01/06 19:08:25 TiTok]: Data (t): 0.0011, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000098 Step: 115200 Total Loss: 3.0446 Recon Loss: 3.0301 
[01/06 19:08:56 TiTok]: Data (t): 0.0011, 53.81/s/gpu Batch (t): 0.5947 LR: 0.000098 Step: 115250 Total Loss: 3.1135 Recon Loss: 3.0991 
[01/06 19:09:26 TiTok]: Data (t): 0.0012, 48.28/s/gpu Batch (t): 0.6628 LR: 0.000098 Step: 115300 Total Loss: 3.0743 Recon Loss: 3.0599 
[01/06 19:09:56 TiTok]: Data (t): 0.0012, 53.32/s/gpu Batch (t): 0.6002 LR: 0.000098 Step: 115350 Total Loss: 3.0810 Recon Loss: 3.0665 
[01/06 19:10:27 TiTok]: Data (t): 0.0017, 54.94/s/gpu Batch (t): 0.5824 LR: 0.000098 Step: 115400 Total Loss: 3.0868 Recon Loss: 3.0723 
[01/06 19:10:57 TiTok]: Data (t): 0.0012, 53.26/s/gpu Batch (t): 0.6009 LR: 0.000098 Step: 115450 Total Loss: 3.0867 Recon Loss: 3.0722 
[01/06 19:11:27 TiTok]: Data (t): 0.0011, 53.09/s/gpu Batch (t): 0.6028 LR: 0.000098 Step: 115500 Total Loss: 3.0786 Recon Loss: 3.0642 
[01/06 19:11:57 TiTok]: Data (t): 0.0009, 52.30/s/gpu Batch (t): 0.6118 LR: 0.000098 Step: 115550 Total Loss: 3.0632 Recon Loss: 3.0488 
[01/06 19:12:27 TiTok]: Data (t): 0.0011, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000097 Step: 115600 Total Loss: 3.1080 Recon Loss: 3.0935 
[01/06 19:12:57 TiTok]: Data (t): 0.0007, 53.41/s/gpu Batch (t): 0.5991 LR: 0.000097 Step: 115650 Total Loss: 3.0699 Recon Loss: 3.0554 
[01/06 19:13:28 TiTok]: Data (t): 0.0011, 54.42/s/gpu Batch (t): 0.5881 LR: 0.000097 Step: 115700 Total Loss: 3.1540 Recon Loss: 3.1394 
[01/06 19:13:58 TiTok]: Data (t): 0.0011, 51.87/s/gpu Batch (t): 0.6169 LR: 0.000097 Step: 115750 Total Loss: 3.1045 Recon Loss: 3.0900 
[01/06 19:14:28 TiTok]: Data (t): 0.0011, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000097 Step: 115800 Total Loss: 3.1022 Recon Loss: 3.0877 
[01/06 19:14:58 TiTok]: Data (t): 0.0011, 53.27/s/gpu Batch (t): 0.6007 LR: 0.000097 Step: 115850 Total Loss: 3.0913 Recon Loss: 3.0768 
[01/06 19:15:28 TiTok]: Data (t): 0.0011, 53.86/s/gpu Batch (t): 0.5942 LR: 0.000097 Step: 115900 Total Loss: 3.1154 Recon Loss: 3.1009 
[01/06 19:15:58 TiTok]: Data (t): 0.0011, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000097 Step: 115950 Total Loss: 3.0875 Recon Loss: 3.0731 
[01/06 19:16:29 TiTok]: Data (t): 0.0007, 46.03/s/gpu Batch (t): 0.6952 LR: 0.000097 Step: 116000 Total Loss: 3.1103 Recon Loss: 3.0958 
[01/06 19:16:59 TiTok]: Data (t): 0.0011, 52.05/s/gpu Batch (t): 0.6147 LR: 0.000097 Step: 116050 Total Loss: 3.0772 Recon Loss: 3.0627 
[01/06 19:17:29 TiTok]: Data (t): 0.0012, 52.19/s/gpu Batch (t): 0.6131 LR: 0.000097 Step: 116100 Total Loss: 3.1312 Recon Loss: 3.1168 
[01/06 19:17:59 TiTok]: Data (t): 0.0007, 53.25/s/gpu Batch (t): 0.6009 LR: 0.000097 Step: 116150 Total Loss: 3.1021 Recon Loss: 3.0876 
[01/06 19:18:29 TiTok]: Data (t): 0.0011, 52.88/s/gpu Batch (t): 0.6052 LR: 0.000097 Step: 116200 Total Loss: 3.1085 Recon Loss: 3.0940 
[01/06 19:19:00 TiTok]: Data (t): 0.0011, 53.39/s/gpu Batch (t): 0.5993 LR: 0.000097 Step: 116250 Total Loss: 3.0747 Recon Loss: 3.0603 
[01/06 19:19:30 TiTok]: Data (t): 0.0007, 53.34/s/gpu Batch (t): 0.6000 LR: 0.000097 Step: 116300 Total Loss: 3.0960 Recon Loss: 3.0815 
[01/06 19:20:00 TiTok]: Data (t): 0.0006, 51.53/s/gpu Batch (t): 0.6210 LR: 0.000097 Step: 116350 Total Loss: 3.0812 Recon Loss: 3.0667 
[01/06 19:20:30 TiTok]: Data (t): 0.0011, 52.28/s/gpu Batch (t): 0.6121 LR: 0.000097 Step: 116400 Total Loss: 3.1410 Recon Loss: 3.1265 
[01/06 19:21:00 TiTok]: Data (t): 0.0011, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000097 Step: 116450 Total Loss: 3.0749 Recon Loss: 3.0603 
[01/06 19:21:31 TiTok]: Data (t): 0.0007, 51.48/s/gpu Batch (t): 0.6216 LR: 0.000097 Step: 116500 Total Loss: 3.1062 Recon Loss: 3.0917 
[01/06 19:22:01 TiTok]: Data (t): 0.0007, 52.29/s/gpu Batch (t): 0.6120 LR: 0.000097 Step: 116550 Total Loss: 3.1009 Recon Loss: 3.0864 
[01/06 19:22:31 TiTok]: Data (t): 0.0011, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000097 Step: 116600 Total Loss: 3.0695 Recon Loss: 3.0550 
[01/06 19:23:01 TiTok]: Data (t): 0.0007, 53.81/s/gpu Batch (t): 0.5947 LR: 0.000097 Step: 116650 Total Loss: 3.0943 Recon Loss: 3.0799 
[01/06 19:23:31 TiTok]: Data (t): 0.0006, 53.32/s/gpu Batch (t): 0.6002 LR: 0.000097 Step: 116700 Total Loss: 3.0441 Recon Loss: 3.0296 
[01/06 19:24:01 TiTok]: Data (t): 0.0007, 52.12/s/gpu Batch (t): 0.6140 LR: 0.000097 Step: 116750 Total Loss: 3.0680 Recon Loss: 3.0535 
[01/06 19:24:31 TiTok]: Data (t): 0.0011, 53.15/s/gpu Batch (t): 0.6021 LR: 0.000097 Step: 116800 Total Loss: 3.1256 Recon Loss: 3.1111 
[01/06 19:25:01 TiTok]: Data (t): 0.0014, 51.82/s/gpu Batch (t): 0.6175 LR: 0.000097 Step: 116850 Total Loss: 3.0793 Recon Loss: 3.0648 
[01/06 19:25:32 TiTok]: Data (t): 0.0012, 52.40/s/gpu Batch (t): 0.6107 LR: 0.000097 Step: 116900 Total Loss: 3.0098 Recon Loss: 2.9953 
[01/06 19:26:02 TiTok]: Data (t): 0.0011, 53.72/s/gpu Batch (t): 0.5957 LR: 0.000097 Step: 116950 Total Loss: 3.0802 Recon Loss: 3.0657 
[01/06 19:26:32 TiTok]: Data (t): 0.0011, 47.04/s/gpu Batch (t): 0.6803 LR: 0.000097 Step: 117000 Total Loss: 3.0844 Recon Loss: 3.0699 
[01/06 19:27:03 TiTok]: Data (t): 0.0011, 51.85/s/gpu Batch (t): 0.6171 LR: 0.000097 Step: 117050 Total Loss: 3.0643 Recon Loss: 3.0499 
[01/06 19:27:33 TiTok]: Data (t): 0.0011, 54.16/s/gpu Batch (t): 0.5908 LR: 0.000097 Step: 117100 Total Loss: 3.1309 Recon Loss: 3.1164 
[01/06 19:28:03 TiTok]: Data (t): 0.0011, 53.16/s/gpu Batch (t): 0.6019 LR: 0.000097 Step: 117150 Total Loss: 3.0838 Recon Loss: 3.0693 
[01/06 19:28:34 TiTok]: Data (t): 0.0011, 52.71/s/gpu Batch (t): 0.6071 LR: 0.000097 Step: 117200 Total Loss: 3.0719 Recon Loss: 3.0574 
[01/06 19:29:04 TiTok]: Data (t): 0.0011, 53.59/s/gpu Batch (t): 0.5972 LR: 0.000097 Step: 117250 Total Loss: 3.1018 Recon Loss: 3.0873 
[01/06 19:29:34 TiTok]: Data (t): 0.0007, 51.69/s/gpu Batch (t): 0.6190 LR: 0.000097 Step: 117300 Total Loss: 3.0421 Recon Loss: 3.0276 
[01/06 19:30:04 TiTok]: Data (t): 0.0012, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000097 Step: 117350 Total Loss: 3.0734 Recon Loss: 3.0590 
[01/06 19:30:34 TiTok]: Data (t): 0.0011, 51.77/s/gpu Batch (t): 0.6181 LR: 0.000097 Step: 117400 Total Loss: 3.0767 Recon Loss: 3.0621 
[01/06 19:31:04 TiTok]: Data (t): 0.0011, 51.93/s/gpu Batch (t): 0.6162 LR: 0.000097 Step: 117450 Total Loss: 3.0939 Recon Loss: 3.0794 
[01/06 19:31:35 TiTok]: Data (t): 0.0011, 52.42/s/gpu Batch (t): 0.6104 LR: 0.000097 Step: 117500 Total Loss: 3.0438 Recon Loss: 3.0294 
[01/06 19:32:05 TiTok]: Data (t): 0.0011, 53.99/s/gpu Batch (t): 0.5927 LR: 0.000097 Step: 117550 Total Loss: 3.1005 Recon Loss: 3.0860 
[01/06 19:32:35 TiTok]: Data (t): 0.0011, 54.38/s/gpu Batch (t): 0.5885 LR: 0.000097 Step: 117600 Total Loss: 3.1349 Recon Loss: 3.1204 
[01/06 19:33:05 TiTok]: Data (t): 0.0006, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000097 Step: 117650 Total Loss: 3.0601 Recon Loss: 3.0456 
[01/06 19:33:35 TiTok]: Data (t): 0.0011, 54.32/s/gpu Batch (t): 0.5891 LR: 0.000097 Step: 117700 Total Loss: 3.0610 Recon Loss: 3.0466 
[01/06 19:34:06 TiTok]: Data (t): 0.0011, 53.01/s/gpu Batch (t): 0.6037 LR: 0.000097 Step: 117750 Total Loss: 3.1065 Recon Loss: 3.0920 
[01/06 19:34:36 TiTok]: Data (t): 0.0012, 53.72/s/gpu Batch (t): 0.5957 LR: 0.000097 Step: 117800 Total Loss: 3.0893 Recon Loss: 3.0748 
[01/06 19:35:06 TiTok]: Data (t): 0.0011, 53.22/s/gpu Batch (t): 0.6013 LR: 0.000097 Step: 117850 Total Loss: 3.0869 Recon Loss: 3.0724 
[01/06 19:35:36 TiTok]: Data (t): 0.0007, 53.33/s/gpu Batch (t): 0.6001 LR: 0.000097 Step: 117900 Total Loss: 3.0480 Recon Loss: 3.0335 
[01/06 19:36:07 TiTok]: Data (t): 0.0017, 53.16/s/gpu Batch (t): 0.6019 LR: 0.000097 Step: 117950 Total Loss: 3.0951 Recon Loss: 3.0806 
[01/06 19:36:37 TiTok]: Data (t): 0.0011, 46.77/s/gpu Batch (t): 0.6843 LR: 0.000097 Step: 118000 Total Loss: 3.0990 Recon Loss: 3.0844 
[01/06 19:37:07 TiTok]: Data (t): 0.0011, 53.72/s/gpu Batch (t): 0.5956 LR: 0.000097 Step: 118050 Total Loss: 3.1081 Recon Loss: 3.0935 
[01/06 19:37:37 TiTok]: Data (t): 0.0011, 51.92/s/gpu Batch (t): 0.6163 LR: 0.000097 Step: 118100 Total Loss: 3.0616 Recon Loss: 3.0471 
[01/06 19:38:07 TiTok]: Data (t): 0.0006, 53.95/s/gpu Batch (t): 0.5932 LR: 0.000097 Step: 118150 Total Loss: 3.0723 Recon Loss: 3.0578 
[01/06 19:38:38 TiTok]: Data (t): 0.0007, 53.31/s/gpu Batch (t): 0.6002 LR: 0.000097 Step: 118200 Total Loss: 3.0455 Recon Loss: 3.0310 
[01/06 19:39:08 TiTok]: Data (t): 0.0011, 51.66/s/gpu Batch (t): 0.6194 LR: 0.000097 Step: 118250 Total Loss: 3.0860 Recon Loss: 3.0714 
[01/06 19:39:38 TiTok]: Data (t): 0.0011, 53.70/s/gpu Batch (t): 0.5959 LR: 0.000097 Step: 118300 Total Loss: 3.1060 Recon Loss: 3.0915 
[01/06 19:40:08 TiTok]: Data (t): 0.0011, 53.14/s/gpu Batch (t): 0.6021 LR: 0.000097 Step: 118350 Total Loss: 3.0745 Recon Loss: 3.0599 
[01/06 19:40:38 TiTok]: Data (t): 0.0011, 49.82/s/gpu Batch (t): 0.6423 LR: 0.000097 Step: 118400 Total Loss: 3.0725 Recon Loss: 3.0580 
[01/06 19:41:09 TiTok]: Data (t): 0.0012, 53.86/s/gpu Batch (t): 0.5941 LR: 0.000097 Step: 118450 Total Loss: 3.0715 Recon Loss: 3.0570 
[01/06 19:41:39 TiTok]: Data (t): 0.0011, 51.70/s/gpu Batch (t): 0.6190 LR: 0.000097 Step: 118500 Total Loss: 3.1135 Recon Loss: 3.0990 
[01/06 19:42:09 TiTok]: Data (t): 0.0013, 52.97/s/gpu Batch (t): 0.6041 LR: 0.000097 Step: 118550 Total Loss: 3.0741 Recon Loss: 3.0595 
[01/06 19:42:39 TiTok]: Data (t): 0.0011, 52.14/s/gpu Batch (t): 0.6137 LR: 0.000097 Step: 118600 Total Loss: 3.0194 Recon Loss: 3.0049 
[01/06 19:43:09 TiTok]: Data (t): 0.0011, 54.05/s/gpu Batch (t): 0.5921 LR: 0.000097 Step: 118650 Total Loss: 3.0954 Recon Loss: 3.0808 
[01/06 19:43:39 TiTok]: Data (t): 0.0012, 52.66/s/gpu Batch (t): 0.6076 LR: 0.000097 Step: 118700 Total Loss: 3.0944 Recon Loss: 3.0798 
[01/06 19:44:09 TiTok]: Data (t): 0.0011, 54.32/s/gpu Batch (t): 0.5891 LR: 0.000097 Step: 118750 Total Loss: 3.0572 Recon Loss: 3.0427 
[01/06 19:44:39 TiTok]: Data (t): 0.0011, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000097 Step: 118800 Total Loss: 3.1092 Recon Loss: 3.0947 
[01/06 19:45:10 TiTok]: Data (t): 0.0012, 52.02/s/gpu Batch (t): 0.6151 LR: 0.000097 Step: 118850 Total Loss: 3.0713 Recon Loss: 3.0567 
[01/06 19:45:40 TiTok]: Data (t): 0.0011, 53.79/s/gpu Batch (t): 0.5949 LR: 0.000097 Step: 118900 Total Loss: 3.0652 Recon Loss: 3.0506 
[01/06 19:46:10 TiTok]: Data (t): 0.0011, 54.04/s/gpu Batch (t): 0.5922 LR: 0.000097 Step: 118950 Total Loss: 3.0736 Recon Loss: 3.0590 
[01/06 19:46:41 TiTok]: Data (t): 0.0007, 45.31/s/gpu Batch (t): 0.7063 LR: 0.000097 Step: 119000 Total Loss: 3.0929 Recon Loss: 3.0784 
[01/06 19:47:11 TiTok]: Data (t): 0.0011, 52.57/s/gpu Batch (t): 0.6087 LR: 0.000097 Step: 119050 Total Loss: 3.0885 Recon Loss: 3.0740 
[01/06 19:47:41 TiTok]: Data (t): 0.0011, 52.27/s/gpu Batch (t): 0.6122 LR: 0.000097 Step: 119100 Total Loss: 3.0651 Recon Loss: 3.0506 
[01/06 19:48:11 TiTok]: Data (t): 0.0011, 52.56/s/gpu Batch (t): 0.6088 LR: 0.000097 Step: 119150 Total Loss: 3.0609 Recon Loss: 3.0463 
[01/06 19:48:41 TiTok]: Data (t): 0.0011, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000097 Step: 119200 Total Loss: 3.0437 Recon Loss: 3.0291 
[01/06 19:49:11 TiTok]: Data (t): 0.0012, 54.53/s/gpu Batch (t): 0.5869 LR: 0.000097 Step: 119250 Total Loss: 3.1247 Recon Loss: 3.1102 
[01/06 19:49:42 TiTok]: Data (t): 0.0013, 53.71/s/gpu Batch (t): 0.5958 LR: 0.000097 Step: 119300 Total Loss: 3.1026 Recon Loss: 3.0880 
[01/06 19:50:12 TiTok]: Data (t): 0.0012, 52.82/s/gpu Batch (t): 0.6058 LR: 0.000097 Step: 119350 Total Loss: 3.0796 Recon Loss: 3.0650 
[01/06 19:50:42 TiTok]: Data (t): 0.0011, 48.70/s/gpu Batch (t): 0.6571 LR: 0.000097 Step: 119400 Total Loss: 3.0871 Recon Loss: 3.0725 
[01/06 19:51:12 TiTok]: Data (t): 0.0012, 53.70/s/gpu Batch (t): 0.5959 LR: 0.000097 Step: 119450 Total Loss: 3.0765 Recon Loss: 3.0619 
[01/06 19:51:42 TiTok]: Data (t): 0.0012, 54.01/s/gpu Batch (t): 0.5925 LR: 0.000097 Step: 119500 Total Loss: 3.1263 Recon Loss: 3.1117 
[01/06 19:52:12 TiTok]: Data (t): 0.0006, 53.31/s/gpu Batch (t): 0.6003 LR: 0.000097 Step: 119550 Total Loss: 3.0842 Recon Loss: 3.0696 
[01/06 19:52:43 TiTok]: Data (t): 0.0007, 49.01/s/gpu Batch (t): 0.6529 LR: 0.000097 Step: 119600 Total Loss: 3.0523 Recon Loss: 3.0377 
[01/06 19:53:13 TiTok]: Data (t): 0.0011, 53.99/s/gpu Batch (t): 0.5927 LR: 0.000097 Step: 119650 Total Loss: 3.0997 Recon Loss: 3.0852 
[01/06 19:53:43 TiTok]: Data (t): 0.0011, 53.16/s/gpu Batch (t): 0.6020 LR: 0.000097 Step: 119700 Total Loss: 3.1054 Recon Loss: 3.0908 
[01/06 19:54:13 TiTok]: Data (t): 0.0011, 53.75/s/gpu Batch (t): 0.5953 LR: 0.000097 Step: 119750 Total Loss: 3.0602 Recon Loss: 3.0456 
[01/06 19:54:44 TiTok]: Data (t): 0.0011, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000097 Step: 119800 Total Loss: 3.1003 Recon Loss: 3.0857 
[01/06 19:55:14 TiTok]: Data (t): 0.0012, 53.79/s/gpu Batch (t): 0.5949 LR: 0.000097 Step: 119850 Total Loss: 3.0871 Recon Loss: 3.0725 
[01/06 19:55:44 TiTok]: Data (t): 0.0011, 49.52/s/gpu Batch (t): 0.6462 LR: 0.000097 Step: 119900 Total Loss: 3.1079 Recon Loss: 3.0933 
[01/06 19:56:14 TiTok]: Data (t): 0.0011, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000097 Step: 119950 Total Loss: 3.0864 Recon Loss: 3.0719 
[01/06 19:56:45 TiTok]: Data (t): 0.0011, 43.45/s/gpu Batch (t): 0.7365 LR: 0.000097 Step: 120000 Total Loss: 3.0608 Recon Loss: 3.0462 
[01/06 19:56:46 TiTok]: Reconstructing images...
[01/06 19:57:22 TiTok]: Data (t): 0.0011, 54.55/s/gpu Batch (t): 0.5866 LR: 0.000097 Step: 120050 Total Loss: 3.0762 Recon Loss: 3.0616 
Epoch 23/199 started.
[01/06 19:57:55 TiTok]: Data (t): 0.0011, 51.66/s/gpu Batch (t): 0.6194 LR: 0.000097 Step: 120100 Total Loss: 3.0833 Recon Loss: 3.0687 
[01/06 19:58:25 TiTok]: Data (t): 0.0012, 53.61/s/gpu Batch (t): 0.5969 LR: 0.000097 Step: 120150 Total Loss: 3.0521 Recon Loss: 3.0375 
[01/06 19:58:55 TiTok]: Data (t): 0.0011, 53.32/s/gpu Batch (t): 0.6001 LR: 0.000097 Step: 120200 Total Loss: 3.1069 Recon Loss: 3.0923 
[01/06 19:59:26 TiTok]: Data (t): 0.0011, 49.72/s/gpu Batch (t): 0.6437 LR: 0.000097 Step: 120250 Total Loss: 3.0554 Recon Loss: 3.0408 
[01/06 19:59:56 TiTok]: Data (t): 0.0011, 52.68/s/gpu Batch (t): 0.6074 LR: 0.000097 Step: 120300 Total Loss: 3.0604 Recon Loss: 3.0458 
[01/06 20:00:26 TiTok]: Data (t): 0.0006, 51.81/s/gpu Batch (t): 0.6177 LR: 0.000097 Step: 120350 Total Loss: 3.0702 Recon Loss: 3.0556 
[01/06 20:00:57 TiTok]: Data (t): 0.0007, 51.24/s/gpu Batch (t): 0.6245 LR: 0.000097 Step: 120400 Total Loss: 3.0797 Recon Loss: 3.0650 
[01/06 20:01:27 TiTok]: Data (t): 0.0014, 52.92/s/gpu Batch (t): 0.6047 LR: 0.000097 Step: 120450 Total Loss: 3.0770 Recon Loss: 3.0624 
[01/06 20:01:57 TiTok]: Data (t): 0.0012, 53.64/s/gpu Batch (t): 0.5966 LR: 0.000097 Step: 120500 Total Loss: 3.0902 Recon Loss: 3.0756 
[01/06 20:02:27 TiTok]: Data (t): 0.0011, 53.95/s/gpu Batch (t): 0.5931 LR: 0.000097 Step: 120550 Total Loss: 3.0491 Recon Loss: 3.0345 
[01/06 20:02:58 TiTok]: Data (t): 0.0012, 52.06/s/gpu Batch (t): 0.6147 LR: 0.000097 Step: 120600 Total Loss: 3.0936 Recon Loss: 3.0790 
[01/06 20:03:28 TiTok]: Data (t): 0.0011, 54.27/s/gpu Batch (t): 0.5897 LR: 0.000097 Step: 120650 Total Loss: 3.0832 Recon Loss: 3.0686 
[01/06 20:03:58 TiTok]: Data (t): 0.0012, 53.31/s/gpu Batch (t): 0.6003 LR: 0.000097 Step: 120700 Total Loss: 3.0561 Recon Loss: 3.0414 
[01/06 20:04:28 TiTok]: Data (t): 0.0011, 52.35/s/gpu Batch (t): 0.6113 LR: 0.000097 Step: 120750 Total Loss: 3.0784 Recon Loss: 3.0638 
[01/06 20:04:58 TiTok]: Data (t): 0.0011, 53.83/s/gpu Batch (t): 0.5944 LR: 0.000097 Step: 120800 Total Loss: 3.0802 Recon Loss: 3.0656 
[01/06 20:05:29 TiTok]: Data (t): 0.0011, 52.75/s/gpu Batch (t): 0.6067 LR: 0.000097 Step: 120850 Total Loss: 3.0990 Recon Loss: 3.0843 
[01/06 20:05:59 TiTok]: Data (t): 0.0011, 53.12/s/gpu Batch (t): 0.6025 LR: 0.000097 Step: 120900 Total Loss: 3.0731 Recon Loss: 3.0585 
[01/06 20:06:29 TiTok]: Data (t): 0.0012, 52.68/s/gpu Batch (t): 0.6075 LR: 0.000097 Step: 120950 Total Loss: 3.0966 Recon Loss: 3.0819 
[01/06 20:07:00 TiTok]: Data (t): 0.0011, 45.38/s/gpu Batch (t): 0.7052 LR: 0.000097 Step: 121000 Total Loss: 3.0652 Recon Loss: 3.0506 
[01/06 20:07:30 TiTok]: Data (t): 0.0011, 52.01/s/gpu Batch (t): 0.6153 LR: 0.000097 Step: 121050 Total Loss: 3.1207 Recon Loss: 3.1061 
[01/06 20:08:00 TiTok]: Data (t): 0.0011, 53.79/s/gpu Batch (t): 0.5950 LR: 0.000097 Step: 121100 Total Loss: 3.0916 Recon Loss: 3.0770 
[01/06 20:08:30 TiTok]: Data (t): 0.0011, 53.18/s/gpu Batch (t): 0.6018 LR: 0.000097 Step: 121150 Total Loss: 3.0600 Recon Loss: 3.0453 
[01/06 20:09:00 TiTok]: Data (t): 0.0011, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000097 Step: 121200 Total Loss: 3.0492 Recon Loss: 3.0346 
[01/06 20:09:30 TiTok]: Data (t): 0.0011, 54.05/s/gpu Batch (t): 0.5921 LR: 0.000097 Step: 121250 Total Loss: 3.0245 Recon Loss: 3.0099 
[01/06 20:10:01 TiTok]: Data (t): 0.0011, 52.09/s/gpu Batch (t): 0.6144 LR: 0.000097 Step: 121300 Total Loss: 3.0487 Recon Loss: 3.0341 
[01/06 20:10:31 TiTok]: Data (t): 0.0011, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000097 Step: 121350 Total Loss: 3.0606 Recon Loss: 3.0460 
[01/06 20:11:01 TiTok]: Data (t): 0.0011, 54.42/s/gpu Batch (t): 0.5880 LR: 0.000097 Step: 121400 Total Loss: 3.0776 Recon Loss: 3.0630 
[01/06 20:11:31 TiTok]: Data (t): 0.0015, 53.94/s/gpu Batch (t): 0.5932 LR: 0.000097 Step: 121450 Total Loss: 3.0279 Recon Loss: 3.0133 
[01/06 20:12:01 TiTok]: Data (t): 0.0006, 53.87/s/gpu Batch (t): 0.5941 LR: 0.000097 Step: 121500 Total Loss: 3.0564 Recon Loss: 3.0418 
[01/06 20:12:31 TiTok]: Data (t): 0.0011, 53.01/s/gpu Batch (t): 0.6037 LR: 0.000097 Step: 121550 Total Loss: 3.0510 Recon Loss: 3.0364 
[01/06 20:13:01 TiTok]: Data (t): 0.0011, 53.31/s/gpu Batch (t): 0.6003 LR: 0.000097 Step: 121600 Total Loss: 3.0852 Recon Loss: 3.0706 
[01/06 20:13:31 TiTok]: Data (t): 0.0011, 53.81/s/gpu Batch (t): 0.5947 LR: 0.000097 Step: 121650 Total Loss: 3.0506 Recon Loss: 3.0360 
[01/06 20:14:02 TiTok]: Data (t): 0.0011, 51.41/s/gpu Batch (t): 0.6224 LR: 0.000097 Step: 121700 Total Loss: 3.1261 Recon Loss: 3.1114 
[01/06 20:14:32 TiTok]: Data (t): 0.0011, 53.46/s/gpu Batch (t): 0.5985 LR: 0.000097 Step: 121750 Total Loss: 3.0543 Recon Loss: 3.0397 
[01/06 20:15:02 TiTok]: Data (t): 0.0011, 51.61/s/gpu Batch (t): 0.6200 LR: 0.000097 Step: 121800 Total Loss: 3.0799 Recon Loss: 3.0653 
[01/06 20:15:33 TiTok]: Data (t): 0.0011, 54.19/s/gpu Batch (t): 0.5906 LR: 0.000097 Step: 121850 Total Loss: 3.0793 Recon Loss: 3.0646 
[01/06 20:16:03 TiTok]: Data (t): 0.0012, 52.64/s/gpu Batch (t): 0.6080 LR: 0.000097 Step: 121900 Total Loss: 3.0797 Recon Loss: 3.0651 
[01/06 20:16:33 TiTok]: Data (t): 0.0011, 52.62/s/gpu Batch (t): 0.6081 LR: 0.000097 Step: 121950 Total Loss: 3.0482 Recon Loss: 3.0336 
[01/06 20:17:03 TiTok]: Data (t): 0.0014, 47.14/s/gpu Batch (t): 0.6789 LR: 0.000097 Step: 122000 Total Loss: 3.0701 Recon Loss: 3.0555 
[01/06 20:17:34 TiTok]: Data (t): 0.0011, 53.78/s/gpu Batch (t): 0.5950 LR: 0.000097 Step: 122050 Total Loss: 3.0440 Recon Loss: 3.0294 
[01/06 20:18:04 TiTok]: Data (t): 0.0011, 52.00/s/gpu Batch (t): 0.6154 LR: 0.000097 Step: 122100 Total Loss: 3.0767 Recon Loss: 3.0620 
[01/06 20:18:34 TiTok]: Data (t): 0.0011, 53.98/s/gpu Batch (t): 0.5928 LR: 0.000097 Step: 122150 Total Loss: 3.0920 Recon Loss: 3.0773 
[01/06 20:19:04 TiTok]: Data (t): 0.0011, 54.24/s/gpu Batch (t): 0.5900 LR: 0.000097 Step: 122200 Total Loss: 3.0688 Recon Loss: 3.0541 
[01/06 20:19:35 TiTok]: Data (t): 0.0011, 53.27/s/gpu Batch (t): 0.6007 LR: 0.000097 Step: 122250 Total Loss: 3.1184 Recon Loss: 3.1037 
[01/06 20:20:05 TiTok]: Data (t): 0.0011, 53.93/s/gpu Batch (t): 0.5934 LR: 0.000097 Step: 122300 Total Loss: 3.0828 Recon Loss: 3.0681 
[01/06 20:20:35 TiTok]: Data (t): 0.0011, 53.64/s/gpu Batch (t): 0.5965 LR: 0.000097 Step: 122350 Total Loss: 3.0601 Recon Loss: 3.0455 
[01/06 20:21:05 TiTok]: Data (t): 0.0014, 53.71/s/gpu Batch (t): 0.5958 LR: 0.000097 Step: 122400 Total Loss: 3.0823 Recon Loss: 3.0676 
[01/06 20:21:35 TiTok]: Data (t): 0.0011, 53.84/s/gpu Batch (t): 0.5943 LR: 0.000097 Step: 122450 Total Loss: 3.0712 Recon Loss: 3.0565 
[01/06 20:22:05 TiTok]: Data (t): 0.0011, 53.77/s/gpu Batch (t): 0.5952 LR: 0.000097 Step: 122500 Total Loss: 3.0835 Recon Loss: 3.0688 
[01/06 20:22:35 TiTok]: Data (t): 0.0012, 51.77/s/gpu Batch (t): 0.6181 LR: 0.000097 Step: 122550 Total Loss: 3.0742 Recon Loss: 3.0594 
[01/06 20:23:05 TiTok]: Data (t): 0.0012, 53.27/s/gpu Batch (t): 0.6007 LR: 0.000097 Step: 122600 Total Loss: 3.0943 Recon Loss: 3.0796 
[01/06 20:23:36 TiTok]: Data (t): 0.0011, 54.36/s/gpu Batch (t): 0.5886 LR: 0.000097 Step: 122650 Total Loss: 3.0681 Recon Loss: 3.0535 
[01/06 20:24:06 TiTok]: Data (t): 0.0012, 53.85/s/gpu Batch (t): 0.5942 LR: 0.000097 Step: 122700 Total Loss: 3.0387 Recon Loss: 3.0240 
[01/06 20:24:36 TiTok]: Data (t): 0.0011, 54.37/s/gpu Batch (t): 0.5886 LR: 0.000097 Step: 122750 Total Loss: 3.0810 Recon Loss: 3.0662 
[01/06 20:25:06 TiTok]: Data (t): 0.0006, 53.75/s/gpu Batch (t): 0.5954 LR: 0.000097 Step: 122800 Total Loss: 3.0612 Recon Loss: 3.0466 
[01/06 20:25:36 TiTok]: Data (t): 0.0007, 52.68/s/gpu Batch (t): 0.6075 LR: 0.000097 Step: 122850 Total Loss: 3.1106 Recon Loss: 3.0959 
[01/06 20:26:06 TiTok]: Data (t): 0.0011, 53.75/s/gpu Batch (t): 0.5954 LR: 0.000097 Step: 122900 Total Loss: 3.1070 Recon Loss: 3.0923 
[01/06 20:26:37 TiTok]: Data (t): 0.0011, 51.92/s/gpu Batch (t): 0.6164 LR: 0.000097 Step: 122950 Total Loss: 3.0382 Recon Loss: 3.0235 
[01/06 20:27:07 TiTok]: Data (t): 0.0012, 45.24/s/gpu Batch (t): 0.7073 LR: 0.000097 Step: 123000 Total Loss: 3.0438 Recon Loss: 3.0291 
[01/06 20:27:37 TiTok]: Data (t): 0.0011, 52.37/s/gpu Batch (t): 0.6110 LR: 0.000097 Step: 123050 Total Loss: 3.0621 Recon Loss: 3.0474 
[01/06 20:28:07 TiTok]: Data (t): 0.0012, 53.11/s/gpu Batch (t): 0.6026 LR: 0.000097 Step: 123100 Total Loss: 3.0547 Recon Loss: 3.0399 
[01/06 20:28:37 TiTok]: Data (t): 0.0011, 51.62/s/gpu Batch (t): 0.6199 LR: 0.000097 Step: 123150 Total Loss: 3.0874 Recon Loss: 3.0727 
[01/06 20:29:08 TiTok]: Data (t): 0.0011, 53.75/s/gpu Batch (t): 0.5954 LR: 0.000097 Step: 123200 Total Loss: 3.0672 Recon Loss: 3.0525 
[01/06 20:29:38 TiTok]: Data (t): 0.0012, 52.85/s/gpu Batch (t): 0.6055 LR: 0.000097 Step: 123250 Total Loss: 3.0576 Recon Loss: 3.0429 
[01/06 20:30:08 TiTok]: Data (t): 0.0011, 53.97/s/gpu Batch (t): 0.5929 LR: 0.000097 Step: 123300 Total Loss: 3.0564 Recon Loss: 3.0416 
[01/06 20:30:38 TiTok]: Data (t): 0.0013, 50.87/s/gpu Batch (t): 0.6290 LR: 0.000097 Step: 123350 Total Loss: 3.0522 Recon Loss: 3.0375 
[01/06 20:31:08 TiTok]: Data (t): 0.0012, 53.79/s/gpu Batch (t): 0.5950 LR: 0.000097 Step: 123400 Total Loss: 3.0823 Recon Loss: 3.0676 
[01/06 20:31:38 TiTok]: Data (t): 0.0011, 52.45/s/gpu Batch (t): 0.6101 LR: 0.000097 Step: 123450 Total Loss: 3.0091 Recon Loss: 2.9943 
[01/06 20:32:08 TiTok]: Data (t): 0.0011, 52.21/s/gpu Batch (t): 0.6129 LR: 0.000097 Step: 123500 Total Loss: 3.0660 Recon Loss: 3.0513 
[01/06 20:32:38 TiTok]: Data (t): 0.0011, 53.46/s/gpu Batch (t): 0.5985 LR: 0.000097 Step: 123550 Total Loss: 3.0391 Recon Loss: 3.0244 
[01/06 20:33:08 TiTok]: Data (t): 0.0011, 53.82/s/gpu Batch (t): 0.5945 LR: 0.000097 Step: 123600 Total Loss: 3.0775 Recon Loss: 3.0628 
[01/06 20:33:38 TiTok]: Data (t): 0.0012, 53.66/s/gpu Batch (t): 0.5964 LR: 0.000097 Step: 123650 Total Loss: 3.0605 Recon Loss: 3.0458 
[01/06 20:34:09 TiTok]: Data (t): 0.0011, 53.78/s/gpu Batch (t): 0.5950 LR: 0.000097 Step: 123700 Total Loss: 3.0272 Recon Loss: 3.0125 
[01/06 20:34:39 TiTok]: Data (t): 0.0011, 52.10/s/gpu Batch (t): 0.6142 LR: 0.000097 Step: 123750 Total Loss: 3.0465 Recon Loss: 3.0318 
[01/06 20:35:09 TiTok]: Data (t): 0.0011, 52.84/s/gpu Batch (t): 0.6056 LR: 0.000097 Step: 123800 Total Loss: 3.0744 Recon Loss: 3.0597 
[01/06 20:35:39 TiTok]: Data (t): 0.0007, 53.86/s/gpu Batch (t): 0.5942 LR: 0.000097 Step: 123850 Total Loss: 3.0701 Recon Loss: 3.0554 
[01/06 20:36:09 TiTok]: Data (t): 0.0006, 53.27/s/gpu Batch (t): 0.6008 LR: 0.000097 Step: 123900 Total Loss: 3.0658 Recon Loss: 3.0511 
[01/06 20:36:39 TiTok]: Data (t): 0.0011, 53.58/s/gpu Batch (t): 0.5973 LR: 0.000097 Step: 123950 Total Loss: 3.0768 Recon Loss: 3.0621 
[01/06 20:37:09 TiTok]: Data (t): 0.0013, 44.89/s/gpu Batch (t): 0.7129 LR: 0.000097 Step: 124000 Total Loss: 3.0650 Recon Loss: 3.0502 
[01/06 20:37:39 TiTok]: Data (t): 0.0011, 54.27/s/gpu Batch (t): 0.5897 LR: 0.000097 Step: 124050 Total Loss: 3.1104 Recon Loss: 3.0957 
[01/06 20:38:09 TiTok]: Data (t): 0.0007, 53.30/s/gpu Batch (t): 0.6004 LR: 0.000097 Step: 124100 Total Loss: 3.0768 Recon Loss: 3.0621 
[01/06 20:38:39 TiTok]: Data (t): 0.0011, 53.96/s/gpu Batch (t): 0.5931 LR: 0.000097 Step: 124150 Total Loss: 3.0806 Recon Loss: 3.0659 
[01/06 20:39:09 TiTok]: Data (t): 0.0014, 53.99/s/gpu Batch (t): 0.5927 LR: 0.000097 Step: 124200 Total Loss: 3.0947 Recon Loss: 3.0800 
[01/06 20:39:39 TiTok]: Data (t): 0.0011, 53.97/s/gpu Batch (t): 0.5929 LR: 0.000097 Step: 124250 Total Loss: 3.0855 Recon Loss: 3.0707 
[01/06 20:40:09 TiTok]: Data (t): 0.0011, 53.61/s/gpu Batch (t): 0.5969 LR: 0.000097 Step: 124300 Total Loss: 3.0966 Recon Loss: 3.0818 
[01/06 20:40:40 TiTok]: Data (t): 0.0011, 53.81/s/gpu Batch (t): 0.5947 LR: 0.000097 Step: 124350 Total Loss: 3.0625 Recon Loss: 3.0477 
[01/06 20:41:10 TiTok]: Data (t): 0.0011, 53.81/s/gpu Batch (t): 0.5946 LR: 0.000097 Step: 124400 Total Loss: 3.0790 Recon Loss: 3.0643 
[01/06 20:41:40 TiTok]: Data (t): 0.0011, 53.78/s/gpu Batch (t): 0.5951 LR: 0.000097 Step: 124450 Total Loss: 3.0538 Recon Loss: 3.0391 
[01/06 20:42:10 TiTok]: Data (t): 0.0013, 53.77/s/gpu Batch (t): 0.5952 LR: 0.000097 Step: 124500 Total Loss: 3.1105 Recon Loss: 3.0957 
[01/06 20:42:40 TiTok]: Data (t): 0.0011, 53.93/s/gpu Batch (t): 0.5933 LR: 0.000097 Step: 124550 Total Loss: 3.0666 Recon Loss: 3.0518 
[01/06 20:43:10 TiTok]: Data (t): 0.0011, 52.67/s/gpu Batch (t): 0.6076 LR: 0.000097 Step: 124600 Total Loss: 3.0618 Recon Loss: 3.0470 
[01/06 20:43:40 TiTok]: Data (t): 0.0009, 51.85/s/gpu Batch (t): 0.6171 LR: 0.000097 Step: 124650 Total Loss: 3.1059 Recon Loss: 3.0911 
[01/06 20:44:10 TiTok]: Data (t): 0.0011, 53.81/s/gpu Batch (t): 0.5947 LR: 0.000097 Step: 124700 Total Loss: 3.0806 Recon Loss: 3.0657 
[01/06 20:44:40 TiTok]: Data (t): 0.0013, 53.61/s/gpu Batch (t): 0.5969 LR: 0.000097 Step: 124750 Total Loss: 3.0617 Recon Loss: 3.0470 
[01/06 20:45:10 TiTok]: Data (t): 0.0011, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000097 Step: 124800 Total Loss: 3.0977 Recon Loss: 3.0829 
[01/06 20:45:41 TiTok]: Data (t): 0.0013, 51.54/s/gpu Batch (t): 0.6209 LR: 0.000097 Step: 124850 Total Loss: 3.0885 Recon Loss: 3.0738 
[01/06 20:46:11 TiTok]: Data (t): 0.0011, 53.84/s/gpu Batch (t): 0.5943 LR: 0.000097 Step: 124900 Total Loss: 3.0488 Recon Loss: 3.0341 
[01/06 20:46:41 TiTok]: Data (t): 0.0011, 53.79/s/gpu Batch (t): 0.5949 LR: 0.000097 Step: 124950 Total Loss: 3.0610 Recon Loss: 3.0463 
[01/06 20:47:12 TiTok]: Data (t): 0.0006, 45.65/s/gpu Batch (t): 0.7010 LR: 0.000097 Step: 125000 Total Loss: 3.1181 Recon Loss: 3.1033 
[01/06 20:47:13 TiTok]: Reconstructing images...
[01/06 20:47:49 TiTok]: Data (t): 0.0011, 53.95/s/gpu Batch (t): 0.5932 LR: 0.000097 Step: 125050 Total Loss: 3.0722 Recon Loss: 3.0575 
Epoch 24/199 started.
[01/06 20:48:21 TiTok]: Data (t): 0.0011, 53.13/s/gpu Batch (t): 0.6023 LR: 0.000097 Step: 125100 Total Loss: 3.0586 Recon Loss: 3.0438 
[01/06 20:48:51 TiTok]: Data (t): 0.0012, 52.12/s/gpu Batch (t): 0.6140 LR: 0.000097 Step: 125150 Total Loss: 3.0881 Recon Loss: 3.0733 
[01/06 20:49:22 TiTok]: Data (t): 0.0011, 51.94/s/gpu Batch (t): 0.6161 LR: 0.000097 Step: 125200 Total Loss: 3.0537 Recon Loss: 3.0389 
[01/06 20:49:52 TiTok]: Data (t): 0.0011, 53.97/s/gpu Batch (t): 0.5929 LR: 0.000097 Step: 125250 Total Loss: 3.0632 Recon Loss: 3.0484 
[01/06 20:50:22 TiTok]: Data (t): 0.0011, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000097 Step: 125300 Total Loss: 3.0077 Recon Loss: 2.9930 
[01/06 20:50:52 TiTok]: Data (t): 0.0011, 50.62/s/gpu Batch (t): 0.6322 LR: 0.000097 Step: 125350 Total Loss: 3.0657 Recon Loss: 3.0509 
[01/06 20:51:23 TiTok]: Data (t): 0.0011, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000097 Step: 125400 Total Loss: 3.0378 Recon Loss: 3.0229 
[01/06 20:51:53 TiTok]: Data (t): 0.0011, 52.51/s/gpu Batch (t): 0.6094 LR: 0.000097 Step: 125450 Total Loss: 3.0817 Recon Loss: 3.0668 
[01/06 20:52:23 TiTok]: Data (t): 0.0011, 53.86/s/gpu Batch (t): 0.5941 LR: 0.000097 Step: 125500 Total Loss: 3.0756 Recon Loss: 3.0608 
[01/06 20:52:53 TiTok]: Data (t): 0.0011, 52.68/s/gpu Batch (t): 0.6075 LR: 0.000097 Step: 125550 Total Loss: 3.0903 Recon Loss: 3.0754 
[01/06 20:53:23 TiTok]: Data (t): 0.0011, 53.08/s/gpu Batch (t): 0.6028 LR: 0.000097 Step: 125600 Total Loss: 3.0559 Recon Loss: 3.0410 
[01/06 20:53:54 TiTok]: Data (t): 0.0011, 53.80/s/gpu Batch (t): 0.5948 LR: 0.000097 Step: 125650 Total Loss: 3.0552 Recon Loss: 3.0404 
[01/06 20:54:24 TiTok]: Data (t): 0.0013, 53.85/s/gpu Batch (t): 0.5943 LR: 0.000097 Step: 125700 Total Loss: 3.0588 Recon Loss: 3.0440 
[01/06 20:54:54 TiTok]: Data (t): 0.0011, 53.94/s/gpu Batch (t): 0.5932 LR: 0.000097 Step: 125750 Total Loss: 3.0789 Recon Loss: 3.0641 
[01/06 20:55:25 TiTok]: Data (t): 0.0011, 52.00/s/gpu Batch (t): 0.6154 LR: 0.000097 Step: 125800 Total Loss: 3.0526 Recon Loss: 3.0377 
[01/06 20:55:55 TiTok]: Data (t): 0.0007, 52.65/s/gpu Batch (t): 0.6078 LR: 0.000097 Step: 125850 Total Loss: 3.0859 Recon Loss: 3.0711 
[01/06 20:56:25 TiTok]: Data (t): 0.0006, 52.99/s/gpu Batch (t): 0.6038 LR: 0.000097 Step: 125900 Total Loss: 3.0435 Recon Loss: 3.0287 
[01/06 20:56:55 TiTok]: Data (t): 0.0006, 52.22/s/gpu Batch (t): 0.6128 LR: 0.000097 Step: 125950 Total Loss: 3.0982 Recon Loss: 3.0834 
[01/06 20:57:25 TiTok]: Data (t): 0.0011, 48.52/s/gpu Batch (t): 0.6596 LR: 0.000097 Step: 126000 Total Loss: 3.0614 Recon Loss: 3.0466 
[01/06 20:57:55 TiTok]: Data (t): 0.0011, 53.79/s/gpu Batch (t): 0.5949 LR: 0.000097 Step: 126050 Total Loss: 3.0703 Recon Loss: 3.0554 
[01/06 20:58:26 TiTok]: Data (t): 0.0011, 52.04/s/gpu Batch (t): 0.6149 LR: 0.000097 Step: 126100 Total Loss: 3.0486 Recon Loss: 3.0337 
[01/06 20:58:56 TiTok]: Data (t): 0.0011, 52.53/s/gpu Batch (t): 0.6092 LR: 0.000097 Step: 126150 Total Loss: 3.0317 Recon Loss: 3.0168 
[01/06 20:59:26 TiTok]: Data (t): 0.0011, 53.75/s/gpu Batch (t): 0.5954 LR: 0.000097 Step: 126200 Total Loss: 3.0727 Recon Loss: 3.0579 
[01/06 20:59:56 TiTok]: Data (t): 0.0011, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000097 Step: 126250 Total Loss: 3.0459 Recon Loss: 3.0310 
[01/06 21:00:26 TiTok]: Data (t): 0.0006, 51.88/s/gpu Batch (t): 0.6168 LR: 0.000097 Step: 126300 Total Loss: 3.0766 Recon Loss: 3.0617 
[01/06 21:00:56 TiTok]: Data (t): 0.0011, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000097 Step: 126350 Total Loss: 3.0909 Recon Loss: 3.0759 
[01/06 21:01:26 TiTok]: Data (t): 0.0011, 46.74/s/gpu Batch (t): 0.6846 LR: 0.000097 Step: 126400 Total Loss: 3.0458 Recon Loss: 3.0309 
[01/06 21:01:57 TiTok]: Data (t): 0.0011, 52.23/s/gpu Batch (t): 0.6127 LR: 0.000097 Step: 126450 Total Loss: 3.0330 Recon Loss: 3.0181 
[01/06 21:02:27 TiTok]: Data (t): 0.0011, 53.74/s/gpu Batch (t): 0.5954 LR: 0.000097 Step: 126500 Total Loss: 3.0725 Recon Loss: 3.0576 
[01/06 21:02:57 TiTok]: Data (t): 0.0011, 53.98/s/gpu Batch (t): 0.5928 LR: 0.000097 Step: 126550 Total Loss: 3.0910 Recon Loss: 3.0762 
[01/06 21:03:27 TiTok]: Data (t): 0.0294, 50.13/s/gpu Batch (t): 0.6384 LR: 0.000097 Step: 126600 Total Loss: 3.0889 Recon Loss: 3.0739 
[01/06 21:03:57 TiTok]: Data (t): 0.0011, 53.83/s/gpu Batch (t): 0.5944 LR: 0.000097 Step: 126650 Total Loss: 3.0714 Recon Loss: 3.0565 
[01/06 21:04:27 TiTok]: Data (t): 0.0011, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000097 Step: 126700 Total Loss: 3.0559 Recon Loss: 3.0410 
[01/06 21:04:57 TiTok]: Data (t): 0.0013, 53.57/s/gpu Batch (t): 0.5974 LR: 0.000097 Step: 126750 Total Loss: 3.0984 Recon Loss: 3.0834 
[01/06 21:05:28 TiTok]: Data (t): 0.0011, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000097 Step: 126800 Total Loss: 3.0627 Recon Loss: 3.0478 
[01/06 21:05:58 TiTok]: Data (t): 0.0011, 53.84/s/gpu Batch (t): 0.5943 LR: 0.000097 Step: 126850 Total Loss: 3.0518 Recon Loss: 3.0368 
[01/06 21:06:28 TiTok]: Data (t): 0.0006, 52.33/s/gpu Batch (t): 0.6115 LR: 0.000097 Step: 126900 Total Loss: 3.0818 Recon Loss: 3.0668 
[01/06 21:06:58 TiTok]: Data (t): 0.0011, 54.25/s/gpu Batch (t): 0.5898 LR: 0.000097 Step: 126950 Total Loss: 3.0906 Recon Loss: 3.0757 
[01/06 21:07:28 TiTok]: Data (t): 0.0011, 47.50/s/gpu Batch (t): 0.6737 LR: 0.000097 Step: 127000 Total Loss: 3.0542 Recon Loss: 3.0393 
[01/06 21:07:58 TiTok]: Data (t): 0.0011, 53.25/s/gpu Batch (t): 0.6010 LR: 0.000097 Step: 127050 Total Loss: 3.0925 Recon Loss: 3.0775 
[01/06 21:08:28 TiTok]: Data (t): 0.0011, 53.86/s/gpu Batch (t): 0.5942 LR: 0.000097 Step: 127100 Total Loss: 3.0452 Recon Loss: 3.0303 
[01/06 21:08:58 TiTok]: Data (t): 0.0011, 53.86/s/gpu Batch (t): 0.5941 LR: 0.000097 Step: 127150 Total Loss: 3.0898 Recon Loss: 3.0749 
[01/06 21:09:29 TiTok]: Data (t): 0.0012, 53.81/s/gpu Batch (t): 0.5946 LR: 0.000097 Step: 127200 Total Loss: 3.0454 Recon Loss: 3.0305 
[01/06 21:09:59 TiTok]: Data (t): 0.0011, 53.81/s/gpu Batch (t): 0.5947 LR: 0.000097 Step: 127250 Total Loss: 3.0797 Recon Loss: 3.0647 
[01/06 21:10:29 TiTok]: Data (t): 0.0011, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000097 Step: 127300 Total Loss: 3.0523 Recon Loss: 3.0373 
[01/06 21:11:00 TiTok]: Data (t): 0.0011, 52.57/s/gpu Batch (t): 0.6087 LR: 0.000097 Step: 127350 Total Loss: 3.0593 Recon Loss: 3.0442 
[01/06 21:11:30 TiTok]: Data (t): 0.0012, 53.42/s/gpu Batch (t): 0.5991 LR: 0.000097 Step: 127400 Total Loss: 3.0840 Recon Loss: 3.0690 
[01/06 21:12:00 TiTok]: Data (t): 0.0006, 51.67/s/gpu Batch (t): 0.6193 LR: 0.000097 Step: 127450 Total Loss: 3.0903 Recon Loss: 3.0754 
[01/06 21:12:30 TiTok]: Data (t): 0.0014, 53.82/s/gpu Batch (t): 0.5945 LR: 0.000097 Step: 127500 Total Loss: 3.0632 Recon Loss: 3.0483 
[01/06 21:13:00 TiTok]: Data (t): 0.0011, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000097 Step: 127550 Total Loss: 3.0594 Recon Loss: 3.0444 
[01/06 21:13:31 TiTok]: Data (t): 0.0011, 51.96/s/gpu Batch (t): 0.6159 LR: 0.000097 Step: 127600 Total Loss: 3.0584 Recon Loss: 3.0435 
[01/06 21:14:01 TiTok]: Data (t): 0.0011, 53.93/s/gpu Batch (t): 0.5934 LR: 0.000097 Step: 127650 Total Loss: 3.0929 Recon Loss: 3.0780 
[01/06 21:14:31 TiTok]: Data (t): 0.0011, 53.64/s/gpu Batch (t): 0.5965 LR: 0.000097 Step: 127700 Total Loss: 3.1018 Recon Loss: 3.0868 
[01/06 21:15:01 TiTok]: Data (t): 0.0011, 54.42/s/gpu Batch (t): 0.5880 LR: 0.000097 Step: 127750 Total Loss: 3.0650 Recon Loss: 3.0500 
[01/06 21:15:31 TiTok]: Data (t): 0.0011, 53.33/s/gpu Batch (t): 0.6000 LR: 0.000097 Step: 127800 Total Loss: 3.0732 Recon Loss: 3.0582 
[01/06 21:16:01 TiTok]: Data (t): 0.0012, 53.84/s/gpu Batch (t): 0.5943 LR: 0.000097 Step: 127850 Total Loss: 3.0755 Recon Loss: 3.0605 
[01/06 21:16:32 TiTok]: Data (t): 0.0011, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000097 Step: 127900 Total Loss: 3.0472 Recon Loss: 3.0323 
[01/06 21:17:02 TiTok]: Data (t): 0.0012, 53.80/s/gpu Batch (t): 0.5948 LR: 0.000097 Step: 127950 Total Loss: 3.0525 Recon Loss: 3.0376 
[01/06 21:17:32 TiTok]: Data (t): 0.0011, 43.90/s/gpu Batch (t): 0.7289 LR: 0.000097 Step: 128000 Total Loss: 3.0523 Recon Loss: 3.0373 
[01/06 21:18:02 TiTok]: Data (t): 0.0011, 53.74/s/gpu Batch (t): 0.5955 LR: 0.000097 Step: 128050 Total Loss: 3.0496 Recon Loss: 3.0347 
[01/06 21:18:32 TiTok]: Data (t): 0.0011, 52.19/s/gpu Batch (t): 0.6131 LR: 0.000097 Step: 128100 Total Loss: 3.0863 Recon Loss: 3.0713 
[01/06 21:19:02 TiTok]: Data (t): 0.0011, 53.79/s/gpu Batch (t): 0.5949 LR: 0.000097 Step: 128150 Total Loss: 3.0385 Recon Loss: 3.0236 
[01/06 21:19:33 TiTok]: Data (t): 0.0011, 53.91/s/gpu Batch (t): 0.5935 LR: 0.000097 Step: 128200 Total Loss: 3.1200 Recon Loss: 3.1050 
[01/06 21:20:03 TiTok]: Data (t): 0.0011, 53.80/s/gpu Batch (t): 0.5948 LR: 0.000097 Step: 128250 Total Loss: 3.0644 Recon Loss: 3.0494 
[01/06 21:20:33 TiTok]: Data (t): 0.0058, 53.80/s/gpu Batch (t): 0.5948 LR: 0.000097 Step: 128300 Total Loss: 3.0554 Recon Loss: 3.0403 
[01/06 21:21:03 TiTok]: Data (t): 0.0011, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000097 Step: 128350 Total Loss: 3.0721 Recon Loss: 3.0570 
[01/06 21:21:33 TiTok]: Data (t): 0.0014, 51.55/s/gpu Batch (t): 0.6207 LR: 0.000097 Step: 128400 Total Loss: 3.0771 Recon Loss: 3.0620 
[01/06 21:22:03 TiTok]: Data (t): 0.0011, 54.10/s/gpu Batch (t): 0.5915 LR: 0.000097 Step: 128450 Total Loss: 3.0496 Recon Loss: 3.0346 
[01/06 21:22:34 TiTok]: Data (t): 0.0011, 50.50/s/gpu Batch (t): 0.6337 LR: 0.000097 Step: 128500 Total Loss: 3.0734 Recon Loss: 3.0583 
[01/06 21:23:04 TiTok]: Data (t): 0.0011, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000097 Step: 128550 Total Loss: 3.0708 Recon Loss: 3.0557 
[01/06 21:23:34 TiTok]: Data (t): 0.0011, 54.39/s/gpu Batch (t): 0.5884 LR: 0.000097 Step: 128600 Total Loss: 3.0737 Recon Loss: 3.0586 
[01/06 21:24:04 TiTok]: Data (t): 0.0011, 53.80/s/gpu Batch (t): 0.5948 LR: 0.000097 Step: 128650 Total Loss: 3.0072 Recon Loss: 2.9922 
[01/06 21:24:34 TiTok]: Data (t): 0.0007, 53.42/s/gpu Batch (t): 0.5991 LR: 0.000097 Step: 128700 Total Loss: 3.0476 Recon Loss: 3.0326 
[01/06 21:25:04 TiTok]: Data (t): 0.0011, 53.99/s/gpu Batch (t): 0.5927 LR: 0.000097 Step: 128750 Total Loss: 3.0606 Recon Loss: 3.0456 
[01/06 21:25:34 TiTok]: Data (t): 0.0011, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000097 Step: 128800 Total Loss: 3.0258 Recon Loss: 3.0108 
[01/06 21:26:04 TiTok]: Data (t): 0.0011, 53.13/s/gpu Batch (t): 0.6023 LR: 0.000097 Step: 128850 Total Loss: 3.0887 Recon Loss: 3.0735 
[01/06 21:26:35 TiTok]: Data (t): 0.0011, 53.04/s/gpu Batch (t): 0.6034 LR: 0.000097 Step: 128900 Total Loss: 3.0520 Recon Loss: 3.0370 
[01/06 21:27:05 TiTok]: Data (t): 0.0011, 53.86/s/gpu Batch (t): 0.5942 LR: 0.000097 Step: 128950 Total Loss: 3.0671 Recon Loss: 3.0520 
[01/06 21:27:35 TiTok]: Data (t): 0.0011, 48.22/s/gpu Batch (t): 0.6636 LR: 0.000097 Step: 129000 Total Loss: 3.1251 Recon Loss: 3.1099 
[01/06 21:28:05 TiTok]: Data (t): 0.0011, 50.16/s/gpu Batch (t): 0.6379 LR: 0.000097 Step: 129050 Total Loss: 3.0377 Recon Loss: 3.0226 
[01/06 21:28:35 TiTok]: Data (t): 0.0006, 51.84/s/gpu Batch (t): 0.6173 LR: 0.000097 Step: 129100 Total Loss: 3.0662 Recon Loss: 3.0510 
[01/06 21:29:05 TiTok]: Data (t): 0.0011, 53.59/s/gpu Batch (t): 0.5971 LR: 0.000097 Step: 129150 Total Loss: 3.0735 Recon Loss: 3.0584 
[01/06 21:29:36 TiTok]: Data (t): 0.0006, 50.27/s/gpu Batch (t): 0.6366 LR: 0.000097 Step: 129200 Total Loss: 3.0882 Recon Loss: 3.0731 
[01/06 21:30:06 TiTok]: Data (t): 0.0011, 52.12/s/gpu Batch (t): 0.6139 LR: 0.000097 Step: 129250 Total Loss: 3.0699 Recon Loss: 3.0548 
[01/06 21:30:36 TiTok]: Data (t): 0.0012, 52.90/s/gpu Batch (t): 0.6049 LR: 0.000097 Step: 129300 Total Loss: 3.0323 Recon Loss: 3.0171 
[01/06 21:31:06 TiTok]: Data (t): 0.0011, 53.61/s/gpu Batch (t): 0.5969 LR: 0.000097 Step: 129350 Total Loss: 3.0499 Recon Loss: 3.0347 
[01/06 21:31:36 TiTok]: Data (t): 0.0011, 53.92/s/gpu Batch (t): 0.5935 LR: 0.000097 Step: 129400 Total Loss: 3.0360 Recon Loss: 3.0208 
[01/06 21:32:06 TiTok]: Data (t): 0.0012, 54.55/s/gpu Batch (t): 0.5866 LR: 0.000097 Step: 129450 Total Loss: 3.0474 Recon Loss: 3.0322 
[01/06 21:32:36 TiTok]: Data (t): 0.0011, 53.71/s/gpu Batch (t): 0.5958 LR: 0.000097 Step: 129500 Total Loss: 3.1070 Recon Loss: 3.0917 
[01/06 21:33:07 TiTok]: Data (t): 0.0011, 53.77/s/gpu Batch (t): 0.5952 LR: 0.000097 Step: 129550 Total Loss: 3.0651 Recon Loss: 3.0498 
[01/06 21:33:37 TiTok]: Data (t): 0.0006, 53.27/s/gpu Batch (t): 0.6007 LR: 0.000097 Step: 129600 Total Loss: 3.0598 Recon Loss: 3.0445 
[01/06 21:34:07 TiTok]: Data (t): 0.0011, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000097 Step: 129650 Total Loss: 3.0537 Recon Loss: 3.0384 
[01/06 21:34:37 TiTok]: Data (t): 0.0011, 53.92/s/gpu Batch (t): 0.5935 LR: 0.000097 Step: 129700 Total Loss: 3.0128 Recon Loss: 2.9974 
[01/06 21:35:07 TiTok]: Data (t): 0.0011, 53.45/s/gpu Batch (t): 0.5986 LR: 0.000097 Step: 129750 Total Loss: 3.0211 Recon Loss: 3.0059 
[01/06 21:35:37 TiTok]: Data (t): 0.0011, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000097 Step: 129800 Total Loss: 3.0641 Recon Loss: 3.0488 
[01/06 21:36:07 TiTok]: Data (t): 0.0011, 54.40/s/gpu Batch (t): 0.5882 LR: 0.000097 Step: 129850 Total Loss: 3.0218 Recon Loss: 3.0065 
[01/06 21:36:37 TiTok]: Data (t): 0.0011, 52.83/s/gpu Batch (t): 0.6057 LR: 0.000097 Step: 129900 Total Loss: 3.0500 Recon Loss: 3.0346 
[01/06 21:37:08 TiTok]: Data (t): 0.0011, 53.71/s/gpu Batch (t): 0.5958 LR: 0.000097 Step: 129950 Total Loss: 3.0789 Recon Loss: 3.0635 
[01/06 21:37:38 TiTok]: Data (t): 0.0011, 46.03/s/gpu Batch (t): 0.6952 LR: 0.000097 Step: 130000 Total Loss: 3.0519 Recon Loss: 3.0365 
[01/06 21:37:39 TiTok]: Reconstructing images...
[01/06 21:38:15 TiTok]: Data (t): 0.0012, 53.66/s/gpu Batch (t): 0.5963 LR: 0.000097 Step: 130050 Total Loss: 3.0617 Recon Loss: 3.0463 
Epoch 25/199 started.
[01/06 21:38:47 TiTok]: Data (t): 0.0007, 51.54/s/gpu Batch (t): 0.6209 LR: 0.000097 Step: 130100 Total Loss: 3.0281 Recon Loss: 3.0126 
[01/06 21:39:17 TiTok]: Data (t): 0.0012, 50.86/s/gpu Batch (t): 0.6292 LR: 0.000097 Step: 130150 Total Loss: 3.0798 Recon Loss: 3.0642 
[01/06 21:39:47 TiTok]: Data (t): 0.0012, 53.74/s/gpu Batch (t): 0.5955 LR: 0.000097 Step: 130200 Total Loss: 3.0611 Recon Loss: 3.0455 
[01/06 21:40:17 TiTok]: Data (t): 0.0006, 52.41/s/gpu Batch (t): 0.6106 LR: 0.000097 Step: 130250 Total Loss: 3.0215 Recon Loss: 3.0059 
[01/06 21:40:47 TiTok]: Data (t): 0.0012, 53.84/s/gpu Batch (t): 0.5943 LR: 0.000097 Step: 130300 Total Loss: 3.0381 Recon Loss: 3.0225 
[01/06 21:41:17 TiTok]: Data (t): 0.0014, 53.81/s/gpu Batch (t): 0.5947 LR: 0.000097 Step: 130350 Total Loss: 3.0643 Recon Loss: 3.0486 
[01/06 21:41:47 TiTok]: Data (t): 0.0007, 53.53/s/gpu Batch (t): 0.5978 LR: 0.000097 Step: 130400 Total Loss: 3.1009 Recon Loss: 3.0853 
[01/06 21:42:17 TiTok]: Data (t): 0.0011, 53.76/s/gpu Batch (t): 0.5952 LR: 0.000097 Step: 130450 Total Loss: 3.0581 Recon Loss: 3.0425 
[01/06 21:42:48 TiTok]: Data (t): 0.0011, 53.98/s/gpu Batch (t): 0.5928 LR: 0.000097 Step: 130500 Total Loss: 3.0454 Recon Loss: 3.0297 
[01/06 21:43:18 TiTok]: Data (t): 0.0011, 53.75/s/gpu Batch (t): 0.5953 LR: 0.000097 Step: 130550 Total Loss: 3.0223 Recon Loss: 3.0065 
[01/06 21:43:48 TiTok]: Data (t): 0.0008, 51.71/s/gpu Batch (t): 0.6188 LR: 0.000097 Step: 130600 Total Loss: 3.0419 Recon Loss: 3.0261 
[01/06 21:44:18 TiTok]: Data (t): 0.0007, 53.28/s/gpu Batch (t): 0.6006 LR: 0.000097 Step: 130650 Total Loss: 3.0548 Recon Loss: 3.0389 
[01/06 21:44:48 TiTok]: Data (t): 0.0011, 53.69/s/gpu Batch (t): 0.5960 LR: 0.000097 Step: 130700 Total Loss: 3.0586 Recon Loss: 3.0423 
[01/06 21:45:18 TiTok]: Data (t): 0.0012, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000097 Step: 130750 Total Loss: 3.0711 Recon Loss: 3.0542 
[01/06 21:45:48 TiTok]: Data (t): 0.0012, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000097 Step: 130800 Total Loss: 3.0552 Recon Loss: 3.0376 
[01/06 21:46:18 TiTok]: Data (t): 0.0012, 52.11/s/gpu Batch (t): 0.6141 LR: 0.000097 Step: 130850 Total Loss: 3.0818 Recon Loss: 3.0626 
[01/06 21:46:48 TiTok]: Data (t): 0.0011, 52.51/s/gpu Batch (t): 0.6094 LR: 0.000097 Step: 130900 Total Loss: 3.0984 Recon Loss: 3.0790 
[01/06 21:47:18 TiTok]: Data (t): 0.0012, 53.69/s/gpu Batch (t): 0.5960 LR: 0.000097 Step: 130950 Total Loss: 3.0923 Recon Loss: 3.0737 
[01/06 21:47:48 TiTok]: Data (t): 0.0007, 40.50/s/gpu Batch (t): 0.7902 LR: 0.000097 Step: 131000 Total Loss: 3.0776 Recon Loss: 3.0594 
[01/06 21:48:19 TiTok]: Data (t): 0.0011, 53.79/s/gpu Batch (t): 0.5949 LR: 0.000097 Step: 131050 Total Loss: 3.0738 Recon Loss: 3.0558 
[01/06 21:48:49 TiTok]: Data (t): 0.0007, 53.19/s/gpu Batch (t): 0.6016 LR: 0.000097 Step: 131100 Total Loss: 3.1242 Recon Loss: 3.1062 
[01/06 21:49:19 TiTok]: Data (t): 0.0012, 53.59/s/gpu Batch (t): 0.5971 LR: 0.000097 Step: 131150 Total Loss: 3.0850 Recon Loss: 3.0670 
[01/06 21:49:49 TiTok]: Data (t): 0.0012, 51.57/s/gpu Batch (t): 0.6205 LR: 0.000097 Step: 131200 Total Loss: 3.0928 Recon Loss: 3.0749 
[01/06 21:50:19 TiTok]: Data (t): 0.0012, 53.22/s/gpu Batch (t): 0.6013 LR: 0.000097 Step: 131250 Total Loss: 3.1006 Recon Loss: 3.0827 
[01/06 21:50:49 TiTok]: Data (t): 0.0011, 51.90/s/gpu Batch (t): 0.6165 LR: 0.000097 Step: 131300 Total Loss: 3.0875 Recon Loss: 3.0695 
[01/06 21:51:20 TiTok]: Data (t): 0.0014, 52.78/s/gpu Batch (t): 0.6063 LR: 0.000097 Step: 131350 Total Loss: 3.0303 Recon Loss: 3.0125 
[01/06 21:51:50 TiTok]: Data (t): 0.0012, 53.83/s/gpu Batch (t): 0.5944 LR: 0.000097 Step: 131400 Total Loss: 3.1117 Recon Loss: 3.0937 
[01/06 21:52:20 TiTok]: Data (t): 0.0008, 53.41/s/gpu Batch (t): 0.5991 LR: 0.000097 Step: 131450 Total Loss: 3.1010 Recon Loss: 3.0830 
[01/06 21:52:50 TiTok]: Data (t): 0.0012, 53.73/s/gpu Batch (t): 0.5956 LR: 0.000097 Step: 131500 Total Loss: 3.0800 Recon Loss: 3.0621 
[01/06 21:53:20 TiTok]: Data (t): 0.0007, 53.46/s/gpu Batch (t): 0.5986 LR: 0.000097 Step: 131550 Total Loss: 3.0769 Recon Loss: 3.0589 
[01/06 21:53:50 TiTok]: Data (t): 0.0012, 53.29/s/gpu Batch (t): 0.6004 LR: 0.000097 Step: 131600 Total Loss: 3.0824 Recon Loss: 3.0644 
[01/06 21:54:20 TiTok]: Data (t): 0.0012, 53.59/s/gpu Batch (t): 0.5972 LR: 0.000097 Step: 131650 Total Loss: 3.1120 Recon Loss: 3.0941 
[01/06 21:54:50 TiTok]: Data (t): 0.0011, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000097 Step: 131700 Total Loss: 3.0718 Recon Loss: 3.0537 
[01/06 21:55:20 TiTok]: Data (t): 0.0012, 51.94/s/gpu Batch (t): 0.6161 LR: 0.000097 Step: 131750 Total Loss: 3.1011 Recon Loss: 3.0831 
[01/06 21:55:50 TiTok]: Data (t): 0.0012, 54.30/s/gpu Batch (t): 0.5893 LR: 0.000097 Step: 131800 Total Loss: 3.1104 Recon Loss: 3.0925 
[01/06 21:56:21 TiTok]: Data (t): 0.0011, 53.39/s/gpu Batch (t): 0.5993 LR: 0.000097 Step: 131850 Total Loss: 3.1375 Recon Loss: 3.1196 
[01/06 21:56:51 TiTok]: Data (t): 0.0011, 53.67/s/gpu Batch (t): 0.5962 LR: 0.000097 Step: 131900 Total Loss: 3.0958 Recon Loss: 3.0779 
[01/06 21:57:21 TiTok]: Data (t): 0.0011, 52.43/s/gpu Batch (t): 0.6103 LR: 0.000097 Step: 131950 Total Loss: 3.1296 Recon Loss: 3.1116 
[01/06 21:57:51 TiTok]: Data (t): 0.0011, 44.66/s/gpu Batch (t): 0.7165 LR: 0.000097 Step: 132000 Total Loss: 3.1074 Recon Loss: 3.0894 
[01/06 21:58:21 TiTok]: Data (t): 0.0008, 53.28/s/gpu Batch (t): 0.6006 LR: 0.000097 Step: 132050 Total Loss: 3.1078 Recon Loss: 3.0898 
[01/06 21:58:51 TiTok]: Data (t): 0.0011, 51.42/s/gpu Batch (t): 0.6223 LR: 0.000097 Step: 132100 Total Loss: 3.0841 Recon Loss: 3.0661 
[01/06 21:59:22 TiTok]: Data (t): 0.0011, 51.40/s/gpu Batch (t): 0.6225 LR: 0.000097 Step: 132150 Total Loss: 3.1009 Recon Loss: 3.0830 
[01/06 21:59:52 TiTok]: Data (t): 0.0014, 52.68/s/gpu Batch (t): 0.6074 LR: 0.000097 Step: 132200 Total Loss: 3.1156 Recon Loss: 3.0975 
[01/06 22:00:22 TiTok]: Data (t): 0.0006, 48.81/s/gpu Batch (t): 0.6556 LR: 0.000097 Step: 132250 Total Loss: 3.0920 Recon Loss: 3.0740 
[01/06 22:00:53 TiTok]: Data (t): 0.0011, 51.23/s/gpu Batch (t): 0.6247 LR: 0.000097 Step: 132300 Total Loss: 3.0905 Recon Loss: 3.0724 
[01/06 22:01:23 TiTok]: Data (t): 0.0011, 53.35/s/gpu Batch (t): 0.5998 LR: 0.000097 Step: 132350 Total Loss: 3.0917 Recon Loss: 3.0737 
[01/06 22:01:53 TiTok]: Data (t): 0.0011, 51.49/s/gpu Batch (t): 0.6215 LR: 0.000097 Step: 132400 Total Loss: 3.0813 Recon Loss: 3.0633 
[01/06 22:02:23 TiTok]: Data (t): 0.0011, 53.73/s/gpu Batch (t): 0.5956 LR: 0.000097 Step: 132450 Total Loss: 3.0631 Recon Loss: 3.0451 
[01/06 22:02:54 TiTok]: Data (t): 0.0011, 53.63/s/gpu Batch (t): 0.5967 LR: 0.000097 Step: 132500 Total Loss: 3.0936 Recon Loss: 3.0756 
[01/06 22:03:24 TiTok]: Data (t): 0.0012, 51.96/s/gpu Batch (t): 0.6159 LR: 0.000097 Step: 132550 Total Loss: 3.1035 Recon Loss: 3.0855 
[01/06 22:03:54 TiTok]: Data (t): 0.0011, 53.86/s/gpu Batch (t): 0.5942 LR: 0.000097 Step: 132600 Total Loss: 3.1222 Recon Loss: 3.1042 
[01/06 22:04:24 TiTok]: Data (t): 0.0011, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000097 Step: 132650 Total Loss: 3.0918 Recon Loss: 3.0738 
[01/06 22:04:54 TiTok]: Data (t): 0.0011, 51.65/s/gpu Batch (t): 0.6196 LR: 0.000097 Step: 132700 Total Loss: 3.0636 Recon Loss: 3.0455 
[01/06 22:05:25 TiTok]: Data (t): 0.0011, 53.80/s/gpu Batch (t): 0.5948 LR: 0.000097 Step: 132750 Total Loss: 3.1241 Recon Loss: 3.1060 
[01/06 22:05:55 TiTok]: Data (t): 0.0006, 53.28/s/gpu Batch (t): 0.6006 LR: 0.000097 Step: 132800 Total Loss: 3.0769 Recon Loss: 3.0589 
[01/06 22:06:25 TiTok]: Data (t): 0.0011, 52.16/s/gpu Batch (t): 0.6135 LR: 0.000097 Step: 132850 Total Loss: 3.0740 Recon Loss: 3.0561 
[01/06 22:06:55 TiTok]: Data (t): 0.0011, 54.37/s/gpu Batch (t): 0.5886 LR: 0.000097 Step: 132900 Total Loss: 3.1012 Recon Loss: 3.0832 
[01/06 22:07:25 TiTok]: Data (t): 0.0011, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000097 Step: 132950 Total Loss: 3.0805 Recon Loss: 3.0625 
[01/06 22:07:56 TiTok]: Data (t): 0.0011, 48.03/s/gpu Batch (t): 0.6662 LR: 0.000097 Step: 133000 Total Loss: 3.1088 Recon Loss: 3.0908 
[01/06 22:08:26 TiTok]: Data (t): 0.0014, 53.80/s/gpu Batch (t): 0.5948 LR: 0.000097 Step: 133050 Total Loss: 3.1072 Recon Loss: 3.0891 
[01/06 22:08:56 TiTok]: Data (t): 0.0011, 54.36/s/gpu Batch (t): 0.5887 LR: 0.000097 Step: 133100 Total Loss: 3.0877 Recon Loss: 3.0696 
[01/06 22:09:26 TiTok]: Data (t): 0.0012, 51.28/s/gpu Batch (t): 0.6240 LR: 0.000097 Step: 133150 Total Loss: 3.0588 Recon Loss: 3.0407 
[01/06 22:09:56 TiTok]: Data (t): 0.0011, 53.13/s/gpu Batch (t): 0.6023 LR: 0.000097 Step: 133200 Total Loss: 3.0723 Recon Loss: 3.0542 
[01/06 22:10:27 TiTok]: Data (t): 0.0011, 53.62/s/gpu Batch (t): 0.5968 LR: 0.000097 Step: 133250 Total Loss: 3.0674 Recon Loss: 3.0493 
[01/06 22:10:57 TiTok]: Data (t): 0.0011, 53.05/s/gpu Batch (t): 0.6032 LR: 0.000097 Step: 133300 Total Loss: 3.1080 Recon Loss: 3.0899 
[01/06 22:11:27 TiTok]: Data (t): 0.0011, 52.18/s/gpu Batch (t): 0.6133 LR: 0.000097 Step: 133350 Total Loss: 3.0710 Recon Loss: 3.0529 
[01/06 22:11:57 TiTok]: Data (t): 0.0011, 53.79/s/gpu Batch (t): 0.5949 LR: 0.000097 Step: 133400 Total Loss: 3.0848 Recon Loss: 3.0667 
[01/06 22:12:27 TiTok]: Data (t): 0.0011, 51.18/s/gpu Batch (t): 0.6252 LR: 0.000097 Step: 133450 Total Loss: 3.0926 Recon Loss: 3.0745 
[01/06 22:12:58 TiTok]: Data (t): 0.0012, 47.58/s/gpu Batch (t): 0.6725 LR: 0.000097 Step: 133500 Total Loss: 3.0804 Recon Loss: 3.0623 
[01/06 22:13:28 TiTok]: Data (t): 0.0012, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000097 Step: 133550 Total Loss: 3.1055 Recon Loss: 3.0873 
[01/06 22:13:58 TiTok]: Data (t): 0.0006, 52.61/s/gpu Batch (t): 0.6082 LR: 0.000097 Step: 133600 Total Loss: 3.0782 Recon Loss: 3.0601 
[01/06 22:14:28 TiTok]: Data (t): 0.0011, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000097 Step: 133650 Total Loss: 3.1089 Recon Loss: 3.0908 
[01/06 22:14:58 TiTok]: Data (t): 0.0011, 52.76/s/gpu Batch (t): 0.6066 LR: 0.000097 Step: 133700 Total Loss: 3.0808 Recon Loss: 3.0627 
[01/06 22:15:28 TiTok]: Data (t): 0.0006, 53.99/s/gpu Batch (t): 0.5927 LR: 0.000097 Step: 133750 Total Loss: 3.0304 Recon Loss: 3.0123 
[01/06 22:15:58 TiTok]: Data (t): 0.0011, 54.41/s/gpu Batch (t): 0.5881 LR: 0.000097 Step: 133800 Total Loss: 3.0975 Recon Loss: 3.0795 
[01/06 22:16:28 TiTok]: Data (t): 0.0012, 53.84/s/gpu Batch (t): 0.5944 LR: 0.000097 Step: 133850 Total Loss: 3.0697 Recon Loss: 3.0516 
[01/06 22:16:58 TiTok]: Data (t): 0.0011, 53.72/s/gpu Batch (t): 0.5957 LR: 0.000097 Step: 133900 Total Loss: 3.0603 Recon Loss: 3.0422 
[01/06 22:17:29 TiTok]: Data (t): 0.0013, 52.11/s/gpu Batch (t): 0.6141 LR: 0.000097 Step: 133950 Total Loss: 3.0962 Recon Loss: 3.0781 
[01/06 22:17:59 TiTok]: Data (t): 0.0011, 45.02/s/gpu Batch (t): 0.7108 LR: 0.000097 Step: 134000 Total Loss: 3.0616 Recon Loss: 3.0436 
[01/06 22:18:29 TiTok]: Data (t): 0.0011, 53.25/s/gpu Batch (t): 0.6009 LR: 0.000097 Step: 134050 Total Loss: 3.1051 Recon Loss: 3.0870 
[01/06 22:18:59 TiTok]: Data (t): 0.0011, 52.65/s/gpu Batch (t): 0.6077 LR: 0.000097 Step: 134100 Total Loss: 3.0838 Recon Loss: 3.0657 
[01/06 22:19:29 TiTok]: Data (t): 0.0011, 53.59/s/gpu Batch (t): 0.5971 LR: 0.000097 Step: 134150 Total Loss: 3.0927 Recon Loss: 3.0746 
[01/06 22:19:59 TiTok]: Data (t): 0.0011, 53.74/s/gpu Batch (t): 0.5955 LR: 0.000097 Step: 134200 Total Loss: 3.0591 Recon Loss: 3.0410 
[01/06 22:20:30 TiTok]: Data (t): 0.0006, 53.74/s/gpu Batch (t): 0.5955 LR: 0.000097 Step: 134250 Total Loss: 3.0631 Recon Loss: 3.0450 
[01/06 22:21:00 TiTok]: Data (t): 0.0011, 53.73/s/gpu Batch (t): 0.5956 LR: 0.000097 Step: 134300 Total Loss: 3.0888 Recon Loss: 3.0707 
[01/06 22:21:30 TiTok]: Data (t): 0.0012, 53.86/s/gpu Batch (t): 0.5941 LR: 0.000097 Step: 134350 Total Loss: 3.0930 Recon Loss: 3.0748 
[01/06 22:22:00 TiTok]: Data (t): 0.0011, 52.18/s/gpu Batch (t): 0.6133 LR: 0.000097 Step: 134400 Total Loss: 3.0565 Recon Loss: 3.0384 
[01/06 22:22:30 TiTok]: Data (t): 0.0012, 53.30/s/gpu Batch (t): 0.6004 LR: 0.000097 Step: 134450 Total Loss: 3.0727 Recon Loss: 3.0545 
[01/06 22:23:00 TiTok]: Data (t): 0.0009, 53.42/s/gpu Batch (t): 0.5990 LR: 0.000097 Step: 134500 Total Loss: 3.1139 Recon Loss: 3.0958 
[01/06 22:23:31 TiTok]: Data (t): 0.0011, 52.03/s/gpu Batch (t): 0.6150 LR: 0.000097 Step: 134550 Total Loss: 3.0831 Recon Loss: 3.0650 
[01/06 22:24:01 TiTok]: Data (t): 0.0011, 52.75/s/gpu Batch (t): 0.6067 LR: 0.000097 Step: 134600 Total Loss: 3.0740 Recon Loss: 3.0559 
[01/06 22:24:31 TiTok]: Data (t): 0.0011, 53.81/s/gpu Batch (t): 0.5947 LR: 0.000097 Step: 134650 Total Loss: 3.0463 Recon Loss: 3.0281 
[01/06 22:25:01 TiTok]: Data (t): 0.0011, 53.80/s/gpu Batch (t): 0.5948 LR: 0.000097 Step: 134700 Total Loss: 3.0643 Recon Loss: 3.0462 
[01/06 22:25:31 TiTok]: Data (t): 0.0011, 52.00/s/gpu Batch (t): 0.6154 LR: 0.000097 Step: 134750 Total Loss: 3.0583 Recon Loss: 3.0401 
[01/06 22:26:01 TiTok]: Data (t): 0.0011, 53.81/s/gpu Batch (t): 0.5947 LR: 0.000097 Step: 134800 Total Loss: 3.0762 Recon Loss: 3.0581 
[01/06 22:26:31 TiTok]: Data (t): 0.0013, 53.70/s/gpu Batch (t): 0.5959 LR: 0.000097 Step: 134850 Total Loss: 3.0839 Recon Loss: 3.0658 
[01/06 22:27:02 TiTok]: Data (t): 0.0007, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000097 Step: 134900 Total Loss: 3.0719 Recon Loss: 3.0538 
[01/06 22:27:32 TiTok]: Data (t): 0.0014, 52.46/s/gpu Batch (t): 0.6100 LR: 0.000097 Step: 134950 Total Loss: 3.0581 Recon Loss: 3.0400 
[01/06 22:28:02 TiTok]: Data (t): 0.0007, 41.83/s/gpu Batch (t): 0.7650 LR: 0.000097 Step: 135000 Total Loss: 3.0963 Recon Loss: 3.0782 
[01/06 22:28:04 TiTok]: Reconstructing images...
[01/06 22:28:38 TiTok]: Data (t): 0.0007, 51.03/s/gpu Batch (t): 0.6271 LR: 0.000097 Step: 135050 Total Loss: 3.1085 Recon Loss: 3.0904 
[01/06 22:29:08 TiTok]: Data (t): 0.0011, 53.74/s/gpu Batch (t): 0.5954 LR: 0.000097 Step: 135100 Total Loss: 3.1014 Recon Loss: 3.0833 
Epoch 26/199 started.
[01/06 22:29:40 TiTok]: Data (t): 0.0011, 52.47/s/gpu Batch (t): 0.6098 LR: 0.000097 Step: 135150 Total Loss: 3.0735 Recon Loss: 3.0554 
[01/06 22:30:11 TiTok]: Data (t): 0.0011, 53.72/s/gpu Batch (t): 0.5956 LR: 0.000096 Step: 135200 Total Loss: 3.0716 Recon Loss: 3.0534 
[01/06 22:30:41 TiTok]: Data (t): 0.0011, 53.99/s/gpu Batch (t): 0.5927 LR: 0.000096 Step: 135250 Total Loss: 3.0701 Recon Loss: 3.0519 
[01/06 22:31:11 TiTok]: Data (t): 0.0011, 52.24/s/gpu Batch (t): 0.6126 LR: 0.000096 Step: 135300 Total Loss: 3.1026 Recon Loss: 3.0845 
[01/06 22:31:41 TiTok]: Data (t): 0.0011, 51.83/s/gpu Batch (t): 0.6174 LR: 0.000096 Step: 135350 Total Loss: 3.0722 Recon Loss: 3.0541 
[01/06 22:32:11 TiTok]: Data (t): 0.0011, 53.86/s/gpu Batch (t): 0.5942 LR: 0.000096 Step: 135400 Total Loss: 3.0477 Recon Loss: 3.0296 
[01/06 22:32:41 TiTok]: Data (t): 0.0011, 52.37/s/gpu Batch (t): 0.6111 LR: 0.000096 Step: 135450 Total Loss: 3.0373 Recon Loss: 3.0191 
[01/06 22:33:11 TiTok]: Data (t): 0.0012, 53.95/s/gpu Batch (t): 0.5931 LR: 0.000096 Step: 135500 Total Loss: 3.0920 Recon Loss: 3.0739 
[01/06 22:33:41 TiTok]: Data (t): 0.0011, 53.85/s/gpu Batch (t): 0.5942 LR: 0.000096 Step: 135550 Total Loss: 3.0890 Recon Loss: 3.0709 
[01/06 22:34:12 TiTok]: Data (t): 0.0011, 52.40/s/gpu Batch (t): 0.6106 LR: 0.000096 Step: 135600 Total Loss: 3.0773 Recon Loss: 3.0591 
[01/06 22:34:42 TiTok]: Data (t): 0.0011, 53.72/s/gpu Batch (t): 0.5957 LR: 0.000096 Step: 135650 Total Loss: 3.0628 Recon Loss: 3.0447 
[01/06 22:35:12 TiTok]: Data (t): 0.0011, 53.84/s/gpu Batch (t): 0.5944 LR: 0.000096 Step: 135700 Total Loss: 3.0833 Recon Loss: 3.0651 
[01/06 22:35:42 TiTok]: Data (t): 0.0012, 53.84/s/gpu Batch (t): 0.5944 LR: 0.000096 Step: 135750 Total Loss: 3.0780 Recon Loss: 3.0598 
[01/06 22:36:12 TiTok]: Data (t): 0.0012, 53.73/s/gpu Batch (t): 0.5955 LR: 0.000096 Step: 135800 Total Loss: 3.0938 Recon Loss: 3.0756 
[01/06 22:36:42 TiTok]: Data (t): 0.0011, 53.86/s/gpu Batch (t): 0.5941 LR: 0.000096 Step: 135850 Total Loss: 3.0677 Recon Loss: 3.0496 
[01/06 22:37:12 TiTok]: Data (t): 0.0007, 53.60/s/gpu Batch (t): 0.5970 LR: 0.000096 Step: 135900 Total Loss: 3.0476 Recon Loss: 3.0295 
[01/06 22:37:42 TiTok]: Data (t): 0.0012, 48.79/s/gpu Batch (t): 0.6559 LR: 0.000096 Step: 135950 Total Loss: 3.0712 Recon Loss: 3.0531 
[01/06 22:38:12 TiTok]: Data (t): 0.0012, 44.25/s/gpu Batch (t): 0.7231 LR: 0.000096 Step: 136000 Total Loss: 3.0785 Recon Loss: 3.0603 
[01/06 22:38:42 TiTok]: Data (t): 0.0014, 53.81/s/gpu Batch (t): 0.5947 LR: 0.000096 Step: 136050 Total Loss: 3.0707 Recon Loss: 3.0526 
[01/06 22:39:12 TiTok]: Data (t): 0.0013, 54.55/s/gpu Batch (t): 0.5866 LR: 0.000096 Step: 136100 Total Loss: 3.0763 Recon Loss: 3.0582 
[01/06 22:39:42 TiTok]: Data (t): 0.0006, 53.32/s/gpu Batch (t): 0.6002 LR: 0.000096 Step: 136150 Total Loss: 3.0941 Recon Loss: 3.0759 
[01/06 22:40:13 TiTok]: Data (t): 0.0011, 53.28/s/gpu Batch (t): 0.6006 LR: 0.000096 Step: 136200 Total Loss: 3.0734 Recon Loss: 3.0553 
[01/06 22:40:43 TiTok]: Data (t): 0.0011, 53.83/s/gpu Batch (t): 0.5944 LR: 0.000096 Step: 136250 Total Loss: 3.0544 Recon Loss: 3.0363 
[01/06 22:41:13 TiTok]: Data (t): 0.0011, 53.56/s/gpu Batch (t): 0.5975 LR: 0.000096 Step: 136300 Total Loss: 3.0965 Recon Loss: 3.0784 
[01/06 22:41:43 TiTok]: Data (t): 0.0011, 53.73/s/gpu Batch (t): 0.5956 LR: 0.000096 Step: 136350 Total Loss: 3.0580 Recon Loss: 3.0398 
[01/06 22:42:13 TiTok]: Data (t): 0.0011, 53.30/s/gpu Batch (t): 0.6003 LR: 0.000096 Step: 136400 Total Loss: 3.1101 Recon Loss: 3.0920 
[01/06 22:42:43 TiTok]: Data (t): 0.0011, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000096 Step: 136450 Total Loss: 3.1213 Recon Loss: 3.1031 
[01/06 22:43:14 TiTok]: Data (t): 0.0011, 53.85/s/gpu Batch (t): 0.5942 LR: 0.000096 Step: 136500 Total Loss: 3.1150 Recon Loss: 3.0969 
[01/06 22:43:44 TiTok]: Data (t): 0.0011, 53.15/s/gpu Batch (t): 0.6021 LR: 0.000096 Step: 136550 Total Loss: 3.0747 Recon Loss: 3.0565 
[01/06 22:44:14 TiTok]: Data (t): 0.0011, 54.34/s/gpu Batch (t): 0.5888 LR: 0.000096 Step: 136600 Total Loss: 3.0696 Recon Loss: 3.0514 
[01/06 22:44:44 TiTok]: Data (t): 0.0011, 51.80/s/gpu Batch (t): 0.6177 LR: 0.000096 Step: 136650 Total Loss: 3.0785 Recon Loss: 3.0603 
[01/06 22:45:14 TiTok]: Data (t): 0.0014, 53.66/s/gpu Batch (t): 0.5963 LR: 0.000096 Step: 136700 Total Loss: 3.0618 Recon Loss: 3.0436 
[01/06 22:45:44 TiTok]: Data (t): 0.0011, 52.49/s/gpu Batch (t): 0.6096 LR: 0.000096 Step: 136750 Total Loss: 3.0881 Recon Loss: 3.0700 
[01/06 22:46:14 TiTok]: Data (t): 0.0013, 53.68/s/gpu Batch (t): 0.5962 LR: 0.000096 Step: 136800 Total Loss: 3.1185 Recon Loss: 3.1004 
[01/06 22:46:44 TiTok]: Data (t): 0.0011, 53.76/s/gpu Batch (t): 0.5952 LR: 0.000096 Step: 136850 Total Loss: 3.0974 Recon Loss: 3.0792 
[01/06 22:47:15 TiTok]: Data (t): 0.0014, 53.86/s/gpu Batch (t): 0.5942 LR: 0.000096 Step: 136900 Total Loss: 3.1035 Recon Loss: 3.0853 
[01/06 22:47:45 TiTok]: Data (t): 0.0011, 51.88/s/gpu Batch (t): 0.6168 LR: 0.000096 Step: 136950 Total Loss: 3.0565 Recon Loss: 3.0383 
[01/06 22:48:15 TiTok]: Data (t): 0.0012, 45.83/s/gpu Batch (t): 0.6982 LR: 0.000096 Step: 137000 Total Loss: 3.0711 Recon Loss: 3.0529 
[01/06 22:48:45 TiTok]: Data (t): 0.0011, 51.83/s/gpu Batch (t): 0.6174 LR: 0.000096 Step: 137050 Total Loss: 3.0945 Recon Loss: 3.0763 
[01/06 22:49:16 TiTok]: Data (t): 0.0011, 52.43/s/gpu Batch (t): 0.6103 LR: 0.000096 Step: 137100 Total Loss: 3.0780 Recon Loss: 3.0598 
[01/06 22:49:46 TiTok]: Data (t): 0.0013, 54.08/s/gpu Batch (t): 0.5917 LR: 0.000096 Step: 137150 Total Loss: 3.0625 Recon Loss: 3.0444 
[01/06 22:50:16 TiTok]: Data (t): 0.0012, 53.98/s/gpu Batch (t): 0.5929 LR: 0.000096 Step: 137200 Total Loss: 3.0557 Recon Loss: 3.0375 
[01/06 22:50:46 TiTok]: Data (t): 0.0011, 54.40/s/gpu Batch (t): 0.5883 LR: 0.000096 Step: 137250 Total Loss: 3.0691 Recon Loss: 3.0510 
[01/06 22:51:17 TiTok]: Data (t): 0.0006, 51.74/s/gpu Batch (t): 0.6185 LR: 0.000096 Step: 137300 Total Loss: 3.0588 Recon Loss: 3.0406 
[01/06 22:51:47 TiTok]: Data (t): 0.0012, 53.80/s/gpu Batch (t): 0.5948 LR: 0.000096 Step: 137350 Total Loss: 3.0444 Recon Loss: 3.0262 
[01/06 22:52:17 TiTok]: Data (t): 0.0011, 53.42/s/gpu Batch (t): 0.5990 LR: 0.000096 Step: 137400 Total Loss: 3.0803 Recon Loss: 3.0622 
[01/06 22:52:47 TiTok]: Data (t): 0.0006, 52.21/s/gpu Batch (t): 0.6129 LR: 0.000096 Step: 137450 Total Loss: 3.0797 Recon Loss: 3.0616 
[01/06 22:53:18 TiTok]: Data (t): 0.0011, 53.03/s/gpu Batch (t): 0.6035 LR: 0.000096 Step: 137500 Total Loss: 3.0735 Recon Loss: 3.0553 
[01/06 22:53:48 TiTok]: Data (t): 0.0012, 53.65/s/gpu Batch (t): 0.5965 LR: 0.000096 Step: 137550 Total Loss: 3.0576 Recon Loss: 3.0395 
[01/06 22:54:18 TiTok]: Data (t): 0.0011, 53.72/s/gpu Batch (t): 0.5956 LR: 0.000096 Step: 137600 Total Loss: 3.0807 Recon Loss: 3.0626 
[01/06 22:54:48 TiTok]: Data (t): 0.0011, 52.10/s/gpu Batch (t): 0.6142 LR: 0.000096 Step: 137650 Total Loss: 3.0964 Recon Loss: 3.0782 
[01/06 22:55:18 TiTok]: Data (t): 0.0011, 53.91/s/gpu Batch (t): 0.5936 LR: 0.000096 Step: 137700 Total Loss: 3.0621 Recon Loss: 3.0439 
[01/06 22:55:48 TiTok]: Data (t): 0.0006, 53.34/s/gpu Batch (t): 0.6000 LR: 0.000096 Step: 137750 Total Loss: 3.0749 Recon Loss: 3.0568 
[01/06 22:56:18 TiTok]: Data (t): 0.0006, 52.45/s/gpu Batch (t): 0.6102 LR: 0.000096 Step: 137800 Total Loss: 3.1159 Recon Loss: 3.0977 
[01/06 22:56:49 TiTok]: Data (t): 0.0013, 53.30/s/gpu Batch (t): 0.6004 LR: 0.000096 Step: 137850 Total Loss: 3.0652 Recon Loss: 3.0469 
[01/06 22:57:19 TiTok]: Data (t): 0.0007, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000096 Step: 137900 Total Loss: 3.1151 Recon Loss: 3.0970 
[01/06 22:57:49 TiTok]: Data (t): 0.0006, 53.17/s/gpu Batch (t): 0.6019 LR: 0.000096 Step: 137950 Total Loss: 3.0821 Recon Loss: 3.0639 
[01/06 22:58:19 TiTok]: Data (t): 0.0012, 45.57/s/gpu Batch (t): 0.7023 LR: 0.000096 Step: 138000 Total Loss: 3.0295 Recon Loss: 3.0114 
[01/06 22:58:49 TiTok]: Data (t): 0.0011, 53.99/s/gpu Batch (t): 0.5927 LR: 0.000096 Step: 138050 Total Loss: 3.0880 Recon Loss: 3.0698 
[01/06 22:59:20 TiTok]: Data (t): 0.0011, 54.43/s/gpu Batch (t): 0.5880 LR: 0.000096 Step: 138100 Total Loss: 3.0653 Recon Loss: 3.0471 
[01/06 22:59:50 TiTok]: Data (t): 0.0012, 53.64/s/gpu Batch (t): 0.5966 LR: 0.000096 Step: 138150 Total Loss: 3.0502 Recon Loss: 3.0320 
[01/06 23:00:20 TiTok]: Data (t): 0.0006, 53.34/s/gpu Batch (t): 0.6000 LR: 0.000096 Step: 138200 Total Loss: 3.0965 Recon Loss: 3.0783 
[01/06 23:00:50 TiTok]: Data (t): 0.0011, 49.24/s/gpu Batch (t): 0.6499 LR: 0.000096 Step: 138250 Total Loss: 3.1270 Recon Loss: 3.1089 
[01/06 23:01:20 TiTok]: Data (t): 0.0011, 53.68/s/gpu Batch (t): 0.5962 LR: 0.000096 Step: 138300 Total Loss: 3.0879 Recon Loss: 3.0697 
[01/06 23:01:51 TiTok]: Data (t): 0.0011, 49.68/s/gpu Batch (t): 0.6442 LR: 0.000096 Step: 138350 Total Loss: 3.0756 Recon Loss: 3.0574 
[01/06 23:02:21 TiTok]: Data (t): 0.0011, 51.36/s/gpu Batch (t): 0.6231 LR: 0.000096 Step: 138400 Total Loss: 3.0569 Recon Loss: 3.0387 
[01/06 23:02:51 TiTok]: Data (t): 0.0011, 53.04/s/gpu Batch (t): 0.6034 LR: 0.000096 Step: 138450 Total Loss: 3.0755 Recon Loss: 3.0573 
[01/06 23:03:21 TiTok]: Data (t): 0.0011, 51.22/s/gpu Batch (t): 0.6247 LR: 0.000096 Step: 138500 Total Loss: 3.0855 Recon Loss: 3.0673 
[01/06 23:03:51 TiTok]: Data (t): 0.0011, 52.89/s/gpu Batch (t): 0.6050 LR: 0.000096 Step: 138550 Total Loss: 3.0902 Recon Loss: 3.0720 
[01/06 23:04:22 TiTok]: Data (t): 0.0011, 54.36/s/gpu Batch (t): 0.5887 LR: 0.000096 Step: 138600 Total Loss: 3.0726 Recon Loss: 3.0544 
[01/06 23:04:52 TiTok]: Data (t): 0.0011, 53.57/s/gpu Batch (t): 0.5973 LR: 0.000096 Step: 138650 Total Loss: 3.0491 Recon Loss: 3.0309 
[01/06 23:05:22 TiTok]: Data (t): 0.0011, 52.86/s/gpu Batch (t): 0.6053 LR: 0.000096 Step: 138700 Total Loss: 3.0573 Recon Loss: 3.0391 
[01/06 23:05:52 TiTok]: Data (t): 0.0011, 53.44/s/gpu Batch (t): 0.5988 LR: 0.000096 Step: 138750 Total Loss: 3.0831 Recon Loss: 3.0650 
[01/06 23:06:22 TiTok]: Data (t): 0.0011, 50.51/s/gpu Batch (t): 0.6335 LR: 0.000096 Step: 138800 Total Loss: 3.0484 Recon Loss: 3.0303 
[01/06 23:06:52 TiTok]: Data (t): 0.0011, 53.82/s/gpu Batch (t): 0.5945 LR: 0.000096 Step: 138850 Total Loss: 3.0690 Recon Loss: 3.0508 
[01/06 23:07:23 TiTok]: Data (t): 0.0011, 52.76/s/gpu Batch (t): 0.6066 LR: 0.000096 Step: 138900 Total Loss: 3.0541 Recon Loss: 3.0359 
[01/06 23:07:53 TiTok]: Data (t): 0.0011, 54.04/s/gpu Batch (t): 0.5921 LR: 0.000096 Step: 138950 Total Loss: 3.0524 Recon Loss: 3.0341 
[01/06 23:08:23 TiTok]: Data (t): 0.0011, 44.46/s/gpu Batch (t): 0.7198 LR: 0.000096 Step: 139000 Total Loss: 3.0624 Recon Loss: 3.0443 
[01/06 23:08:53 TiTok]: Data (t): 0.0011, 53.44/s/gpu Batch (t): 0.5988 LR: 0.000096 Step: 139050 Total Loss: 3.0553 Recon Loss: 3.0371 
[01/06 23:09:23 TiTok]: Data (t): 0.0014, 52.25/s/gpu Batch (t): 0.6124 LR: 0.000096 Step: 139100 Total Loss: 3.0645 Recon Loss: 3.0463 
[01/06 23:09:54 TiTok]: Data (t): 0.0012, 51.64/s/gpu Batch (t): 0.6197 LR: 0.000096 Step: 139150 Total Loss: 3.0911 Recon Loss: 3.0730 
[01/06 23:10:24 TiTok]: Data (t): 0.0011, 51.97/s/gpu Batch (t): 0.6157 LR: 0.000096 Step: 139200 Total Loss: 3.0487 Recon Loss: 3.0305 
[01/06 23:10:54 TiTok]: Data (t): 0.0012, 53.54/s/gpu Batch (t): 0.5977 LR: 0.000096 Step: 139250 Total Loss: 3.0795 Recon Loss: 3.0614 
[01/06 23:11:25 TiTok]: Data (t): 0.0011, 53.70/s/gpu Batch (t): 0.5959 LR: 0.000096 Step: 139300 Total Loss: 3.0622 Recon Loss: 3.0440 
[01/06 23:11:55 TiTok]: Data (t): 0.0011, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000096 Step: 139350 Total Loss: 3.0318 Recon Loss: 3.0136 
[01/06 23:12:25 TiTok]: Data (t): 0.0006, 48.98/s/gpu Batch (t): 0.6534 LR: 0.000096 Step: 139400 Total Loss: 3.0800 Recon Loss: 3.0618 
[01/06 23:12:56 TiTok]: Data (t): 0.0015, 53.27/s/gpu Batch (t): 0.6007 LR: 0.000096 Step: 139450 Total Loss: 3.0886 Recon Loss: 3.0703 
[01/06 23:13:26 TiTok]: Data (t): 0.0011, 53.75/s/gpu Batch (t): 0.5954 LR: 0.000096 Step: 139500 Total Loss: 3.0257 Recon Loss: 3.0076 
[01/06 23:13:56 TiTok]: Data (t): 0.0011, 48.08/s/gpu Batch (t): 0.6655 LR: 0.000096 Step: 139550 Total Loss: 3.0504 Recon Loss: 3.0321 
[01/06 23:14:26 TiTok]: Data (t): 0.0014, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000096 Step: 139600 Total Loss: 3.0969 Recon Loss: 3.0787 
[01/06 23:14:56 TiTok]: Data (t): 0.0006, 54.39/s/gpu Batch (t): 0.5883 LR: 0.000096 Step: 139650 Total Loss: 3.0973 Recon Loss: 3.0791 
[01/06 23:15:27 TiTok]: Data (t): 0.0011, 52.10/s/gpu Batch (t): 0.6142 LR: 0.000096 Step: 139700 Total Loss: 3.0761 Recon Loss: 3.0578 
[01/06 23:15:57 TiTok]: Data (t): 0.0011, 53.78/s/gpu Batch (t): 0.5950 LR: 0.000096 Step: 139750 Total Loss: 3.0338 Recon Loss: 3.0156 
[01/06 23:16:27 TiTok]: Data (t): 0.0011, 52.15/s/gpu Batch (t): 0.6136 LR: 0.000096 Step: 139800 Total Loss: 3.0651 Recon Loss: 3.0469 
[01/06 23:16:57 TiTok]: Data (t): 0.0011, 53.17/s/gpu Batch (t): 0.6018 LR: 0.000096 Step: 139850 Total Loss: 3.0379 Recon Loss: 3.0197 
[01/06 23:17:27 TiTok]: Data (t): 0.0011, 53.68/s/gpu Batch (t): 0.5962 LR: 0.000096 Step: 139900 Total Loss: 3.0440 Recon Loss: 3.0259 
[01/06 23:17:58 TiTok]: Data (t): 0.0011, 53.65/s/gpu Batch (t): 0.5965 LR: 0.000096 Step: 139950 Total Loss: 3.0606 Recon Loss: 3.0425 
[01/06 23:18:28 TiTok]: Data (t): 0.0011, 46.41/s/gpu Batch (t): 0.6895 LR: 0.000096 Step: 140000 Total Loss: 3.0864 Recon Loss: 3.0681 
[01/06 23:18:30 TiTok]: Reconstructing images...
[01/06 23:19:06 TiTok]: Data (t): 0.0011, 52.83/s/gpu Batch (t): 0.6057 LR: 0.000096 Step: 140050 Total Loss: 3.1079 Recon Loss: 3.0897 
[01/06 23:19:36 TiTok]: Data (t): 0.0011, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000096 Step: 140100 Total Loss: 3.0745 Recon Loss: 3.0563 
Epoch 27/199 started.
[01/06 23:20:08 TiTok]: Data (t): 0.0012, 52.99/s/gpu Batch (t): 0.6039 LR: 0.000096 Step: 140150 Total Loss: 3.0799 Recon Loss: 3.0617 
[01/06 23:20:38 TiTok]: Data (t): 0.0012, 51.68/s/gpu Batch (t): 0.6191 LR: 0.000096 Step: 140200 Total Loss: 3.0347 Recon Loss: 3.0166 
[01/06 23:21:08 TiTok]: Data (t): 0.0011, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000096 Step: 140250 Total Loss: 3.0902 Recon Loss: 3.0720 
[01/06 23:21:39 TiTok]: Data (t): 0.0006, 52.61/s/gpu Batch (t): 0.6083 LR: 0.000096 Step: 140300 Total Loss: 3.0876 Recon Loss: 3.0695 
[01/06 23:22:09 TiTok]: Data (t): 0.0012, 51.03/s/gpu Batch (t): 0.6271 LR: 0.000096 Step: 140350 Total Loss: 3.1011 Recon Loss: 3.0829 
[01/06 23:22:39 TiTok]: Data (t): 0.0011, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000096 Step: 140400 Total Loss: 3.0481 Recon Loss: 3.0300 
[01/06 23:23:09 TiTok]: Data (t): 0.0011, 54.09/s/gpu Batch (t): 0.5916 LR: 0.000096 Step: 140450 Total Loss: 3.0938 Recon Loss: 3.0756 
[01/06 23:23:39 TiTok]: Data (t): 0.0012, 53.77/s/gpu Batch (t): 0.5951 LR: 0.000096 Step: 140500 Total Loss: 3.0458 Recon Loss: 3.0276 
[01/06 23:24:09 TiTok]: Data (t): 0.0011, 53.54/s/gpu Batch (t): 0.5977 LR: 0.000096 Step: 140550 Total Loss: 3.0794 Recon Loss: 3.0612 
[01/06 23:24:39 TiTok]: Data (t): 0.0011, 51.64/s/gpu Batch (t): 0.6196 LR: 0.000096 Step: 140600 Total Loss: 3.0591 Recon Loss: 3.0410 
[01/06 23:25:09 TiTok]: Data (t): 0.0012, 52.11/s/gpu Batch (t): 0.6141 LR: 0.000096 Step: 140650 Total Loss: 3.0623 Recon Loss: 3.0441 
[01/06 23:25:39 TiTok]: Data (t): 0.0011, 53.79/s/gpu Batch (t): 0.5950 LR: 0.000096 Step: 140700 Total Loss: 3.0549 Recon Loss: 3.0367 
[01/06 23:26:09 TiTok]: Data (t): 0.0011, 53.03/s/gpu Batch (t): 0.6034 LR: 0.000096 Step: 140750 Total Loss: 3.0624 Recon Loss: 3.0443 
[01/06 23:26:39 TiTok]: Data (t): 0.0013, 53.85/s/gpu Batch (t): 0.5943 LR: 0.000096 Step: 140800 Total Loss: 3.0845 Recon Loss: 3.0663 
[01/06 23:27:09 TiTok]: Data (t): 0.0219, 52.57/s/gpu Batch (t): 0.6087 LR: 0.000096 Step: 140850 Total Loss: 3.0763 Recon Loss: 3.0580 
[01/06 23:27:39 TiTok]: Data (t): 0.0011, 53.67/s/gpu Batch (t): 0.5962 LR: 0.000096 Step: 140900 Total Loss: 3.0375 Recon Loss: 3.0193 
[01/06 23:28:10 TiTok]: Data (t): 0.0011, 53.96/s/gpu Batch (t): 0.5930 LR: 0.000096 Step: 140950 Total Loss: 3.0602 Recon Loss: 3.0420 
[01/06 23:28:40 TiTok]: Data (t): 0.0012, 45.35/s/gpu Batch (t): 0.7056 LR: 0.000096 Step: 141000 Total Loss: 3.0648 Recon Loss: 3.0466 
[01/06 23:29:10 TiTok]: Data (t): 0.0011, 53.69/s/gpu Batch (t): 0.5960 LR: 0.000096 Step: 141050 Total Loss: 3.0131 Recon Loss: 2.9949 
[01/06 23:29:40 TiTok]: Data (t): 0.0011, 53.76/s/gpu Batch (t): 0.5952 LR: 0.000096 Step: 141100 Total Loss: 3.0842 Recon Loss: 3.0661 
[01/06 23:30:10 TiTok]: Data (t): 0.0011, 53.85/s/gpu Batch (t): 0.5942 LR: 0.000096 Step: 141150 Total Loss: 3.0787 Recon Loss: 3.0606 
[01/06 23:30:40 TiTok]: Data (t): 0.0011, 52.15/s/gpu Batch (t): 0.6136 LR: 0.000096 Step: 141200 Total Loss: 3.0949 Recon Loss: 3.0766 
[01/06 23:31:10 TiTok]: Data (t): 0.0012, 53.67/s/gpu Batch (t): 0.5962 LR: 0.000096 Step: 141250 Total Loss: 3.0791 Recon Loss: 3.0608 
[01/06 23:31:40 TiTok]: Data (t): 0.0011, 49.51/s/gpu Batch (t): 0.6464 LR: 0.000096 Step: 141300 Total Loss: 3.0419 Recon Loss: 3.0236 
[01/06 23:32:10 TiTok]: Data (t): 0.0014, 51.18/s/gpu Batch (t): 0.6253 LR: 0.000096 Step: 141350 Total Loss: 3.0304 Recon Loss: 3.0122 
[01/06 23:32:40 TiTok]: Data (t): 0.0011, 53.66/s/gpu Batch (t): 0.5963 LR: 0.000096 Step: 141400 Total Loss: 3.0854 Recon Loss: 3.0671 
[01/06 23:33:10 TiTok]: Data (t): 0.0012, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000096 Step: 141450 Total Loss: 3.0824 Recon Loss: 3.0642 
[01/06 23:33:40 TiTok]: Data (t): 0.0011, 53.73/s/gpu Batch (t): 0.5956 LR: 0.000096 Step: 141500 Total Loss: 3.0668 Recon Loss: 3.0485 
[01/06 23:34:11 TiTok]: Data (t): 0.0012, 52.01/s/gpu Batch (t): 0.6153 LR: 0.000096 Step: 141550 Total Loss: 3.0081 Recon Loss: 2.9899 
[01/06 23:34:40 TiTok]: Data (t): 0.0011, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000096 Step: 141600 Total Loss: 3.0349 Recon Loss: 3.0167 
[01/06 23:35:11 TiTok]: Data (t): 0.0011, 51.43/s/gpu Batch (t): 0.6222 LR: 0.000096 Step: 141650 Total Loss: 3.0606 Recon Loss: 3.0424 
[01/06 23:35:41 TiTok]: Data (t): 0.0007, 53.38/s/gpu Batch (t): 0.5995 LR: 0.000096 Step: 141700 Total Loss: 3.0414 Recon Loss: 3.0232 
[01/06 23:36:11 TiTok]: Data (t): 0.0012, 53.77/s/gpu Batch (t): 0.5951 LR: 0.000096 Step: 141750 Total Loss: 3.0618 Recon Loss: 3.0435 
[01/06 23:36:41 TiTok]: Data (t): 0.0011, 53.85/s/gpu Batch (t): 0.5942 LR: 0.000096 Step: 141800 Total Loss: 3.0980 Recon Loss: 3.0798 
[01/06 23:37:11 TiTok]: Data (t): 0.0007, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000096 Step: 141850 Total Loss: 3.0918 Recon Loss: 3.0736 
[01/06 23:37:41 TiTok]: Data (t): 0.0011, 53.15/s/gpu Batch (t): 0.6020 LR: 0.000096 Step: 141900 Total Loss: 3.0534 Recon Loss: 3.0352 
[01/06 23:38:11 TiTok]: Data (t): 0.0011, 51.53/s/gpu Batch (t): 0.6210 LR: 0.000096 Step: 141950 Total Loss: 3.0681 Recon Loss: 3.0499 
[01/06 23:38:41 TiTok]: Data (t): 0.0011, 47.59/s/gpu Batch (t): 0.6724 LR: 0.000096 Step: 142000 Total Loss: 3.0494 Recon Loss: 3.0312 
[01/06 23:39:11 TiTok]: Data (t): 0.0013, 53.97/s/gpu Batch (t): 0.5929 LR: 0.000096 Step: 142050 Total Loss: 3.0373 Recon Loss: 3.0190 
[01/06 23:39:41 TiTok]: Data (t): 0.0011, 53.93/s/gpu Batch (t): 0.5933 LR: 0.000096 Step: 142100 Total Loss: 3.0479 Recon Loss: 3.0297 
[01/06 23:40:11 TiTok]: Data (t): 0.0011, 53.85/s/gpu Batch (t): 0.5943 LR: 0.000096 Step: 142150 Total Loss: 3.0336 Recon Loss: 3.0154 
[01/06 23:40:41 TiTok]: Data (t): 0.0006, 52.79/s/gpu Batch (t): 0.6061 LR: 0.000096 Step: 142200 Total Loss: 3.0556 Recon Loss: 3.0373 
[01/06 23:41:11 TiTok]: Data (t): 0.0011, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000096 Step: 142250 Total Loss: 3.0841 Recon Loss: 3.0660 
[01/06 23:41:41 TiTok]: Data (t): 0.0011, 53.70/s/gpu Batch (t): 0.5959 LR: 0.000096 Step: 142300 Total Loss: 3.0940 Recon Loss: 3.0757 
[01/06 23:42:11 TiTok]: Data (t): 0.0011, 53.81/s/gpu Batch (t): 0.5947 LR: 0.000096 Step: 142350 Total Loss: 3.0741 Recon Loss: 3.0559 
[01/06 23:42:41 TiTok]: Data (t): 0.0011, 53.79/s/gpu Batch (t): 0.5949 LR: 0.000096 Step: 142400 Total Loss: 3.0505 Recon Loss: 3.0323 
[01/06 23:43:11 TiTok]: Data (t): 0.0011, 52.03/s/gpu Batch (t): 0.6151 LR: 0.000096 Step: 142450 Total Loss: 3.0569 Recon Loss: 3.0387 
[01/06 23:43:41 TiTok]: Data (t): 0.0011, 53.80/s/gpu Batch (t): 0.5948 LR: 0.000096 Step: 142500 Total Loss: 3.0905 Recon Loss: 3.0723 
[01/06 23:44:11 TiTok]: Data (t): 0.0007, 51.54/s/gpu Batch (t): 0.6208 LR: 0.000096 Step: 142550 Total Loss: 3.0929 Recon Loss: 3.0747 
[01/06 23:44:41 TiTok]: Data (t): 0.0012, 53.69/s/gpu Batch (t): 0.5960 LR: 0.000096 Step: 142600 Total Loss: 3.0260 Recon Loss: 3.0078 
[01/06 23:45:11 TiTok]: Data (t): 0.0011, 53.77/s/gpu Batch (t): 0.5952 LR: 0.000096 Step: 142650 Total Loss: 3.1014 Recon Loss: 3.0832 
[01/06 23:45:41 TiTok]: Data (t): 0.0012, 53.71/s/gpu Batch (t): 0.5958 LR: 0.000096 Step: 142700 Total Loss: 3.0474 Recon Loss: 3.0292 
[01/06 23:46:11 TiTok]: Data (t): 0.0011, 53.93/s/gpu Batch (t): 0.5934 LR: 0.000096 Step: 142750 Total Loss: 3.0629 Recon Loss: 3.0448 
[01/06 23:46:42 TiTok]: Data (t): 0.0006, 51.83/s/gpu Batch (t): 0.6174 LR: 0.000096 Step: 142800 Total Loss: 3.0316 Recon Loss: 3.0134 
[01/06 23:47:12 TiTok]: Data (t): 0.0011, 53.80/s/gpu Batch (t): 0.5948 LR: 0.000096 Step: 142850 Total Loss: 3.0781 Recon Loss: 3.0598 
[01/06 23:47:42 TiTok]: Data (t): 0.0011, 51.93/s/gpu Batch (t): 0.6162 LR: 0.000096 Step: 142900 Total Loss: 3.0688 Recon Loss: 3.0506 
[01/06 23:48:12 TiTok]: Data (t): 0.0011, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000096 Step: 142950 Total Loss: 3.1051 Recon Loss: 3.0869 
[01/06 23:48:42 TiTok]: Data (t): 0.0011, 46.28/s/gpu Batch (t): 0.6915 LR: 0.000096 Step: 143000 Total Loss: 3.0495 Recon Loss: 3.0313 
[01/06 23:49:12 TiTok]: Data (t): 0.0011, 53.74/s/gpu Batch (t): 0.5955 LR: 0.000096 Step: 143050 Total Loss: 3.0876 Recon Loss: 3.0693 
[01/06 23:49:42 TiTok]: Data (t): 0.0011, 53.85/s/gpu Batch (t): 0.5943 LR: 0.000096 Step: 143100 Total Loss: 3.0770 Recon Loss: 3.0588 
[01/06 23:50:12 TiTok]: Data (t): 0.0012, 51.62/s/gpu Batch (t): 0.6200 LR: 0.000096 Step: 143150 Total Loss: 3.0219 Recon Loss: 3.0037 
[01/06 23:50:42 TiTok]: Data (t): 0.0012, 51.40/s/gpu Batch (t): 0.6225 LR: 0.000096 Step: 143200 Total Loss: 3.0263 Recon Loss: 3.0081 
[01/06 23:51:12 TiTok]: Data (t): 0.0011, 53.81/s/gpu Batch (t): 0.5947 LR: 0.000096 Step: 143250 Total Loss: 3.0705 Recon Loss: 3.0523 
[01/06 23:51:42 TiTok]: Data (t): 0.0006, 53.98/s/gpu Batch (t): 0.5928 LR: 0.000096 Step: 143300 Total Loss: 3.0750 Recon Loss: 3.0568 
[01/06 23:52:12 TiTok]: Data (t): 0.0012, 53.70/s/gpu Batch (t): 0.5959 LR: 0.000096 Step: 143350 Total Loss: 3.0764 Recon Loss: 3.0582 
[01/06 23:52:43 TiTok]: Data (t): 0.0011, 54.44/s/gpu Batch (t): 0.5878 LR: 0.000096 Step: 143400 Total Loss: 3.0772 Recon Loss: 3.0589 
[01/06 23:53:12 TiTok]: Data (t): 0.0011, 53.71/s/gpu Batch (t): 0.5958 LR: 0.000096 Step: 143450 Total Loss: 3.0300 Recon Loss: 3.0118 
[01/06 23:53:42 TiTok]: Data (t): 0.0012, 53.68/s/gpu Batch (t): 0.5962 LR: 0.000096 Step: 143500 Total Loss: 3.0865 Recon Loss: 3.0683 
[01/06 23:54:12 TiTok]: Data (t): 0.0014, 53.85/s/gpu Batch (t): 0.5943 LR: 0.000096 Step: 143550 Total Loss: 3.1342 Recon Loss: 3.1159 
[01/06 23:54:42 TiTok]: Data (t): 0.0012, 53.65/s/gpu Batch (t): 0.5965 LR: 0.000096 Step: 143600 Total Loss: 3.0416 Recon Loss: 3.0235 
[01/06 23:55:13 TiTok]: Data (t): 0.0011, 52.41/s/gpu Batch (t): 0.6106 LR: 0.000096 Step: 143650 Total Loss: 3.0791 Recon Loss: 3.0609 
[01/06 23:55:43 TiTok]: Data (t): 0.0013, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000096 Step: 143700 Total Loss: 3.0728 Recon Loss: 3.0546 
[01/06 23:56:13 TiTok]: Data (t): 0.0011, 53.61/s/gpu Batch (t): 0.5969 LR: 0.000096 Step: 143750 Total Loss: 3.0550 Recon Loss: 3.0368 
[01/06 23:56:43 TiTok]: Data (t): 0.0011, 52.47/s/gpu Batch (t): 0.6099 LR: 0.000096 Step: 143800 Total Loss: 3.0700 Recon Loss: 3.0518 
[01/06 23:57:13 TiTok]: Data (t): 0.0011, 53.79/s/gpu Batch (t): 0.5949 LR: 0.000096 Step: 143850 Total Loss: 3.0391 Recon Loss: 3.0209 
[01/06 23:57:43 TiTok]: Data (t): 0.0012, 53.83/s/gpu Batch (t): 0.5944 LR: 0.000096 Step: 143900 Total Loss: 3.0198 Recon Loss: 3.0016 
[01/06 23:58:13 TiTok]: Data (t): 0.0011, 53.84/s/gpu Batch (t): 0.5943 LR: 0.000096 Step: 143950 Total Loss: 3.0703 Recon Loss: 3.0520 
[01/06 23:58:43 TiTok]: Data (t): 0.0012, 46.37/s/gpu Batch (t): 0.6901 LR: 0.000096 Step: 144000 Total Loss: 3.0654 Recon Loss: 3.0471 
[01/06 23:59:13 TiTok]: Data (t): 0.0011, 53.70/s/gpu Batch (t): 0.5959 LR: 0.000096 Step: 144050 Total Loss: 3.0796 Recon Loss: 3.0614 
[01/06 23:59:43 TiTok]: Data (t): 0.0011, 53.34/s/gpu Batch (t): 0.5999 LR: 0.000096 Step: 144100 Total Loss: 3.0518 Recon Loss: 3.0335 
[01/07 00:00:13 TiTok]: Data (t): 0.0011, 53.22/s/gpu Batch (t): 0.6012 LR: 0.000096 Step: 144150 Total Loss: 3.0513 Recon Loss: 3.0330 
[01/07 00:00:43 TiTok]: Data (t): 0.0011, 53.29/s/gpu Batch (t): 0.6004 LR: 0.000096 Step: 144200 Total Loss: 3.0559 Recon Loss: 3.0377 
[01/07 00:01:13 TiTok]: Data (t): 0.0011, 53.56/s/gpu Batch (t): 0.5974 LR: 0.000096 Step: 144250 Total Loss: 3.0302 Recon Loss: 3.0120 
[01/07 00:01:43 TiTok]: Data (t): 0.0011, 52.14/s/gpu Batch (t): 0.6138 LR: 0.000096 Step: 144300 Total Loss: 3.0310 Recon Loss: 3.0128 
[01/07 00:02:13 TiTok]: Data (t): 0.0013, 53.92/s/gpu Batch (t): 0.5935 LR: 0.000096 Step: 144350 Total Loss: 3.0633 Recon Loss: 3.0451 
[01/07 00:02:43 TiTok]: Data (t): 0.0011, 53.42/s/gpu Batch (t): 0.5991 LR: 0.000096 Step: 144400 Total Loss: 3.0514 Recon Loss: 3.0331 
[01/07 00:03:13 TiTok]: Data (t): 0.0266, 53.37/s/gpu Batch (t): 0.5996 LR: 0.000096 Step: 144450 Total Loss: 3.0908 Recon Loss: 3.0726 
[01/07 00:03:43 TiTok]: Data (t): 0.0011, 51.74/s/gpu Batch (t): 0.6184 LR: 0.000096 Step: 144500 Total Loss: 3.0829 Recon Loss: 3.0647 
[01/07 00:04:13 TiTok]: Data (t): 0.0011, 53.86/s/gpu Batch (t): 0.5941 LR: 0.000096 Step: 144550 Total Loss: 3.0427 Recon Loss: 3.0245 
[01/07 00:04:43 TiTok]: Data (t): 0.0011, 53.76/s/gpu Batch (t): 0.5952 LR: 0.000096 Step: 144600 Total Loss: 3.0726 Recon Loss: 3.0544 
[01/07 00:05:13 TiTok]: Data (t): 0.0006, 53.27/s/gpu Batch (t): 0.6007 LR: 0.000096 Step: 144650 Total Loss: 3.0608 Recon Loss: 3.0426 
[01/07 00:05:44 TiTok]: Data (t): 0.0012, 53.89/s/gpu Batch (t): 0.5938 LR: 0.000096 Step: 144700 Total Loss: 3.0603 Recon Loss: 3.0420 
[01/07 00:06:14 TiTok]: Data (t): 0.0006, 52.12/s/gpu Batch (t): 0.6139 LR: 0.000096 Step: 144750 Total Loss: 3.0627 Recon Loss: 3.0445 
[01/07 00:06:44 TiTok]: Data (t): 0.0011, 53.93/s/gpu Batch (t): 0.5934 LR: 0.000096 Step: 144800 Total Loss: 3.0032 Recon Loss: 2.9849 
[01/07 00:07:14 TiTok]: Data (t): 0.0012, 53.14/s/gpu Batch (t): 0.6022 LR: 0.000096 Step: 144850 Total Loss: 3.0303 Recon Loss: 3.0121 
[01/07 00:07:44 TiTok]: Data (t): 0.0011, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000096 Step: 144900 Total Loss: 3.0770 Recon Loss: 3.0588 
[01/07 00:08:14 TiTok]: Data (t): 0.0014, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000096 Step: 144950 Total Loss: 3.0499 Recon Loss: 3.0317 
[01/07 00:08:44 TiTok]: Data (t): 0.0011, 46.64/s/gpu Batch (t): 0.6861 LR: 0.000096 Step: 145000 Total Loss: 3.0434 Recon Loss: 3.0252 
[01/07 00:08:46 TiTok]: Reconstructing images...
[01/07 00:09:22 TiTok]: Data (t): 0.0012, 53.73/s/gpu Batch (t): 0.5956 LR: 0.000096 Step: 145050 Total Loss: 3.0666 Recon Loss: 3.0484 
[01/07 00:09:52 TiTok]: Data (t): 0.0011, 53.92/s/gpu Batch (t): 0.5935 LR: 0.000096 Step: 145100 Total Loss: 3.0425 Recon Loss: 3.0243 
Epoch 28/199 started.
[01/07 00:10:24 TiTok]: Data (t): 0.0011, 53.54/s/gpu Batch (t): 0.5977 LR: 0.000096 Step: 145150 Total Loss: 3.0805 Recon Loss: 3.0622 
[01/07 00:10:54 TiTok]: Data (t): 0.0013, 52.91/s/gpu Batch (t): 0.6048 LR: 0.000096 Step: 145200 Total Loss: 3.0661 Recon Loss: 3.0478 
[01/07 00:11:24 TiTok]: Data (t): 0.0007, 52.47/s/gpu Batch (t): 0.6099 LR: 0.000096 Step: 145250 Total Loss: 3.0772 Recon Loss: 3.0590 
[01/07 00:11:54 TiTok]: Data (t): 0.0012, 53.91/s/gpu Batch (t): 0.5935 LR: 0.000096 Step: 145300 Total Loss: 3.0370 Recon Loss: 3.0188 
[01/07 00:12:24 TiTok]: Data (t): 0.0012, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000096 Step: 145350 Total Loss: 3.0321 Recon Loss: 3.0139 
[01/07 00:12:54 TiTok]: Data (t): 0.0007, 48.48/s/gpu Batch (t): 0.6601 LR: 0.000096 Step: 145400 Total Loss: 3.0397 Recon Loss: 3.0214 
[01/07 00:13:25 TiTok]: Data (t): 0.0013, 52.07/s/gpu Batch (t): 0.6146 LR: 0.000096 Step: 145450 Total Loss: 3.0553 Recon Loss: 3.0371 
[01/07 00:13:55 TiTok]: Data (t): 0.0011, 53.85/s/gpu Batch (t): 0.5943 LR: 0.000096 Step: 145500 Total Loss: 3.0674 Recon Loss: 3.0491 
[01/07 00:14:25 TiTok]: Data (t): 0.0011, 53.40/s/gpu Batch (t): 0.5993 LR: 0.000096 Step: 145550 Total Loss: 3.0836 Recon Loss: 3.0654 
[01/07 00:14:55 TiTok]: Data (t): 0.0013, 52.87/s/gpu Batch (t): 0.6053 LR: 0.000096 Step: 145600 Total Loss: 3.0246 Recon Loss: 3.0065 
[01/07 00:15:25 TiTok]: Data (t): 0.0011, 53.86/s/gpu Batch (t): 0.5941 LR: 0.000096 Step: 145650 Total Loss: 3.0786 Recon Loss: 3.0604 
[01/07 00:15:55 TiTok]: Data (t): 0.0011, 53.92/s/gpu Batch (t): 0.5935 LR: 0.000096 Step: 145700 Total Loss: 3.0248 Recon Loss: 3.0066 
[01/07 00:16:25 TiTok]: Data (t): 0.0012, 53.75/s/gpu Batch (t): 0.5953 LR: 0.000096 Step: 145750 Total Loss: 3.0339 Recon Loss: 3.0156 
[01/07 00:16:55 TiTok]: Data (t): 0.0011, 53.73/s/gpu Batch (t): 0.5956 LR: 0.000096 Step: 145800 Total Loss: 3.0092 Recon Loss: 2.9910 
[01/07 00:17:25 TiTok]: Data (t): 0.0006, 52.11/s/gpu Batch (t): 0.6141 LR: 0.000096 Step: 145850 Total Loss: 3.0491 Recon Loss: 3.0309 
[01/07 00:17:56 TiTok]: Data (t): 0.0011, 53.90/s/gpu Batch (t): 0.5937 LR: 0.000096 Step: 145900 Total Loss: 3.0568 Recon Loss: 3.0386 
[01/07 00:18:26 TiTok]: Data (t): 0.0011, 54.22/s/gpu Batch (t): 0.5902 LR: 0.000096 Step: 145950 Total Loss: 3.0685 Recon Loss: 3.0503 
[01/07 00:18:56 TiTok]: Data (t): 0.0011, 46.62/s/gpu Batch (t): 0.6865 LR: 0.000096 Step: 146000 Total Loss: 3.0619 Recon Loss: 3.0437 
[01/07 00:19:26 TiTok]: Data (t): 0.0011, 53.75/s/gpu Batch (t): 0.5954 LR: 0.000096 Step: 146050 Total Loss: 3.0733 Recon Loss: 3.0550 
[01/07 00:19:56 TiTok]: Data (t): 0.0011, 53.66/s/gpu Batch (t): 0.5963 LR: 0.000096 Step: 146100 Total Loss: 3.0368 Recon Loss: 3.0186 
[01/07 00:20:26 TiTok]: Data (t): 0.0006, 53.25/s/gpu Batch (t): 0.6010 LR: 0.000096 Step: 146150 Total Loss: 3.0501 Recon Loss: 3.0319 
[01/07 00:20:56 TiTok]: Data (t): 0.0012, 54.32/s/gpu Batch (t): 0.5891 LR: 0.000096 Step: 146200 Total Loss: 3.0425 Recon Loss: 3.0243 
[01/07 00:21:27 TiTok]: Data (t): 0.0011, 53.82/s/gpu Batch (t): 0.5945 LR: 0.000096 Step: 146250 Total Loss: 3.0241 Recon Loss: 3.0059 
[01/07 00:21:57 TiTok]: Data (t): 0.0011, 53.93/s/gpu Batch (t): 0.5933 LR: 0.000096 Step: 146300 Total Loss: 3.0332 Recon Loss: 3.0149 
[01/07 00:22:27 TiTok]: Data (t): 0.0012, 53.67/s/gpu Batch (t): 0.5962 LR: 0.000096 Step: 146350 Total Loss: 3.0304 Recon Loss: 3.0122 
[01/07 00:22:57 TiTok]: Data (t): 0.0006, 52.99/s/gpu Batch (t): 0.6039 LR: 0.000096 Step: 146400 Total Loss: 3.0597 Recon Loss: 3.0415 
[01/07 00:23:28 TiTok]: Data (t): 0.0011, 52.90/s/gpu Batch (t): 0.6050 LR: 0.000096 Step: 146450 Total Loss: 3.0530 Recon Loss: 3.0348 
[01/07 00:23:58 TiTok]: Data (t): 0.0011, 51.42/s/gpu Batch (t): 0.6223 LR: 0.000096 Step: 146500 Total Loss: 3.0681 Recon Loss: 3.0499 
[01/07 00:24:28 TiTok]: Data (t): 0.0012, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000096 Step: 146550 Total Loss: 3.1116 Recon Loss: 3.0934 
[01/07 00:24:58 TiTok]: Data (t): 0.0011, 52.82/s/gpu Batch (t): 0.6059 LR: 0.000096 Step: 146600 Total Loss: 3.0281 Recon Loss: 3.0098 
[01/07 00:25:29 TiTok]: Data (t): 0.0006, 53.79/s/gpu Batch (t): 0.5949 LR: 0.000096 Step: 146650 Total Loss: 3.0536 Recon Loss: 3.0354 
[01/07 00:25:59 TiTok]: Data (t): 0.0007, 53.72/s/gpu Batch (t): 0.5957 LR: 0.000096 Step: 146700 Total Loss: 3.0487 Recon Loss: 3.0305 
[01/07 00:26:29 TiTok]: Data (t): 0.0011, 53.88/s/gpu Batch (t): 0.5939 LR: 0.000096 Step: 146750 Total Loss: 3.0825 Recon Loss: 3.0643 
[01/07 00:26:59 TiTok]: Data (t): 0.0011, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000096 Step: 146800 Total Loss: 2.9896 Recon Loss: 2.9713 
[01/07 00:27:29 TiTok]: Data (t): 0.0014, 54.34/s/gpu Batch (t): 0.5889 LR: 0.000096 Step: 146850 Total Loss: 3.0503 Recon Loss: 3.0321 
[01/07 00:27:59 TiTok]: Data (t): 0.0007, 52.42/s/gpu Batch (t): 0.6105 LR: 0.000096 Step: 146900 Total Loss: 3.0207 Recon Loss: 3.0024 
[01/07 00:28:30 TiTok]: Data (t): 0.0011, 53.70/s/gpu Batch (t): 0.5959 LR: 0.000096 Step: 146950 Total Loss: 3.0020 Recon Loss: 2.9838 
[01/07 00:29:00 TiTok]: Data (t): 0.0011, 45.46/s/gpu Batch (t): 0.7039 LR: 0.000096 Step: 147000 Total Loss: 3.0786 Recon Loss: 3.0604 
[01/07 00:29:30 TiTok]: Data (t): 0.0011, 53.83/s/gpu Batch (t): 0.5944 LR: 0.000096 Step: 147050 Total Loss: 3.0230 Recon Loss: 3.0048 
[01/07 00:30:00 TiTok]: Data (t): 0.0011, 54.02/s/gpu Batch (t): 0.5923 LR: 0.000096 Step: 147100 Total Loss: 3.0677 Recon Loss: 3.0495 
[01/07 00:30:30 TiTok]: Data (t): 0.0011, 52.95/s/gpu Batch (t): 0.6044 LR: 0.000096 Step: 147150 Total Loss: 3.0615 Recon Loss: 3.0433 
[01/07 00:31:00 TiTok]: Data (t): 0.0006, 53.19/s/gpu Batch (t): 0.6017 LR: 0.000096 Step: 147200 Total Loss: 3.0480 Recon Loss: 3.0297 
[01/07 00:31:31 TiTok]: Data (t): 0.0011, 52.88/s/gpu Batch (t): 0.6051 LR: 0.000096 Step: 147250 Total Loss: 3.0196 Recon Loss: 3.0015 
[01/07 00:32:01 TiTok]: Data (t): 0.0012, 53.60/s/gpu Batch (t): 0.5970 LR: 0.000096 Step: 147300 Total Loss: 3.0410 Recon Loss: 3.0228 
[01/07 00:32:31 TiTok]: Data (t): 0.0006, 51.17/s/gpu Batch (t): 0.6254 LR: 0.000096 Step: 147350 Total Loss: 3.0241 Recon Loss: 3.0059 
[01/07 00:33:01 TiTok]: Data (t): 0.0011, 53.93/s/gpu Batch (t): 0.5933 LR: 0.000096 Step: 147400 Total Loss: 3.0453 Recon Loss: 3.0270 
[01/07 00:33:32 TiTok]: Data (t): 0.0011, 53.68/s/gpu Batch (t): 0.5961 LR: 0.000096 Step: 147450 Total Loss: 3.0324 Recon Loss: 3.0142 
[01/07 00:34:02 TiTok]: Data (t): 0.0011, 54.33/s/gpu Batch (t): 0.5890 LR: 0.000096 Step: 147500 Total Loss: 3.0393 Recon Loss: 3.0211 
[01/07 00:34:32 TiTok]: Data (t): 0.0011, 54.00/s/gpu Batch (t): 0.5926 LR: 0.000096 Step: 147550 Total Loss: 3.0864 Recon Loss: 3.0682 
[01/07 00:35:02 TiTok]: Data (t): 0.0011, 53.51/s/gpu Batch (t): 0.5980 LR: 0.000096 Step: 147600 Total Loss: 3.0639 Recon Loss: 3.0457 
[01/07 00:35:32 TiTok]: Data (t): 0.0012, 53.99/s/gpu Batch (t): 0.5927 LR: 0.000096 Step: 147650 Total Loss: 3.0603 Recon Loss: 3.0420 
[01/07 00:36:02 TiTok]: Data (t): 0.0011, 54.27/s/gpu Batch (t): 0.5897 LR: 0.000096 Step: 147700 Total Loss: 3.0786 Recon Loss: 3.0603 
[01/07 00:36:33 TiTok]: Data (t): 0.0011, 53.77/s/gpu Batch (t): 0.5952 LR: 0.000096 Step: 147750 Total Loss: 3.0258 Recon Loss: 3.0076 
[01/07 00:37:03 TiTok]: Data (t): 0.0011, 53.88/s/gpu Batch (t): 0.5940 LR: 0.000096 Step: 147800 Total Loss: 3.0485 Recon Loss: 3.0303 
[01/07 00:37:32 TiTok]: Data (t): 0.0011, 53.85/s/gpu Batch (t): 0.5942 LR: 0.000096 Step: 147850 Total Loss: 3.0204 Recon Loss: 3.0021 
[01/07 00:38:03 TiTok]: Data (t): 0.0011, 53.64/s/gpu Batch (t): 0.5966 LR: 0.000096 Step: 147900 Total Loss: 3.0893 Recon Loss: 3.0710 
[01/07 00:38:33 TiTok]: Data (t): 0.0006, 53.97/s/gpu Batch (t): 0.5930 LR: 0.000096 Step: 147950 Total Loss: 3.0306 Recon Loss: 3.0123 
[01/07 00:39:03 TiTok]: Data (t): 0.0011, 46.79/s/gpu Batch (t): 0.6839 LR: 0.000096 Step: 148000 Total Loss: 3.0884 Recon Loss: 3.0701 
[01/07 00:39:33 TiTok]: Data (t): 0.0006, 51.77/s/gpu Batch (t): 0.6181 LR: 0.000096 Step: 148050 Total Loss: 3.0341 Recon Loss: 3.0159 
[01/07 00:40:03 TiTok]: Data (t): 0.0011, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000096 Step: 148100 Total Loss: 3.0797 Recon Loss: 3.0615 
[01/07 00:40:33 TiTok]: Data (t): 0.0012, 51.74/s/gpu Batch (t): 0.6185 LR: 0.000096 Step: 148150 Total Loss: 3.0331 Recon Loss: 3.0149 
[01/07 00:41:04 TiTok]: Data (t): 0.0011, 53.81/s/gpu Batch (t): 0.5947 LR: 0.000096 Step: 148200 Total Loss: 3.0465 Recon Loss: 3.0283 
[01/07 00:41:34 TiTok]: Data (t): 0.0011, 52.79/s/gpu Batch (t): 0.6062 LR: 0.000096 Step: 148250 Total Loss: 3.0938 Recon Loss: 3.0756 
[01/07 00:42:04 TiTok]: Data (t): 0.0007, 53.29/s/gpu Batch (t): 0.6005 LR: 0.000096 Step: 148300 Total Loss: 3.0278 Recon Loss: 3.0095 
[01/07 00:42:34 TiTok]: Data (t): 0.0011, 53.81/s/gpu Batch (t): 0.5947 LR: 0.000096 Step: 148350 Total Loss: 3.0327 Recon Loss: 3.0144 
[01/07 00:43:04 TiTok]: Data (t): 0.0006, 53.58/s/gpu Batch (t): 0.5972 LR: 0.000096 Step: 148400 Total Loss: 3.0599 Recon Loss: 3.0416 
[01/07 00:43:34 TiTok]: Data (t): 0.0009, 53.18/s/gpu Batch (t): 0.6017 LR: 0.000096 Step: 148450 Total Loss: 3.0401 Recon Loss: 3.0219 
[01/07 00:44:04 TiTok]: Data (t): 0.0006, 53.87/s/gpu Batch (t): 0.5940 LR: 0.000096 Step: 148500 Total Loss: 3.0290 Recon Loss: 3.0109 
[01/07 00:44:34 TiTok]: Data (t): 0.0011, 53.86/s/gpu Batch (t): 0.5941 LR: 0.000096 Step: 148550 Total Loss: 3.0486 Recon Loss: 3.0303 
[01/07 00:45:04 TiTok]: Data (t): 0.0011, 54.29/s/gpu Batch (t): 0.5895 LR: 0.000096 Step: 148600 Total Loss: 3.0124 Recon Loss: 2.9942 
[01/07 00:45:35 TiTok]: Data (t): 0.0007, 53.20/s/gpu Batch (t): 0.6015 LR: 0.000096 Step: 148650 Total Loss: 3.0204 Recon Loss: 3.0022 
[01/07 00:46:05 TiTok]: Data (t): 0.0011, 51.56/s/gpu Batch (t): 0.6206 LR: 0.000096 Step: 148700 Total Loss: 3.0517 Recon Loss: 3.0335 
[01/07 00:46:35 TiTok]: Data (t): 0.0011, 53.77/s/gpu Batch (t): 0.5952 LR: 0.000096 Step: 148750 Total Loss: 3.0597 Recon Loss: 3.0414 
[01/07 00:47:05 TiTok]: Data (t): 0.0006, 53.26/s/gpu Batch (t): 0.6009 LR: 0.000096 Step: 148800 Total Loss: 3.0499 Recon Loss: 3.0316 
[01/07 00:47:36 TiTok]: Data (t): 0.0011, 52.60/s/gpu Batch (t): 0.6084 LR: 0.000096 Step: 148850 Total Loss: 3.0413 Recon Loss: 3.0231 
[01/07 00:48:06 TiTok]: Data (t): 0.0012, 53.87/s/gpu Batch (t): 0.5941 LR: 0.000096 Step: 148900 Total Loss: 3.0202 Recon Loss: 3.0020 
[01/07 00:48:36 TiTok]: Data (t): 0.0006, 49.45/s/gpu Batch (t): 0.6471 LR: 0.000096 Step: 148950 Total Loss: 3.0205 Recon Loss: 3.0023 
[01/07 00:49:06 TiTok]: Data (t): 0.0011, 45.01/s/gpu Batch (t): 0.7109 LR: 0.000096 Step: 149000 Total Loss: 3.0562 Recon Loss: 3.0381 
[01/07 00:49:36 TiTok]: Data (t): 0.0011, 51.98/s/gpu Batch (t): 0.6156 LR: 0.000096 Step: 149050 Total Loss: 3.0708 Recon Loss: 3.0526 
[01/07 00:50:06 TiTok]: Data (t): 0.0011, 53.95/s/gpu Batch (t): 0.5931 LR: 0.000096 Step: 149100 Total Loss: 3.0334 Recon Loss: 3.0152 
[01/07 00:50:37 TiTok]: Data (t): 0.0014, 51.95/s/gpu Batch (t): 0.6159 LR: 0.000096 Step: 149150 Total Loss: 3.0341 Recon Loss: 3.0159 
[01/07 00:51:07 TiTok]: Data (t): 0.0011, 53.72/s/gpu Batch (t): 0.5956 LR: 0.000096 Step: 149200 Total Loss: 3.0411 Recon Loss: 3.0228 
[01/07 00:51:37 TiTok]: Data (t): 0.0011, 54.30/s/gpu Batch (t): 0.5893 LR: 0.000096 Step: 149250 Total Loss: 3.0542 Recon Loss: 3.0359 
[01/07 00:52:07 TiTok]: Data (t): 0.0029, 51.19/s/gpu Batch (t): 0.6252 LR: 0.000096 Step: 149300 Total Loss: 3.0432 Recon Loss: 3.0250 
[01/07 00:52:37 TiTok]: Data (t): 0.0012, 52.76/s/gpu Batch (t): 0.6066 LR: 0.000096 Step: 149350 Total Loss: 3.0596 Recon Loss: 3.0414 
[01/07 00:53:08 TiTok]: Data (t): 0.0011, 53.94/s/gpu Batch (t): 0.5933 LR: 0.000096 Step: 149400 Total Loss: 3.0322 Recon Loss: 3.0140 
[01/07 00:53:38 TiTok]: Data (t): 0.0011, 53.62/s/gpu Batch (t): 0.5968 LR: 0.000096 Step: 149450 Total Loss: 3.0567 Recon Loss: 3.0385 
[01/07 00:54:08 TiTok]: Data (t): 0.0011, 54.42/s/gpu Batch (t): 0.5880 LR: 0.000096 Step: 149500 Total Loss: 3.0680 Recon Loss: 3.0497 
[01/07 00:54:38 TiTok]: Data (t): 0.0012, 53.94/s/gpu Batch (t): 0.5933 LR: 0.000096 Step: 149550 Total Loss: 3.0000 Recon Loss: 2.9818 
[01/07 00:55:08 TiTok]: Data (t): 0.0012, 53.73/s/gpu Batch (t): 0.5955 LR: 0.000096 Step: 149600 Total Loss: 3.0500 Recon Loss: 3.0318 
[01/07 00:55:39 TiTok]: Data (t): 0.0011, 52.14/s/gpu Batch (t): 0.6137 LR: 0.000096 Step: 149650 Total Loss: 3.0395 Recon Loss: 3.0213 
[01/07 00:56:09 TiTok]: Data (t): 0.0014, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000096 Step: 149700 Total Loss: 3.0579 Recon Loss: 3.0397 
[01/07 00:56:39 TiTok]: Data (t): 0.0011, 50.20/s/gpu Batch (t): 0.6375 LR: 0.000096 Step: 149750 Total Loss: 3.0463 Recon Loss: 3.0281 
[01/07 00:57:09 TiTok]: Data (t): 0.0011, 53.83/s/gpu Batch (t): 0.5945 LR: 0.000096 Step: 149800 Total Loss: 3.0408 Recon Loss: 3.0225 
[01/07 00:57:39 TiTok]: Data (t): 0.0007, 53.83/s/gpu Batch (t): 0.5944 LR: 0.000096 Step: 149850 Total Loss: 3.0616 Recon Loss: 3.0433 
[01/07 00:58:09 TiTok]: Data (t): 0.0011, 53.82/s/gpu Batch (t): 0.5946 LR: 0.000096 Step: 149900 Total Loss: 3.0466 Recon Loss: 3.0284 
[01/07 00:58:40 TiTok]: Data (t): 0.0011, 51.65/s/gpu Batch (t): 0.6196 LR: 0.000096 Step: 149950 Total Loss: 3.0418 Recon Loss: 3.0235 
[01/07 00:59:10 TiTok]: Data (t): 0.0006, 50.38/s/gpu Batch (t): 0.6352 LR: 0.000096 Step: 150000 Total Loss: 3.0491 Recon Loss: 3.0309 
Model weights saved in titok_s128_matryoshka_annealing_stage1_run1/checkpoint-150000/unwrapped_model/pytorch_model.bin
[01/07 00:59:12 TiTok]: Saved state to titok_s128_matryoshka_annealing_stage1_run1/checkpoint-150000
Model weights saved in titok_s128_matryoshka_annealing_stage1_run1/checkpoint-150000/ema_model/pytorch_model.bin
[01/07 00:59:38 TiTok]: Reconstructing images...
[01/07 00:59:44 TiTok]: Computing metrics on the validation set.
[01/07 01:19:01 TiTok]: EMA EVALUATION with 100.0% tokensStep: 150000 
[01/07 01:19:01 TiTok]: {'CodebookEntropy': tensor(11.9839, device='cuda:0', dtype=torch.float64),
 'CodebookUsage': 1.0,
 'InceptionScore': 134.7548125954618,
 'rFID': 7.131269544556687}
[01/07 01:37:43 TiTok]: EMA EVALUATION with 75.0% tokensStep: 150000 
[01/07 01:37:43 TiTok]: {'CodebookEntropy': tensor(11.9839, device='cuda:0', dtype=torch.float64),
 'CodebookUsage': 1.0,
 'InceptionScore': 83.85616287825627,
 'rFID': 15.066007081834982}
[01/07 01:55:10 TiTok]: EMA EVALUATION with 50.0% tokensStep: 150000 
[01/07 01:55:10 TiTok]: {'CodebookEntropy': tensor(11.9839, device='cuda:0', dtype=torch.float64),
 'CodebookUsage': 1.0,
 'InceptionScore': 14.224513097104376,
 'rFID': 87.73676932962042}
